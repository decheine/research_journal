## Motivation

So I have been looking into ways that we can use information theory measures (ITMs) to do summary statistics for large data structures. The data structures can be thought of as cubes where we care about the latitude, longitude and the time for different variables. Concretely, we are looking at a 4D dataset in the form of
$$
\mathbf{X} = X(\phi,\lambda, t, z)
$$
where $\phi$ is the latitude, $\lambda$ is the longitude, $t$ is the time, $z$ is the variable. So we have a dataset where we have spatial, temporal and variable considerations. 

---

## Part I - Sensitivity Analysis

* We look at sensitivity methods in the context of kernel methods
  * Latent Function Only - $y=f(x)$
* "Open the black box"
* Non-Bayesian Context
  * (KRR, SVM, KDE, HSIC)
  * Application - Earth Data

---

## Part II - Input Uncertainty Quantification

* We add another level of complexity
  * Uncertain Inputs $x\sim \mathbb{P}$
  * Bayesian Model Context - $y=f(x)+\epsilon$
* Input Uncertainty
  * Crude Approximation - (**Paper**: eGP)
  * Fully Bayesian Approach Scaled - (**Paper**: eGP2.0)
* Application - IASI Data

---

## Part III - Similarity Analysis

* Last Level of complexity
* Measure similarity directly
  * $\text{sim}(\mathcal{X,Y})$
* Single Variable: RBIG 4 EO
  * Application - Spatial, Temporal Data
* Different Variables: HSIC
  * Experiment - Empirical Studies
