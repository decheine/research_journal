{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Twitter: jejjohnson Github: jejjohnson Website: jejjohnson.netlify.app Research Journal: jejjohnson.github.io/research_journal I recently ran into someone at a conference who said, \" a lot of research dies in Graduate students laptops \" (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. This is my research journal of various topics that I'm working with. My research is mostly in using Machine learning methods in various applications of remote sensing, ocean and climate sciences. I focus on three factors: Data Representation - We know that spatial-temporal information is important; can we effectively capture this in ML models? Modeling Uncertainty - Which ML models allow us to capture any facets of uncertainty (input, model, output) in our models. Similarity Measures - What is similarity and how are we able to measure it using ML? These are very difficult questions but I think they are very helpful for the community in general. The predominant algorithms of interest include kernel methods like Gaussian Processes (GPs) and Invertible Flows like Gaussianization Flows (GFs) for density and dependence estimation. Some other things you'll find are: Python and good coding practices Remote Computing and efficiency hacks I hoard a lot of links... Deep Learning in practice Eventually some blog posts... My Appendices \u00b6 My notes that I have been accumulating over the years. These will eventually go into my thesis, current/future publications and hopefully I'll master it enough to teach to someone one day. My Talks \u00b6 Some talks that I have done along with code and slides. My Resources \u00b6 My resources for all things python and tech. I like to tinker with different packages so I try to document my findings. My Projects \u00b6 My projects that I am involved in mostly related to academia. My Thesis \u00b6 My Thesis and everything related to it. I decided to compile all of my notes in markdown and of course the final product will be available in LaTeX format. My Tutorials \u00b6 I do like to give back to the community. So I have compiled some tutorials that will hopefully be helpful to other people. My Snippets \u00b6 I write lots of bits of code everywhere and it tends to be disorganized. I'm trying to organize my bits of code everywhere into digestable snippets; kind of like a personal reference. My Blogs \u00b6 Nothing here yet. But I have some ideas about which blogs I'd like to do.","title":"Overview"},{"location":"#overview","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Twitter: jejjohnson Github: jejjohnson Website: jejjohnson.netlify.app Research Journal: jejjohnson.github.io/research_journal I recently ran into someone at a conference who said, \" a lot of research dies in Graduate students laptops \" (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. This is my research journal of various topics that I'm working with. My research is mostly in using Machine learning methods in various applications of remote sensing, ocean and climate sciences. I focus on three factors: Data Representation - We know that spatial-temporal information is important; can we effectively capture this in ML models? Modeling Uncertainty - Which ML models allow us to capture any facets of uncertainty (input, model, output) in our models. Similarity Measures - What is similarity and how are we able to measure it using ML? These are very difficult questions but I think they are very helpful for the community in general. The predominant algorithms of interest include kernel methods like Gaussian Processes (GPs) and Invertible Flows like Gaussianization Flows (GFs) for density and dependence estimation. Some other things you'll find are: Python and good coding practices Remote Computing and efficiency hacks I hoard a lot of links... Deep Learning in practice Eventually some blog posts...","title":"Overview"},{"location":"#my-appendices","text":"My notes that I have been accumulating over the years. These will eventually go into my thesis, current/future publications and hopefully I'll master it enough to teach to someone one day.","title":"My Appendices"},{"location":"#my-talks","text":"Some talks that I have done along with code and slides.","title":"My Talks"},{"location":"#my-resources","text":"My resources for all things python and tech. I like to tinker with different packages so I try to document my findings.","title":"My Resources"},{"location":"#my-projects","text":"My projects that I am involved in mostly related to academia.","title":"My Projects"},{"location":"#my-thesis","text":"My Thesis and everything related to it. I decided to compile all of my notes in markdown and of course the final product will be available in LaTeX format.","title":"My Thesis"},{"location":"#my-tutorials","text":"I do like to give back to the community. So I have compiled some tutorials that will hopefully be helpful to other people.","title":"My Tutorials"},{"location":"#my-snippets","text":"I write lots of bits of code everywhere and it tends to be disorganized. I'm trying to organize my bits of code everywhere into digestable snippets; kind of like a personal reference.","title":"My Snippets"},{"location":"#my-blogs","text":"Nothing here yet. But I have some ideas about which blogs I'd like to do.","title":"My Blogs"},{"location":"definitions/","text":"Definitions \u00b6 Just a growing list of definitions. Things to Avoid \u00b6 Just a list of principles or things that I really want to avoid when doing science. I leave these here as a reminder. Desire to achieve a sought for (or simply publishable) result leads to fiddling with the data points, the modelling assumptions, or the research hypotheses themselves. - Satelli ( prezi ) P-Hacking \u00b6 Fishing for favourable p-values Paper : A. Gelman and E. Loken, \u201cThe garden of forking paths: Why multiple comparisons can be a problem, even when there is no \u2018fishing expedition\u2019 or \u2018p-hacking\u2019 and the research hypothesis was posited ahead of time,\u201d 2013 Leamer, E. E. Tantalus on the Road to Asymptopia. J. Econ. Perspect. 24, 31\u201346 (2010) HARKing \u00b6 Formulating the research **H**ypothesis **A**fter the **R**esults are **K**nown Paper : Kerr, N. L., HARKing: Hypothesizing After the Results are Known. Personal. Soc. Psychol. Rev. 2, 196217 (1998) Seed Hacking \u00b6 Finding the right seed or initializing for the ML model that gives the best results. Data Fishing Expedition \u00b6 Finding data that gives your model the best results","title":"Definitions"},{"location":"definitions/#definitions","text":"Just a growing list of definitions.","title":"Definitions"},{"location":"definitions/#things-to-avoid","text":"Just a list of principles or things that I really want to avoid when doing science. I leave these here as a reminder. Desire to achieve a sought for (or simply publishable) result leads to fiddling with the data points, the modelling assumptions, or the research hypotheses themselves. - Satelli ( prezi )","title":"Things to Avoid"},{"location":"definitions/#p-hacking","text":"Fishing for favourable p-values Paper : A. Gelman and E. Loken, \u201cThe garden of forking paths: Why multiple comparisons can be a problem, even when there is no \u2018fishing expedition\u2019 or \u2018p-hacking\u2019 and the research hypothesis was posited ahead of time,\u201d 2013 Leamer, E. E. Tantalus on the Road to Asymptopia. J. Econ. Perspect. 24, 31\u201346 (2010)","title":"P-Hacking"},{"location":"definitions/#harking","text":"Formulating the research **H**ypothesis **A**fter the **R**esults are **K**nown Paper : Kerr, N. L., HARKing: Hypothesizing After the Results are Known. Personal. Soc. Psychol. Rev. 2, 196217 (1998)","title":"HARKing"},{"location":"definitions/#seed-hacking","text":"Finding the right seed or initializing for the ML model that gives the best results.","title":"Seed Hacking"},{"location":"definitions/#data-fishing-expedition","text":"Finding data that gives your model the best results","title":"Data Fishing Expedition"},{"location":"logistics/","text":"Logistics \u00b6 Notebooks \u00b6 I really like notebooks as a way to display information. Strategy Do Experiments Upload Notebook (meta data, sections, comments) Refactor Notebook (Clean, sections, labels, only important info) Re-Upload Notebook Organize and MKDocsify-it","title":"Logistics"},{"location":"logistics/#logistics","text":"","title":"Logistics"},{"location":"logistics/#notebooks","text":"I really like notebooks as a way to display information. Strategy Do Experiments Upload Notebook (meta data, sections, comments) Refactor Notebook (Clean, sections, labels, only important info) Re-Upload Notebook Organize and MKDocsify-it","title":"Notebooks"},{"location":"Explorers/","text":"Explorers \u00b6 Colab Notebook Templates \u00b6 Jax Exploration PyTorch Exploration TensorFlow Exploration","title":"Explorers"},{"location":"Explorers/#explorers","text":"","title":"Explorers"},{"location":"Explorers/#colab-notebook-templates","text":"Jax Exploration PyTorch Exploration TensorFlow Exploration","title":"Colab Notebook Templates"},{"location":"Explorers/BNNs/","text":"Bayesian Neural Networks Working Group \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: isp.uv.es/working_groups/bnn Summary \u00b6 This is the working group page for exploring Bayesian Neural Networks (BNNs). Recently BNNs have started to become popular in the Machine learning literature as well as the applied sciences literature. Most research groups are interested because of the 'principled' approach to handling uncertainty within your data. Many traditional ML approaches don't account for uncertainty and make this The adoption of Deep learning methods and easy-to-use open-source software has also aided in the growning popularity. It is now easier to implement and try various models without having to do things from scratch. It's a good time to see if the Bayesian methodology works for your problem as the field has started to progress. In this working group we will working with the Bayesian methodology from 3 perspectives: Theory , Practice , and Application . Even though there are 3 parts, we will be heavily driven by the application portion. After talking with the lab, we have a list of possible applications where we think BNNs would be appropriate. This could determine the direction of our exploration as a principled approach to dealing with your problem does require us to think about our data and which approaches will be appropriate. We will also adopt a balance between the methods that seem to work in practice (e.g. Drop-Out, Ensembles) and the methods that \"would be nice\" (e.g. VI-based layers, Deep Gaussian Processes, SWAG). This means that we will include things that approximate NNs such as drop-out and architectures that are a mixture of standard NNs and probabilistic models. I outline each of the sections in detail below. Sections Outline \u00b6 Theory We will look at some of the staple papers which started the BNN approach as well as some of the SOTA approaches. In addition to Bayes related material, we will also take a look at some things related to uncertainty and neural networks. Practice We will go over some key probabilistic programming aspects. This is different than the standard Neural network architecture and can be a bit difficult to fully grasp. I think with adequate training in the software side of things, your life will be must easier and you will be able to correctly and efficiently implement algorithms and concepts. Applications This will be somewhat application driven, at least for the practical aspects. In the end, the groups have all come with hopes that they can use some of these techniques in the near future. We currently have the following pending applications: * Emulation Data * Ocean Data (Multi-Output) * Medical Data * Gap Filling Data Format ( TBD ) \u00b6 I would like to balance the 3 things I've mentioned above. I would like to spend time in the meetings discussing the theory and concepts. And then we can have a few sessions discussing some programming concepts to ensure that we can be doing practice on the way. Perhaps the individual groups can work on the applications in their free time. Requirements \u00b6 This is not a beginners working group so there will be some expectations from the people attending if they're going to participate. They are not strictly required, but I suggest them because I think it would make the experience better for everyone . Familiarity with Bayesian Concepts (Prior, Likelihood, Posterior, Evidence, etc) Prior Programming experience; preferably in Python ( practical sessions ) Familiarity with Neural networks and the terminology (gradients, loss, optimization, etc) Logistics \u00b6 When * Start: February, 2020 * Duration: TBD Where * ISP Open Office Leads * J. Emmanuel Johnson * Kristoffer Wickstrom Resources \u00b6 Literature * Papers * SOTA Resources * Videos Software * DL Frameworks * Overview * TensorFlow","title":"Bayesian Neural Networks Working Group"},{"location":"Explorers/BNNs/#bayesian-neural-networks-working-group","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: isp.uv.es/working_groups/bnn","title":"Bayesian Neural Networks Working Group"},{"location":"Explorers/BNNs/#summary","text":"This is the working group page for exploring Bayesian Neural Networks (BNNs). Recently BNNs have started to become popular in the Machine learning literature as well as the applied sciences literature. Most research groups are interested because of the 'principled' approach to handling uncertainty within your data. Many traditional ML approaches don't account for uncertainty and make this The adoption of Deep learning methods and easy-to-use open-source software has also aided in the growning popularity. It is now easier to implement and try various models without having to do things from scratch. It's a good time to see if the Bayesian methodology works for your problem as the field has started to progress. In this working group we will working with the Bayesian methodology from 3 perspectives: Theory , Practice , and Application . Even though there are 3 parts, we will be heavily driven by the application portion. After talking with the lab, we have a list of possible applications where we think BNNs would be appropriate. This could determine the direction of our exploration as a principled approach to dealing with your problem does require us to think about our data and which approaches will be appropriate. We will also adopt a balance between the methods that seem to work in practice (e.g. Drop-Out, Ensembles) and the methods that \"would be nice\" (e.g. VI-based layers, Deep Gaussian Processes, SWAG). This means that we will include things that approximate NNs such as drop-out and architectures that are a mixture of standard NNs and probabilistic models. I outline each of the sections in detail below.","title":"Summary"},{"location":"Explorers/BNNs/#sections-outline","text":"Theory We will look at some of the staple papers which started the BNN approach as well as some of the SOTA approaches. In addition to Bayes related material, we will also take a look at some things related to uncertainty and neural networks. Practice We will go over some key probabilistic programming aspects. This is different than the standard Neural network architecture and can be a bit difficult to fully grasp. I think with adequate training in the software side of things, your life will be must easier and you will be able to correctly and efficiently implement algorithms and concepts. Applications This will be somewhat application driven, at least for the practical aspects. In the end, the groups have all come with hopes that they can use some of these techniques in the near future. We currently have the following pending applications: * Emulation Data * Ocean Data (Multi-Output) * Medical Data * Gap Filling Data","title":"Sections Outline"},{"location":"Explorers/BNNs/#format-tbd","text":"I would like to balance the 3 things I've mentioned above. I would like to spend time in the meetings discussing the theory and concepts. And then we can have a few sessions discussing some programming concepts to ensure that we can be doing practice on the way. Perhaps the individual groups can work on the applications in their free time.","title":"Format (TBD)"},{"location":"Explorers/BNNs/#requirements","text":"This is not a beginners working group so there will be some expectations from the people attending if they're going to participate. They are not strictly required, but I suggest them because I think it would make the experience better for everyone . Familiarity with Bayesian Concepts (Prior, Likelihood, Posterior, Evidence, etc) Prior Programming experience; preferably in Python ( practical sessions ) Familiarity with Neural networks and the terminology (gradients, loss, optimization, etc)","title":"Requirements"},{"location":"Explorers/BNNs/#logistics","text":"When * Start: February, 2020 * Duration: TBD Where * ISP Open Office Leads * J. Emmanuel Johnson * Kristoffer Wickstrom","title":"Logistics"},{"location":"Explorers/BNNs/#resources","text":"Literature * Papers * SOTA Resources * Videos Software * DL Frameworks * Overview * TensorFlow","title":"Resources"},{"location":"Explorers/BNNs/sidebar/","text":"Home Page Literature * Papers Resources * Videos Explorers Groups * For not so Dummies Software * Overview * TensorFlow","title":"Sidebar"},{"location":"Explorers/BNNs/code/","text":"Programming Exercises \u00b6 I found some really good programming exercises in this class webpage . They have lectures as well. It seems like they implement a lot of things from scratch. Topics include: Probabilistic Modeling Bayesian Linear Regression Bayesian Networks Monte Carlo Methods Variational Inference & Expectation Propagation Message Passing Gaussian Processes","title":"Programming Exercises"},{"location":"Explorers/BNNs/code/#programming-exercises","text":"I found some really good programming exercises in this class webpage . They have lectures as well. It seems like they implement a lot of things from scratch. Topics include: Probabilistic Modeling Bayesian Linear Regression Bayesian Networks Monte Carlo Methods Variational Inference & Expectation Propagation Message Passing Gaussian Processes","title":"Programming Exercises"},{"location":"Explorers/BNNs/code/resources/","text":"From Scratch \u00b6 Introduction \u00b6 NN and Bayesian Learning Class notes with PyTorch/Pyro Code Probability & Statistics \u00b6 An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma - Blog Probabilistic Programming Concepts - Computational Statistics (2019) - Notes Bayesian regression with linear basis function models - Martin Krasser - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Multivariate Normal Distribution Primer - blog Neural Networks \u00b6 What is torch.nn reall? - Jeremy Howard - docs Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog Deep Learning Fundamentals - Eric Ma, Scipy 2019 - Video & Notebook | blog Bayesian Neural Networks \u00b6 Weight Uncertainty in Neural Networks Tutorial - Josh Feldman (2018) - blog Weight Uncertainty in Neural Networks - Nitarshan Rajkumar (2018) - blog Variational inference in Bayesian neural networks - Martin Krasser - blog Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 - Brendan Hasz - Blog Inference \u00b6 Expectation Maximization \u00b6 Algorithm Breakdown: Expectation Maximization - Ritchie Vink - blog Latent variable models part 1: Gaussian mixture models and the EM algorithm - Martin Krasser - blog Laplace Approximation \u00b6 Monte Carlo \u00b6 Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Variational Inference \u00b6 Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference - Brendan Hasz - Blog From Expectation Maximization to Stochastic Variational Inference - Martin Krasser - blog Latent variable models part 2 - Stochastic variational inference and variational autoencoders - Martin Krasser - blog Automatic Differentiation Variational Inference - Arthur Lui (2020) - blog Paper Bayesian Linear Regression ADVI using PyTorch - Arthur Lui (2020) - blog | Paper Gaussian Processes \u00b6 Gaussian Processes - Martin Krasser - blog 4-Part Tutorial Understanding Gaussian Processes Fitting a GP GP Kernels","title":"Resources"},{"location":"Explorers/BNNs/code/resources/#from-scratch","text":"","title":"From Scratch"},{"location":"Explorers/BNNs/code/resources/#introduction","text":"NN and Bayesian Learning Class notes with PyTorch/Pyro Code","title":"Introduction"},{"location":"Explorers/BNNs/code/resources/#probability-statistics","text":"An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma - Blog Probabilistic Programming Concepts - Computational Statistics (2019) - Notes Bayesian regression with linear basis function models - Martin Krasser - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Multivariate Normal Distribution Primer - blog","title":"Probability &amp; Statistics"},{"location":"Explorers/BNNs/code/resources/#neural-networks","text":"What is torch.nn reall? - Jeremy Howard - docs Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog Deep Learning Fundamentals - Eric Ma, Scipy 2019 - Video & Notebook | blog","title":"Neural Networks"},{"location":"Explorers/BNNs/code/resources/#bayesian-neural-networks","text":"Weight Uncertainty in Neural Networks Tutorial - Josh Feldman (2018) - blog Weight Uncertainty in Neural Networks - Nitarshan Rajkumar (2018) - blog Variational inference in Bayesian neural networks - Martin Krasser - blog Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 - Brendan Hasz - Blog","title":"Bayesian Neural Networks"},{"location":"Explorers/BNNs/code/resources/#inference","text":"","title":"Inference"},{"location":"Explorers/BNNs/code/resources/#expectation-maximization","text":"Algorithm Breakdown: Expectation Maximization - Ritchie Vink - blog Latent variable models part 1: Gaussian mixture models and the EM algorithm - Martin Krasser - blog","title":"Expectation Maximization"},{"location":"Explorers/BNNs/code/resources/#laplace-approximation","text":"","title":"Laplace Approximation"},{"location":"Explorers/BNNs/code/resources/#monte-carlo","text":"Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog","title":"Monte Carlo"},{"location":"Explorers/BNNs/code/resources/#variational-inference","text":"Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference - Brendan Hasz - Blog From Expectation Maximization to Stochastic Variational Inference - Martin Krasser - blog Latent variable models part 2 - Stochastic variational inference and variational autoencoders - Martin Krasser - blog Automatic Differentiation Variational Inference - Arthur Lui (2020) - blog Paper Bayesian Linear Regression ADVI using PyTorch - Arthur Lui (2020) - blog | Paper","title":"Variational Inference"},{"location":"Explorers/BNNs/code/resources/#gaussian-processes","text":"Gaussian Processes - Martin Krasser - blog 4-Part Tutorial Understanding Gaussian Processes Fitting a GP GP Kernels","title":"Gaussian Processes"},{"location":"Explorers/BNNs/code/software/","text":"Software \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So what to choose? \u00b6 There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health. List of Software \u00b6 There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning. Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Software"},{"location":"Explorers/BNNs/code/software/#software","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020","title":"Software"},{"location":"Explorers/BNNs/code/software/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"Explorers/BNNs/code/software/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"Explorers/BNNs/code/software/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.","title":"Convergence of the Libraries"},{"location":"Explorers/BNNs/code/software/#so-what-to-choose","text":"There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"So what to choose?"},{"location":"Explorers/BNNs/code/software/#list-of-software","text":"There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning.","title":"List of Software"},{"location":"Explorers/BNNs/code/software/#core-packages","text":"","title":"Core Packages"},{"location":"Explorers/BNNs/code/software/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"Explorers/BNNs/code/software/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"Explorers/BNNs/code/software/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/","text":"TensorFlow 2.0 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com These are notes that I took based off of lectures 1 and 2 given by Francois Chollet. Architecture \u00b6 1. Engine Module \u00b6 This is basically the model definition. It has the following parts Layer Network - this contains the DAG of Layers (internal component) Model - this contains the network and is used to do the training and evaluation loops Sequential - wraps a list of layers 2 Various Classes (and subclasses) \u00b6 Layers Metric Loss Callback Optimizer Regularizers, Constraints? Layer Class \u00b6 This is the core abstraction in the API. Everything is a Layer or it at least interacts closely with the Layer . What can it do? \u00b6 Computation This manages the computation. It takes in batch inputs / batch outputs. Assumes no interactions between samples Eager or Graph execution Training and Inference model Masking (e.g. time series, missing features) Manages State This keeps track of what's trainable or not trainable. class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () self . weights = ... trainable self . bias = ... not trainable Track Losses & Metrics Up class Linear ( tf . keras . Layer ): def call ( self , x ): # calculate kl divergence kl_loss = ... # add loss self . add_loss ( ... ) Type Checking Frozen or UnFrozen (fine-tuning, batch-norm, GANS) Can build DAGs - Sequential Form Mixed Precio What do they not do? \u00b6 Gradients Device Placement Distribution-specific logic Only batch-wise computation. Basic Layer \u00b6 We are going to create a base layer # create linear layer class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # weights variable w_init = tf . random_normal_initializer ()( shape = ( input_dim , units )) self . w = tf . Variable ( initial_value = w_init , trainable = True ) # bias parameter b_init = tf . zeros_initializer ()( shape = ( units ,)) self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b # data x_train = tf . ones ( 2 , 2 ) # initialize linear layer linear_layer = Linear ( 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x ) Better Basic Layer \u00b6 class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , ** kwargs ): super () . __init__ () self . units = units Notice how we didn't construct the weights when we initialized the class (constructor). This is nice because now we can construct our layer without having to know what the input dimension will be. We can simply specify the units. Instead we create a build method and that has the weights specified. def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + self . b The rest doesn't change. We can initialize the liner layer just with the units. This is called 'Lazy loading' linear_layer = Linear ( 32 ) It will call .build(x.shape) to get the dimensions of the dataset. y = linear_layer ( x ) Nested Layers \u00b6 We can nest Layers (as many) layers as we want actually. For example: Multi-Layer Perceptron \u00b6 class MLP ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x MLP Block \u00b6 class MLPB ( Layer ): def __init__ ( self ): super () . __init__ () self . mlp_1 = MLPBlock ( 32 ) self . mlp_2 = MLPBlock ( 32 ) self . mlp_3 = MLPBlock ( 1 ) def call ( self , inputs ): x = self . mlp_1 ( x ) x = self . mlp_2 ( x ) return x = self . mlp_3 ( x ) Basic Training \u00b6 So assuming that we have our linear layer, we can do some basic training procedure. # initialize model lr_model = Linear ( 32 ) # loss function loss_fn = tf . keras . losses . MSELoss () # optimizer optimizer = tf . keras . optimizers . Adam () # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = linear_model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , lr_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , lr_model . trainable_weights )) Losses \u00b6 We can add losses on the fly. For example, we can add a small activation regularizer in the call function for the MLP layer that we made above: class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x Now when we call the layer, we get the activation loss. mlp_layer = MLP ( 32 ) y = mlp_layer ( x ) Now it gets reset everytime we call it. Modified Training Loop \u00b6 mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function optimizer = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions for minibatch loss = loss_fn ( y , preds ) # loss value for minibatch loss += sum ( mlp_model . losses ) # extra losses from forward pass # find gradients grads = tape . gradients ( loss , mlp_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , mlp_model . trainable_weights )) Useful for: KL-Divergence Weight Regularization Activation Regularization Note : There is some context. The inner layers are also reset when their parent layer is called. Serialization \u00b6 class Linear ( tf . keras . Layer ): def __init__ () ... def get_config ( self ): config super () . get_config () config . update ({ 'units' : self . units }) return config Training Mode \u00b6 Allows you to do training versus inference mode. You simply need to add an extra argument in the cal() method. ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Some good examples: Batch Normalization Probabilistic Models (MC Variational Inference) Model Class \u00b6 This handles top-level functionality. The Model class does everything the Layer class can do, i.e. it is the same except with more available methods. In the literature, we refer to this as a \"model\", e.g. a deep learning model, a machine learning model, or as a \"network\", e.g. a deep neural network. In the literature, we refer to a Layer as something with a closed sequence of operations. For example a convolutional layer or a recurrent layer. Sometimes we also refer layers within layers as a block. For example a ResNet block or an Attention block. So ultimately, you would define the Layer class to do the inner computation blocks and the Model class to do the outer model with what you do to train and save. Training functionality .compile() .fit() .evaulate() .predict() Saving We have the .save() method which includes: configuration (topology) state (weights) optimiser Summarization & Visualization .summary() plot_model() Compile \u00b6 This option give configurations: optimizer Loss When you have the model class and you run .compile() , you are running the graph in graph execution model. So you are basically compiling the graph. If we want to run it eagerly: we need to set the paramter run_eagerly to be True . mlp = MLP () mlp . compile ( optimizer = Adam (), loss = MSELoss (), run_eagerly = True ) Fit \u00b6 How the data will be fit: The training procedure. Callbacks Data Epochs Functional Model \u00b6","title":"TensorFlow 2.0"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#tensorflow-20","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com These are notes that I took based off of lectures 1 and 2 given by Francois Chollet.","title":"TensorFlow 2.0"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#architecture","text":"","title":"Architecture"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#1-engine-module","text":"This is basically the model definition. It has the following parts Layer Network - this contains the DAG of Layers (internal component) Model - this contains the network and is used to do the training and evaluation loops Sequential - wraps a list of layers","title":"1. Engine Module"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#2-various-classes-and-subclasses","text":"Layers Metric Loss Callback Optimizer Regularizers, Constraints?","title":"2 Various Classes (and subclasses)"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#layer-class","text":"This is the core abstraction in the API. Everything is a Layer or it at least interacts closely with the Layer .","title":"Layer Class"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#what-can-it-do","text":"Computation This manages the computation. It takes in batch inputs / batch outputs. Assumes no interactions between samples Eager or Graph execution Training and Inference model Masking (e.g. time series, missing features) Manages State This keeps track of what's trainable or not trainable. class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () self . weights = ... trainable self . bias = ... not trainable Track Losses & Metrics Up class Linear ( tf . keras . Layer ): def call ( self , x ): # calculate kl divergence kl_loss = ... # add loss self . add_loss ( ... ) Type Checking Frozen or UnFrozen (fine-tuning, batch-norm, GANS) Can build DAGs - Sequential Form Mixed Precio","title":"What can it do?"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#what-do-they-not-do","text":"Gradients Device Placement Distribution-specific logic Only batch-wise computation.","title":"What do they not do?"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#basic-layer","text":"We are going to create a base layer # create linear layer class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # weights variable w_init = tf . random_normal_initializer ()( shape = ( input_dim , units )) self . w = tf . Variable ( initial_value = w_init , trainable = True ) # bias parameter b_init = tf . zeros_initializer ()( shape = ( units ,)) self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b # data x_train = tf . ones ( 2 , 2 ) # initialize linear layer linear_layer = Linear ( 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x )","title":"Basic Layer"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#better-basic-layer","text":"class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , ** kwargs ): super () . __init__ () self . units = units Notice how we didn't construct the weights when we initialized the class (constructor). This is nice because now we can construct our layer without having to know what the input dimension will be. We can simply specify the units. Instead we create a build method and that has the weights specified. def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + self . b The rest doesn't change. We can initialize the liner layer just with the units. This is called 'Lazy loading' linear_layer = Linear ( 32 ) It will call .build(x.shape) to get the dimensions of the dataset. y = linear_layer ( x )","title":"Better Basic Layer"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#nested-layers","text":"We can nest Layers (as many) layers as we want actually. For example:","title":"Nested Layers"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#multi-layer-perceptron","text":"class MLP ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x","title":"Multi-Layer Perceptron"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#mlp-block","text":"class MLPB ( Layer ): def __init__ ( self ): super () . __init__ () self . mlp_1 = MLPBlock ( 32 ) self . mlp_2 = MLPBlock ( 32 ) self . mlp_3 = MLPBlock ( 1 ) def call ( self , inputs ): x = self . mlp_1 ( x ) x = self . mlp_2 ( x ) return x = self . mlp_3 ( x )","title":"MLP Block"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#basic-training","text":"So assuming that we have our linear layer, we can do some basic training procedure. # initialize model lr_model = Linear ( 32 ) # loss function loss_fn = tf . keras . losses . MSELoss () # optimizer optimizer = tf . keras . optimizers . Adam () # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = linear_model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , lr_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , lr_model . trainable_weights ))","title":"Basic Training"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#losses","text":"We can add losses on the fly. For example, we can add a small activation regularizer in the call function for the MLP layer that we made above: class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x Now when we call the layer, we get the activation loss. mlp_layer = MLP ( 32 ) y = mlp_layer ( x ) Now it gets reset everytime we call it.","title":"Losses"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#modified-training-loop","text":"mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function optimizer = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions for minibatch loss = loss_fn ( y , preds ) # loss value for minibatch loss += sum ( mlp_model . losses ) # extra losses from forward pass # find gradients grads = tape . gradients ( loss , mlp_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , mlp_model . trainable_weights )) Useful for: KL-Divergence Weight Regularization Activation Regularization Note : There is some context. The inner layers are also reset when their parent layer is called.","title":"Modified Training Loop"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#serialization","text":"class Linear ( tf . keras . Layer ): def __init__ () ... def get_config ( self ): config super () . get_config () config . update ({ 'units' : self . units }) return config","title":"Serialization"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#training-mode","text":"Allows you to do training versus inference mode. You simply need to add an extra argument in the cal() method. ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Some good examples: Batch Normalization Probabilistic Models (MC Variational Inference)","title":"Training Mode"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#model-class","text":"This handles top-level functionality. The Model class does everything the Layer class can do, i.e. it is the same except with more available methods. In the literature, we refer to this as a \"model\", e.g. a deep learning model, a machine learning model, or as a \"network\", e.g. a deep neural network. In the literature, we refer to a Layer as something with a closed sequence of operations. For example a convolutional layer or a recurrent layer. Sometimes we also refer layers within layers as a block. For example a ResNet block or an Attention block. So ultimately, you would define the Layer class to do the inner computation blocks and the Model class to do the outer model with what you do to train and save. Training functionality .compile() .fit() .evaulate() .predict() Saving We have the .save() method which includes: configuration (topology) state (weights) optimiser Summarization & Visualization .summary() plot_model()","title":"Model Class"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#compile","text":"This option give configurations: optimizer Loss When you have the model class and you run .compile() , you are running the graph in graph execution model. So you are basically compiling the graph. If we want to run it eagerly: we need to set the paramter run_eagerly to be True . mlp = MLP () mlp . compile ( optimizer = Adam (), loss = MSELoss (), run_eagerly = True )","title":"Compile"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#fit","text":"How the data will be fit: The training procedure. Callbacks Data Epochs","title":"Fit"},{"location":"Explorers/BNNs/code/my_notes/tensorflow/#functional-model","text":"","title":"Functional Model"},{"location":"Explorers/BNNs/code/packages/numpyro/","text":"","title":"Numpyro"},{"location":"Explorers/BNNs/code/packages/pyro/","text":"Pyro \u00b6 Recommended Resources \u00b6 Tutorials \u00b6 Sample Code \u00b6 Pyro Docs Probabilistic Programming with VI: Under the Hood A Prelude to Pyro Experimenting with Pyro's Hidden Native Support for Bayesian Neural Networks Bayesian Inference: How we are able to chase the Posterior","title":"Pyro"},{"location":"Explorers/BNNs/code/packages/pyro/#pyro","text":"","title":"Pyro"},{"location":"Explorers/BNNs/code/packages/pyro/#recommended-resources","text":"","title":"Recommended Resources"},{"location":"Explorers/BNNs/code/packages/pyro/#tutorials","text":"","title":"Tutorials"},{"location":"Explorers/BNNs/code/packages/pyro/#sample-code","text":"Pyro Docs Probabilistic Programming with VI: Under the Hood A Prelude to Pyro Experimenting with Pyro's Hidden Native Support for Bayesian Neural Networks Bayesian Inference: How we are able to chase the Posterior","title":"Sample Code"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/","text":"Explorers Group: TF 2.X and PyTorch for not so Dummies \u00b6 Date : Tuesday, 21 Feb 2020 Time : 1600 Location : IPL OE Lead : J. Emmanuel Johnson Blurb \u00b6 TensorFlow (TF) is one of the most popular automatic differentiation (autograd) libraries in the world right now being used in production as well as research. Backed by Google (or Alphabet, Inc.), it has a huge company support system and the open-source community is massive so there is a ton of code available for a lot of the state-of-the-art (SOTA) machine learning algorithms. A more recent autograd library, PyTorch , was founded by Facebook and has risen to be the second most popular autograd library available. It's overtaken TensorFlow in the research industry because it is more Pythonic and it uses dynamic graphs which better suited the research community. TensorFlow recently got an update (TF 2.X) that has integrated keras (a high-level TF wrapper) and incorporated some more PyTorch-like design principles. So we will be going over some of the key features that you need to know to get you started on your Machine Learning and/or Deep Learning journey using TensorFlow and/or PyTorch. What To Expect \u00b6 This will be fairly high level but we will have some code examples. I will be presenting an overview of the nature/status of deep learning software and then we will be going through a google colab notebook outlining some of the key features of TensorFlow and PyTorch and answering any questions people have. It should take more more than 1.5 hours. Some other things: This will be in Python so familiarity with the language is expected and/or a strong familiarity with programming. I expect some level of machine learning and/or deep learning background to be able to keep up with some of the terminology, e.g. optimization, loss function, etc. This will not be a live coding session but you're more than welcome to bring your own laptops. Even if you think this is too high-level (or low-level) but are still interested, check the resources I've listed below. There might be some useful tutorials for you. Resources \u00b6 I have put the materials online. Please go through them if you get a chance. In particular I will be giving a short introduction about the nature of a deep learning library and we will spend the rest of the time walking through the key features of this colab notebook. The resources I've linked will be updated over the weekend and over the next few months so check back for updates if you're still interested. Introduction TensorFlow Spirit Animal \u00b6 Did you know that some Koala bears carry a strand of chlamydia (Herpes)? Some news reports even got so far as to kill them due to potential human infections. A moment of silence for the remaining 70% of the population of our moody, furry, cuddly friends as they deal with the crazy Austrailian wildfires .","title":"Explorers Group: TF 2.X and PyTorch for not so Dummies"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/#explorers-group-tf-2x-and-pytorch-for-not-so-dummies","text":"Date : Tuesday, 21 Feb 2020 Time : 1600 Location : IPL OE Lead : J. Emmanuel Johnson","title":"Explorers Group: TF 2.X and PyTorch for not so Dummies"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/#blurb","text":"TensorFlow (TF) is one of the most popular automatic differentiation (autograd) libraries in the world right now being used in production as well as research. Backed by Google (or Alphabet, Inc.), it has a huge company support system and the open-source community is massive so there is a ton of code available for a lot of the state-of-the-art (SOTA) machine learning algorithms. A more recent autograd library, PyTorch , was founded by Facebook and has risen to be the second most popular autograd library available. It's overtaken TensorFlow in the research industry because it is more Pythonic and it uses dynamic graphs which better suited the research community. TensorFlow recently got an update (TF 2.X) that has integrated keras (a high-level TF wrapper) and incorporated some more PyTorch-like design principles. So we will be going over some of the key features that you need to know to get you started on your Machine Learning and/or Deep Learning journey using TensorFlow and/or PyTorch.","title":"Blurb"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/#what-to-expect","text":"This will be fairly high level but we will have some code examples. I will be presenting an overview of the nature/status of deep learning software and then we will be going through a google colab notebook outlining some of the key features of TensorFlow and PyTorch and answering any questions people have. It should take more more than 1.5 hours. Some other things: This will be in Python so familiarity with the language is expected and/or a strong familiarity with programming. I expect some level of machine learning and/or deep learning background to be able to keep up with some of the terminology, e.g. optimization, loss function, etc. This will not be a live coding session but you're more than welcome to bring your own laptops. Even if you think this is too high-level (or low-level) but are still interested, check the resources I've listed below. There might be some useful tutorials for you.","title":"What To Expect"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/#resources","text":"I have put the materials online. Please go through them if you get a chance. In particular I will be giving a short introduction about the nature of a deep learning library and we will spend the rest of the time walking through the key features of this colab notebook. The resources I've linked will be updated over the weekend and over the next few months so check back for updates if you're still interested. Introduction TensorFlow","title":"Resources"},{"location":"Explorers/BNNs/other/explorers_2020_01_tf/#spirit-animal","text":"Did you know that some Koala bears carry a strand of chlamydia (Herpes)? Some news reports even got so far as to kill them due to potential human infections. A moment of silence for the remaining 70% of the population of our moody, furry, cuddly friends as they deal with the crazy Austrailian wildfires .","title":"Spirit Animal"},{"location":"Explorers/BNNs/other/people/","text":"Key Figures \u00b6 Yarin Gal - One key person who is behind the popular 'drop-out' technique.","title":"Key Figures"},{"location":"Explorers/BNNs/other/people/#key-figures","text":"Yarin Gal - One key person who is behind the popular 'drop-out' technique.","title":"Key Figures"},{"location":"Explorers/BNNs/other/polemica/","text":"Controversy \u00b6 Controversal Opinions \u00b6 Original Tweet Neil Lawrence Backlash Andrew Gordon Wilson [Rebutle]((https://twitter.com/andrewgwils/status/1210354001041969152?s=09) | Essay","title":"Controversy"},{"location":"Explorers/BNNs/other/polemica/#controversy","text":"","title":"Controversy"},{"location":"Explorers/BNNs/other/polemica/#controversal-opinions","text":"Original Tweet Neil Lawrence Backlash Andrew Gordon Wilson [Rebutle]((https://twitter.com/andrewgwils/status/1210354001041969152?s=09) | Essay","title":"Controversal Opinions"},{"location":"Explorers/BNNs/other/videos/","text":"Videos \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 2020-01-20 Talks \u00b6 Bayesian Deep Learning \u00b6 An Attempt at Demystifying Bayesian Deep Learning (2017) - Eric Ma, PyData - Youtube | Notebooks | Slides A nice and simple talk. Gives some background and motivation. Great figures. Bayesian Deep Learning: Primer (2019) - Wilson et. al. - Youtube Good talk. A bit advanced stuff later in the talk. But the beginning is great. Bayesian Neural Networks: A function space Tour - GPSS 2019 - Video | Slides An interesting talk from a GP perspective. TensorFlow Probability: Learning with Confidence - Dillion, TF Dev '19 - Youtube Bayesian Deep Learning - Gal, MLSS '19 - Part I | Part II | Part III Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS '19 - Part I | Part II | Part III Tutorials \u00b6 Bayesian Deep Learning - Mohammad Emtiyaz Khan, NeurIPS '19 - Part I | Part II | Slides Workshops \u00b6 DeepBayes 2019 - Playlist NeurIPS 2019 - Website","title":"Videos"},{"location":"Explorers/BNNs/other/videos/#videos","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 2020-01-20","title":"Videos"},{"location":"Explorers/BNNs/other/videos/#talks","text":"","title":"Talks"},{"location":"Explorers/BNNs/other/videos/#bayesian-deep-learning","text":"An Attempt at Demystifying Bayesian Deep Learning (2017) - Eric Ma, PyData - Youtube | Notebooks | Slides A nice and simple talk. Gives some background and motivation. Great figures. Bayesian Deep Learning: Primer (2019) - Wilson et. al. - Youtube Good talk. A bit advanced stuff later in the talk. But the beginning is great. Bayesian Neural Networks: A function space Tour - GPSS 2019 - Video | Slides An interesting talk from a GP perspective. TensorFlow Probability: Learning with Confidence - Dillion, TF Dev '19 - Youtube Bayesian Deep Learning - Gal, MLSS '19 - Part I | Part II | Part III Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS '19 - Part I | Part II | Part III","title":"Bayesian Deep Learning"},{"location":"Explorers/BNNs/other/videos/#tutorials","text":"Bayesian Deep Learning - Mohammad Emtiyaz Khan, NeurIPS '19 - Part I | Part II | Slides","title":"Tutorials"},{"location":"Explorers/BNNs/other/videos/#workshops","text":"DeepBayes 2019 - Playlist NeurIPS 2019 - Website","title":"Workshops"},{"location":"Explorers/BNNs/prezis/test/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson Second slide \u00b6 Best quote ever. Note: speaker notes FTW!","title":"TF2.X and PyTorch"},{"location":"Explorers/BNNs/prezis/test/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"Explorers/BNNs/prezis/test/#second-slide","text":"Best quote ever. Note: speaker notes FTW!","title":"Second slide"},{"location":"Explorers/BNNs/theory/","text":"","title":"Index"},{"location":"Explorers/BNNs/theory/optimization/","text":"On Empirical Comparisons of Optimizers for Deep Learning - (2019)","title":"Optimization"},{"location":"Explorers/BNNs/theory/papers/","text":"Papers \u00b6 This page highlights some of the key papers that we will go over during the working group. I have organized the sections into topics and each section will have the appropriate literature. I will keep the papers that we will go in detail near the top and any supporting papers will go at the bottom. !> Warning : This is not the final list. This is merely a guide that we will pivot off of. Bayesian \u00b6 The prior can generally only be understood in the context of the likelihood - Gelman et. al. (2017) - arxiv | blog Introduction to Bayesian Deep Learning \u00b6 Practical Deep Learning with Bayesian Principles - Warner & Neal (1997) - arxiv Towards Bayesian Deep Learning: A Survey - Wang et. al. (2016) Approximately Bayesian \u00b6 Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - Gal & Ghahramani - Paper | Code | Tutorial | Blog Computer Vision \u00b6 What Uncertainties do We Need for BDL for CV - Kendall & Gal (2017) - arxiv | A Comprehensive guide to Bayesian Convolutional NeuralNetwork with Variational Inference - Shridhar et. al. (2018) - arxiv | code (PyTorch) SOTA \u00b6 Bayesian Deep Learning and a Probabilistic Perspective of Generalization - AGW & Pavel Izmailov - PDF The Case for Bayesian Deep Learning - AGW - ARXIV Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning - Zhang et al - arxiv | Code Stochastic Weighted Averaging - SWA | Paper Stochastic Weighted Averaging - SWAG | Paper Stochastic Weighted Averaging - Multi-SWAG","title":"Papers"},{"location":"Explorers/BNNs/theory/papers/#papers","text":"This page highlights some of the key papers that we will go over during the working group. I have organized the sections into topics and each section will have the appropriate literature. I will keep the papers that we will go in detail near the top and any supporting papers will go at the bottom. !> Warning : This is not the final list. This is merely a guide that we will pivot off of.","title":"Papers"},{"location":"Explorers/BNNs/theory/papers/#bayesian","text":"The prior can generally only be understood in the context of the likelihood - Gelman et. al. (2017) - arxiv | blog","title":"Bayesian"},{"location":"Explorers/BNNs/theory/papers/#introduction-to-bayesian-deep-learning","text":"Practical Deep Learning with Bayesian Principles - Warner & Neal (1997) - arxiv Towards Bayesian Deep Learning: A Survey - Wang et. al. (2016)","title":"Introduction to Bayesian Deep Learning"},{"location":"Explorers/BNNs/theory/papers/#approximately-bayesian","text":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - Gal & Ghahramani - Paper | Code | Tutorial | Blog","title":"Approximately Bayesian"},{"location":"Explorers/BNNs/theory/papers/#computer-vision","text":"What Uncertainties do We Need for BDL for CV - Kendall & Gal (2017) - arxiv | A Comprehensive guide to Bayesian Convolutional NeuralNetwork with Variational Inference - Shridhar et. al. (2018) - arxiv | code (PyTorch)","title":"Computer Vision"},{"location":"Explorers/BNNs/theory/papers/#sota","text":"Bayesian Deep Learning and a Probabilistic Perspective of Generalization - AGW & Pavel Izmailov - PDF The Case for Bayesian Deep Learning - AGW - ARXIV Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning - Zhang et al - arxiv | Code Stochastic Weighted Averaging - SWA | Paper Stochastic Weighted Averaging - SWAG | Paper Stochastic Weighted Averaging - Multi-SWAG","title":"SOTA"},{"location":"Explorers/BNNs/theory/prob_nn/","text":"Neural Networks with Uncertainty \u00b6 Author: J. Emmanuel Johnson Date: 1 st October, 2019 Synopsis \u00b6 This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression, What is Uncertainty? \u00b6 Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Distribution Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input. Uncertainty in the Error Generalization \u00b6 First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model. Uncertainty Over Functions \u00b6 In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 . Aleatoric Uncertainty, \\sigma^2 \\sigma^2 \u00b6 This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day... Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 \u00b6 The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find Overview of Architecture Types \u00b6 So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put Generic Neural Networks (NN) \u00b6 This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence Probabilistic Neural Networks (PNN) \u00b6 This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions. Learning: Maximum Likelihood \u00b6 Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} Final Layer \u00b6 This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric) Heteroscedastic Noise Model \u00b6 This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns . Gaussian Process (Deep Kernel Learning) \u00b6 Bayesian Benchmarks \u00b6 So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni Resources \u00b6 Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Neural Networks with Uncertainty"},{"location":"Explorers/BNNs/theory/prob_nn/#neural-networks-with-uncertainty","text":"Author: J. Emmanuel Johnson Date: 1 st October, 2019","title":"Neural Networks with Uncertainty"},{"location":"Explorers/BNNs/theory/prob_nn/#synopsis","text":"This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression,","title":"Synopsis"},{"location":"Explorers/BNNs/theory/prob_nn/#what-is-uncertainty","text":"Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Distribution Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input.","title":"What is Uncertainty?"},{"location":"Explorers/BNNs/theory/prob_nn/#uncertainty-in-the-error-generalization","text":"First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model.","title":"Uncertainty in the Error Generalization"},{"location":"Explorers/BNNs/theory/prob_nn/#uncertainty-over-functions","text":"In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 .","title":"Uncertainty Over Functions"},{"location":"Explorers/BNNs/theory/prob_nn/#aleatoric-uncertainty-sigma2sigma2","text":"This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day...","title":"Aleatoric Uncertainty, \\sigma^2\\sigma^2"},{"location":"Explorers/BNNs/theory/prob_nn/#epistemic-uncertainty-nu_2nu_2","text":"The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find","title":"Epistemic Uncertainty, \\nu_{**}^2\\nu_{**}^2"},{"location":"Explorers/BNNs/theory/prob_nn/#overview-of-architecture-types","text":"So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put","title":"Overview of Architecture Types"},{"location":"Explorers/BNNs/theory/prob_nn/#generic-neural-networks-nn","text":"This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence","title":"Generic Neural Networks (NN)"},{"location":"Explorers/BNNs/theory/prob_nn/#probabilistic-neural-networks-pnn","text":"This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions.","title":"Probabilistic Neural Networks (PNN)"},{"location":"Explorers/BNNs/theory/prob_nn/#learning-maximum-likelihood","text":"Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned}","title":"Learning: Maximum Likelihood"},{"location":"Explorers/BNNs/theory/prob_nn/#final-layer","text":"This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric)","title":"Final Layer"},{"location":"Explorers/BNNs/theory/prob_nn/#heteroscedastic-noise-model","text":"This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns .","title":"Heteroscedastic Noise Model"},{"location":"Explorers/BNNs/theory/prob_nn/#gaussian-process-deep-kernel-learning","text":"","title":"Gaussian Process (Deep Kernel Learning)"},{"location":"Explorers/BNNs/theory/prob_nn/#bayesian-benchmarks","text":"So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni","title":"Bayesian Benchmarks"},{"location":"Explorers/BNNs/theory/prob_nn/#resources","text":"Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"Explorers/BNNs/theory/resources/","text":"Uncertainty Decomposition in BNNs with Latent Variables - arxiv Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Discussions \u00b6 Variational Bayesian Inference vs Monte-Carlo Dropout for Uncertainty Quantification in DL - reddit So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni Resources \u00b6 Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"Explorers/BNNs/theory/resources/#discussions","text":"Variational Bayesian Inference vs Monte-Carlo Dropout for Uncertainty Quantification in DL - reddit So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni","title":"Discussions"},{"location":"Explorers/BNNs/theory/resources/#resources","text":"Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"Explorers/BNNs/theory/sota/","text":"State-of-the-Art \u00b6 SWAG - Video","title":"State-of-the-Art"},{"location":"Explorers/BNNs/theory/sota/#state-of-the-art","text":"SWAG - Video","title":"State-of-the-Art"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/","text":"Neural Networks with Uncertainty \u00b6 Author: J. Emmanuel Johnson Date: 1 st October, 2019 Synopsis \u00b6 This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression, Overview of Architecture Types \u00b6 So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put Generic Neural Networks (NN) \u00b6 This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence Probabilistic Neural Networks (PNN) \u00b6 This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions. Learning: Maximum Likelihood \u00b6 Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} Final Layer \u00b6 This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric) Heteroscedastic Noise Model \u00b6 This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns . Gaussian Process (Deep Kernel Learning) \u00b6 Bayesian Benchmarks \u00b6","title":"Neural Networks with Uncertainty"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#neural-networks-with-uncertainty","text":"Author: J. Emmanuel Johnson Date: 1 st October, 2019","title":"Neural Networks with Uncertainty"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#synopsis","text":"This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression,","title":"Synopsis"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#overview-of-architecture-types","text":"So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put","title":"Overview of Architecture Types"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#generic-neural-networks-nn","text":"This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence","title":"Generic Neural Networks (NN)"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#probabilistic-neural-networks-pnn","text":"This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions.","title":"Probabilistic Neural Networks (PNN)"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#learning-maximum-likelihood","text":"Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned}","title":"Learning: Maximum Likelihood"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#final-layer","text":"This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric)","title":"Final Layer"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#heteroscedastic-noise-model","text":"This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns .","title":"Heteroscedastic Noise Model"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#gaussian-process-deep-kernel-learning","text":"","title":"Gaussian Process (Deep Kernel Learning)"},{"location":"Explorers/BNNs/theory/uncertainty_in_neural_networks/#bayesian-benchmarks","text":"","title":"Bayesian Benchmarks"},{"location":"Explorers/BNNs/theory/videos/","text":"Videos \u00b6 Introduction to Bayesian Deep Learning \u00b6","title":"Videos"},{"location":"Explorers/BNNs/theory/videos/#videos","text":"","title":"Videos"},{"location":"Explorers/BNNs/theory/videos/#introduction-to-bayesian-deep-learning","text":"","title":"Introduction to Bayesian Deep Learning"},{"location":"Explorers/starspots/links/","text":"StarSpots \u00b6 This is my dump for information regarding starspots. Links \u00b6 Blog Posts \u00b6 Earth to exoplanet: Hunting for planets w. ML - Google Blog (Dec-2017) Open Sourcing Hunt for Exoplanets - Google AI Blog (Mar-2018) Using ML to Find Planets - NOVA (Dec-2018) Identifying Exoplanets w. NN - TensorFlow Blog (Nov-2019) Exoplanet hunting using Machine Learning - towardsdatascience (Dec-2019) Papers \u00b6 Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90 - Shallue & Vanderburg (2018) Machine-learning Inference of the Interior Structure of Low-mass Exoplanets - Baumester et. al. (2020) ML Pipeline for Exoplanet Classification - Sturrock et al (2019) CEESA meets machine learning: A Constant Elasticity Earth Similarity Approach to habitability and classification of exoplanets Scientific Domain Knowledge Improves Exoplanet Transit Classification with Deep Learning - Ansdell et. al. (2018) - NASA FDL Improving Exoplanet Detection Power: Multivariate Gaussian Process Models for Stellar Activity - Jones et. al. (Dec-2017) A uniform analysis of HD 209458bSpitzer/IRAClightcurves with Gaussian process models - Evans et. al. Disentangling Time-series Spectra with Gaussian Processes: Applications to Radial Velocity Analysis - Czekala et. al. (May-2017) A uniform analysis of HD 209458b Spitzer/IRAC light curves withGaussian process models - Evans et. al. () | PDF Bayesian Methods for Exoplanet Science - Parviainen (Nov-2017) An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval - Cobb et. al. (May-2019) Thesis \u00b6 The use of Gaussian processesin the analysis of stellar noise in exoplanet search Software \u00b6 Exoplanet ML - ML models and utilities for exoplanet science - TF exoplanet - exoplanet toolkit using PyMC3 | Demo w. GPs Blog: PyTorch + PyMC3 Using a Bayesian Neural Network in the Detection of Exoplanets - Esperanza Lopez Aguilera Presentation \u00b6 Gaussian processes: the next step in exoplanet data analysis - Aigrain Scaling Gaussian Processesand the search for exoplanets - Foreman-Mackey et. al. (2019) Improvement on Exoplanet Detection Methods and Analysis via Gaussian Process Fitting Techniques Using a Bayesian Neural Network in the Detection of Exoplanets - Esperanza Lopez Aguilera","title":"StarSpots"},{"location":"Explorers/starspots/links/#starspots","text":"This is my dump for information regarding starspots.","title":"StarSpots"},{"location":"Explorers/starspots/links/#links","text":"","title":"Links"},{"location":"Explorers/starspots/links/#blog-posts","text":"Earth to exoplanet: Hunting for planets w. ML - Google Blog (Dec-2017) Open Sourcing Hunt for Exoplanets - Google AI Blog (Mar-2018) Using ML to Find Planets - NOVA (Dec-2018) Identifying Exoplanets w. NN - TensorFlow Blog (Nov-2019) Exoplanet hunting using Machine Learning - towardsdatascience (Dec-2019)","title":"Blog Posts"},{"location":"Explorers/starspots/links/#papers","text":"Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90 - Shallue & Vanderburg (2018) Machine-learning Inference of the Interior Structure of Low-mass Exoplanets - Baumester et. al. (2020) ML Pipeline for Exoplanet Classification - Sturrock et al (2019) CEESA meets machine learning: A Constant Elasticity Earth Similarity Approach to habitability and classification of exoplanets Scientific Domain Knowledge Improves Exoplanet Transit Classification with Deep Learning - Ansdell et. al. (2018) - NASA FDL Improving Exoplanet Detection Power: Multivariate Gaussian Process Models for Stellar Activity - Jones et. al. (Dec-2017) A uniform analysis of HD 209458bSpitzer/IRAClightcurves with Gaussian process models - Evans et. al. Disentangling Time-series Spectra with Gaussian Processes: Applications to Radial Velocity Analysis - Czekala et. al. (May-2017) A uniform analysis of HD 209458b Spitzer/IRAC light curves withGaussian process models - Evans et. al. () | PDF Bayesian Methods for Exoplanet Science - Parviainen (Nov-2017) An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval - Cobb et. al. (May-2019)","title":"Papers"},{"location":"Explorers/starspots/links/#thesis","text":"The use of Gaussian processesin the analysis of stellar noise in exoplanet search","title":"Thesis"},{"location":"Explorers/starspots/links/#software","text":"Exoplanet ML - ML models and utilities for exoplanet science - TF exoplanet - exoplanet toolkit using PyMC3 | Demo w. GPs Blog: PyTorch + PyMC3 Using a Bayesian Neural Network in the Detection of Exoplanets - Esperanza Lopez Aguilera","title":"Software"},{"location":"Explorers/starspots/links/#presentation","text":"Gaussian processes: the next step in exoplanet data analysis - Aigrain Scaling Gaussian Processesand the search for exoplanets - Foreman-Mackey et. al. (2019) Improvement on Exoplanet Detection Methods and Analysis via Gaussian Process Fitting Techniques Using a Bayesian Neural Network in the Detection of Exoplanets - Esperanza Lopez Aguilera","title":"Presentation"},{"location":"appendix/","text":"My Appendices \u00b6 Concepts \u00b6 Matrix Tricks Sleeper Concepts Bayesian \u00b6 Introduction Regression Variational Inference Monte Carlo Gaussian Distribution Gaussian Processes \u00b6 Introduction Sparse Gaussian Processes Input Uncertainty Taylor Expansion Variational Information Kernels \u00b6 HSIC MMD Information Theory \u00b6 Entropy Formulas Variation of Information Density Estimation \u00b6 Histograms Logistic","title":"My Appendices"},{"location":"appendix/#my-appendices","text":"","title":"My Appendices"},{"location":"appendix/#concepts","text":"Matrix Tricks Sleeper Concepts","title":"Concepts"},{"location":"appendix/#bayesian","text":"Introduction Regression Variational Inference Monte Carlo Gaussian Distribution","title":"Bayesian"},{"location":"appendix/#gaussian-processes","text":"Introduction Sparse Gaussian Processes Input Uncertainty Taylor Expansion Variational Information","title":"Gaussian Processes"},{"location":"appendix/#kernels","text":"HSIC MMD","title":"Kernels"},{"location":"appendix/#information-theory","text":"Entropy Formulas Variation of Information","title":"Information Theory"},{"location":"appendix/#density-estimation","text":"Histograms Logistic","title":"Density Estimation"},{"location":"appendix/bayesian/exponential/","text":"Exponential Family of Distributions \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle Resources \u00b6 A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Exponential Family of Distributions"},{"location":"appendix/bayesian/exponential/#exponential-family-of-distributions","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle","title":"Exponential Family of Distributions"},{"location":"appendix/bayesian/exponential/#resources","text":"A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Resources"},{"location":"appendix/bayesian/gaussian/","text":"Gaussian Distributions \u00b6 Univariate Gaussian \u00b6 \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) Multivariate Gaussian \u00b6 \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} Joint Gaussian Distribution \u00b6 \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} Marginal Distribution \\mathcal{P}(\\cdot) \\mathcal{P}(\\cdot) \u00b6 We have the marginal distribution of x x \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) and in integral form: \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy and we have the marginal distribution of y y \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) Conditional Distribution \\mathcal{P}(\\cdot | \\cdot) \\mathcal{P}(\\cdot | \\cdot) \u00b6 We have the conditional distribution of x x given y y . \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) where: \\mu_{a|b} = a + BC^{-1}(y-b) \\mu_{a|b} = a + BC^{-1}(y-b) \\Sigma_{a|b} = A - BC^{-1}B^T \\Sigma_{a|b} = A - BC^{-1}B^T and we have the marginal distribution of y y given x x \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) where: \\mu_{b|a} = b + AC^{-1}(x-a) \\mu_{b|a} = b + AC^{-1}(x-a) \\Sigma_{b|a} = B - AC^{-1}A^T \\Sigma_{b|a} = B - AC^{-1}A^T basically mirror opposites of each other. But this might be useful to know later when we deal with trying to find the marginal distributions of Gaussian process functions. Source : Sampling from a Normal Distribution - blog A really nice blog with nice plots of joint distributions. Two was to derive the conditional distributions - stack How to generate Gaussian samples = blog Multivariate Gaussians and Detereminant - Lecturee Notes Bandwidth Selection \u00b6 Scotts sigma = np . power ( n_samples , - 1.0 / ( d_dimensions + 4 )) Silverman sigma = np . power ( n_samples * ( d_dimensions + 2.0 ) / 4.0 , - 1.0 / ( d_dimensions + 4 ) Gaussian Distribution \u00b6 PDF \u00b6 f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) Likelihood \u00b6 - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi Alternative Representation \u00b6 X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda) Reparameterization \u00b6 So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition Entropy \u00b6 1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| KL-Divergence (Relative Entropy) \u00b6 KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"Gaussian"},{"location":"appendix/bayesian/gaussian/#gaussian-distributions","text":"","title":"Gaussian Distributions"},{"location":"appendix/bayesian/gaussian/#univariate-gaussian","text":"\\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right)","title":"Univariate Gaussian"},{"location":"appendix/bayesian/gaussian/#multivariate-gaussian","text":"\\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned}","title":"Multivariate Gaussian"},{"location":"appendix/bayesian/gaussian/#joint-gaussian-distribution","text":"\\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned}","title":"Joint Gaussian Distribution"},{"location":"appendix/bayesian/gaussian/#marginal-distribution-mathcalpcdotmathcalpcdot","text":"We have the marginal distribution of x x \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) and in integral form: \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy and we have the marginal distribution of y y \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) \\mathcal{P}(y) \\sim \\mathcal{N}(b, B)","title":"Marginal Distribution \\mathcal{P}(\\cdot)\\mathcal{P}(\\cdot)"},{"location":"appendix/bayesian/gaussian/#conditional-distribution-mathcalpcdot-cdotmathcalpcdot-cdot","text":"We have the conditional distribution of x x given y y . \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) where: \\mu_{a|b} = a + BC^{-1}(y-b) \\mu_{a|b} = a + BC^{-1}(y-b) \\Sigma_{a|b} = A - BC^{-1}B^T \\Sigma_{a|b} = A - BC^{-1}B^T and we have the marginal distribution of y y given x x \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) where: \\mu_{b|a} = b + AC^{-1}(x-a) \\mu_{b|a} = b + AC^{-1}(x-a) \\Sigma_{b|a} = B - AC^{-1}A^T \\Sigma_{b|a} = B - AC^{-1}A^T basically mirror opposites of each other. But this might be useful to know later when we deal with trying to find the marginal distributions of Gaussian process functions. Source : Sampling from a Normal Distribution - blog A really nice blog with nice plots of joint distributions. Two was to derive the conditional distributions - stack How to generate Gaussian samples = blog Multivariate Gaussians and Detereminant - Lecturee Notes","title":"Conditional Distribution \\mathcal{P}(\\cdot | \\cdot)\\mathcal{P}(\\cdot | \\cdot)"},{"location":"appendix/bayesian/gaussian/#bandwidth-selection","text":"Scotts sigma = np . power ( n_samples , - 1.0 / ( d_dimensions + 4 )) Silverman sigma = np . power ( n_samples * ( d_dimensions + 2.0 ) / 4.0 , - 1.0 / ( d_dimensions + 4 )","title":"Bandwidth Selection"},{"location":"appendix/bayesian/gaussian/#gaussian-distribution","text":"","title":"Gaussian Distribution"},{"location":"appendix/bayesian/gaussian/#pdf","text":"f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right)","title":"PDF"},{"location":"appendix/bayesian/gaussian/#likelihood","text":"- \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi","title":"Likelihood"},{"location":"appendix/bayesian/gaussian/#alternative-representation","text":"X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda)","title":"Alternative Representation"},{"location":"appendix/bayesian/gaussian/#reparameterization","text":"So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition","title":"Reparameterization"},{"location":"appendix/bayesian/gaussian/#entropy","text":"1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|","title":"Entropy"},{"location":"appendix/bayesian/gaussian/#kl-divergence-relative-entropy","text":"KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"KL-Divergence (Relative Entropy)"},{"location":"appendix/bayesian/inference/","text":"Solving Hard Integral Problems \u00b6 Source | Deisenroth - Sampling Advances in VI - Notebook Numerical Integration (low dimension) Bayesian Quadrature Expectation Propagation Conjugate Priors (Gaussian Likelihood w/ GP Prior) Subset Methods (Nystrom) Fast Linear Algebra (Krylov, Fast Transforms, KD-Trees) Variational Methods (Laplace, Mean-Field, Expectation Propagation) Monte Carlo Methods (Gibbs, Metropolis-Hashings, Particle Filter) Inference \u00b6 Maximum Likelihood \u00b6 Sources : Intro to Quantitative Econ w. Python Laplace Approximation \u00b6 This is where we approximate the posterior with a Gaussian distribution \\mathcal{N}(\\mu, A^{-1}) \\mathcal{N}(\\mu, A^{-1}) . w=w_{map} w=w_{map} , finds a mode (local max) of p(w|D) p(w|D) A = \\nabla\\nabla \\log p(D|w) p(w) A = \\nabla\\nabla \\log p(D|w) p(w) - very expensive calculation Only captures a single mode and discards the probability mass similar to the KLD in one direction. Markov Chain Monte Carlo \u00b6 We can produce samples from the exact posterior by defining a specific Monte Carlo chain. We actually do this in practice with NNs because of the stochastic training regimes. We modify the SGD algorithm to define a scalable MCMC sampler. Here is a visual demonstration of some popular MCMC samplers. Variational Inference \u00b6 Definition : We can find the best approximation within a given family w.r.t. KL-Divergence. $$ \\text{KLD}[q||p] = \\int_w q(w) \\log \\frac{q(w)}{p(w|D)}dw $$ Let q(w)=\\mathcal{N}(\\mu, S) q(w)=\\mathcal{N}(\\mu, S) and then we minimize KLD (q||p) (q||p) to find the parameters \\mu, S \\mu, S . \"Approximate the posterior, not the model\" - James Hensman.","title":"Solving Hard Integral Problems"},{"location":"appendix/bayesian/inference/#solving-hard-integral-problems","text":"Source | Deisenroth - Sampling Advances in VI - Notebook Numerical Integration (low dimension) Bayesian Quadrature Expectation Propagation Conjugate Priors (Gaussian Likelihood w/ GP Prior) Subset Methods (Nystrom) Fast Linear Algebra (Krylov, Fast Transforms, KD-Trees) Variational Methods (Laplace, Mean-Field, Expectation Propagation) Monte Carlo Methods (Gibbs, Metropolis-Hashings, Particle Filter)","title":"Solving Hard Integral Problems"},{"location":"appendix/bayesian/inference/#inference","text":"","title":"Inference"},{"location":"appendix/bayesian/inference/#maximum-likelihood","text":"Sources : Intro to Quantitative Econ w. Python","title":"Maximum Likelihood"},{"location":"appendix/bayesian/inference/#laplace-approximation","text":"This is where we approximate the posterior with a Gaussian distribution \\mathcal{N}(\\mu, A^{-1}) \\mathcal{N}(\\mu, A^{-1}) . w=w_{map} w=w_{map} , finds a mode (local max) of p(w|D) p(w|D) A = \\nabla\\nabla \\log p(D|w) p(w) A = \\nabla\\nabla \\log p(D|w) p(w) - very expensive calculation Only captures a single mode and discards the probability mass similar to the KLD in one direction.","title":"Laplace Approximation"},{"location":"appendix/bayesian/inference/#markov-chain-monte-carlo","text":"We can produce samples from the exact posterior by defining a specific Monte Carlo chain. We actually do this in practice with NNs because of the stochastic training regimes. We modify the SGD algorithm to define a scalable MCMC sampler. Here is a visual demonstration of some popular MCMC samplers.","title":"Markov Chain Monte Carlo"},{"location":"appendix/bayesian/inference/#variational-inference","text":"Definition : We can find the best approximation within a given family w.r.t. KL-Divergence. $$ \\text{KLD}[q||p] = \\int_w q(w) \\log \\frac{q(w)}{p(w|D)}dw $$ Let q(w)=\\mathcal{N}(\\mu, S) q(w)=\\mathcal{N}(\\mu, S) and then we minimize KLD (q||p) (q||p) to find the parameters \\mu, S \\mu, S . \"Approximate the posterior, not the model\" - James Hensman.","title":"Variational Inference"},{"location":"appendix/bayesian/intro/","text":"Bayesian: Language of Uncertainty \u00b6 Formulation \u00b6 A model is something that links inputs to outputs. If we are given data, X \\in \\mathbb{R}^{NxD} X \\in \\mathbb{R}^{NxD} , and observations, y y , we ideally would want to know these two entities are related. That relationship (or transformation) from the data X X to the observations y y is what we would call a model, \\mathcal{M} \\mathcal{M} . graph LR A[X] --> B[Model]; B --> C[y] More concretely, let X\\in \\mathbb{R}^{NxD} X\\in \\mathbb{R}^{NxD} and y \\in \\mathbb{R}^{N} y \\in \\mathbb{R}^{N} where N N is the number of samples and D D is the number of dimensions/features. In a transformation sense, we could think of it as a function, f f that maps the data from X X to y y , or f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} . To put it simply, we have the following equation to describe our model. y = f(X) y = f(X) But if we put a statistical spin on it and say that X X is a random variabe (r.v.), X \\sim \\mathbb{P} X \\sim \\mathbb{P} . We typically don't know \\mathbb{P} \\mathbb{P} or else there really would not be a problem. Or even worse, let's say that there is actually noise in our observation so we're not entirely 100% sure that each input, x x corresponds to each output, y y . Fortunately, we have mathematics where we can easily find some mathematical framework to transform our problem into a way we can easily solve. In this case, we can use the mathematics of probability theory to express the uncertainty and noise that come with our model, \\mathcal{M} \\mathcal{M} . More specifically, we can use Bayes rule to give us inverse probabilities that allow us to use inference; basically using our data to infer unknown quantities, model aspects and (most importantly) make predictions. Bayes Rule in Words \u00b6 In a Machine Learning problem, we almost always have the following components: Data Model which we believe can describe our data, parameters which can be changed/tuned to fit the data Goal Learn the parameters given the data which points belong to which cluster predict function outputs predict future labels predict the lower dimensional embedding/representation The Bayesian framework works best when you think about it from a probabilistic standpoint. \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} I've seen some people ( here , here ) have some sort of equivalence between Model, \\mathcal{M} \\mathcal{M} and Hypothesis, \\mathcal{H} \\mathcal{H} . In this particular instance, think of the \\mathcal{M} \\mathcal{M} as the best possible outcome that we can achieve to map x x to y y correctly . And think of \\mathcal{H} \\mathcal{H} as a set of possible formulas we could use; like in a Universe where we have all of the possible formulas and collection of parameters. I quite like the term Hypothesis because it adds another level of abstraction when thinking about the problem. But at the same time I feel like this extra layer of abstraction is not something I like to think about all of the time. Let's break down each of these components. P(\\text{ Model }) P(\\text{ Model }) - Prior Probability P(\\text{ Data } | \\text{}) P(\\text{ Data } | \\text{}) - Evidence, Normalization Constant P(\\text{ Model } | \\text{ Data }) P(\\text{ Model } | \\text{ Data }) - Posterior Probability P(\\text{ Data } | \\text{ Model }) P(\\text{ Data } | \\text{ Model }) - Likelihood Let's change the notation to something a bit more common. P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} where: * P(\\mathcal{D}|\\theta, \\mathcal{M}) P(\\mathcal{D}|\\theta, \\mathcal{M}) - Likelihood of the parameters, \\theta \\theta in model \\mathcal{M} \\mathcal{M} Likelihood of the parameters ( not of the data ). For every set of parameters, I can assign a probability to some observable data. * P(\\theta | \\mathcal{M}) P(\\theta | \\mathcal{M}) - prior probability of \\theta \\theta This expresses the distribution and the uncertainty of the parameters that define my model. It's a way of constraining the range of values that can occur. Expert knowledge in this area is crucial if you would like Physics-aware machine learning models. * P(\\mathcal{D}|\\mathcal{M}) P(\\mathcal{D}|\\mathcal{M}) - The normalization constant (the marginal likelihood) This term seems to give us a lot of problems ??? but this is an artifact of Bayes Rule where in order to obtain my Posterior, I need to renormalize. * P(\\theta | \\mathcal{D,M}) P(\\theta | \\mathcal{D,M}) - Posterior of \\theta \\theta given data \\mathcal{D} \\mathcal{D} T There are few things that are different. First of all, every single component is conditioned on a model \\mathcal{M} \\mathcal{M} . This is to say, given that I have described my model, here are the configurations that this model requires. So we're really staying true to the model based Machine Learning instead of the Toolbox method. Also, I've changed the data to be denoted as \\mathcal{D} \\mathcal{D} where \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} . Maximum A Posteriori (MAP) Estimation \u00b6 Notice how this doesn't really help us use or evaluate the parameters for the model that we've learned. That's when it comes to predictions. If we look at the sum for Bayes rule P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) , we can write something similar for actual datapoints, P(x|\\mathcal{D,M}) P(x|\\mathcal{D,M}) . P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta This integral is by d\\theta d\\theta because we want to integrate out the parameters (WHY???) . There is now a joint distribution between x x and \\theta \\theta . We actually don't have access to that. But we do have access to the posterior probability of \\theta \\theta given the data \\mathcal{D} \\mathcal{D} . So using the product rule ( P(x,y)=P(x)P(y|x) P(x,y)=P(x)P(y|x) ), we can split apart that quantity to obtain: P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta Now dissecting this formula a little further, we now have the average predictions for all \\theta \\theta 's which we weight by the posteriors. This arises a natural ensemble scheme where we are averaging models. Also worth noting is that there is no optimization within any of these quantities. Practically, my brain is telling me that that is a bit useless but I guess in the Bayesian framework, that's not really all that necessary. But yes, optimal model parameters would be needed for actually making decisions on future data. We can write a loss function \\mathcal{L} \\mathcal{L} w.r.t. \\theta \\theta to express how one would optimize this quantity: \\mathcal{L}(\\theta^*)= \\mathcal{L}(\\theta^*)= Model Comparison \u00b6 P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} The most interesting one is deriving the maximum likelihood formulation (Ocamz Razer, Model Evidence, Integrated Likelihood): what's the probability that the data I have came from the model P(\\mathcal{D|M}) P(\\mathcal{D|M}) . Again using Bayes sum rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta Once again, we don't have access to this joint distribution, but we do have access to the likelihood and the prior. So, again, we can decompose this joint distribution by using the Bayes product rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta QUESTIONS \u00b6 Marginalize? Normalize? Why is it so hard? Intractability? Posterior Likelihood Prior --- \u00b6 Supplementary \u00b6 Bait and Switch \u00b6 MAP estimate is easy to make it work but can do some weird stuff. Bayesian - hard to make it work but sometimes makes more sense. Maximum Likelihood \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) MAP \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) Type II Maximum Likelihood \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw Type II MAP: \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) Tips and Tricks for Practicioners \u00b6 Set initial hyper-parameters with domain knowledge Standardize input data Set initial length scales to \\sigma_l \\approx 0.5 \\sigma_l \\approx 0.5 Standardize targets \\mathbf{y} \\mathbf{y} Set initial signal variance to \\sigma_f\\approx 1.0 \\sigma_f\\approx 1.0 Set noise level initially high \\sigma_n \\approx 0.5 \\times \\sigma_f \\sigma_n \\approx 0.5 \\times \\sigma_f Random restarts Penalize high signal-to-noise ratios ( \\frac{\\sigma_f}{\\sigma_n} \\frac{\\sigma_f}{\\sigma_n} ) Resources \u00b6 Deisenroth - Model Selection Bayes Theorem Net Probabilistic World - What is Bayes Theorem | The Anatomy of Bayes Theorem Zhoubin Gharamani - YouTube Talk Learning From Data - Website TODO: FlowChart of Learning and Probabilistic Learning Linear Regression Experiments A Probabilistic View of Linear Regression - Keng (2016) - blog A Probabilistic Interpretation of Regularization - Brian Keng (2016) - blog Code : FlowCharts - Mermaid | VSC Ext Definitiions \u00b6 Bayes Rule Inference - Some conclusion reached given by some evidence or reasoning Probability - An expression of belief Probabilistic Modeling","title":"Bayesian: Language of Uncertainty"},{"location":"appendix/bayesian/intro/#bayesian-language-of-uncertainty","text":"","title":"Bayesian: Language of Uncertainty"},{"location":"appendix/bayesian/intro/#formulation","text":"A model is something that links inputs to outputs. If we are given data, X \\in \\mathbb{R}^{NxD} X \\in \\mathbb{R}^{NxD} , and observations, y y , we ideally would want to know these two entities are related. That relationship (or transformation) from the data X X to the observations y y is what we would call a model, \\mathcal{M} \\mathcal{M} . graph LR A[X] --> B[Model]; B --> C[y] More concretely, let X\\in \\mathbb{R}^{NxD} X\\in \\mathbb{R}^{NxD} and y \\in \\mathbb{R}^{N} y \\in \\mathbb{R}^{N} where N N is the number of samples and D D is the number of dimensions/features. In a transformation sense, we could think of it as a function, f f that maps the data from X X to y y , or f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} . To put it simply, we have the following equation to describe our model. y = f(X) y = f(X) But if we put a statistical spin on it and say that X X is a random variabe (r.v.), X \\sim \\mathbb{P} X \\sim \\mathbb{P} . We typically don't know \\mathbb{P} \\mathbb{P} or else there really would not be a problem. Or even worse, let's say that there is actually noise in our observation so we're not entirely 100% sure that each input, x x corresponds to each output, y y . Fortunately, we have mathematics where we can easily find some mathematical framework to transform our problem into a way we can easily solve. In this case, we can use the mathematics of probability theory to express the uncertainty and noise that come with our model, \\mathcal{M} \\mathcal{M} . More specifically, we can use Bayes rule to give us inverse probabilities that allow us to use inference; basically using our data to infer unknown quantities, model aspects and (most importantly) make predictions.","title":"Formulation"},{"location":"appendix/bayesian/intro/#bayes-rule-in-words","text":"In a Machine Learning problem, we almost always have the following components: Data Model which we believe can describe our data, parameters which can be changed/tuned to fit the data Goal Learn the parameters given the data which points belong to which cluster predict function outputs predict future labels predict the lower dimensional embedding/representation The Bayesian framework works best when you think about it from a probabilistic standpoint. \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} I've seen some people ( here , here ) have some sort of equivalence between Model, \\mathcal{M} \\mathcal{M} and Hypothesis, \\mathcal{H} \\mathcal{H} . In this particular instance, think of the \\mathcal{M} \\mathcal{M} as the best possible outcome that we can achieve to map x x to y y correctly . And think of \\mathcal{H} \\mathcal{H} as a set of possible formulas we could use; like in a Universe where we have all of the possible formulas and collection of parameters. I quite like the term Hypothesis because it adds another level of abstraction when thinking about the problem. But at the same time I feel like this extra layer of abstraction is not something I like to think about all of the time. Let's break down each of these components. P(\\text{ Model }) P(\\text{ Model }) - Prior Probability P(\\text{ Data } | \\text{}) P(\\text{ Data } | \\text{}) - Evidence, Normalization Constant P(\\text{ Model } | \\text{ Data }) P(\\text{ Model } | \\text{ Data }) - Posterior Probability P(\\text{ Data } | \\text{ Model }) P(\\text{ Data } | \\text{ Model }) - Likelihood Let's change the notation to something a bit more common. P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} where: * P(\\mathcal{D}|\\theta, \\mathcal{M}) P(\\mathcal{D}|\\theta, \\mathcal{M}) - Likelihood of the parameters, \\theta \\theta in model \\mathcal{M} \\mathcal{M} Likelihood of the parameters ( not of the data ). For every set of parameters, I can assign a probability to some observable data. * P(\\theta | \\mathcal{M}) P(\\theta | \\mathcal{M}) - prior probability of \\theta \\theta This expresses the distribution and the uncertainty of the parameters that define my model. It's a way of constraining the range of values that can occur. Expert knowledge in this area is crucial if you would like Physics-aware machine learning models. * P(\\mathcal{D}|\\mathcal{M}) P(\\mathcal{D}|\\mathcal{M}) - The normalization constant (the marginal likelihood) This term seems to give us a lot of problems ??? but this is an artifact of Bayes Rule where in order to obtain my Posterior, I need to renormalize. * P(\\theta | \\mathcal{D,M}) P(\\theta | \\mathcal{D,M}) - Posterior of \\theta \\theta given data \\mathcal{D} \\mathcal{D} T There are few things that are different. First of all, every single component is conditioned on a model \\mathcal{M} \\mathcal{M} . This is to say, given that I have described my model, here are the configurations that this model requires. So we're really staying true to the model based Machine Learning instead of the Toolbox method. Also, I've changed the data to be denoted as \\mathcal{D} \\mathcal{D} where \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} .","title":"Bayes Rule in Words"},{"location":"appendix/bayesian/intro/#maximum-a-posteriori-map-estimation","text":"Notice how this doesn't really help us use or evaluate the parameters for the model that we've learned. That's when it comes to predictions. If we look at the sum for Bayes rule P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) , we can write something similar for actual datapoints, P(x|\\mathcal{D,M}) P(x|\\mathcal{D,M}) . P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta This integral is by d\\theta d\\theta because we want to integrate out the parameters (WHY???) . There is now a joint distribution between x x and \\theta \\theta . We actually don't have access to that. But we do have access to the posterior probability of \\theta \\theta given the data \\mathcal{D} \\mathcal{D} . So using the product rule ( P(x,y)=P(x)P(y|x) P(x,y)=P(x)P(y|x) ), we can split apart that quantity to obtain: P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta Now dissecting this formula a little further, we now have the average predictions for all \\theta \\theta 's which we weight by the posteriors. This arises a natural ensemble scheme where we are averaging models. Also worth noting is that there is no optimization within any of these quantities. Practically, my brain is telling me that that is a bit useless but I guess in the Bayesian framework, that's not really all that necessary. But yes, optimal model parameters would be needed for actually making decisions on future data. We can write a loss function \\mathcal{L} \\mathcal{L} w.r.t. \\theta \\theta to express how one would optimize this quantity: \\mathcal{L}(\\theta^*)= \\mathcal{L}(\\theta^*)=","title":"Maximum A Posteriori (MAP) Estimation"},{"location":"appendix/bayesian/intro/#model-comparison","text":"P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} The most interesting one is deriving the maximum likelihood formulation (Ocamz Razer, Model Evidence, Integrated Likelihood): what's the probability that the data I have came from the model P(\\mathcal{D|M}) P(\\mathcal{D|M}) . Again using Bayes sum rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta Once again, we don't have access to this joint distribution, but we do have access to the likelihood and the prior. So, again, we can decompose this joint distribution by using the Bayes product rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta","title":"Model Comparison"},{"location":"appendix/bayesian/intro/#questions","text":"Marginalize? Normalize? Why is it so hard? Intractability? Posterior Likelihood Prior","title":"QUESTIONS"},{"location":"appendix/bayesian/intro/#-","text":"","title":"---"},{"location":"appendix/bayesian/intro/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/bayesian/intro/#bait-and-switch","text":"MAP estimate is easy to make it work but can do some weird stuff. Bayesian - hard to make it work but sometimes makes more sense. Maximum Likelihood \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) MAP \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) Type II Maximum Likelihood \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw Type II MAP: \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha)","title":"Bait and Switch"},{"location":"appendix/bayesian/intro/#tips-and-tricks-for-practicioners","text":"Set initial hyper-parameters with domain knowledge Standardize input data Set initial length scales to \\sigma_l \\approx 0.5 \\sigma_l \\approx 0.5 Standardize targets \\mathbf{y} \\mathbf{y} Set initial signal variance to \\sigma_f\\approx 1.0 \\sigma_f\\approx 1.0 Set noise level initially high \\sigma_n \\approx 0.5 \\times \\sigma_f \\sigma_n \\approx 0.5 \\times \\sigma_f Random restarts Penalize high signal-to-noise ratios ( \\frac{\\sigma_f}{\\sigma_n} \\frac{\\sigma_f}{\\sigma_n} )","title":"Tips and Tricks for Practicioners"},{"location":"appendix/bayesian/intro/#resources","text":"Deisenroth - Model Selection Bayes Theorem Net Probabilistic World - What is Bayes Theorem | The Anatomy of Bayes Theorem Zhoubin Gharamani - YouTube Talk Learning From Data - Website TODO: FlowChart of Learning and Probabilistic Learning Linear Regression Experiments A Probabilistic View of Linear Regression - Keng (2016) - blog A Probabilistic Interpretation of Regularization - Brian Keng (2016) - blog Code : FlowCharts - Mermaid | VSC Ext","title":"Resources"},{"location":"appendix/bayesian/intro/#definitiions","text":"Bayes Rule Inference - Some conclusion reached given by some evidence or reasoning Probability - An expression of belief Probabilistic Modeling","title":"Definitiions"},{"location":"appendix/bayesian/kde/","text":"Kernel Density Estimation \u00b6 Resources \u00b6 Built-In \u00b6 Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/kde/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/kde/#resources","text":"","title":"Resources"},{"location":"appendix/bayesian/kde/#built-in","text":"Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Built-In"},{"location":"appendix/bayesian/kl_divergence/","text":"KL Divergence \u00b6 Typical : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x VI : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x Positive and Reverse KL \u00b6 Density Ratio Estimation for KL Divergence Minimization between Implicit Distributions - Blog Resources : YouTube Aurelien Geron - Short Intro to Entropy, Cross-Entropy and KL-Divergence Ben Lambert - Through Secret Codes Zhoubin - Video > A nice talk where he highlights the asymptotic conditions for MLE. The proof is sketched using the minimization of the KLD function. Blog Anna-Lena Popkes KLD Explained KLD for ML Reverse Vs Forward KL KL-Divergence as an Objective Function NF Slides (MLE context) Edward Class Notes Stanford - MLE | Consistency and Asymptotic Normality of the MLE | Fisher Information, Cramer-Raw LB | MLE Model Mispecification Code KLD py NumPy/SciPy Recipes","title":"KL Divergence"},{"location":"appendix/bayesian/kl_divergence/#kl-divergence","text":"Typical : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x VI : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x","title":"KL Divergence"},{"location":"appendix/bayesian/kl_divergence/#positive-and-reverse-kl","text":"Density Ratio Estimation for KL Divergence Minimization between Implicit Distributions - Blog Resources : YouTube Aurelien Geron - Short Intro to Entropy, Cross-Entropy and KL-Divergence Ben Lambert - Through Secret Codes Zhoubin - Video > A nice talk where he highlights the asymptotic conditions for MLE. The proof is sketched using the minimization of the KLD function. Blog Anna-Lena Popkes KLD Explained KLD for ML Reverse Vs Forward KL KL-Divergence as an Objective Function NF Slides (MLE context) Edward Class Notes Stanford - MLE | Consistency and Asymptotic Normality of the MLE | Fisher Information, Cramer-Raw LB | MLE Model Mispecification Code KLD py NumPy/SciPy Recipes","title":"Positive and Reverse KL"},{"location":"appendix/bayesian/mixtures/","text":"Mixture Models \u00b6 Resources \u00b6 Overview \u00b6 Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog Code \u00b6 Built-in \u00b6 Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn. From Scratch \u00b6 ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"Mixture Models"},{"location":"appendix/bayesian/mixtures/#mixture-models","text":"","title":"Mixture Models"},{"location":"appendix/bayesian/mixtures/#resources","text":"","title":"Resources"},{"location":"appendix/bayesian/mixtures/#overview","text":"Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog","title":"Overview"},{"location":"appendix/bayesian/mixtures/#code","text":"","title":"Code"},{"location":"appendix/bayesian/mixtures/#built-in","text":"Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn.","title":"Built-in"},{"location":"appendix/bayesian/mixtures/#from-scratch","text":"ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"From Scratch"},{"location":"appendix/bayesian/monte_carlo/","text":"Monte Carlo Sampling \u00b6 MC in Python MC in Python Computational Statistics Parallel MC Sampling with multiprocessing Parallel in Python Parallel w. Ray Parallel MC w. Dask Comp Stats Parallel MC Sims in Python pt 1 MC n IS","title":"Monte carlo"},{"location":"appendix/bayesian/monte_carlo/#monte-carlo-sampling","text":"MC in Python MC in Python Computational Statistics Parallel MC Sampling with multiprocessing Parallel in Python Parallel w. Ray Parallel MC w. Dask Comp Stats Parallel MC Sims in Python pt 1 MC n IS","title":"Monte Carlo Sampling"},{"location":"appendix/bayesian/pdf_est/","text":"PDF Estimation \u00b6 Main Idea \u00b6 Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx Likelihood \u00b6 Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] Stochastic Gradient Descent \u00b6 Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages. Example \u00b6 Mixture of Gaussians \u00b6 p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise. Histogram Method \u00b6 Gotchas \u00b6 Search Sorted \u00b6 Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_est/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_est/#main-idea","text":"Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx","title":"Main Idea"},{"location":"appendix/bayesian/pdf_est/#likelihood","text":"Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}]","title":"Likelihood"},{"location":"appendix/bayesian/pdf_est/#stochastic-gradient-descent","text":"Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages.","title":"Stochastic Gradient Descent"},{"location":"appendix/bayesian/pdf_est/#example","text":"","title":"Example"},{"location":"appendix/bayesian/pdf_est/#mixture-of-gaussians","text":"p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise.","title":"Mixture of Gaussians"},{"location":"appendix/bayesian/pdf_est/#histogram-method","text":"","title":"Histogram Method"},{"location":"appendix/bayesian/pdf_est/#gotchas","text":"","title":"Gotchas"},{"location":"appendix/bayesian/pdf_est/#search-sorted","text":"Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"Search Sorted"},{"location":"appendix/bayesian/pdf_estimation/","text":"PDF Estimation \u00b6 Histograms \u00b6 Bayesian Blocks for Histograms \u00b6 Histograms done the right way AstroML Example Dynamic Programming Example - Jake VanderPlas Kernel Density Estimation \u00b6 Comparison of 1D Density Estimators Compares Normal, Bayesian Blocks, Nearest Neighbors, Kernel Density KNN + Sklearn Density Estimation Gives a nice implementation of knn to do density estimation","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_estimation/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_estimation/#histograms","text":"","title":"Histograms"},{"location":"appendix/bayesian/pdf_estimation/#bayesian-blocks-for-histograms","text":"Histograms done the right way AstroML Example Dynamic Programming Example - Jake VanderPlas","title":"Bayesian Blocks for Histograms"},{"location":"appendix/bayesian/pdf_estimation/#kernel-density-estimation","text":"Comparison of 1D Density Estimators Compares Normal, Bayesian Blocks, Nearest Neighbors, Kernel Density KNN + Sklearn Density Estimation Gives a nice implementation of knn to do density estimation","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/regression/","text":"Regression \u00b6 Bayesian Regression Inference Bayesian Regression \u00b6 Model \u00b6 In typical regression problems we have some data \\mathcal{D} \\mathcal{D} which consists of some input-output pairs X,y X,y . We wish to find a function f(\\cdot) f(\\cdot) that maps the data X X to y y . We also assume that there is some noise in the outputs \\epsilon_y \\epsilon_y . We can also have noise on the inputs X X but we will discuss that at a later time. So concretely, we have: $$ \\begin{aligned} y &= w : x + \\epsilon_y \\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\end{aligned} $$ Let's demonstrate this by generating N data points from the true distribution. As seen from the figure above, the points that we generated line somewhere along the true line. Of course, we are privvy to see the true like but an algorithm might have trouble with such few points. In addition, we can see the weight space is quite large as well. One thing we can do is maximize the likelihood that y y comes from some normal distribution \\mathcal{N} \\mathcal{N} with some mean \\mu \\mu and standard deviation \\sigma^2 \\sigma^2 . $$ \\mathcal{F} = \\underset{w}{\\text{max}} \\log \\mathcal{N} (y_i | w : x_i, \\sigma^2) $$ So we will use the mean squared error (MSE) error as a loss function for our problem as maximizing the likelihood is equivalent to minimizing the MSE. Proof: max MLE = min MSE The likelihood of our model is: $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log p(y_i|x_i,\\theta)$$ And for simplicity, we assume the noise $\\epsilon$ comes from a Gaussian distribution and that it is constant. So we can rewrite our likelihood as $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\mathcal{N}(y_i | \\mathbf{x_i, w}, \\sigma^2)$$ Plugging in the full formula for the Gaussian distribution with some simplifications gives us: $$ \\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_e^2}} \\exp\\left( - \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} \\right) $$ We can use the log rule $\\log ab = \\log a + \\log b$ to rewrite this expression to separate the constant term from the exponential. Also, $\\log e^x = x$. $$ \\log p(y|\\mathbf{X,w}) = - \\frac{N}{2} \\log 2 \\pi \\sigma_e^2 - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} $$ So, the first term is constant so that we can ignore that in our loss function. We can do the same for the denominator for the second term. Let's simplify it to make our life easier. $$ \\log p(y|\\mathbf{X,w}) = - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ So we want to maximize this quantity: in other words, I want to find the parameter $\\mathbf{w}$ s.t. this equation is maximum. $$ \\mathbf{w}_{MLE} = \\argmax_{\\mathbf{w}} - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ We can rewrite this expression because the maximum of a negative quantity is the same as minimizing a positive quantity. $$ \\mathbf{w}_{MLE} = \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ This is the same as the MSE error expression; with the edition of a scalar value $1/N$. $$ \\begin{aligned} \\mathbf{w}_{MLE} &= \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 \\\\ &= \\argmin_{\\mathbf{w}} \\text{MSE} \\end{aligned} $$ **Note**: If we did not know $\\sigma_y^2$ then we would have to optimize this as well. Inference \u00b6","title":"Regression"},{"location":"appendix/bayesian/regression/#regression","text":"Bayesian Regression Inference","title":"Regression"},{"location":"appendix/bayesian/regression/#bayesian-regression","text":"","title":"Bayesian Regression"},{"location":"appendix/bayesian/regression/#model","text":"In typical regression problems we have some data \\mathcal{D} \\mathcal{D} which consists of some input-output pairs X,y X,y . We wish to find a function f(\\cdot) f(\\cdot) that maps the data X X to y y . We also assume that there is some noise in the outputs \\epsilon_y \\epsilon_y . We can also have noise on the inputs X X but we will discuss that at a later time. So concretely, we have: $$ \\begin{aligned} y &= w : x + \\epsilon_y \\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\end{aligned} $$ Let's demonstrate this by generating N data points from the true distribution. As seen from the figure above, the points that we generated line somewhere along the true line. Of course, we are privvy to see the true like but an algorithm might have trouble with such few points. In addition, we can see the weight space is quite large as well. One thing we can do is maximize the likelihood that y y comes from some normal distribution \\mathcal{N} \\mathcal{N} with some mean \\mu \\mu and standard deviation \\sigma^2 \\sigma^2 . $$ \\mathcal{F} = \\underset{w}{\\text{max}} \\log \\mathcal{N} (y_i | w : x_i, \\sigma^2) $$ So we will use the mean squared error (MSE) error as a loss function for our problem as maximizing the likelihood is equivalent to minimizing the MSE. Proof: max MLE = min MSE The likelihood of our model is: $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log p(y_i|x_i,\\theta)$$ And for simplicity, we assume the noise $\\epsilon$ comes from a Gaussian distribution and that it is constant. So we can rewrite our likelihood as $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\mathcal{N}(y_i | \\mathbf{x_i, w}, \\sigma^2)$$ Plugging in the full formula for the Gaussian distribution with some simplifications gives us: $$ \\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_e^2}} \\exp\\left( - \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} \\right) $$ We can use the log rule $\\log ab = \\log a + \\log b$ to rewrite this expression to separate the constant term from the exponential. Also, $\\log e^x = x$. $$ \\log p(y|\\mathbf{X,w}) = - \\frac{N}{2} \\log 2 \\pi \\sigma_e^2 - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} $$ So, the first term is constant so that we can ignore that in our loss function. We can do the same for the denominator for the second term. Let's simplify it to make our life easier. $$ \\log p(y|\\mathbf{X,w}) = - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ So we want to maximize this quantity: in other words, I want to find the parameter $\\mathbf{w}$ s.t. this equation is maximum. $$ \\mathbf{w}_{MLE} = \\argmax_{\\mathbf{w}} - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ We can rewrite this expression because the maximum of a negative quantity is the same as minimizing a positive quantity. $$ \\mathbf{w}_{MLE} = \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ This is the same as the MSE error expression; with the edition of a scalar value $1/N$. $$ \\begin{aligned} \\mathbf{w}_{MLE} &= \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 \\\\ &= \\argmin_{\\mathbf{w}} \\text{MSE} \\end{aligned} $$ **Note**: If we did not know $\\sigma_y^2$ then we would have to optimize this as well.","title":"Model"},{"location":"appendix/bayesian/regression/#inference","text":"","title":"Inference"},{"location":"appendix/bayesian/uniform/","text":"Uniform Distribution \u00b6 Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"appendix/bayesian/uniform/#uniform-distribution","text":"Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"appendix/bayesian/variational_inference/","text":"Variational Inference \u00b6 Motivations \u00b6 Variational inference is the most scalable inference method the machine learning community has (as of 2019). Tutorials https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/ https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/ ELBO - Derivation \u00b6 Let's start with the marginal likelihood function. \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using some algebra, let's take the log of the marginal likelihood (evidence): \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution q(\\mathbf x) q(\\mathbf x) . We get the following equation: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now with Jensen's inequality, we have the relationship f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] . We would like to put the \\log \\log function inside of the integral. Jensen's inequality allows us to do this. If we let f(\\cdot)= \\log(\\cdot) f(\\cdot)= \\log(\\cdot) then we get the Jensen's equality for a concave function, f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] . In this case if we match the terms to each component to the inequality, we have \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] So now finally we have both terms in the inequality. Summarizing everything we have the following relationship: log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x I'm going to switch up the terminology just to make it easier aesthetically. I'm going to let \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) be \\log \\mathcal{P}(y|\\theta) \\log \\mathcal{P}(y|\\theta) and \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) . So basically: \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence. \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} The punchline of this (after many calculated manipulations), is that we obtain an optimization equation \\mathcal{F}(\\theta) \\mathcal{F}(\\theta) : \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] where: Approximate posterior distribution: q(x) q(x) The best match to the true posterior \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . This is what we want to calculate. Reconstruction Cost: \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] The expected log-likelihood measure of how well the samples from q(x) q(x) are able to explain the data y y . Penalty: \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] Ensures that the explanation of the data q(x) q(x) doesn't deviate too far from your beliefs \\mathcal{P}(x) \\mathcal{P}(x) . (Okham's razor constraint) Source : VI Tutorial - Shakir Mohamed If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. ### Comments on q(x) q(x) We have now transformed our problem from an integration problem to an optimization problem where we optimize for q(x) q(x) directly. Many people tend to simplify q q but we could easily write some dependencies on the data for example q(x|\\mathcal{D}) q(x|\\mathcal{D}) . We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence. Typically q(x) q(x) is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem. Many people say 'tighten the bound' but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood. ## Pros and Cons ### Why Variational Inference? Applicable to all probabilistic models Transforms a problem from integration to one of optimization Convergence assessment Principled and Scalable approach to model selection Compact representation of posterior distribution Faster to converge Numerically stable Modern Computing Architectures (GPUs) Why Not Variational Inference? \u00b6 Approximate posterior only Difficulty in optimization due to local minima Under-estimates the variance of posterior Limited theory and guarantees for variational mehtods Resources \u00b6 Tutorial Series - Why? | ELBO | MC ELBO | Reparameterization | MC ELBO unBias | MC ELBO PyTorch | Talk Blog Posts: Neural Variational Inference Classical Theory Scaling Up BlackBox Mode VAEs and Helmholtz Machines Importance Weighted AEs Neural Samplers and Hierarchical VI Importance Weighted Hierarchical VI | Video Normal Approximation to the Posterior Distribution - blog Lower Bound Understaing the Variational Lower Bound Deriving the Variational Lower Bound Summaries Advances in Variational Inference Presentations VI Shakir Deisenroth - VI | IT Bayesian Non-Parametrics and Priors over functions here Reviews * From EM to SVI * Variational Inference * VI- Review for Statisticians * Tutorial on VI * VI w/ Code * VI - Mean Field * VI Tutorial * GMM * VI in GMM * GMM Pyro | Pyro * GMM PyTorch | PyTorch | PyTorchy Code Extensions Neural Samplers and Hierarchical Variational Inference From Scratch \u00b6 Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0 Blog Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Algorithm Breakdown: Expectation Maximization - blog Variational Inference \u00b6 Variational Bayes and The Mean-Field Approximation - Keng (2017) - blog","title":"Variational Inference"},{"location":"appendix/bayesian/variational_inference/#variational-inference","text":"","title":"Variational Inference"},{"location":"appendix/bayesian/variational_inference/#motivations","text":"Variational inference is the most scalable inference method the machine learning community has (as of 2019). Tutorials https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/ https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/","title":"Motivations"},{"location":"appendix/bayesian/variational_inference/#elbo-derivation","text":"Let's start with the marginal likelihood function. \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using some algebra, let's take the log of the marginal likelihood (evidence): \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution q(\\mathbf x) q(\\mathbf x) . We get the following equation: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now with Jensen's inequality, we have the relationship f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] . We would like to put the \\log \\log function inside of the integral. Jensen's inequality allows us to do this. If we let f(\\cdot)= \\log(\\cdot) f(\\cdot)= \\log(\\cdot) then we get the Jensen's equality for a concave function, f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] . In this case if we match the terms to each component to the inequality, we have \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] So now finally we have both terms in the inequality. Summarizing everything we have the following relationship: log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x I'm going to switch up the terminology just to make it easier aesthetically. I'm going to let \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) be \\log \\mathcal{P}(y|\\theta) \\log \\mathcal{P}(y|\\theta) and \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) . So basically: \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence. \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} The punchline of this (after many calculated manipulations), is that we obtain an optimization equation \\mathcal{F}(\\theta) \\mathcal{F}(\\theta) : \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] where: Approximate posterior distribution: q(x) q(x) The best match to the true posterior \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . This is what we want to calculate. Reconstruction Cost: \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] The expected log-likelihood measure of how well the samples from q(x) q(x) are able to explain the data y y . Penalty: \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] Ensures that the explanation of the data q(x) q(x) doesn't deviate too far from your beliefs \\mathcal{P}(x) \\mathcal{P}(x) . (Okham's razor constraint) Source : VI Tutorial - Shakir Mohamed If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. ### Comments on q(x) q(x) We have now transformed our problem from an integration problem to an optimization problem where we optimize for q(x) q(x) directly. Many people tend to simplify q q but we could easily write some dependencies on the data for example q(x|\\mathcal{D}) q(x|\\mathcal{D}) . We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence. Typically q(x) q(x) is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem. Many people say 'tighten the bound' but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood. ## Pros and Cons ### Why Variational Inference? Applicable to all probabilistic models Transforms a problem from integration to one of optimization Convergence assessment Principled and Scalable approach to model selection Compact representation of posterior distribution Faster to converge Numerically stable Modern Computing Architectures (GPUs)","title":"ELBO - Derivation"},{"location":"appendix/bayesian/variational_inference/#why-not-variational-inference","text":"Approximate posterior only Difficulty in optimization due to local minima Under-estimates the variance of posterior Limited theory and guarantees for variational mehtods","title":"Why Not Variational Inference?"},{"location":"appendix/bayesian/variational_inference/#resources","text":"Tutorial Series - Why? | ELBO | MC ELBO | Reparameterization | MC ELBO unBias | MC ELBO PyTorch | Talk Blog Posts: Neural Variational Inference Classical Theory Scaling Up BlackBox Mode VAEs and Helmholtz Machines Importance Weighted AEs Neural Samplers and Hierarchical VI Importance Weighted Hierarchical VI | Video Normal Approximation to the Posterior Distribution - blog Lower Bound Understaing the Variational Lower Bound Deriving the Variational Lower Bound Summaries Advances in Variational Inference Presentations VI Shakir Deisenroth - VI | IT Bayesian Non-Parametrics and Priors over functions here Reviews * From EM to SVI * Variational Inference * VI- Review for Statisticians * Tutorial on VI * VI w/ Code * VI - Mean Field * VI Tutorial * GMM * VI in GMM * GMM Pyro | Pyro * GMM PyTorch | PyTorch | PyTorchy Code Extensions Neural Samplers and Hierarchical Variational Inference","title":"Resources"},{"location":"appendix/bayesian/variational_inference/#from-scratch","text":"Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0 Blog Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Algorithm Breakdown: Expectation Maximization - blog","title":"From Scratch"},{"location":"appendix/bayesian/variational_inference/#variational-inference_1","text":"Variational Bayes and The Mean-Field Approximation - Keng (2017) - blog","title":"Variational Inference"},{"location":"appendix/bayesian/neural_networks/dropout/","text":"Dropout \u00b6 Code \u00b6 from tensorflow.keras.layers import Input , Dense , Dropout inputs = Input ( shape = ( 1 ,)) x = Dense ( 512 , activation = \"relu\" )( inputs ) x = Dropout ( 0.5 )( x , training = True ) x = Dense ( 512 , activation = \"relu\" )( x ) x = Dropout ( 0.5 )( x , training = True ) outputs = Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . compile ( loss = \"mse\" , optimizer = \"adam\" ) model . fit ( x_train , y_train ) # do stochastic forward passes on x_test samples = [ model . predict ( x_test ) for _ in range ( 100 )] # predictive mean mu = np . mean ( samples , axis = 0 ) # predictive standard deviation var = np . var ( samples , axis = 0 ) var = np . percentile ( var , [ 5 , 95 ], axis = 0 ) # get bounds, 2 std (95% confidence interval) upper , lower = mu - 2 * var ** 0.5 , mu - 2 * var ** 0.5 # plot plt . plot ( x_test , mu ) plt . fill_between ( x_test , lower , upper , alpha = 0.1 ) Source : Yarin Gal - MLSS2019 Slides Resources \u00b6 Bayesian Deep Learning (MLSS 2019)","title":"Dropout"},{"location":"appendix/bayesian/neural_networks/dropout/#dropout","text":"","title":"Dropout"},{"location":"appendix/bayesian/neural_networks/dropout/#code","text":"from tensorflow.keras.layers import Input , Dense , Dropout inputs = Input ( shape = ( 1 ,)) x = Dense ( 512 , activation = \"relu\" )( inputs ) x = Dropout ( 0.5 )( x , training = True ) x = Dense ( 512 , activation = \"relu\" )( x ) x = Dropout ( 0.5 )( x , training = True ) outputs = Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . compile ( loss = \"mse\" , optimizer = \"adam\" ) model . fit ( x_train , y_train ) # do stochastic forward passes on x_test samples = [ model . predict ( x_test ) for _ in range ( 100 )] # predictive mean mu = np . mean ( samples , axis = 0 ) # predictive standard deviation var = np . var ( samples , axis = 0 ) var = np . percentile ( var , [ 5 , 95 ], axis = 0 ) # get bounds, 2 std (95% confidence interval) upper , lower = mu - 2 * var ** 0.5 , mu - 2 * var ** 0.5 # plot plt . plot ( x_test , mu ) plt . fill_between ( x_test , lower , upper , alpha = 0.1 ) Source : Yarin Gal - MLSS2019 Slides","title":"Code"},{"location":"appendix/bayesian/neural_networks/dropout/#resources","text":"Bayesian Deep Learning (MLSS 2019)","title":"Resources"},{"location":"appendix/bayesian/neural_networks/literature/","text":"","title":"Literature"},{"location":"appendix/bayesian/neural_networks/regression_master/","text":"","title":"Regression master"},{"location":"appendix/concepts/frameworks/","text":"Frameworks \u00b6 Principles to Products \u00b6 Shakir Mohammed Applications Assistive Tech. Advanced Science Climate and Energy Healthcare Fairness and Safety Autonomous Systems Reasoning Planning Explanation Rapid Learning World Simulation Objects and Relations Information Uncertainty Information Gain Causality Prediction Principles Probability Theory Bayesian Analysis Hypothesis Testing Estimation Theory Asymptotics","title":"Frameworks"},{"location":"appendix/concepts/frameworks/#frameworks","text":"","title":"Frameworks"},{"location":"appendix/concepts/frameworks/#principles-to-products","text":"Shakir Mohammed Applications Assistive Tech. Advanced Science Climate and Energy Healthcare Fairness and Safety Autonomous Systems Reasoning Planning Explanation Rapid Learning World Simulation Objects and Relations Information Uncertainty Information Gain Causality Prediction Principles Probability Theory Bayesian Analysis Hypothesis Testing Estimation Theory Asymptotics","title":"Principles to Products"},{"location":"appendix/concepts/lin_alg/","text":"Linear Algebra Tricks \u00b6 Frobenius Norm (Hilbert-Schmidt Norm) Intiution Formulation Code Frobenius Norm Frobenius Norm (or Hilbert-Schmidt Norm) a matrix Frobenius Norm (Hilbert-Schmidt Norm) \u00b6 Intiution \u00b6 The Frobenius norm is the common matrix-based norm. Formulation \u00b6 \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Proof Let $A=U\\Sigma V^\\top$ be the Singular Value Decomposition of A. Then $$||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2$$ If $\\lambda_i^2$ are the eigenvalues of $AA^\\top$ and $A^\\top A$, then we can show $$ \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} $$ Code \u00b6 Eigenvalues sigma_xy = covariance ( X , Y ) eigvals = np . linalg . eigvals ( sigma_xy ) f_norm = np . sum ( eigvals ** 2 ) Trace sigma_xy = covariance ( X , Y ) f_norm = np . trace ( X @ X . T ) ** 2 Einsum X -= np . mean ( X , axis = 1 ) Y -= np . mean ( Y , axis = 1 ) f_norm = np . einsum ( 'ij,ji->' , X @ X . T ) Refactor f_norm = np . linalg . norm ( X @ X . T ) Frobenius Norm \u00b6 ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F Frobenius Norm (or Hilbert-Schmidt Norm) a matrix \u00b6 \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Details Let A=U\\Sigma V^\\top A=U\\Sigma V^\\top be the Singular Value Decomposition of A. Then ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 If \\lambda_i^2 \\lambda_i^2 are the eigenvalues of AA^\\top AA^\\top and A^\\top A A^\\top A , then we can show \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned}","title":"Linear Algebra Tricks"},{"location":"appendix/concepts/lin_alg/#linear-algebra-tricks","text":"Frobenius Norm (Hilbert-Schmidt Norm) Intiution Formulation Code Frobenius Norm Frobenius Norm (or Hilbert-Schmidt Norm) a matrix","title":"Linear Algebra Tricks"},{"location":"appendix/concepts/lin_alg/#frobenius-norm-hilbert-schmidt-norm","text":"","title":"Frobenius Norm (Hilbert-Schmidt Norm)"},{"location":"appendix/concepts/lin_alg/#intiution","text":"The Frobenius norm is the common matrix-based norm.","title":"Intiution"},{"location":"appendix/concepts/lin_alg/#formulation","text":"\\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Proof Let $A=U\\Sigma V^\\top$ be the Singular Value Decomposition of A. Then $$||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2$$ If $\\lambda_i^2$ are the eigenvalues of $AA^\\top$ and $A^\\top A$, then we can show $$ \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} $$","title":"Formulation"},{"location":"appendix/concepts/lin_alg/#code","text":"Eigenvalues sigma_xy = covariance ( X , Y ) eigvals = np . linalg . eigvals ( sigma_xy ) f_norm = np . sum ( eigvals ** 2 ) Trace sigma_xy = covariance ( X , Y ) f_norm = np . trace ( X @ X . T ) ** 2 Einsum X -= np . mean ( X , axis = 1 ) Y -= np . mean ( Y , axis = 1 ) f_norm = np . einsum ( 'ij,ji->' , X @ X . T ) Refactor f_norm = np . linalg . norm ( X @ X . T )","title":"Code"},{"location":"appendix/concepts/lin_alg/#frobenius-norm","text":"||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F","title":"Frobenius Norm"},{"location":"appendix/concepts/lin_alg/#frobenius-norm-or-hilbert-schmidt-norm-a-matrix","text":"\\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Details Let A=U\\Sigma V^\\top A=U\\Sigma V^\\top be the Singular Value Decomposition of A. Then ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 If \\lambda_i^2 \\lambda_i^2 are the eigenvalues of AA^\\top AA^\\top and A^\\top A A^\\top A , then we can show \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned}","title":"Frobenius Norm (or Hilbert-Schmidt Norm) a matrix"},{"location":"appendix/concepts/matrix_tricks/","text":"Multivariate Normal Covariance Matrices and the Cholesky Decomposition","title":"Matrix tricks"},{"location":"appendix/concepts/sleeper_concepts/","text":"Key Concepts \u00b6 Definitions \u00b6 Mathematics Geometry: Studying Shapes and Spaces Algebra: Studying Relationships Probability: Belief, Uncertainty Calculus: Mathematics of Change Concepts \u00b6 MLE vs MSE vs KLD - jessica yung MLE vs Cross Validation Jensen's Inequality Error Propagation Law of Iterative Expectations Robustness - Symmetry/Equivariance Model Steps/Principles Occam's Razor - selecting less complex models Kolmogrov Complexity Universal Prior Maximum Likelihood Minimum Description Length Uncertainty What is it? Learning Theory, Epistemic\\Bayesian, aleatoric, Out-of-Distribution Bayes Theorem, Similarity","title":"Key Concepts"},{"location":"appendix/concepts/sleeper_concepts/#key-concepts","text":"","title":"Key Concepts"},{"location":"appendix/concepts/sleeper_concepts/#definitions","text":"Mathematics Geometry: Studying Shapes and Spaces Algebra: Studying Relationships Probability: Belief, Uncertainty Calculus: Mathematics of Change","title":"Definitions"},{"location":"appendix/concepts/sleeper_concepts/#concepts","text":"MLE vs MSE vs KLD - jessica yung MLE vs Cross Validation Jensen's Inequality Error Propagation Law of Iterative Expectations Robustness - Symmetry/Equivariance Model Steps/Principles Occam's Razor - selecting less complex models Kolmogrov Complexity Universal Prior Maximum Likelihood Minimum Description Length Uncertainty What is it? Learning Theory, Epistemic\\Bayesian, aleatoric, Out-of-Distribution Bayes Theorem, Similarity","title":"Concepts"},{"location":"appendix/concepts/splines/","text":"Splines \u00b6 Example - CDF Functions \u00b6 Example from the AstroML Book - Code | Text Invert Interpolations - StackOverFlow Maximum Wind Speed Prediction at the Sprogo station - Scipy Lecture Notes Approximation by Spline Functions and Parametric Spline Curves with SciPy - Vadym Pasko Optimization and Root Finding - Computational Statistics","title":"Splines"},{"location":"appendix/concepts/splines/#splines","text":"","title":"Splines"},{"location":"appendix/concepts/splines/#example-cdf-functions","text":"Example from the AstroML Book - Code | Text Invert Interpolations - StackOverFlow Maximum Wind Speed Prediction at the Sprogo station - Scipy Lecture Notes Approximation by Spline Functions and Parametric Spline Curves with SciPy - Vadym Pasko Optimization and Root Finding - Computational Statistics","title":"Example - CDF Functions"},{"location":"appendix/concepts/calculus/autograd/","text":"Automatic Differentiation \u00b6 Tutorials \u00b6 Automatic Differentiation - Matthew James Johnson (2017) - Video","title":"Automatic Differentiation"},{"location":"appendix/concepts/calculus/autograd/#automatic-differentiation","text":"","title":"Automatic Differentiation"},{"location":"appendix/concepts/calculus/autograd/#tutorials","text":"Automatic Differentiation - Matthew James Johnson (2017) - Video","title":"Tutorials"},{"location":"appendix/concepts/calculus/change_of_variables/","text":"Change of Variables \u00b6 This is after making some transformation function, we can find the probability of that function by simply multiplying Normalizing Flows \u00b6 First we will apply the change of variables formula from the perspective of parametric Gaussianization. Recall that we have our original data distribution \\mathcal{x} \\mathcal{x} and we want to find some transformation z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) such that z z is drawn from a Gaussian distribution z\\sim \\mathcal{N}(0, \\mathbf{I}) z\\sim \\mathcal{N}(0, \\mathbf{I}) . graph LR A((X)) -- D --> B((Z)) \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| Let z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) , we can simplify the notation a bit: \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| Now we can rewrite this equation in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} Let's do the same thing as above but from the perspective of normalized flows (at least the original idea). I've seen the perspective of a transformation \\mathcal{G} \\mathcal{G} that maps data from a latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . graph LR A((Z)) -- G --> B((X)) In this instance, we have a generator \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} that transforms the data from the latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . We can describe this as x=\\mathcal{G}_{\\theta}(z) x=\\mathcal{G}_{\\theta}(z) , so therefore going from \\mathcal{Z} \\mathcal{Z} to \\mathcal{X} \\mathcal{X} is given by this equation z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) . So first, let's write out the transformation not including the function values. \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| Now let's add in the function values taking into account that z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) : \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| Here, we have something different because we have the determinant of a function's inverse. We assume that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} is invertible which would allow us to use the inverse function theorem to move the inverse outside of the \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} . \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| And now we can use the fact that the determinant of the inverse of the Jacobian of invertible function is simply the inverse of the determinant of the Jacobian of the invertible function. In words, that's a lot to unpack, but it basically means that: \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} So with this last idea in mind, we can finally construct the final form: \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} Again, we can write this in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| Resources : * Youtube: * Professor Leonard - How to Change Variables in Multiple Integrals (Jacobian) * mrgonzalezWHS - Change of Variables. Jacobian * Kishore Kashyap - Transformations I | Transformations II * MathInsight * Double Integrals | Example * Course * Cambridge * Transforming Density Functions * Transforming Bivariate Density Functions * Pauls Online Math Notes * Change of Variables Teaching Notes Transforming Density Functions Transformation of RVs","title":"Change of Variables"},{"location":"appendix/concepts/calculus/change_of_variables/#change-of-variables","text":"This is after making some transformation function, we can find the probability of that function by simply multiplying","title":"Change of Variables"},{"location":"appendix/concepts/calculus/change_of_variables/#normalizing-flows","text":"First we will apply the change of variables formula from the perspective of parametric Gaussianization. Recall that we have our original data distribution \\mathcal{x} \\mathcal{x} and we want to find some transformation z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) such that z z is drawn from a Gaussian distribution z\\sim \\mathcal{N}(0, \\mathbf{I}) z\\sim \\mathcal{N}(0, \\mathbf{I}) . graph LR A((X)) -- D --> B((Z)) \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| Let z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) , we can simplify the notation a bit: \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| Now we can rewrite this equation in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} Let's do the same thing as above but from the perspective of normalized flows (at least the original idea). I've seen the perspective of a transformation \\mathcal{G} \\mathcal{G} that maps data from a latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . graph LR A((Z)) -- G --> B((X)) In this instance, we have a generator \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} that transforms the data from the latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . We can describe this as x=\\mathcal{G}_{\\theta}(z) x=\\mathcal{G}_{\\theta}(z) , so therefore going from \\mathcal{Z} \\mathcal{Z} to \\mathcal{X} \\mathcal{X} is given by this equation z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) . So first, let's write out the transformation not including the function values. \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| Now let's add in the function values taking into account that z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) : \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| Here, we have something different because we have the determinant of a function's inverse. We assume that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} is invertible which would allow us to use the inverse function theorem to move the inverse outside of the \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} . \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| And now we can use the fact that the determinant of the inverse of the Jacobian of invertible function is simply the inverse of the determinant of the Jacobian of the invertible function. In words, that's a lot to unpack, but it basically means that: \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} So with this last idea in mind, we can finally construct the final form: \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} Again, we can write this in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| Resources : * Youtube: * Professor Leonard - How to Change Variables in Multiple Integrals (Jacobian) * mrgonzalezWHS - Change of Variables. Jacobian * Kishore Kashyap - Transformations I | Transformations II * MathInsight * Double Integrals | Example * Course * Cambridge * Transforming Density Functions * Transforming Bivariate Density Functions * Pauls Online Math Notes * Change of Variables Teaching Notes Transforming Density Functions Transformation of RVs","title":"Normalizing Flows"},{"location":"appendix/concepts/calculus/identity_trick/","text":"Identity Trick \u00b6 I think the slides from the MLSS 2018 meeting is the only place that I have encountered anyone actually explicitly mentioning this Identity trick. Given an integral problem: p(x) = \\int p(x|z)p(z)dz p(x) = \\int p(x|z)p(z)dz I can multiply by an arbitrary distribution which is equivalent to 1. p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz Then I can regroup and reweight the integral p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz This results in a different expectation that we initially had p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] Examples: Importance Sampling Manipulate Stochastic gradients Derive Probability bounds RL for policy corrections","title":"[Identity Trick](https://www.shakirm.com/slides/MLSS2018-Madrid-ProbThinking.pdf)"},{"location":"appendix/concepts/calculus/identity_trick/#identity-trick","text":"I think the slides from the MLSS 2018 meeting is the only place that I have encountered anyone actually explicitly mentioning this Identity trick. Given an integral problem: p(x) = \\int p(x|z)p(z)dz p(x) = \\int p(x|z)p(z)dz I can multiply by an arbitrary distribution which is equivalent to 1. p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz Then I can regroup and reweight the integral p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz This results in a different expectation that we initially had p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] Examples: Importance Sampling Manipulate Stochastic gradients Derive Probability bounds RL for policy corrections","title":"Identity Trick"},{"location":"appendix/concepts/calculus/inverse_function/","text":"Inverse Function Theorem \u00b6 Resources : * Wiki * YouTube * Prof Ghist Math - Inverse Function Theorem * The Infinite Looper - Inv Fun Theorem * Professor Leonard - Fundamental Theorem of Calculus | Derivatives of Inverse Functions Source : Mathematics for Machine Learning - Deisenroth (2019) Change of Variables: A Precursor to Normalizing Flow - Rui Shu Pattern Recognition and Machine Learning - Bishop (2006) Often we are faced with the situation where we do not know the distribution of our data. But perhaps we know the distribution of a transformation of our data, e.g. if we know that X X is a r.v. that is uniformly distributed, then what is the distribution of X^2 + X + c X^2 + X + c ? In this case, we want to understand what's the relationship between the distribution we know and the transformed distribution. One way to do so is to use the inverse transform theorem which directly uses the cumulative distribution function (CDF). Let's say we have u \\sim \\mathcal U(0,1) u \\sim \\mathcal U(0,1) and some invertible function f(\\cdot) f(\\cdot) that maps X \\sim \\mathcal P X \\sim \\mathcal P to u u . x = f(u) x = f(u) Now, we want to know the probability of x x when all we know is the probability of u u . \\mathcal P(x)=\\mathcal P(f(u)=x) \\mathcal P(x)=\\mathcal P(f(u)=x) So solving for u u in that equation gives us: \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) Now we see that u=f^{-1}(x) u=f^{-1}(x) which gives us a direct formulation for moving from the uniform distribution space \\mathcal U \\mathcal U to a different probability distribution space \\mathcal P \\mathcal P . Probability Integral Transform Resources * Brilliant Does a nice example where they talk about the problems with fat-tailed distributions. * Wiki * CrossValidated * How does the inverse transform method work * Help me understand the quantile (inverse CDF) function * Youtube * Ben Hambert - Intro to Inv Transform Sampling * Mathematical Monk * Intro | General Case | Invertible Case * Code Review - Inverse Transform Sampling * R Markdown - Inverse Transform Sampling * Using Chebyshev - Blog | Code * CDFs - Super powerful way to visualize data and also is uniformly distriuted * Histograms and CDFs - blog * Why We Love CDFS so Much and not histograms - Blog * Boundary Issues * Confidence Band from DKW inequality - code * Make Monotonic - code * Matplotlib example of CDF bins versus theoretical (not smooth) - Code * Alternatives * KDE * Statsmodels Implementation - code | Univariate * KDE vs Histograms - blog * Empirical CDF * The Empirical Distribution Function - blog * Plotting an Empirical CDF In python - blog * Scipy histogram - code * Empirical CDF Function - code * ECDFs - notebook Derivative of an Inverse Function \u00b6 MathInsight - Link","title":"Inverse Function Theorem"},{"location":"appendix/concepts/calculus/inverse_function/#inverse-function-theorem","text":"Resources : * Wiki * YouTube * Prof Ghist Math - Inverse Function Theorem * The Infinite Looper - Inv Fun Theorem * Professor Leonard - Fundamental Theorem of Calculus | Derivatives of Inverse Functions Source : Mathematics for Machine Learning - Deisenroth (2019) Change of Variables: A Precursor to Normalizing Flow - Rui Shu Pattern Recognition and Machine Learning - Bishop (2006) Often we are faced with the situation where we do not know the distribution of our data. But perhaps we know the distribution of a transformation of our data, e.g. if we know that X X is a r.v. that is uniformly distributed, then what is the distribution of X^2 + X + c X^2 + X + c ? In this case, we want to understand what's the relationship between the distribution we know and the transformed distribution. One way to do so is to use the inverse transform theorem which directly uses the cumulative distribution function (CDF). Let's say we have u \\sim \\mathcal U(0,1) u \\sim \\mathcal U(0,1) and some invertible function f(\\cdot) f(\\cdot) that maps X \\sim \\mathcal P X \\sim \\mathcal P to u u . x = f(u) x = f(u) Now, we want to know the probability of x x when all we know is the probability of u u . \\mathcal P(x)=\\mathcal P(f(u)=x) \\mathcal P(x)=\\mathcal P(f(u)=x) So solving for u u in that equation gives us: \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) Now we see that u=f^{-1}(x) u=f^{-1}(x) which gives us a direct formulation for moving from the uniform distribution space \\mathcal U \\mathcal U to a different probability distribution space \\mathcal P \\mathcal P . Probability Integral Transform Resources * Brilliant Does a nice example where they talk about the problems with fat-tailed distributions. * Wiki * CrossValidated * How does the inverse transform method work * Help me understand the quantile (inverse CDF) function * Youtube * Ben Hambert - Intro to Inv Transform Sampling * Mathematical Monk * Intro | General Case | Invertible Case * Code Review - Inverse Transform Sampling * R Markdown - Inverse Transform Sampling * Using Chebyshev - Blog | Code * CDFs - Super powerful way to visualize data and also is uniformly distriuted * Histograms and CDFs - blog * Why We Love CDFS so Much and not histograms - Blog * Boundary Issues * Confidence Band from DKW inequality - code * Make Monotonic - code * Matplotlib example of CDF bins versus theoretical (not smooth) - Code * Alternatives * KDE * Statsmodels Implementation - code | Univariate * KDE vs Histograms - blog * Empirical CDF * The Empirical Distribution Function - blog * Plotting an Empirical CDF In python - blog * Scipy histogram - code * Empirical CDF Function - code * ECDFs - notebook","title":"Inverse Function Theorem"},{"location":"appendix/concepts/calculus/inverse_function/#derivative-of-an-inverse-function","text":"MathInsight - Link","title":"Derivative of an Inverse Function"},{"location":"appendix/concepts/calculus/jensens/","text":"Jensens Inequality \u00b6 This theorem is one of those sleeper theorems which comes up in a big way in many machine learning problems. The Jensen inequality theorem states that for a convex function f f , \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) A convex function (or concave up) is when there exists a minimum to that function. If we take two points on any part of the graph and draw a line between them, we will be above or at (as a limit) the minimum point of the graph. We can flip the signs for a concave function. But we want the convex property because then it means it has a minimum value and this is useful for minimization strategies. Recall from Calculus class 101: let's look at the function f(x)=\\log x f(x)=\\log x . We can use the second derivative test to find out if a function is convex or not. If f'(x) \\geq 0 f'(x) \\geq 0 then it is concave up (or convex). I'll map out the derivatives below: f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} You'll see that -\\frac{1}{x^2}\\leq 0 -\\frac{1}{x^2}\\leq 0 for x \\in [0, \\infty) x \\in [0, \\infty) . This means that \\log x \\log x is a concave function. So, the solution to this if we want a convex function is to take the negative \\log \\log (which adds intuition as to why we typically take the negative log likelihood of many functions). Variational Inference \u00b6 Typically in the VI literature, they add this Jensen inequality property in order to come up with the Evidence Lower Bound (ELBO). But I never understood how it worked because I didn't know if they wanted the convex or concave. If we think of the loss function of the likelihood \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) and the ELBO \\mathcal{F}(q, \\theta) \\mathcal{F}(q, \\theta) . Take a look at the figure from Figure: Showing Typically we use the \\log \\log function when it's used Resources * Computational Statistics * Blog * Sleeper Theorems * DIT Package * Ox Educ - Intuition | Proof * MIT OpenCourseWare - Intro Prob. | Inequalitiese, Convergence and Weak Law of Large Numbers","title":"Jensens Inequality"},{"location":"appendix/concepts/calculus/jensens/#jensens-inequality","text":"This theorem is one of those sleeper theorems which comes up in a big way in many machine learning problems. The Jensen inequality theorem states that for a convex function f f , \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) A convex function (or concave up) is when there exists a minimum to that function. If we take two points on any part of the graph and draw a line between them, we will be above or at (as a limit) the minimum point of the graph. We can flip the signs for a concave function. But we want the convex property because then it means it has a minimum value and this is useful for minimization strategies. Recall from Calculus class 101: let's look at the function f(x)=\\log x f(x)=\\log x . We can use the second derivative test to find out if a function is convex or not. If f'(x) \\geq 0 f'(x) \\geq 0 then it is concave up (or convex). I'll map out the derivatives below: f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} You'll see that -\\frac{1}{x^2}\\leq 0 -\\frac{1}{x^2}\\leq 0 for x \\in [0, \\infty) x \\in [0, \\infty) . This means that \\log x \\log x is a concave function. So, the solution to this if we want a convex function is to take the negative \\log \\log (which adds intuition as to why we typically take the negative log likelihood of many functions).","title":"Jensens Inequality"},{"location":"appendix/concepts/calculus/jensens/#variational-inference","text":"Typically in the VI literature, they add this Jensen inequality property in order to come up with the Evidence Lower Bound (ELBO). But I never understood how it worked because I didn't know if they wanted the convex or concave. If we think of the loss function of the likelihood \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) and the ELBO \\mathcal{F}(q, \\theta) \\mathcal{F}(q, \\theta) . Take a look at the figure from Figure: Showing Typically we use the \\log \\log function when it's used Resources * Computational Statistics * Blog * Sleeper Theorems * DIT Package * Ox Educ - Intuition | Proof * MIT OpenCourseWare - Intro Prob. | Inequalitiese, Convergence and Weak Law of Large Numbers","title":"Variational Inference"},{"location":"appendix/density/","text":"Density Estimation \u00b6 Overview of Methods \u00b6 Method Property Linear Uniform Distance Adaptive Kernels Adaptive, Smoothing Information Distribution Example \u00b6 TODO : Histogram of data distributions; Example","title":"Density Estimation"},{"location":"appendix/density/#density-estimation","text":"","title":"Density Estimation"},{"location":"appendix/density/#overview-of-methods","text":"Method Property Linear Uniform Distance Adaptive Kernels Adaptive, Smoothing Information Distribution","title":"Overview of Methods"},{"location":"appendix/density/#example","text":"TODO : Histogram of data distributions; Example","title":"Example"},{"location":"appendix/density/cde/","text":"Conditional Density Estimation \u00b6 Conditional Density Estimation with NNs: Best Practices and Benchmarks - arxiv (2019)","title":"Conditional Density Estimation"},{"location":"appendix/density/cde/#conditional-density-estimation","text":"Conditional Density Estimation with NNs: Best Practices and Benchmarks - arxiv (2019)","title":"Conditional Density Estimation"},{"location":"appendix/density/generative_modeling/","text":"Generative Modeling \u00b6 Resources \u00b6 Generative modelling with deep latent variable models A good presentation with some nice high-level concepts","title":"Generative Modeling"},{"location":"appendix/density/generative_modeling/#generative-modeling","text":"","title":"Generative Modeling"},{"location":"appendix/density/generative_modeling/#resources","text":"Generative modelling with deep latent variable models A good presentation with some nice high-level concepts","title":"Resources"},{"location":"appendix/density/histograms/","text":"Histograms \u00b6 Resources \u00b6 Histogram Speeds in Python Implementations \u00b6 Python Histogram Plotting: Numpy, Matplotlib, Pandas & Seaborn Numba Implementation Impriving Histogram w. Numpy Histogram w/o PyLab Fast Histogram","title":"Histograms"},{"location":"appendix/density/histograms/#histograms","text":"","title":"Histograms"},{"location":"appendix/density/histograms/#resources","text":"Histogram Speeds in Python","title":"Resources"},{"location":"appendix/density/histograms/#implementations","text":"Python Histogram Plotting: Numpy, Matplotlib, Pandas & Seaborn Numba Implementation Impriving Histogram w. Numpy Histogram w/o PyLab Fast Histogram","title":"Implementations"},{"location":"appendix/density/logistic/","text":"Logistic Distribution \u00b6 Cheat Sheet \u00b6 PDF Logistic Distribution f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) (-\\infty, \\infty) CDF Logistic Distribution F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) \\rightarrow [0, 1] (-\\infty, \\infty) \\rightarrow [0, 1] Quantile Function Logistic Distribution F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) where p \\sim \\mathcal{U}([0,1]) p \\sim \\mathcal{U}([0,1]) . Inverse sampling Log SoftMax \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) PyTorch Function - Functional.log_softmax Sigmoid \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} PyTorch Function - Functional.sigmoid Log Sigmoid \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) PyTorch Function - Functional.logsigmoid Log Sum Exponential \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) PyTorch Function - torch.logsumexp SoftPlus \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) PyTorch Function - Function.softplus","title":"Logistic Distribution"},{"location":"appendix/density/logistic/#logistic-distribution","text":"","title":"Logistic Distribution"},{"location":"appendix/density/logistic/#cheat-sheet","text":"PDF Logistic Distribution f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) (-\\infty, \\infty) CDF Logistic Distribution F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) \\rightarrow [0, 1] (-\\infty, \\infty) \\rightarrow [0, 1] Quantile Function Logistic Distribution F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) where p \\sim \\mathcal{U}([0,1]) p \\sim \\mathcal{U}([0,1]) . Inverse sampling Log SoftMax \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) PyTorch Function - Functional.log_softmax Sigmoid \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} PyTorch Function - Functional.sigmoid Log Sigmoid \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) PyTorch Function - Functional.logsigmoid Log Sum Exponential \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) PyTorch Function - torch.logsumexp SoftPlus \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) PyTorch Function - Function.softplus","title":"Cheat Sheet"},{"location":"appendix/density/flows/earth_science/","text":"Applications in Earth Science \u00b6 Personally, I haven't seen too many applications in Earth sciences. I've seen it show up explicitly in two scenarios: inverse problems and density estimation of the Earth. Literature \u00b6 Normalizing Flows on Tori and Spheres - Rezende et. al. (2020) - arxiv > They use normalizing flows for density estimation on different complex geometrices (e.g. Toris or Spheres). They have an example where they do it on the Earth. Not much explanation but it looked really cool. Analyzing Inverse Problems with Invertible Neural Networks - Ardizzone et. al. (2019) - arxiv > They use Normalizing flows (what they call invertible neural networks) to solve inverse problems. They also have a nice toolbox and reproducible code for this paper. Hybrid Models with Deep and Invertible Features - Nalisnick et. al. (2019) - plmr > While this is not directly applied to Earth sciences, I think this is a really cool idea. To get a probability distribution, train a normalizing flow on data. And then couple that to a machine learning model. Super neat! No code though. Couldn't find anything...","title":"Applications in Earth Science"},{"location":"appendix/density/flows/earth_science/#applications-in-earth-science","text":"Personally, I haven't seen too many applications in Earth sciences. I've seen it show up explicitly in two scenarios: inverse problems and density estimation of the Earth.","title":"Applications in Earth Science"},{"location":"appendix/density/flows/earth_science/#literature","text":"Normalizing Flows on Tori and Spheres - Rezende et. al. (2020) - arxiv > They use normalizing flows for density estimation on different complex geometrices (e.g. Toris or Spheres). They have an example where they do it on the Earth. Not much explanation but it looked really cool. Analyzing Inverse Problems with Invertible Neural Networks - Ardizzone et. al. (2019) - arxiv > They use Normalizing flows (what they call invertible neural networks) to solve inverse problems. They also have a nice toolbox and reproducible code for this paper. Hybrid Models with Deep and Invertible Features - Nalisnick et. al. (2019) - plmr > While this is not directly applied to Earth sciences, I think this is a really cool idea. To get a probability distribution, train a normalizing flow on data. And then couple that to a machine learning model. Super neat! No code though. Couldn't find anything...","title":"Literature"},{"location":"appendix/density/flows/gaussinization/","text":"Resources \u00b6 Projection Pursuit Implementation - github","title":"Gaussinization"},{"location":"appendix/density/flows/gaussinization/#resources","text":"Projection Pursuit Implementation - github","title":"Resources"},{"location":"appendix/density/flows/householder/","text":"HouseHolder Flows \u00b6 Literature \u00b6 History \u00b6 Improving Variational Auto-Encoders using Householder Flow - Tomczak & Welling (2017) - arxiv Silverster Normalizing Flows for Variational Inference - Van Den Berg et. al. (2018) - UAI - paper Related \u00b6 Variational Inference with Orthogonal Normalizing Flows - Hasenclever et. al. (2017) - arxiv Improving Normalizing Flows via Better Orthogonal Parameterizations Golinski et. al. (2019)- ICML Workshop Relevant \u00b6 Variational AutoEncoder with Optimizing Gaussian Mixture Model Priors - Guo et al. (2020) - PDF Variational inference with Gaussian mixture model and householder flow - Liu et. al. (2018) - ...","title":"HouseHolder Flows"},{"location":"appendix/density/flows/householder/#householder-flows","text":"","title":"HouseHolder Flows"},{"location":"appendix/density/flows/householder/#literature","text":"","title":"Literature"},{"location":"appendix/density/flows/householder/#history","text":"Improving Variational Auto-Encoders using Householder Flow - Tomczak & Welling (2017) - arxiv Silverster Normalizing Flows for Variational Inference - Van Den Berg et. al. (2018) - UAI - paper","title":"History"},{"location":"appendix/density/flows/householder/#related","text":"Variational Inference with Orthogonal Normalizing Flows - Hasenclever et. al. (2017) - arxiv Improving Normalizing Flows via Better Orthogonal Parameterizations Golinski et. al. (2019)- ICML Workshop","title":"Related"},{"location":"appendix/density/flows/householder/#relevant","text":"Variational AutoEncoder with Optimizing Gaussian Mixture Model Priors - Guo et al. (2020) - PDF Variational inference with Gaussian mixture model and householder flow - Liu et. al. (2018) - ...","title":"Relevant"},{"location":"appendix/density/flows/literature/","text":"Normalizing Flows Literature \u00b6 Training \u00b6 Can use Reverse-KL Can use Forward-KL (aka Maximum Likelihood Estimation) Generally possible to sample from the model Maximum Likelihood Training \\log p_\\theta(\\mathbf{x}) = \\log p_\\mathbf{z}(f(\\mathbf{x})) + \\log \\left| \\det \\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) \\right| \\log p_\\theta(\\mathbf{x}) = \\log p_\\mathbf{z}(f(\\mathbf{x})) + \\log \\left| \\det \\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) \\right| Stochastic Gradients \\nabla_\\theta \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x}) \\right] = \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\nabla_\\theta \\log p_\\theta(\\mathbf{x}) \\right] \\nabla_\\theta \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x}) \\right] = \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\nabla_\\theta \\log p_\\theta(\\mathbf{x}) \\right] Stochastic Scales to Large Datasets Converges to True minimum Large body of supportive software Summarizing \u00b6 Almost all papers are trying to do some form of creating a clever jacobian so that it is relatively cheap to calculate and work with. I like the slides in this presentation which attempts to summarize the different methods and how they are related. Jacobian Type Methods Determinant Identities Planar NF, Sylvester NF Coupling Blocks NICE, Real NVP, GLOW AutoRegressive Inverse AF, Neural AF, Masked AF Unbiased Estimation FFJORD, Residual Flows Diagonal Gaussianization Flows, GDN Automatic Differentiation \u00b6 According to a talk by Ricky Chen: For a full Jacobian, need d d separate passes. In general, a Jacobian diagonal has the same cost as the full jacobian . Not sure I understand this. But apparently, one could use HollowNets to efficiently compute dimension-wise derivatives of order k k . Source : Ricky Chen page Interesting \u00b6 Continuous Normalizing flows \u00b6 FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models - Grathwohl & Chen et. al. (2018) - arxiv Stochastic Normalizing Flows - Hodgkinson et. al. (2020) - arxiv","title":"Normalizing Flows Literature"},{"location":"appendix/density/flows/literature/#normalizing-flows-literature","text":"","title":"Normalizing Flows Literature"},{"location":"appendix/density/flows/literature/#training","text":"Can use Reverse-KL Can use Forward-KL (aka Maximum Likelihood Estimation) Generally possible to sample from the model Maximum Likelihood Training \\log p_\\theta(\\mathbf{x}) = \\log p_\\mathbf{z}(f(\\mathbf{x})) + \\log \\left| \\det \\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) \\right| \\log p_\\theta(\\mathbf{x}) = \\log p_\\mathbf{z}(f(\\mathbf{x})) + \\log \\left| \\det \\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) \\right| Stochastic Gradients \\nabla_\\theta \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x}) \\right] = \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\nabla_\\theta \\log p_\\theta(\\mathbf{x}) \\right] \\nabla_\\theta \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x}) \\right] = \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\nabla_\\theta \\log p_\\theta(\\mathbf{x}) \\right] Stochastic Scales to Large Datasets Converges to True minimum Large body of supportive software","title":"Training"},{"location":"appendix/density/flows/literature/#summarizing","text":"Almost all papers are trying to do some form of creating a clever jacobian so that it is relatively cheap to calculate and work with. I like the slides in this presentation which attempts to summarize the different methods and how they are related. Jacobian Type Methods Determinant Identities Planar NF, Sylvester NF Coupling Blocks NICE, Real NVP, GLOW AutoRegressive Inverse AF, Neural AF, Masked AF Unbiased Estimation FFJORD, Residual Flows Diagonal Gaussianization Flows, GDN","title":"Summarizing"},{"location":"appendix/density/flows/literature/#automatic-differentiation","text":"According to a talk by Ricky Chen: For a full Jacobian, need d d separate passes. In general, a Jacobian diagonal has the same cost as the full jacobian . Not sure I understand this. But apparently, one could use HollowNets to efficiently compute dimension-wise derivatives of order k k . Source : Ricky Chen page","title":"Automatic Differentiation"},{"location":"appendix/density/flows/literature/#interesting","text":"","title":"Interesting"},{"location":"appendix/density/flows/literature/#continuous-normalizing-flows","text":"FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models - Grathwohl & Chen et. al. (2018) - arxiv Stochastic Normalizing Flows - Hodgkinson et. al. (2020) - arxiv","title":"Continuous Normalizing flows"},{"location":"appendix/density/flows/mixture_cdfs/","text":"Continuous Mixture CDFs \u00b6 Author: J. Emmanuel Johnson Paper: Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design - Ho et. al. (2019) We take K Logistics. x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, where \\theta=[\\pi, \\mu, \\beta] \\theta=[\\pi, \\mu, \\beta] are the mixture params. \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) Domain \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) CDF Function F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) Source : Flow++ Code Structure \u00b6 Forward Transform Mixture Log CDF(x) Logit Function Mixture Log PDF Inverse Transformation Sigmoid Function Mixture Inverse CDF Mixture Log PDF Mixture of Logistics Coupling Layer \u00b6 Resources \u00b6 Flow++ Model Implementation with a Logistic Mixture Layer. Features the forward and backwards transformation with a bisection search. Uses PyTorch. Flow-Demos | Composition Flows Good Demo showing a basic CDF Flow Model. Also shows the composite flows. However, there is no inverse function. DPP Code Same as above but a better structure in my opinion. Gaussian Mixture CDF - Jax Jax Implementation. No inverse but at least I can see the potential Jax version Gaussian Mixture Model - INN 4 Inverse Problems | Tests | Technical Report They have a nice technical report which talks about how one can train a mixture density network with full covariance matrices. Literature \u00b6 Relevant \u00b6 Variational AutoEncoder with Optimizing Gaussian Mixture Model Priors - Guo et al. (2020) - PDF Variational inference with Gaussian mixture model and householder flow - Liu et. al. (2018) - ...","title":"Continuous Mixture CDFs"},{"location":"appendix/density/flows/mixture_cdfs/#continuous-mixture-cdfs","text":"Author: J. Emmanuel Johnson Paper: Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design - Ho et. al. (2019) We take K Logistics. x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, where \\theta=[\\pi, \\mu, \\beta] \\theta=[\\pi, \\mu, \\beta] are the mixture params. \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) Domain \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) CDF Function F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) Source : Flow++","title":"Continuous Mixture CDFs"},{"location":"appendix/density/flows/mixture_cdfs/#code-structure","text":"Forward Transform Mixture Log CDF(x) Logit Function Mixture Log PDF Inverse Transformation Sigmoid Function Mixture Inverse CDF Mixture Log PDF","title":"Code Structure"},{"location":"appendix/density/flows/mixture_cdfs/#mixture-of-logistics-coupling-layer","text":"","title":"Mixture of Logistics Coupling Layer"},{"location":"appendix/density/flows/mixture_cdfs/#resources","text":"Flow++ Model Implementation with a Logistic Mixture Layer. Features the forward and backwards transformation with a bisection search. Uses PyTorch. Flow-Demos | Composition Flows Good Demo showing a basic CDF Flow Model. Also shows the composite flows. However, there is no inverse function. DPP Code Same as above but a better structure in my opinion. Gaussian Mixture CDF - Jax Jax Implementation. No inverse but at least I can see the potential Jax version Gaussian Mixture Model - INN 4 Inverse Problems | Tests | Technical Report They have a nice technical report which talks about how one can train a mixture density network with full covariance matrices.","title":"Resources"},{"location":"appendix/density/flows/mixture_cdfs/#literature","text":"","title":"Literature"},{"location":"appendix/density/flows/mixture_cdfs/#relevant","text":"Variational AutoEncoder with Optimizing Gaussian Mixture Model Priors - Guo et al. (2020) - PDF Variational inference with Gaussian mixture model and householder flow - Liu et. al. (2018) - ...","title":"Relevant"},{"location":"appendix/density/gaussianization/","text":"RBIG for Spatial-Temporal Representation Analysis \u00b6 Notes \u00b6 Overview * Representation * My experience, PCA * Valeros previous paper * Schrodinger Eigenmaps * Model vs Density * Intro to Simo Sarkaa * Change of Variables to be Gaussian * Model as Gaussian * Density Estimation * Neural Splines Paper * Intro to Information * Neuroscience paper (why?) * variation of info paper - motivation 4 representation * Intro to RBIG * Gaussianization Flows paper * Intro to GPs * Gustau Paper Examples * Earth Science Data Cubes * Different Regions * Minicubes Visualization * Taylor Diagram - IT Measures * PySim Package * Line Plots - Model vs Density | MSE vs Info * Maps * DataShader * NASA Software? Algorithms * (GP vs RBIG) * GPFlow * pyRBIG * Taylor Diagrams * PySim","title":"RBIG for Spatial-Temporal Representation Analysis"},{"location":"appendix/density/gaussianization/#rbig-for-spatial-temporal-representation-analysis","text":"","title":"RBIG for Spatial-Temporal Representation Analysis"},{"location":"appendix/density/gaussianization/#notes","text":"Overview * Representation * My experience, PCA * Valeros previous paper * Schrodinger Eigenmaps * Model vs Density * Intro to Simo Sarkaa * Change of Variables to be Gaussian * Model as Gaussian * Density Estimation * Neural Splines Paper * Intro to Information * Neuroscience paper (why?) * variation of info paper - motivation 4 representation * Intro to RBIG * Gaussianization Flows paper * Intro to GPs * Gustau Paper Examples * Earth Science Data Cubes * Different Regions * Minicubes Visualization * Taylor Diagram - IT Measures * PySim Package * Line Plots - Model vs Density | MSE vs Info * Maps * DataShader * NASA Software? Algorithms * (GP vs RBIG) * GPFlow * pyRBIG * Taylor Diagrams * PySim","title":"Notes"},{"location":"appendix/density/gaussianization/ideas/","text":"RBIG 2.0 Ideas \u00b6 Neural Spline Flows - github Linear Rational Splines - github \\mathcal{X} \\rightarrow \\mathcal{Z} \\mathcal{X} \\rightarrow \\mathcal{Z} Option I F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] - CDF \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Logit Function Option II F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Quantile Gaussian Function CDF Function \u00b6 Mixture of Gaussians Mixture of Logistics Linear Quadratic Splines Rational Quadratic Splines NonLinear Squared Flows Logit Function \u00b6 Logit Function Gaussian Quantile Function Rotation \u00b6 HouseHolder Transforms SVD QR Decomposition LU Decomposition","title":"RBIG 2.0 Ideas"},{"location":"appendix/density/gaussianization/ideas/#rbig-20-ideas","text":"Neural Spline Flows - github Linear Rational Splines - github \\mathcal{X} \\rightarrow \\mathcal{Z} \\mathcal{X} \\rightarrow \\mathcal{Z} Option I F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] - CDF \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Logit Function Option II F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Quantile Gaussian Function","title":"RBIG 2.0 Ideas"},{"location":"appendix/density/gaussianization/ideas/#cdf-function","text":"Mixture of Gaussians Mixture of Logistics Linear Quadratic Splines Rational Quadratic Splines NonLinear Squared Flows","title":"CDF Function"},{"location":"appendix/density/gaussianization/ideas/#logit-function","text":"Logit Function Gaussian Quantile Function","title":"Logit Function"},{"location":"appendix/density/gaussianization/ideas/#rotation","text":"HouseHolder Transforms SVD QR Decomposition LU Decomposition","title":"Rotation"},{"location":"appendix/density/gaussianization/unscented/","text":"Unscented Transformation \u00b6 Resources \u00b6 The Unscented Kalman Filter - Wan & Van Der Merwe - PDF Unscented Kalman Filter Tutorial - Terejanu - PDF FilterPy - Function | Notebook State Estimation and Localization for Self-Driving Cars - Coursera Video | Course MiniDemo - Code Extended and Unscented GPs - Steinburg & Bonilla (2017) - PDF Unscented Gaussian Process Latent Variable Model: Learning from Uncertain Inputs w. Intractable Kernels - R.M.A. de Souza et. al. (2019) - PDF GP-UKF: Unscented Kalman Filters with Gaussian Process Predictionand Observation Models - Ko et. al. (2017) - PDF Variational Inference for Latent Variables and UncertainInputs in Gaussian Processes - Damianou et. al. (2016) - PDF SSMToyBox Bayesian Quadrature Oxford EmuKit GPyTorch","title":"Unscented Transformation"},{"location":"appendix/density/gaussianization/unscented/#unscented-transformation","text":"","title":"Unscented Transformation"},{"location":"appendix/density/gaussianization/unscented/#resources","text":"The Unscented Kalman Filter - Wan & Van Der Merwe - PDF Unscented Kalman Filter Tutorial - Terejanu - PDF FilterPy - Function | Notebook State Estimation and Localization for Self-Driving Cars - Coursera Video | Course MiniDemo - Code Extended and Unscented GPs - Steinburg & Bonilla (2017) - PDF Unscented Gaussian Process Latent Variable Model: Learning from Uncertain Inputs w. Intractable Kernels - R.M.A. de Souza et. al. (2019) - PDF GP-UKF: Unscented Kalman Filters with Gaussian Process Predictionand Observation Models - Ko et. al. (2017) - PDF Variational Inference for Latent Variables and UncertainInputs in Gaussian Processes - Damianou et. al. (2016) - PDF SSMToyBox Bayesian Quadrature Oxford EmuKit GPyTorch","title":"Resources"},{"location":"appendix/density/gaussianization/applications/climate/","text":"","title":"Climate"},{"location":"appendix/density/gaussianization/applications/demo_innf/","text":"Demo: Gaussianization \u00b6 Data \u00b6 RBIG Model \u00b6 Initialize Model \u00b6 # rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) Fit Model to Data \u00b6 # run RBIG model rbig_clf . fit ( X ); Visualization \u00b6 1. Marginal Gaussianization \u00b6 # rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V 2. Rotation \u00b6","title":"Demo: Gaussianization"},{"location":"appendix/density/gaussianization/applications/demo_innf/#demo-gaussianization","text":"","title":"Demo: Gaussianization"},{"location":"appendix/density/gaussianization/applications/demo_innf/#data","text":"","title":"Data"},{"location":"appendix/density/gaussianization/applications/demo_innf/#rbig-model","text":"","title":"RBIG Model"},{"location":"appendix/density/gaussianization/applications/demo_innf/#initialize-model","text":"# rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base )","title":"Initialize Model"},{"location":"appendix/density/gaussianization/applications/demo_innf/#fit-model-to-data","text":"# run RBIG model rbig_clf . fit ( X );","title":"Fit Model to Data"},{"location":"appendix/density/gaussianization/applications/demo_innf/#visualization","text":"","title":"Visualization"},{"location":"appendix/density/gaussianization/applications/demo_innf/#1-marginal-gaussianization","text":"# rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V","title":"1. Marginal Gaussianization"},{"location":"appendix/density/gaussianization/applications/demo_innf/#2-rotation","text":"","title":"2. Rotation"},{"location":"appendix/density/gaussianization/applications/drought/","text":"","title":"Drought"},{"location":"appendix/density/gaussianization/applications/spatial_temporal/","text":"","title":"Spatial temporal"},{"location":"appendix/density/gaussianization/software/coupling/","text":"Coupling Layers \u00b6 A univariate bijective differentiable function \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} , parameterized by \\theta \\theta . Note : It needs to be strictly monotonic. Non-Linear Squared Flows \u00b6 \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} where \\theta=[a,b,c,d,h] \\theta=[a,b,c,d,h] Invertible Inverse is analytically computable (root of cubic polynomial) Paper : Latent Normalizing Flows for Discrete Sequences - Ziegler & Rush (2019) Continuous Mixture CDFs \u00b6 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 where \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K The function F_{\\theta_2}(x, \\pi, \\mu, \\sigma) F_{\\theta_2}(x, \\pi, \\mu, \\sigma) is a CDF mixture distribution of K K logistic functions, post-composed with an inverse Sigmoid function, logit = \\log p / (1-p) = \\log p / (1-p) . So the full function is: F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) Some notes: * \\text{logit}:[0,1] \\rightarrow \\mathbb{R} \\text{logit}:[0,1] \\rightarrow \\mathbb{R} - ensure the right range for \\hat{f} \\hat{f} * Inverse: done numerically w/ the bisection algorithm * \\nabla_x F(\\cdot) \\nabla_x F(\\cdot) - it's a mixture of PDFs of logistic mixture distribution (i.e. linear combination of hyperbolic secant functions) Paper : Flow++ - Ho et. al. (2019) Splines \u00b6 A spline is a piece-wise polynomial or a piece-rational function which is specified by K+1 K+1 points (x_i,y_i)_{i=0}^K (x_i,y_i)_{i=0}^K called knots which a spline is passed. In particular, I am interested in rational-quadratic splines. Models a coupling layer \\hat{f}_\\theta(x) \\hat{f}_\\theta(x) as a monotone rational-quadratic spline on the interval [-B, B] [-B, B] , and outside the interval as an identity function. Paper : Neural Spline Flows, Durkan et. al. (2019)","title":"Coupling Layers"},{"location":"appendix/density/gaussianization/software/coupling/#coupling-layers","text":"A univariate bijective differentiable function \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} , parameterized by \\theta \\theta . Note : It needs to be strictly monotonic.","title":"Coupling Layers"},{"location":"appendix/density/gaussianization/software/coupling/#non-linear-squared-flows","text":"\\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} where \\theta=[a,b,c,d,h] \\theta=[a,b,c,d,h] Invertible Inverse is analytically computable (root of cubic polynomial) Paper : Latent Normalizing Flows for Discrete Sequences - Ziegler & Rush (2019)","title":"Non-Linear Squared Flows"},{"location":"appendix/density/gaussianization/software/coupling/#continuous-mixture-cdfs","text":"\\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 where \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K The function F_{\\theta_2}(x, \\pi, \\mu, \\sigma) F_{\\theta_2}(x, \\pi, \\mu, \\sigma) is a CDF mixture distribution of K K logistic functions, post-composed with an inverse Sigmoid function, logit = \\log p / (1-p) = \\log p / (1-p) . So the full function is: F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) Some notes: * \\text{logit}:[0,1] \\rightarrow \\mathbb{R} \\text{logit}:[0,1] \\rightarrow \\mathbb{R} - ensure the right range for \\hat{f} \\hat{f} * Inverse: done numerically w/ the bisection algorithm * \\nabla_x F(\\cdot) \\nabla_x F(\\cdot) - it's a mixture of PDFs of logistic mixture distribution (i.e. linear combination of hyperbolic secant functions) Paper : Flow++ - Ho et. al. (2019)","title":"Continuous Mixture CDFs"},{"location":"appendix/density/gaussianization/software/coupling/#splines","text":"A spline is a piece-wise polynomial or a piece-rational function which is specified by K+1 K+1 points (x_i,y_i)_{i=0}^K (x_i,y_i)_{i=0}^K called knots which a spline is passed. In particular, I am interested in rational-quadratic splines. Models a coupling layer \\hat{f}_\\theta(x) \\hat{f}_\\theta(x) as a monotone rational-quadratic spline on the interval [-B, B] [-B, B] , and outside the interval as an identity function. Paper : Neural Spline Flows, Durkan et. al. (2019)","title":"Splines"},{"location":"appendix/density/gaussianization/software/losses/","text":"Loss Functions \u00b6 Recall the change of variables formulation to calculate the probability: p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| and we can also calculate the log probability like so: \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| where z=\\mathcal{G}_\\theta(x) z=\\mathcal{G}_\\theta(x) . Negative Log-Likelihood \u00b6 -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] Empirically, this can be calculated by: -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| Non-Gaussianity \u00b6 Another perspective is the \"Non-Gaussianity\" of your data. J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] If we assume that the probability of p_x(x)=c p_x(x)=c because it will never change, it means that the only thing we have to do is minimize the 2 nd and 3 rd terms. \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} which we can find empirically: J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) ! Question : What's the difference between the two equations? Perhaps part 1, you fit a Gaussian... Change in Total Correlation \u00b6 Change in Non-Gaussianity \u00b6 \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] Empirically, we can calculate this by: \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)|","title":"Loss Functions"},{"location":"appendix/density/gaussianization/software/losses/#loss-functions","text":"Recall the change of variables formulation to calculate the probability: p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| and we can also calculate the log probability like so: \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| where z=\\mathcal{G}_\\theta(x) z=\\mathcal{G}_\\theta(x) .","title":"Loss Functions"},{"location":"appendix/density/gaussianization/software/losses/#negative-log-likelihood","text":"-\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] Empirically, this can be calculated by: -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)|","title":"Negative Log-Likelihood"},{"location":"appendix/density/gaussianization/software/losses/#non-gaussianity","text":"Another perspective is the \"Non-Gaussianity\" of your data. J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] If we assume that the probability of p_x(x)=c p_x(x)=c because it will never change, it means that the only thing we have to do is minimize the 2 nd and 3 rd terms. \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} which we can find empirically: J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) ! Question : What's the difference between the two equations? Perhaps part 1, you fit a Gaussian...","title":"Non-Gaussianity"},{"location":"appendix/density/gaussianization/software/losses/#change-in-total-correlation","text":"","title":"Change in Total Correlation"},{"location":"appendix/density/gaussianization/software/losses/#change-in-non-gaussianity","text":"\\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] Empirically, we can calculate this by: \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)|","title":"Change in Non-Gaussianity"},{"location":"appendix/density/gaussianization/software/marginal_gauss/","text":"Marginal Gaussianization \u00b6 A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform Idea \u00b6 The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration. High-Level Instructions \u00b6 Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d)) Mathematical Details \u00b6 For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian. Data \u00b6 In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution. Marginal Uniformization \u00b6 The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' Histogram Estimation \u00b6 Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1. Gaussianization of Uniform Variable \u00b6 In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' Log Determinant Jacobian \u00b6 \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} Log-Likelihood of the Data \u00b6 \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) Quantile Transform \u00b6 Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch KDE Transform \u00b6 Spline Functions \u00b6 Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper Gaussian Transform \u00b6","title":"Marginal Gaussianization"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#marginal-gaussianization","text":"A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform","title":"Marginal Gaussianization"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#idea","text":"The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration.","title":"Idea"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#high-level-instructions","text":"Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d))","title":"High-Level Instructions"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#mathematical-details","text":"For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian.","title":"Mathematical Details"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#data","text":"In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution.","title":"Data"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#marginal-uniformization","text":"The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d'","title":"Marginal Uniformization"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#histogram-estimation","text":"Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1.","title":"Histogram Estimation"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#gaussianization-of-uniform-variable","text":"In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d'","title":"Gaussianization of Uniform Variable"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#log-determinant-jacobian","text":"\\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)}","title":"Log Determinant Jacobian"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#log-likelihood-of-the-data","text":"\\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N})","title":"Log-Likelihood of the Data"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#quantile-transform","text":"Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch","title":"Quantile Transform"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#kde-transform","text":"","title":"KDE Transform"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#spline-functions","text":"Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper","title":"Spline Functions"},{"location":"appendix/density/gaussianization/software/marginal_gauss/#gaussian-transform","text":"","title":"Gaussian Transform"},{"location":"appendix/density/gaussianization/software/marginal_uni/","text":"Marginal Uniformization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020 Forward Transformation \u00b6 In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) . Boundary Issues \u00b6 The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails. Reverse Transformation \u00b6 This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems. Derivative \u00b6 In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} Log Abs Determinant Jacobian \u00b6 This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values. Probability (Computing the Density) \u00b6 So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Marginal Uniformization"},{"location":"appendix/density/gaussianization/software/marginal_uni/#marginal-uniformization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020","title":"Marginal Uniformization"},{"location":"appendix/density/gaussianization/software/marginal_uni/#forward-transformation","text":"In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) .","title":"Forward Transformation"},{"location":"appendix/density/gaussianization/software/marginal_uni/#boundary-issues","text":"The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails.","title":"Boundary Issues"},{"location":"appendix/density/gaussianization/software/marginal_uni/#reverse-transformation","text":"This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems.","title":"Reverse Transformation"},{"location":"appendix/density/gaussianization/software/marginal_uni/#derivative","text":"In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned}","title":"Derivative"},{"location":"appendix/density/gaussianization/software/marginal_uni/#log-abs-determinant-jacobian","text":"This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values.","title":"Log Abs Determinant Jacobian"},{"location":"appendix/density/gaussianization/software/marginal_uni/#probability-computing-the-density","text":"So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Probability (Computing the Density)"},{"location":"appendix/density/gaussianization/software/refactor/","text":"Refactoring RBIG (RBIG 1.1) \u00b6 Components \u00b6 Flow \u00b6 Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian Normalizing Flow \u00b6 This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian Normalizing Flow Model \u00b6 This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform Ideal Case \u00b6 Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"appendix/density/gaussianization/software/refactor/#refactoring-rbig-rbig-11","text":"","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"appendix/density/gaussianization/software/refactor/#components","text":"","title":"Components"},{"location":"appendix/density/gaussianization/software/refactor/#flow","text":"Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian","title":"Flow"},{"location":"appendix/density/gaussianization/software/refactor/#normalizing-flow","text":"This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian","title":"Normalizing Flow"},{"location":"appendix/density/gaussianization/software/refactor/#normalizing-flow-model","text":"This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform","title":"Normalizing Flow Model"},{"location":"appendix/density/gaussianization/software/refactor/#ideal-case","text":"Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Ideal Case"},{"location":"appendix/density/gaussianization/software/rotation/","text":"Rotation \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook Main Idea \u00b6 Rotation Matrix \u00b6 Foward Transformation \u00b6 Reverse Transformation \u00b6 Jacobian \u00b6 The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 . Log Jacobian \u00b6 As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0 Decompositions \u00b6 QR Decomposition \u00b6 A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular Singular Value Decomposition \u00b6 Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary Eigendecomposition \u00b6 Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Polar Decomposition \u00b6 A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6","title":"Rotation"},{"location":"appendix/density/gaussianization/software/rotation/#rotation","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook","title":"Rotation"},{"location":"appendix/density/gaussianization/software/rotation/#main-idea","text":"","title":"Main Idea"},{"location":"appendix/density/gaussianization/software/rotation/#rotation-matrix","text":"","title":"Rotation Matrix"},{"location":"appendix/density/gaussianization/software/rotation/#foward-transformation","text":"","title":"Foward Transformation"},{"location":"appendix/density/gaussianization/software/rotation/#reverse-transformation","text":"","title":"Reverse Transformation"},{"location":"appendix/density/gaussianization/software/rotation/#jacobian","text":"The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 .","title":"Jacobian"},{"location":"appendix/density/gaussianization/software/rotation/#log-jacobian","text":"As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0","title":"Log Jacobian"},{"location":"appendix/density/gaussianization/software/rotation/#decompositions","text":"","title":"Decompositions"},{"location":"appendix/density/gaussianization/software/rotation/#qr-decomposition","text":"A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular","title":"QR Decomposition"},{"location":"appendix/density/gaussianization/software/rotation/#singular-value-decomposition","text":"Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary","title":"Singular Value Decomposition"},{"location":"appendix/density/gaussianization/software/rotation/#eigendecomposition","text":"Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Eigendecomposition"},{"location":"appendix/density/gaussianization/software/rotation/#polar-decomposition","text":"A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Polar Decomposition"},{"location":"appendix/density/gaussianization/software/rotation/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"appendix/density/gaussianization/software/rotation/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"appendix/density/gaussianization/software/rotation/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"appendix/density/gaussianization/software/rotation/#jacobian_1","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"appendix/density/gaussianization/software/rotation/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"appendix/density/gaussianization/theory/gaussianization/","text":"Gaussianization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization Rotation-Based Iterative Gaussianization Rotation-Based Iterative Gaussianization Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence References Why Gaussianization? \u00b6 Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . Loss Function \u00b6 as shown in the equation from the original paper . Negentropy \u00b6 Methods \u00b6 Projection Pursuit \u00b6 Gaussianization \u00b6 Rotation-Based Iterative Gaussianization \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Rotation-Based Iterative Gaussianization \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space. Information Theory Measures \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} KL-Divergence \u00b6 Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} References \u00b6","title":"Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#gaussianization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization Rotation-Based Iterative Gaussianization Rotation-Based Iterative Gaussianization Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence References","title":"Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#why-gaussianization","text":"Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"appendix/density/gaussianization/theory/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"appendix/density/gaussianization/theory/gaussianization/#loss-function","text":"as shown in the equation from the original paper .","title":"Loss Function"},{"location":"appendix/density/gaussianization/theory/gaussianization/#negentropy","text":"","title":"Negentropy"},{"location":"appendix/density/gaussianization/theory/gaussianization/#methods","text":"","title":"Methods"},{"location":"appendix/density/gaussianization/theory/gaussianization/#projection-pursuit","text":"","title":"Projection Pursuit"},{"location":"appendix/density/gaussianization/theory/gaussianization/#gaussianization_1","text":"","title":"Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#rotation-based-iterative-gaussianization","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Rotation-Based Iterative Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#rotation-based-iterative-gaussianization_1","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Rotation-Based Iterative Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"appendix/density/gaussianization/theory/gaussianization/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"appendix/density/gaussianization/theory/gaussianization/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"appendix/density/gaussianization/theory/gaussianization/#information-theory-measures","text":"Caption : Information Theory measures in a nutshell.","title":"Information Theory Measures"},{"location":"appendix/density/gaussianization/theory/gaussianization/#information","text":"","title":"Information"},{"location":"appendix/density/gaussianization/theory/gaussianization/#entropy","text":"","title":"Entropy"},{"location":"appendix/density/gaussianization/theory/gaussianization/#mutual-information","text":"Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned}","title":"Mutual Information"},{"location":"appendix/density/gaussianization/theory/gaussianization/#kl-divergence","text":"Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"KL-Divergence"},{"location":"appendix/density/gaussianization/theory/gaussianization/#references","text":"","title":"References"},{"location":"appendix/density/gaussianization/theory/itm/","text":"Information Theory Measures \u00b6 Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD) Summary \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Total Correlation (Mutual Information) \u00b6 This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF Kullback-Leibler Divergence (KLD) \u00b6","title":"Information Theory Measures"},{"location":"appendix/density/gaussianization/theory/itm/#information-theory-measures","text":"Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD)","title":"Information Theory Measures"},{"location":"appendix/density/gaussianization/theory/itm/#summary","text":"Caption : Information Theory measures in a nutshell.","title":"Summary"},{"location":"appendix/density/gaussianization/theory/itm/#information","text":"","title":"Information"},{"location":"appendix/density/gaussianization/theory/itm/#entropy","text":"","title":"Entropy"},{"location":"appendix/density/gaussianization/theory/itm/#mutual-information","text":"","title":"Mutual Information"},{"location":"appendix/density/gaussianization/theory/itm/#total-correlation-mutual-information","text":"This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF","title":"Total Correlation (Mutual Information)"},{"location":"appendix/density/gaussianization/theory/itm/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"appendix/density/gaussianization/theory/literature/","text":"Literature Review \u00b6 Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization Theory \u00b6 Gaussianization \u00b6 The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks RBIG \u00b6 The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications Generalized Divisive Normalization \u00b6 This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Literature Review"},{"location":"appendix/density/gaussianization/theory/literature/#literature-review","text":"Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization","title":"Literature Review"},{"location":"appendix/density/gaussianization/theory/literature/#theory","text":"","title":"Theory"},{"location":"appendix/density/gaussianization/theory/literature/#gaussianization","text":"The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Gaussianization"},{"location":"appendix/density/gaussianization/theory/literature/#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks","title":"Journal Articles"},{"location":"appendix/density/gaussianization/theory/literature/#rbig","text":"The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications","title":"RBIG"},{"location":"appendix/density/gaussianization/theory/literature/#generalized-divisive-normalization","text":"This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Generalized Divisive Normalization"},{"location":"appendix/density/gaussianization/theory/related/","text":"Related Methods \u00b6 Deep Density Destructors Main Idea Normalizing Flows Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations Deep Density Destructors \u00b6 Main Idea \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| Normalizing Flows \u00b6 Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} Loss Function \u00b6 We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless. Sampling \u00b6 If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} Choice of Transformations \u00b6 The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian. Prior Distribution \u00b6 This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple. Jacobian \u00b6 This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed. Resources \u00b6 Best Tutorials \u00b6 Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang Survey of Literature \u00b6 Neural Density Estimators \u00b6 Deep Density Destructors \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/ Tutorials \u00b6 RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019) Algorithms \u00b6 * RBIG Upgrades \u00b6 Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF Cutting Edge \u00b6 Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code Github Implementations \u00b6 Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Related Methods"},{"location":"appendix/density/gaussianization/theory/related/#related-methods","text":"Deep Density Destructors Main Idea Normalizing Flows Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations","title":"Related Methods"},{"location":"appendix/density/gaussianization/theory/related/#deep-density-destructors","text":"","title":"Deep Density Destructors"},{"location":"appendix/density/gaussianization/theory/related/#main-idea","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Main Idea"},{"location":"appendix/density/gaussianization/theory/related/#normalizing-flows","text":"Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned}","title":"Normalizing Flows"},{"location":"appendix/density/gaussianization/theory/related/#loss-function","text":"We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless.","title":"Loss Function"},{"location":"appendix/density/gaussianization/theory/related/#sampling","text":"If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned}","title":"Sampling"},{"location":"appendix/density/gaussianization/theory/related/#choice-of-transformations","text":"The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian.","title":"Choice of Transformations"},{"location":"appendix/density/gaussianization/theory/related/#prior-distribution","text":"This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple.","title":"Prior Distribution"},{"location":"appendix/density/gaussianization/theory/related/#jacobian","text":"This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed.","title":"Jacobian"},{"location":"appendix/density/gaussianization/theory/related/#resources","text":"","title":"Resources"},{"location":"appendix/density/gaussianization/theory/related/#best-tutorials","text":"Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang","title":"Best Tutorials"},{"location":"appendix/density/gaussianization/theory/related/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"appendix/density/gaussianization/theory/related/#neural-density-estimators","text":"","title":"Neural Density Estimators"},{"location":"appendix/density/gaussianization/theory/related/#deep-density-destructors_1","text":"","title":"Deep Density Destructors"},{"location":"appendix/density/gaussianization/theory/related/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"appendix/density/gaussianization/theory/related/#tutorials","text":"RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019)","title":"Tutorials"},{"location":"appendix/density/gaussianization/theory/related/#algorithms","text":"*","title":"Algorithms"},{"location":"appendix/density/gaussianization/theory/related/#rbig-upgrades","text":"Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF","title":"RBIG Upgrades"},{"location":"appendix/density/gaussianization/theory/related/#cutting-edge","text":"Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code","title":"Cutting Edge"},{"location":"appendix/density/gaussianization/theory/related/#github-implementations","text":"Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Github Implementations"},{"location":"appendix/gps/1_introduction/","text":"GP from Scratch \u00b6 This post will go through how we can build a GP regression model from scratch. I will be going over the formulation as well as how we can code this up from scratch. I did this before a long time ago but I've learned a lot about GPs since then. So I'm putting all of my knowledge together so that I can get a good implementation that goes in parallel with the theory. I am also interested in furthering my research on uncertain GPs where I go over how we can look at input error in GPs. Materials The full code can be found in the colab notebook. Later I will refactor everything into a script so I can use it in the future. Colab Notebook Good News It took me approximately 12 hours in total to code this up from scratch. That's significantly better than last time as that time easily took me a week and some change. And I still had problems with the code afterwards. That's progress, no? Resources I saw quite a few tutorials that inspired me to do this tutorial. Blog Post - Excellent blog post that goes over GPs with step-by-step. Necessary equations only. Blog Post Series - Peter Roelants Good blog post series that go through more finer details of GPs using TensorFlow. Definition \u00b6 The first thing to understand about GPs is that we are actively placing a distribution \\mathcal{P}(f) \\mathcal{P}(f) on functions f f where these functions can be infinitely long function values f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . A GP generalizes the multivariate Gaussian distribution to infinitely many variables. A GP is a collection of random variables f_1, f_2, \\ldots f_1, f_2, \\ldots , any finite number of which is Gaussian distributed. A GP defines a distribution over functions p(f) p(f) which can be used for Bayesian regression. (Zhoubin) Another nice definition is: Gaussian Process : Any set of function variables \\{f_n \\}^{N}_{n=1} \\{f_n \\}^{N}_{n=1} has a joint Gaussian distribution with mean function m m . (Deisenroth) The nice thing is that this is provided by a mean function \\mu \\mu and covariance matrix \\mathbf{K} \\mathbf{K} Bayesian Inference Problem \u00b6 Objective \u00b6 Let's have some data set, \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) Model \u00b6 \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\\\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\\\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} The prior on f f is a GP distribution, the likelihood is Gaussian, therefore the posterior on f f is also a GP, P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} So we can make predictions: P(y_*|x_*, \\mathcal{D}) = \\int P(y_*|x_*, \\mathcal{D})P(f|\\mathcal{D})df P(y_*|x_*, \\mathcal{D}) = \\int P(y_*|x_*, \\mathcal{D})P(f|\\mathcal{D})df We can also do model comparison by way of the marginal likelihood (evidence) so that we can compare and tune the covariance functions P(y|X) = \\int P(y|f,X)P(f)df P(y|X) = \\int P(y|f,X)P(f)df Bayesian Treatment \u00b6 So now how does this look in terms of the Bayes theorem in words: \\text{Posterior} = \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}} \\text{Posterior} = \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}} And mathematically: p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) Gaussian Process Regression \u00b6 We only need a few elements to define a Gaussian process in itself. Just a mean function \\mu \\mu , a covariance matrix \\mathbf{K}_\\theta \\mathbf{K}_\\theta and some data, \\mathcal{D} \\mathcal{D} . Code class GPR : def __init__ ( self , mu , kernel , X , y , noise_variance = 1e-6 ): self . mu = mu self . kernel = kernel self . x_train = x_train self . y_train = y_train self . noise_variance = noise_variance Gaussian Process Prior \u00b6 This is the basis of the GP method. Under the assumption that we mentioned above: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) where m_\\theta m_\\theta is a mean function and \\mathbf{K} \\mathbf{K} is a covariance function We kind of treat these functions as a vector of function values up to infinity in theory f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . But in particular we look at the distribution over the function values, for example f_i=f(x_i) f_i=f(x_i) . So let's look at the joint distribution between N N function values f_N f_N and all other function values f_\\infty f_\\infty . This is 'normally distributed' so we can write the joint distribution roughly as: \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\\\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\\\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\\\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\\\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) where \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} and \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} (or m\\rightarrow \\infty m\\rightarrow \\infty ) to be more precise. So again, any marginal distribution of a joint Gaussian distribution is still a Gaussian distribution. So if we integrate over all of the functions from the infinite portion, we get: We can even get more specific and split the f_N f_N into training f_{\\text{train}} f_{\\text{train}} and testing f_{\\text{test}} f_{\\text{test}} . It's simply a matter of manipulating joint Gaussian distributions. So again, calculating the marginals: \\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned} and we arrive at a joint Gaussian distribution of the training and testing which is still normally distributed due to the marginalization. Code \u00b6 Code Mean Function Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function. def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ]) The output of the mean function is size \\mathbb{R}^{N} \\mathbb{R}^{N} . Kernel Function The most common kernel function you will see in the literature is the Radial Basis Function (RBF). It's a universal approximator and it performs fairly well on most datasets. If your dataset becomes non-linear, then it may start to fail as it is a really smooth function. The kernel function is defined as: k(x,y) = \\sigma_f \\exp \\left( - \\gamma || x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\gamma || x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) We also have the more robust version of the RBF with a separate length scale per dimension called the Automatic Relavance Determination (ARD) kernel. k(x,y) = \\sigma_f \\exp \\left( - || x / \\sigma_\\lambda - y / \\sigma_\\lambda ||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - || x / \\sigma_\\lambda - y / \\sigma_\\lambda ||^2_2 \\right) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) Remember : These are functions so they take in vectors \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and output a scalar value. Kernel Matrix The kernel function in the tab over shows how we can calculate the kernel for an input vector. But we need every single combination # Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x ) Sampling from Prior \u00b6 Now, something a bit more practical, generally speaking when we program the sampling portion of the prior, we need data. The kernel function is as is and has already been defined with its appropriate parameters. Furthermore, we already have defined the mean function \\mu \\mu when we initialized the mean function above. So we just need to pass the function through the multivariate normal function along with the number of samples we would like to draw from the prior. Code # initialize parameters params = { 'gamma' : 10. , 'length_scale' : 1e-3 , } n_samples = 10 # condition on 10 samples test_X = X [: n_samples , :] . copy () # random samples from data distribution # GP Prior functions (mu, sigma) mu_f = zero_mean cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 # number of random functions to draw key = jax . random . PRNGKey ( 0 ) # Jax random numbers boilerplate code y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,)) Likelihood (noise model) \u00b6 p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) This comes from our assumption as stated above from y=f(x)+\\epsilon y=f(x)+\\epsilon . Alternative Notation: * y\\sim \\mathcal{N}(f, \\sigma_n^2) y\\sim \\mathcal{N}(f, \\sigma_n^2) * \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) Posterior \u00b6 Alternative Notation: \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) Code This will easily be the longest function that we need for the GP. In my version, it's not necessary for training the GP. But it is necessary for testing. Posterior def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False , return_cov = False ): ( mu_func , cov_func ) = prior_params # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), Y - mu_func ( X ) . reshape ( - 1 , 1 ) ) # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ # calculate transform kernel KxX = cov_func ( params , X_new , X ) # Calculate the Mean mu_y = jnp . dot ( KxX , alpha ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== v = jax . scipy . linalg . cho_solve (( L , lower ), KxX . T ) # Calculate kernel matrix for inputs Kxx = cov_func ( params , X_new , X_new ) cov_y = Kxx - jnp . dot ( KxX , v ) # Likelihood Noise if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # return variance (diagonals of covariance) if return_cov is not True : cov_y = jnp . diag ( cov_y ) return mu_y , cov_y Cholesky A lot of times just straight solving the K^{-1}y=\\alpha K^{-1}y=\\alpha will give you problems. Many times you'll get an error about the matrix being ill-conditioned and non positive semi-definite. So we have to rectify that with the Cholesky decomposition. K K should be a positive semi-definite matrix so, there are more stable ways to solve this. We can use the cholesky decomposition which decomposes K K into a product of two lower triangular matrices: K = LL^\\top K = LL^\\top We do this because: it's less expensive to calculate the inverse of a triangular matrix it's easier to solve systems of equations Ax=b Ax=b . Cholesky Factorization There are two convenience terms that allow you to calculate the cholesky decomposition: cho_factor - calculates the decomposition K \\rightarrow L K \\rightarrow L cho_solve - solves the system of equations problem LL^\\top \\alpha=y LL^\\top \\alpha=y def cholesky_factorization ( K , Y ): # cho factor the cholesky, K = LL^T L = jax . scipy . linalg . cho_factor ( K , lower = True ) # alpha, LL^T alpha=y alpha = jax . scipy . linalg . cho_solve ( L , Y ) return L , alpha Note : If you want to get the cholesky matrix by itself and operator on it without the cho_factor function, then you should call the cholesky function directly. The cho_factor puts random (inexpensive) values in the part of the triangle that's not necessary. Whereas the cholesky adds zeros there instead. Variance Term The variance term also makes use of the K^{-1} K^{-1} . So naturally, we can use the already factored cholesky decompsition to calculate the term. \\begin{aligned} k_* K^{-1}k_* &= (Lv)^\\top K^{-1}Lv\\\\ &= v^\\top L^\\top (LL^\\top)^{-1} Lv\\\\ &= v^{\\top}L^\\top L^{-\\top}L^{-1}v\\\\ &= v^\\top v \\end{aligned} \\begin{aligned} k_* K^{-1}k_* &= (Lv)^\\top K^{-1}Lv\\\\ &= v^\\top L^\\top (LL^\\top)^{-1} Lv\\\\ &= v^{\\top}L^\\top L^{-\\top}L^{-1}v\\\\ &= v^\\top v \\end{aligned} v = jax . scipy . linalg . cho_solve (( L , lower ), KxX . T ) var = np . dot ( KxX , v ) Joint Probability Distribution \u00b6 To make GPs useful, we want to actually make predictions. This stems from the using the joint distribution of the training data and test data with the formula shown above used to condition on multivariate Gaussians. In terms of the GP function space, we have \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\\\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\\\ K_{*x} & K_{**} \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\\\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\\\ K_{*x} & K_{**} \\end{bmatrix} \\right) \\end{aligned} Then solving for the marginals, we can come up with the predictive test points. \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) where: \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} Marginal Log-Likelihood \u00b6 The prior m(x), K m(x), K have hyper-parameters \\theta \\theta . So learning a \\mathcal{GP} \\mathcal{GP} implies inferring hyper-parameters from the model. p(Y|X,\\theta)=\\int p(Y|f)p(f|X, \\theta)df p(Y|X,\\theta)=\\int p(Y|f)p(f|X, \\theta)df However, we are not interested in f f directly. We can marginalize it out via the integral equation. The marginal of a Gaussian is Gaussian. Note : Typically we use the \\log \\log likelihood instead of a pure likelihood. This is purely for computational purposes. The \\log \\log function is monotonic so it doesn't alter the location of the extreme points of the function. Furthermore we typically minimize the -\\log -\\log instead of the maximum \\log \\log for purely practical reasons. One way to train these functions is to use Maximum A Posterior (MAP) of the hyper-parameters \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} Marginal Likelihood (Evidence) \u00b6 p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df where: * p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) * p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) Note that all we're doing is simply describing each of these elements specifically because all of these quantities are Gaussian distributed. p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df So the product of two Gaussians is simply a Gaussian. That along with the notion that the integral of all the functions is a normal distribution with mean \\mu \\mu and covariance K K . p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) Proof Using the Gaussian identities: \\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned} \\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned} So we can use the same reasoning to combine the prior and the likelihood to get the posterior \\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned} \\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned} Source : Alternative Derivation for Log Likelihood - blog Marginal Log-Likelihood \u00b6 TODO Proof of Marginal Log-Likelihood Now we need a cost function that will allow us to get the best hyperparameters that fit our data. \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| Inverting N\\times N N\\times N matrices is the worse part about GPs in general. There are many techniques to be able to handle them, but for basics, it can become a problem. Furthermore, inverting this Kernel matrix tends to have problems being positive semi-definite . One way we can make this more efficient is to do the cholesky decomposition and then solve our problem that way. Cholesky Components \u00b6 Let \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) . We can write the log likelihood in terms of the cholesky decomposition. \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\underbrace{\\mathbf{L}^{-1}y}_{\\alpha}||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\underbrace{\\mathbf{L}^{-1}y}_{\\alpha}||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} This gives us a computational complexity of \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) Code I will demonstrate two ways to do this: We will use the equations above We will refactor this and use the built-in function From Scratch def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # =========================== # 3. Marginal Log-Likelihood # =========================== log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) # same as dot(Y.T, alpha) log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) return - log_likelihood . sum () Refactored def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior, mu(), cov(,) # ========================== mu_x = mu_f ( Ytrain ) Kxx = cov_f ( params , Xtrain , Xtrain ) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) # =========================== # 3. Marginal Log-Likelihood # =========================== # get log probability log_prob = jax . scipy . stats . multivariate_normal . logpdf ( x = Ytrain . T , mean = mu_x , cov = K_gp ) # sum dimensions and return neg mll return - log_prob . sum () source - Dai, GPSS 2018 Training \u00b6 Code Log Params We often have problems when it comes to using optimizers. A lot of times they just don't seem to want to converge and the gradients seem to not change no matter what happens. One trick we can do is to make the optimizer solve a transformed version of the parameters. And then we can take a softmax so that they converge properly. f(x) = \\ln (1 + \\exp(x)) f(x) = \\ln (1 + \\exp(x)) Jax has a built-in function so we'll just use that. def saturate ( params ): return { ikey : jax . nn . softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} Experimental Parameters logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 50 ) # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { 'gamma' : 2.0 , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1. , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Training Step # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Experimental Loop # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # TRAINING PARARMETERS n_epochs = 500 learning_rate = 0.1 losses = list () postfix = {} import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params ) Resources \u00b6 Surrogates: GP Modeling, Design, and Optimization for the Applied Sciences - Gramacy - Online Book","title":"Basic GP"},{"location":"appendix/gps/1_introduction/#gp-from-scratch","text":"This post will go through how we can build a GP regression model from scratch. I will be going over the formulation as well as how we can code this up from scratch. I did this before a long time ago but I've learned a lot about GPs since then. So I'm putting all of my knowledge together so that I can get a good implementation that goes in parallel with the theory. I am also interested in furthering my research on uncertain GPs where I go over how we can look at input error in GPs. Materials The full code can be found in the colab notebook. Later I will refactor everything into a script so I can use it in the future. Colab Notebook Good News It took me approximately 12 hours in total to code this up from scratch. That's significantly better than last time as that time easily took me a week and some change. And I still had problems with the code afterwards. That's progress, no? Resources I saw quite a few tutorials that inspired me to do this tutorial. Blog Post - Excellent blog post that goes over GPs with step-by-step. Necessary equations only. Blog Post Series - Peter Roelants Good blog post series that go through more finer details of GPs using TensorFlow.","title":"GP from Scratch"},{"location":"appendix/gps/1_introduction/#definition","text":"The first thing to understand about GPs is that we are actively placing a distribution \\mathcal{P}(f) \\mathcal{P}(f) on functions f f where these functions can be infinitely long function values f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . A GP generalizes the multivariate Gaussian distribution to infinitely many variables. A GP is a collection of random variables f_1, f_2, \\ldots f_1, f_2, \\ldots , any finite number of which is Gaussian distributed. A GP defines a distribution over functions p(f) p(f) which can be used for Bayesian regression. (Zhoubin) Another nice definition is: Gaussian Process : Any set of function variables \\{f_n \\}^{N}_{n=1} \\{f_n \\}^{N}_{n=1} has a joint Gaussian distribution with mean function m m . (Deisenroth) The nice thing is that this is provided by a mean function \\mu \\mu and covariance matrix \\mathbf{K} \\mathbf{K}","title":"Definition"},{"location":"appendix/gps/1_introduction/#bayesian-inference-problem","text":"","title":"Bayesian Inference Problem"},{"location":"appendix/gps/1_introduction/#objective","text":"Let's have some data set, \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y)","title":"Objective"},{"location":"appendix/gps/1_introduction/#model","text":"\\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\\\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\\\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} The prior on f f is a GP distribution, the likelihood is Gaussian, therefore the posterior on f f is also a GP, P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} So we can make predictions: P(y_*|x_*, \\mathcal{D}) = \\int P(y_*|x_*, \\mathcal{D})P(f|\\mathcal{D})df P(y_*|x_*, \\mathcal{D}) = \\int P(y_*|x_*, \\mathcal{D})P(f|\\mathcal{D})df We can also do model comparison by way of the marginal likelihood (evidence) so that we can compare and tune the covariance functions P(y|X) = \\int P(y|f,X)P(f)df P(y|X) = \\int P(y|f,X)P(f)df","title":"Model"},{"location":"appendix/gps/1_introduction/#bayesian-treatment","text":"So now how does this look in terms of the Bayes theorem in words: \\text{Posterior} = \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}} \\text{Posterior} = \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}} And mathematically: p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*)","title":"Bayesian Treatment"},{"location":"appendix/gps/1_introduction/#gaussian-process-regression","text":"We only need a few elements to define a Gaussian process in itself. Just a mean function \\mu \\mu , a covariance matrix \\mathbf{K}_\\theta \\mathbf{K}_\\theta and some data, \\mathcal{D} \\mathcal{D} . Code class GPR : def __init__ ( self , mu , kernel , X , y , noise_variance = 1e-6 ): self . mu = mu self . kernel = kernel self . x_train = x_train self . y_train = y_train self . noise_variance = noise_variance","title":"Gaussian Process Regression"},{"location":"appendix/gps/1_introduction/#gaussian-process-prior","text":"This is the basis of the GP method. Under the assumption that we mentioned above: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) where m_\\theta m_\\theta is a mean function and \\mathbf{K} \\mathbf{K} is a covariance function We kind of treat these functions as a vector of function values up to infinity in theory f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . But in particular we look at the distribution over the function values, for example f_i=f(x_i) f_i=f(x_i) . So let's look at the joint distribution between N N function values f_N f_N and all other function values f_\\infty f_\\infty . This is 'normally distributed' so we can write the joint distribution roughly as: \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\\\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\\\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\\\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\\\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) where \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} and \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} (or m\\rightarrow \\infty m\\rightarrow \\infty ) to be more precise. So again, any marginal distribution of a joint Gaussian distribution is still a Gaussian distribution. So if we integrate over all of the functions from the infinite portion, we get: We can even get more specific and split the f_N f_N into training f_{\\text{train}} f_{\\text{train}} and testing f_{\\text{test}} f_{\\text{test}} . It's simply a matter of manipulating joint Gaussian distributions. So again, calculating the marginals: \\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned} and we arrive at a joint Gaussian distribution of the training and testing which is still normally distributed due to the marginalization.","title":"Gaussian Process Prior"},{"location":"appendix/gps/1_introduction/#code","text":"Code Mean Function Honestly, I never work with mean functions. I always assume a zero-mean function and that's it. I don't really know anyone who works with mean functions either. I've seen it used in deep Gaussian processes but I have no expertise in which mean functions to use. So, we'll follow the community standard for now: zero mean function. def zero_mean ( x ): return jnp . zeros ( x . shape [ 0 ]) The output of the mean function is size \\mathbb{R}^{N} \\mathbb{R}^{N} . Kernel Function The most common kernel function you will see in the literature is the Radial Basis Function (RBF). It's a universal approximator and it performs fairly well on most datasets. If your dataset becomes non-linear, then it may start to fail as it is a really smooth function. The kernel function is defined as: k(x,y) = \\sigma_f \\exp \\left( - \\gamma || x - y||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - \\gamma || x - y||^2_2 \\right) # Squared Euclidean Distance Formula @jax . jit def sqeuclidean_distance ( x , y ): return jnp . sum (( x - y ) ** 2 ) # RBF Kernel @jax . jit def rbf_kernel ( params , x , y ): return jnp . exp ( - params [ 'gamma' ] * sqeuclidean_distance ( x , y )) We also have the more robust version of the RBF with a separate length scale per dimension called the Automatic Relavance Determination (ARD) kernel. k(x,y) = \\sigma_f \\exp \\left( - || x / \\sigma_\\lambda - y / \\sigma_\\lambda ||^2_2 \\right) k(x,y) = \\sigma_f \\exp \\left( - || x / \\sigma_\\lambda - y / \\sigma_\\lambda ||^2_2 \\right) # ARD Kernel @jax . jit def ard_kernel ( params , x , y ): # divide by the length scale x = x / params [ 'length_scale' ] y = y / params [ 'length_scale' ] # return the ard kernel return params [ 'var_f' ] * jnp . exp ( - sqeuclidean_distance ( x , y ) ) Remember : These are functions so they take in vectors \\mathbf{x} \\in \\mathbb{R}^{D} \\mathbf{x} \\in \\mathbb{R}^{D} and output a scalar value. Kernel Matrix The kernel function in the tab over shows how we can calculate the kernel for an input vector. But we need every single combination # Gram Matrix def gram ( func , params , x , y ): return jax . vmap ( lambda x1 : jax . vmap ( lambda y1 : func ( params , x1 , y1 ))( y ))( x )","title":"Code"},{"location":"appendix/gps/1_introduction/#sampling-from-prior","text":"Now, something a bit more practical, generally speaking when we program the sampling portion of the prior, we need data. The kernel function is as is and has already been defined with its appropriate parameters. Furthermore, we already have defined the mean function \\mu \\mu when we initialized the mean function above. So we just need to pass the function through the multivariate normal function along with the number of samples we would like to draw from the prior. Code # initialize parameters params = { 'gamma' : 10. , 'length_scale' : 1e-3 , } n_samples = 10 # condition on 10 samples test_X = X [: n_samples , :] . copy () # random samples from data distribution # GP Prior functions (mu, sigma) mu_f = zero_mean cov_f = functools . partial ( gram , rbf_kernel ) mu_x , cov_x = gp_prior ( params , mu_f = mu_f , cov_f = cov_f , x = test_X ) # make it semi-positive definite with jitter jitter = 1e-6 cov_x_ = cov_x + jitter * jnp . eye ( cov_x . shape [ 0 ]) n_functions = 10 # number of random functions to draw key = jax . random . PRNGKey ( 0 ) # Jax random numbers boilerplate code y_samples = jax . random . multivariate_normal ( key , mu_x , cov_x_ , shape = ( n_functions ,))","title":"Sampling from Prior"},{"location":"appendix/gps/1_introduction/#likelihood-noise-model","text":"p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) This comes from our assumption as stated above from y=f(x)+\\epsilon y=f(x)+\\epsilon . Alternative Notation: * y\\sim \\mathcal{N}(f, \\sigma_n^2) y\\sim \\mathcal{N}(f, \\sigma_n^2) * \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i)","title":"Likelihood (noise model)"},{"location":"appendix/gps/1_introduction/#posterior","text":"Alternative Notation: \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) Code This will easily be the longest function that we need for the GP. In my version, it's not necessary for training the GP. But it is necessary for testing. Posterior def posterior ( params , prior_params , X , Y , X_new , likelihood_noise = False , return_cov = False ): ( mu_func , cov_func ) = prior_params # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ \"likelihood_noise\" ] + 1e-7 ) * jnp . eye ( Kxx . shape [ 0 ]), Y - mu_func ( X ) . reshape ( - 1 , 1 ) ) # ================================ # 4. PREDICTIVE MEAN DISTRIBUTION # ================================ # calculate transform kernel KxX = cov_func ( params , X_new , X ) # Calculate the Mean mu_y = jnp . dot ( KxX , alpha ) # ===================================== # 5. PREDICTIVE COVARIANCE DISTRIBUTION # ===================================== v = jax . scipy . linalg . cho_solve (( L , lower ), KxX . T ) # Calculate kernel matrix for inputs Kxx = cov_func ( params , X_new , X_new ) cov_y = Kxx - jnp . dot ( KxX , v ) # Likelihood Noise if likelihood_noise is True : cov_y += params [ 'likelihood_noise' ] # return variance (diagonals of covariance) if return_cov is not True : cov_y = jnp . diag ( cov_y ) return mu_y , cov_y Cholesky A lot of times just straight solving the K^{-1}y=\\alpha K^{-1}y=\\alpha will give you problems. Many times you'll get an error about the matrix being ill-conditioned and non positive semi-definite. So we have to rectify that with the Cholesky decomposition. K K should be a positive semi-definite matrix so, there are more stable ways to solve this. We can use the cholesky decomposition which decomposes K K into a product of two lower triangular matrices: K = LL^\\top K = LL^\\top We do this because: it's less expensive to calculate the inverse of a triangular matrix it's easier to solve systems of equations Ax=b Ax=b . Cholesky Factorization There are two convenience terms that allow you to calculate the cholesky decomposition: cho_factor - calculates the decomposition K \\rightarrow L K \\rightarrow L cho_solve - solves the system of equations problem LL^\\top \\alpha=y LL^\\top \\alpha=y def cholesky_factorization ( K , Y ): # cho factor the cholesky, K = LL^T L = jax . scipy . linalg . cho_factor ( K , lower = True ) # alpha, LL^T alpha=y alpha = jax . scipy . linalg . cho_solve ( L , Y ) return L , alpha Note : If you want to get the cholesky matrix by itself and operator on it without the cho_factor function, then you should call the cholesky function directly. The cho_factor puts random (inexpensive) values in the part of the triangle that's not necessary. Whereas the cholesky adds zeros there instead. Variance Term The variance term also makes use of the K^{-1} K^{-1} . So naturally, we can use the already factored cholesky decompsition to calculate the term. \\begin{aligned} k_* K^{-1}k_* &= (Lv)^\\top K^{-1}Lv\\\\ &= v^\\top L^\\top (LL^\\top)^{-1} Lv\\\\ &= v^{\\top}L^\\top L^{-\\top}L^{-1}v\\\\ &= v^\\top v \\end{aligned} \\begin{aligned} k_* K^{-1}k_* &= (Lv)^\\top K^{-1}Lv\\\\ &= v^\\top L^\\top (LL^\\top)^{-1} Lv\\\\ &= v^{\\top}L^\\top L^{-\\top}L^{-1}v\\\\ &= v^\\top v \\end{aligned} v = jax . scipy . linalg . cho_solve (( L , lower ), KxX . T ) var = np . dot ( KxX , v )","title":"Posterior"},{"location":"appendix/gps/1_introduction/#joint-probability-distribution","text":"To make GPs useful, we want to actually make predictions. This stems from the using the joint distribution of the training data and test data with the formula shown above used to condition on multivariate Gaussians. In terms of the GP function space, we have \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\\\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\\\ K_{*x} & K_{**} \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\\\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\\\ K_{*x} & K_{**} \\end{bmatrix} \\right) \\end{aligned} Then solving for the marginals, we can come up with the predictive test points. \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) where: \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top}","title":"Joint Probability Distribution"},{"location":"appendix/gps/1_introduction/#marginal-log-likelihood","text":"The prior m(x), K m(x), K have hyper-parameters \\theta \\theta . So learning a \\mathcal{GP} \\mathcal{GP} implies inferring hyper-parameters from the model. p(Y|X,\\theta)=\\int p(Y|f)p(f|X, \\theta)df p(Y|X,\\theta)=\\int p(Y|f)p(f|X, \\theta)df However, we are not interested in f f directly. We can marginalize it out via the integral equation. The marginal of a Gaussian is Gaussian. Note : Typically we use the \\log \\log likelihood instead of a pure likelihood. This is purely for computational purposes. The \\log \\log function is monotonic so it doesn't alter the location of the extreme points of the function. Furthermore we typically minimize the -\\log -\\log instead of the maximum \\log \\log for purely practical reasons. One way to train these functions is to use Maximum A Posterior (MAP) of the hyper-parameters \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned}","title":"Marginal Log-Likelihood"},{"location":"appendix/gps/1_introduction/#marginal-likelihood-evidence","text":"p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df where: * p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) * p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) Note that all we're doing is simply describing each of these elements specifically because all of these quantities are Gaussian distributed. p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df So the product of two Gaussians is simply a Gaussian. That along with the notion that the integral of all the functions is a normal distribution with mean \\mu \\mu and covariance K K . p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) Proof Using the Gaussian identities: \\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned} \\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned} So we can use the same reasoning to combine the prior and the likelihood to get the posterior \\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned} \\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned} Source : Alternative Derivation for Log Likelihood - blog","title":"Marginal Likelihood (Evidence)"},{"location":"appendix/gps/1_introduction/#marginal-log-likelihood_1","text":"TODO Proof of Marginal Log-Likelihood Now we need a cost function that will allow us to get the best hyperparameters that fit our data. \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| Inverting N\\times N N\\times N matrices is the worse part about GPs in general. There are many techniques to be able to handle them, but for basics, it can become a problem. Furthermore, inverting this Kernel matrix tends to have problems being positive semi-definite . One way we can make this more efficient is to do the cholesky decomposition and then solve our problem that way.","title":"Marginal Log-Likelihood"},{"location":"appendix/gps/1_introduction/#cholesky-components","text":"Let \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) . We can write the log likelihood in terms of the cholesky decomposition. \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\underbrace{\\mathbf{L}^{-1}y}_{\\alpha}||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\underbrace{\\mathbf{L}^{-1}y}_{\\alpha}||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} This gives us a computational complexity of \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) Code I will demonstrate two ways to do this: We will use the equations above We will refactor this and use the built-in function From Scratch def nll_scratch ( gp_priors , params , X , Y ) -> float : ( mu_func , cov_func ) = gp_priors # ========================== # 1. GP PRIOR # ========================== mu_x , Kxx = gp_prior ( params , mu_f = mu_func , cov_f = cov_func , x = X ) # =========================== # 2. CHOLESKY FACTORIZATION # =========================== ( L , lower ), alpha = cholesky_factorization ( Kxx + ( params [ 'likelihood_noise' ] + 1e-5 ) * jnp . eye ( Kxx . shape [ 0 ]), Y ) # =========================== # 3. Marginal Log-Likelihood # =========================== log_likelihood = - 0.5 * jnp . einsum ( \"ik,ik->k\" , Y , alpha ) # same as dot(Y.T, alpha) log_likelihood -= jnp . sum ( jnp . log ( jnp . diag ( L ))) log_likelihood -= ( Kxx . shape [ 0 ] / 2 ) * jnp . log ( 2 * jnp . pi ) return - log_likelihood . sum () Refactored def marginal_likelihood ( prior_params , params , Xtrain , Ytrain ): # unpack params ( mu_func , cov_func ) = prior_params # ========================== # 1. GP Prior, mu(), cov(,) # ========================== mu_x = mu_f ( Ytrain ) Kxx = cov_f ( params , Xtrain , Xtrain ) # =========================== # 2. GP Likelihood # =========================== K_gp = Kxx + ( params [ 'likelihood_noise' ] + 1e-6 ) * jnp . eye ( Kxx . shape [ 0 ]) # =========================== # 3. Marginal Log-Likelihood # =========================== # get log probability log_prob = jax . scipy . stats . multivariate_normal . logpdf ( x = Ytrain . T , mean = mu_x , cov = K_gp ) # sum dimensions and return neg mll return - log_prob . sum () source - Dai, GPSS 2018","title":"Cholesky Components"},{"location":"appendix/gps/1_introduction/#training","text":"Code Log Params We often have problems when it comes to using optimizers. A lot of times they just don't seem to want to converge and the gradients seem to not change no matter what happens. One trick we can do is to make the optimizer solve a transformed version of the parameters. And then we can take a softmax so that they converge properly. f(x) = \\ln (1 + \\exp(x)) f(x) = \\ln (1 + \\exp(x)) Jax has a built-in function so we'll just use that. def saturate ( params ): return { ikey : jax . nn . softplus ( ivalue ) for ( ikey , ivalue ) in params . items ()} Experimental Parameters logger . setLevel ( logging . INFO ) X , y , Xtest , ytest = get_data ( 50 ) # PRIOR FUNCTIONS (mean, covariance) mu_f = zero_mean cov_f = functools . partial ( gram , rbf_kernel ) gp_priors = ( mu_f , cov_f ) # Kernel, Likelihood parameters params = { 'gamma' : 2.0 , # 'length_scale': 1.0, # 'var_f': 1.0, 'likelihood_noise' : 1. , } # saturate parameters with likelihoods params = saturate ( params ) # LOSS FUNCTION mll_loss = jax . jit ( functools . partial ( marginal_likelihood , gp_priors )) # GRADIENT LOSS FUNCTION dloss = jax . jit ( jax . grad ( mll_loss )) Training Step # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss Experimental Loop # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = 1e-2 ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) # TRAINING PARARMETERS n_epochs = 500 learning_rate = 0.1 losses = list () postfix = {} import tqdm with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # update params postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { jax . nn . softplus ( params [ ikey ]) : .2f } \" # save loss values losses . append ( value . mean ()) # update progress bar postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" bar . set_postfix ( postfix ) # saturate params params = saturate ( params )","title":"Training"},{"location":"appendix/gps/1_introduction/#resources","text":"Surrogates: GP Modeling, Design, and Optimization for the Applied Sciences - Gramacy - Online Book","title":"Resources"},{"location":"appendix/gps/2_sparse_gps/","text":"Sparse Gaussian Processes \u00b6 [toc] Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories: Subset of Data (Transformation, Random Sampling) Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte) Inducing Points (SoR, FITC, DTC, KISS-GP) Linear Algebra (Toeplitz, Kronecker, ) Approximate Inference (Variational Methods) Each of these methods ultimately augment the model so that the largest computation goes from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(MN^2) \\mathcal{O}(MN^2) where M<<N M<<N . Subset of Data \u00b6 This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you're more likely to have. It's an age old rule that says if you want better predictions, it's often better just to have more data. A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space \\mathcal{X} \\in \\mathbb{R}^{N \\times D} \\mathcal{X} \\in \\mathbb{R}^{N \\times D} to subset data space \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} which is found through an eigen decomposition scheme. In GPs we calculate a kernel matrix \\mathbf K \\in \\mathbb{R}^{N \\times N} \\mathbf K \\in \\mathbb{R}^{N \\times N} . If N N is large enough, then throughout the marginal likelihood, we need to calculate \\mathbf K^{-1} \\mathbf K^{-1} and |\\mathbf K| |\\mathbf K| which has \\mathcal{O}(N^3) \\mathcal{O}(N^3) operations and \\mathcal{O}(N^2) \\mathcal{O}(N^2) memory costs. So we make an approximate matrix \\mathbf {\\tilde{K}} \\mathbf {\\tilde{K}} given by the following formula: \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} where: * K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} - the kernel matrix for the subspace \\mathcal{Z} \\mathcal{Z} * K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} - the transformation matrix from the data space \\mathcal{X} \\mathcal{X} to the subspace \\mathcal{Z} \\mathcal{Z} * K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} - the approximate kernel matrix of the data space \\mathcal{X} \\mathcal{X} Below is an example of where this would be applicable where we just implement this method where we just transform the day. from sklearn.kernel_approximation import Nystroem from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_processes.kernels import RBF # Initialize Nystrom transform nystrom_map = Nystrom ( random_state = 1 , n_components = 1 ) # Transform Data X_transformed = nystrom_map . fit_transform ( X ) # initialize GPR model = GPR () # fit GP model model . fit ( X_transformed , y ) Kernel Approximations \u00b6 Pivoting off of the method above, we So now when we calculate the log likelihood term \\log \\mathcal{P}(y|X,\\theta) \\log \\mathcal{P}(y|X,\\theta) we can have an approximation: \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) Notice how we haven't actually changing our formulation because we still have to calculate the inverse of \\tilde{K} \\tilde{K} which is \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} . Using the Woodbury matrix identity for the kernel approximation form ( Sherman-Morrison Formula ): (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} Now the matrix that we need to invert is (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} which is (M \\times M) (M \\times M) which is considerably smaller if M << N M << N . So the overall computational complexity reduces to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) . Inducing Points \u00b6 Deisenroth - GPs for Big Data - MLSS2015 Dai - Scalable GPs - MLSS2018 Sparse GPs - Inducing Points Summary \u00b6 So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data \\mathcal{Z} \\mathcal{Z} from the data space \\mathcal{D} \\mathcal{D} . We typically have some approximate matrix \\mathbf{Q} \\mathbf{Q} which approximates the kernel matrix \\mathbf{K} \\mathbf{K} : \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix \\mathbf{K} \\mathbf{K} . Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down: \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} The data fit term penalizes the data lying outside the covariance ellipse, the complexity penalty is the integral of the data fit term over all possible observations \\mathbf{y} \\mathbf{y} which characterizes the volume of possible datasets, the trace term ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods. Algorithm \\mathbf{G} \\mathbf{G} \\mathbf{T} \\mathbf{T} FITC diag (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} 0 VFE \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} DTC \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} 0 Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will argue for approximating the inference but I think it's important to be pragmatic about these sorts of things. Observations about the Sparse GPs \u00b6 VFE Overestimates noise variance Improves with additional inducing inputs Recovers the full GP Posterior Hindered by local optima FITC Can severly underestimate the noise variance May ignore additional inducing inputs Does not recover the full GP posterior Relies on Local Optima Some parameter initialization strategies: * K-Means * Initially fixing the hyperparameters * Random Restarts An interesting solution to find good hyperparameters for VFE: Find parameters with FITC solution Initialize GP model of VFE with FITC solutions Find parameters with VFE. Source: * Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - Paper * Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - Thesis Variational Compression \u00b6 Figure : This graphical model shows the relationship between the data X X , the labels y y and the augmented labels z z . This is a concept I've came across that seeks to give a stronger argument for using an augmented space \\mathcal Z\\in \\mathbb{R}^{M \\times D} \\mathcal Z\\in \\mathbb{R}^{M \\times D} instead of just the data space \\mathcal X \\in \\mathbb{R}^{N \\times D} \\mathcal X \\in \\mathbb{R}^{N \\times D} . This has allowed us to reduce the computational complexity of all of our most expensive calculations from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) when we are learning the best parameters for our GP models. The term variational compression comes from the notion that we want to suppress the function valuse f f with some auxilary variables u u . It's kind of like reducing the data space \\mathcal X \\mathcal X with the auxilary data space \\mathcal Z \\mathcal Z in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You'll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms. Joint Distribution - Augmented Space \\mathcal{P}(f,u) \\mathcal{P}(f,u) \u00b6 Let's add an additional set of variables u u that's jointly Gaussian with our original function f f . p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) We have a new space where we have introduced some auxilary variables u u to be modeled jointly with f f . Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution \\mathcal{P}(f|u) \\mathcal{P}(f|u) and marginal distribution \\mathcal{P}(u) \\mathcal{P}(u) in terms of the joint distribution \\mathcal P (f,u) \\mathcal P (f,u) using conditional probability rules. \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) where: Conditional Dist.: \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} Augmented space Prior: \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) We could actually marginalize out u u to get back to the standard GP prior \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) . But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain , \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) . We want to 'compress' the data space \\mathcal X \\mathcal X and subsequently the function space of f f . So now let's write the complete joint distribution which includes the data likelihood and the augmented latent variable space: \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} We have a new term which is the familiar GP likelihood term \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) . The rest of the terms we have already defined above. So now you can kind of see how we're attempting to compress the conditional distribution f f . We no longer need the prior for X X or f f in order to obtain the joint distribution for our model. The prior we have is \\mathcal P (u) \\mathcal P (u) which is kind of a made up variable. From henceforth, I will be omitting the dependency on X X and Z Z as they're not important for the argument that follows. But keep it in the back of your mind that that dependency does exist. Conditional Distribution - \\mathcal{P}(y|u) \\mathcal{P}(y|u) \u00b6 The next step would be to try and condition on f f and u u to obtain the conditional distribution of y y given u u , \\mathcal{P}(y|u) \\mathcal{P}(y|u) . We can rearrange the terms of the formula above like so: \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) and using the conditional probability rules P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} we can simplify the formula even further: \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) So, what are we looking at? We are looking at the new joint distribution of y y and f f given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the f f 's. In doing so, we obtain the conditional density that we set off to explore: \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df where: * \\mathcal{P}(y|f) \\mathcal{P}(y|f) - Likelihood * \\mathcal{P}(f|u) \\mathcal{P}(f|u) - Conditional Distribution The last step would be to try and see if we can calculate \\mathcal{P}(y) \\mathcal{P}(y) because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the u u 's I get after grouping the terms: \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du which reduces to: \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data \\mathcal P (y*|y) \\mathcal P (y*|y) . The two important issues are highlighted in that equation alone: We now have the same bottleneck on our parameter for u u as we do for standard Bayesian parametric modeling. The computation of \\mathcal P (y|u) \\mathcal P (y|u) is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior \\mathcal P (u) \\mathcal P (u) . Variational Bound on \\mathcal P (y|u) \\mathcal P (y|u) \u00b6 We've shown the difficulties of actually obtaining the probability density function of \\mathcal{P}(y) \\mathcal{P}(y) but in this section we're just going to show that we can obtain a lower bound for the conditional density function \\mathcal{P}(y|u) \\mathcal{P}(y|u) \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df I'll do the 4.5 classic steps in order to arrive at a variational lower bound: Given an integral problem , take the \\log \\log of both sides of the function. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df Introduce the variational parameter q(f) q(f) as a proposal with the Identity trick. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df Use Jensen's inequality for the log function to rearrange the formula to highlight the importance weight and provide a bound for \\mathcal{F}(q) \\mathcal{F}(q) : \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) Rearrange to look like an expectation and KL divergence using targeted \\log \\log rules: \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df Simplify notation to look like every paper in ML that uses VI to profit and obtain the variational lower bound . \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] Titsias Innovation: et q(f) = \\mathcal{P}(f|u) q(f) = \\mathcal{P}(f|u) . \u00b6 According to Titsias et al. (2009) he looked at what happens if we let q(f)=\\mathcal P (f|u) q(f)=\\mathcal P (f|u) . For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less. \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df As a thought experiment though, what would happen if we had thee true posterior of \\mathcal{P}(f|y,u) \\mathcal{P}(f|y,u) and an approximating density of \\mathcal{P}(f|u) \\mathcal{P}(f|u) ? Well, we can take the KL KL divergence of that quantity and we get the following: \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between \\mathcal{P}(f|u) \\mathcal{P}(f|u) and \\mathcal{P}(f|u) \\mathcal{P}(f|u) . Maximizing the bound will try to find the optimal compression and looks at the information between y y and u u . He does not that there is no bound and it is an exact bound when u=f u=f . I believe that's related to the GPFlow derivation of variational GPs implementation but I don't have more information on this. Sources : Deep Gaussian Processes - MLSS 2019 Gaussian Processes for Big Data - Hensman et. al. (2013) Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Scalable Variational Gaussian Process Classification - Hensman et. al. (2015) ELBOs \u00b6 Let \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} Lower Bound \u00b6 \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) where: \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} Nystrom approximation \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} Uncertainty Based Correction Variational Bound on \\mathcal P (y) \\mathcal P (y) \u00b6 In this scenario, we marginalize out the remaining u u 's and we can get an error bound on the \\mathcal P(y) \\mathcal P(y) \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) * James Hensman - GPSS 2015 | Aweseome Graphical Models The explicit form of the lower bound \\mathcal{P}(y) \\mathcal{P}(y) for is gives us: \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Stochastic Variational Inference \u00b6 Supplementary Material \u00b6 Important Formulas \u00b6 These formulas come up when we're looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix \\mathbf K\\in \\mathbb R^{N\\times N} \\mathbf K\\in \\mathbb R^{N\\times N} which implies we need to calculate the inverse \\mathbf K^{-1} \\mathbf K^{-1} and the determinant | | det \\mathbf K| \\mathbf K| which both require \\mathcal{O}(N^3) \\mathcal{O}(N^3) . These formulas below are useful when we want to avoid those computational complexity counts. Nystrom Approximation \u00b6 \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} Sherman-Morrison-Woodbury Formula \u00b6 (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} Sylvester Determinant Theorem \u00b6 \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| Resources \u00b6 Code Walk-Throughs \u00b6 VFE Approximation for GPs, the gory Details | More Notes | Summary of Inducing Point Methods A good walkthrough of the essential equations and how we can implement them from scratch SparseGP - Alaya-in-Matrix Another good walkthrough with everything well defined such that its easy to replicate. Math Walkthroughs \u00b6 Gonzalo Blog Step-by-Step Derivations Papers \u00b6 Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. * A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula. Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Presentations \u00b6 Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014) Notes \u00b6 On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014) Blogs \u00b6 Variational Free Energy for Sparse GPs - Gonzalo https://github.com/Alaya-in-Matrix/SparseGP","title":"Sparse Gaussian Processes"},{"location":"appendix/gps/2_sparse_gps/#sparse-gaussian-processes","text":"[toc] Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories: Subset of Data (Transformation, Random Sampling) Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte) Inducing Points (SoR, FITC, DTC, KISS-GP) Linear Algebra (Toeplitz, Kronecker, ) Approximate Inference (Variational Methods) Each of these methods ultimately augment the model so that the largest computation goes from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(MN^2) \\mathcal{O}(MN^2) where M<<N M<<N .","title":"Sparse Gaussian Processes"},{"location":"appendix/gps/2_sparse_gps/#subset-of-data","text":"This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you're more likely to have. It's an age old rule that says if you want better predictions, it's often better just to have more data. A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space \\mathcal{X} \\in \\mathbb{R}^{N \\times D} \\mathcal{X} \\in \\mathbb{R}^{N \\times D} to subset data space \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} which is found through an eigen decomposition scheme. In GPs we calculate a kernel matrix \\mathbf K \\in \\mathbb{R}^{N \\times N} \\mathbf K \\in \\mathbb{R}^{N \\times N} . If N N is large enough, then throughout the marginal likelihood, we need to calculate \\mathbf K^{-1} \\mathbf K^{-1} and |\\mathbf K| |\\mathbf K| which has \\mathcal{O}(N^3) \\mathcal{O}(N^3) operations and \\mathcal{O}(N^2) \\mathcal{O}(N^2) memory costs. So we make an approximate matrix \\mathbf {\\tilde{K}} \\mathbf {\\tilde{K}} given by the following formula: \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} where: * K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} - the kernel matrix for the subspace \\mathcal{Z} \\mathcal{Z} * K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} - the transformation matrix from the data space \\mathcal{X} \\mathcal{X} to the subspace \\mathcal{Z} \\mathcal{Z} * K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} - the approximate kernel matrix of the data space \\mathcal{X} \\mathcal{X} Below is an example of where this would be applicable where we just implement this method where we just transform the day. from sklearn.kernel_approximation import Nystroem from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_processes.kernels import RBF # Initialize Nystrom transform nystrom_map = Nystrom ( random_state = 1 , n_components = 1 ) # Transform Data X_transformed = nystrom_map . fit_transform ( X ) # initialize GPR model = GPR () # fit GP model model . fit ( X_transformed , y )","title":"Subset of Data"},{"location":"appendix/gps/2_sparse_gps/#kernel-approximations","text":"Pivoting off of the method above, we So now when we calculate the log likelihood term \\log \\mathcal{P}(y|X,\\theta) \\log \\mathcal{P}(y|X,\\theta) we can have an approximation: \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) Notice how we haven't actually changing our formulation because we still have to calculate the inverse of \\tilde{K} \\tilde{K} which is \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} . Using the Woodbury matrix identity for the kernel approximation form ( Sherman-Morrison Formula ): (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} Now the matrix that we need to invert is (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} which is (M \\times M) (M \\times M) which is considerably smaller if M << N M << N . So the overall computational complexity reduces to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) .","title":"Kernel Approximations"},{"location":"appendix/gps/2_sparse_gps/#inducing-points","text":"Deisenroth - GPs for Big Data - MLSS2015 Dai - Scalable GPs - MLSS2018","title":"Inducing Points"},{"location":"appendix/gps/2_sparse_gps/#sparse-gps-inducing-points-summary","text":"So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data \\mathcal{Z} \\mathcal{Z} from the data space \\mathcal{D} \\mathcal{D} . We typically have some approximate matrix \\mathbf{Q} \\mathbf{Q} which approximates the kernel matrix \\mathbf{K} \\mathbf{K} : \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix \\mathbf{K} \\mathbf{K} . Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down: \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} The data fit term penalizes the data lying outside the covariance ellipse, the complexity penalty is the integral of the data fit term over all possible observations \\mathbf{y} \\mathbf{y} which characterizes the volume of possible datasets, the trace term ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods. Algorithm \\mathbf{G} \\mathbf{G} \\mathbf{T} \\mathbf{T} FITC diag (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} 0 VFE \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} DTC \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} 0 Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will argue for approximating the inference but I think it's important to be pragmatic about these sorts of things.","title":"Sparse GPs - Inducing Points Summary"},{"location":"appendix/gps/2_sparse_gps/#observations-about-the-sparse-gps","text":"VFE Overestimates noise variance Improves with additional inducing inputs Recovers the full GP Posterior Hindered by local optima FITC Can severly underestimate the noise variance May ignore additional inducing inputs Does not recover the full GP posterior Relies on Local Optima Some parameter initialization strategies: * K-Means * Initially fixing the hyperparameters * Random Restarts An interesting solution to find good hyperparameters for VFE: Find parameters with FITC solution Initialize GP model of VFE with FITC solutions Find parameters with VFE. Source: * Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - Paper * Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - Thesis","title":"Observations about the Sparse GPs"},{"location":"appendix/gps/2_sparse_gps/#variational-compression","text":"Figure : This graphical model shows the relationship between the data X X , the labels y y and the augmented labels z z . This is a concept I've came across that seeks to give a stronger argument for using an augmented space \\mathcal Z\\in \\mathbb{R}^{M \\times D} \\mathcal Z\\in \\mathbb{R}^{M \\times D} instead of just the data space \\mathcal X \\in \\mathbb{R}^{N \\times D} \\mathcal X \\in \\mathbb{R}^{N \\times D} . This has allowed us to reduce the computational complexity of all of our most expensive calculations from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) when we are learning the best parameters for our GP models. The term variational compression comes from the notion that we want to suppress the function valuse f f with some auxilary variables u u . It's kind of like reducing the data space \\mathcal X \\mathcal X with the auxilary data space \\mathcal Z \\mathcal Z in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You'll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms.","title":"Variational Compression"},{"location":"appendix/gps/2_sparse_gps/#joint-distribution-augmented-space-mathcalpfumathcalpfu","text":"Let's add an additional set of variables u u that's jointly Gaussian with our original function f f . p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) We have a new space where we have introduced some auxilary variables u u to be modeled jointly with f f . Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution \\mathcal{P}(f|u) \\mathcal{P}(f|u) and marginal distribution \\mathcal{P}(u) \\mathcal{P}(u) in terms of the joint distribution \\mathcal P (f,u) \\mathcal P (f,u) using conditional probability rules. \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) where: Conditional Dist.: \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} Augmented space Prior: \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) We could actually marginalize out u u to get back to the standard GP prior \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) . But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain , \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) . We want to 'compress' the data space \\mathcal X \\mathcal X and subsequently the function space of f f . So now let's write the complete joint distribution which includes the data likelihood and the augmented latent variable space: \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} We have a new term which is the familiar GP likelihood term \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) . The rest of the terms we have already defined above. So now you can kind of see how we're attempting to compress the conditional distribution f f . We no longer need the prior for X X or f f in order to obtain the joint distribution for our model. The prior we have is \\mathcal P (u) \\mathcal P (u) which is kind of a made up variable. From henceforth, I will be omitting the dependency on X X and Z Z as they're not important for the argument that follows. But keep it in the back of your mind that that dependency does exist.","title":"Joint Distribution - Augmented Space \\mathcal{P}(f,u)\\mathcal{P}(f,u)"},{"location":"appendix/gps/2_sparse_gps/#conditional-distribution-mathcalpyumathcalpyu","text":"The next step would be to try and condition on f f and u u to obtain the conditional distribution of y y given u u , \\mathcal{P}(y|u) \\mathcal{P}(y|u) . We can rearrange the terms of the formula above like so: \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) and using the conditional probability rules P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} we can simplify the formula even further: \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) So, what are we looking at? We are looking at the new joint distribution of y y and f f given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the f f 's. In doing so, we obtain the conditional density that we set off to explore: \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df where: * \\mathcal{P}(y|f) \\mathcal{P}(y|f) - Likelihood * \\mathcal{P}(f|u) \\mathcal{P}(f|u) - Conditional Distribution The last step would be to try and see if we can calculate \\mathcal{P}(y) \\mathcal{P}(y) because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the u u 's I get after grouping the terms: \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du which reduces to: \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data \\mathcal P (y*|y) \\mathcal P (y*|y) . The two important issues are highlighted in that equation alone: We now have the same bottleneck on our parameter for u u as we do for standard Bayesian parametric modeling. The computation of \\mathcal P (y|u) \\mathcal P (y|u) is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior \\mathcal P (u) \\mathcal P (u) .","title":"Conditional Distribution - \\mathcal{P}(y|u)\\mathcal{P}(y|u)"},{"location":"appendix/gps/2_sparse_gps/#variational-bound-on-mathcal-p-yumathcal-p-yu","text":"We've shown the difficulties of actually obtaining the probability density function of \\mathcal{P}(y) \\mathcal{P}(y) but in this section we're just going to show that we can obtain a lower bound for the conditional density function \\mathcal{P}(y|u) \\mathcal{P}(y|u) \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df I'll do the 4.5 classic steps in order to arrive at a variational lower bound: Given an integral problem , take the \\log \\log of both sides of the function. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df Introduce the variational parameter q(f) q(f) as a proposal with the Identity trick. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df Use Jensen's inequality for the log function to rearrange the formula to highlight the importance weight and provide a bound for \\mathcal{F}(q) \\mathcal{F}(q) : \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) Rearrange to look like an expectation and KL divergence using targeted \\log \\log rules: \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df Simplify notation to look like every paper in ML that uses VI to profit and obtain the variational lower bound . \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right]","title":"Variational Bound on \\mathcal P (y|u)\\mathcal P (y|u)"},{"location":"appendix/gps/2_sparse_gps/#titsias-innovation-et-qf-mathcalpfuqf-mathcalpfu","text":"According to Titsias et al. (2009) he looked at what happens if we let q(f)=\\mathcal P (f|u) q(f)=\\mathcal P (f|u) . For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less. \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df As a thought experiment though, what would happen if we had thee true posterior of \\mathcal{P}(f|y,u) \\mathcal{P}(f|y,u) and an approximating density of \\mathcal{P}(f|u) \\mathcal{P}(f|u) ? Well, we can take the KL KL divergence of that quantity and we get the following: \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between \\mathcal{P}(f|u) \\mathcal{P}(f|u) and \\mathcal{P}(f|u) \\mathcal{P}(f|u) . Maximizing the bound will try to find the optimal compression and looks at the information between y y and u u . He does not that there is no bound and it is an exact bound when u=f u=f . I believe that's related to the GPFlow derivation of variational GPs implementation but I don't have more information on this. Sources : Deep Gaussian Processes - MLSS 2019 Gaussian Processes for Big Data - Hensman et. al. (2013) Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Scalable Variational Gaussian Process Classification - Hensman et. al. (2015)","title":"Titsias Innovation: et q(f) = \\mathcal{P}(f|u)q(f) = \\mathcal{P}(f|u)."},{"location":"appendix/gps/2_sparse_gps/#elbos","text":"Let \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top}","title":"ELBOs"},{"location":"appendix/gps/2_sparse_gps/#lower-bound","text":"\\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) where: \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} Nystrom approximation \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} Uncertainty Based Correction","title":"Lower Bound"},{"location":"appendix/gps/2_sparse_gps/#variational-bound-on-mathcal-p-ymathcal-p-y","text":"In this scenario, we marginalize out the remaining u u 's and we can get an error bound on the \\mathcal P(y) \\mathcal P(y) \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) * James Hensman - GPSS 2015 | Aweseome Graphical Models The explicit form of the lower bound \\mathcal{P}(y) \\mathcal{P}(y) for is gives us: \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014)","title":"Variational Bound on \\mathcal P (y)\\mathcal P (y)"},{"location":"appendix/gps/2_sparse_gps/#stochastic-variational-inference","text":"","title":"Stochastic Variational Inference"},{"location":"appendix/gps/2_sparse_gps/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"appendix/gps/2_sparse_gps/#important-formulas","text":"These formulas come up when we're looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix \\mathbf K\\in \\mathbb R^{N\\times N} \\mathbf K\\in \\mathbb R^{N\\times N} which implies we need to calculate the inverse \\mathbf K^{-1} \\mathbf K^{-1} and the determinant | | det \\mathbf K| \\mathbf K| which both require \\mathcal{O}(N^3) \\mathcal{O}(N^3) . These formulas below are useful when we want to avoid those computational complexity counts.","title":"Important Formulas"},{"location":"appendix/gps/2_sparse_gps/#nystrom-approximation","text":"\\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top}","title":"Nystrom Approximation"},{"location":"appendix/gps/2_sparse_gps/#sherman-morrison-woodbury-formula","text":"(\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top}","title":"Sherman-Morrison-Woodbury Formula"},{"location":"appendix/gps/2_sparse_gps/#sylvester-determinant-theorem","text":"\\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right|","title":"Sylvester Determinant Theorem"},{"location":"appendix/gps/2_sparse_gps/#resources","text":"","title":"Resources"},{"location":"appendix/gps/2_sparse_gps/#code-walk-throughs","text":"VFE Approximation for GPs, the gory Details | More Notes | Summary of Inducing Point Methods A good walkthrough of the essential equations and how we can implement them from scratch SparseGP - Alaya-in-Matrix Another good walkthrough with everything well defined such that its easy to replicate.","title":"Code Walk-Throughs"},{"location":"appendix/gps/2_sparse_gps/#math-walkthroughs","text":"Gonzalo Blog Step-by-Step Derivations","title":"Math Walkthroughs"},{"location":"appendix/gps/2_sparse_gps/#papers","text":"Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. * A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.","title":"Papers"},{"location":"appendix/gps/2_sparse_gps/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Thesis Explain"},{"location":"appendix/gps/2_sparse_gps/#presentations","text":"Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014)","title":"Presentations"},{"location":"appendix/gps/2_sparse_gps/#notes","text":"On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014)","title":"Notes"},{"location":"appendix/gps/2_sparse_gps/#blogs","text":"Variational Free Energy for Sparse GPs - Gonzalo https://github.com/Alaya-in-Matrix/SparseGP","title":"Blogs"},{"location":"appendix/gps/3_input_error/","text":"GPs and Uncertain Inputs through the Ages \u00b6 Summary \u00b6 Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution Standard GP Formulation \u00b6 Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood Stochastic Test Points \u00b6 This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance. Setup \u00b6 Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step. GP Predictive Distribution \u00b6 So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation. Numerical Integration \u00b6 The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ). Approximate Gaussian Distribution \u00b6 Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations. Stochastic Measurements \u00b6 A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this. Noisy-Input GP (NIGP) \u00b6 We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings. Expected Derivative \u00b6 Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances . Moment Matching \u00b6 The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion. Variational Strategies \u00b6 What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ). My Method - Marriage of Two Strategies \u00b6 I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below. Model \u00b6 Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic. Inference \u00b6 So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter Supplementary Material \u00b6 Moment Matching \u00b6 In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function. Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Propagation of Variances \u00b6 Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) + NIGP - Propagating Variances \u00b6 Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation Real Results with Variance Estimates \u00b6 Figure : Absolute Error From a GP Model Figure : Standard GP Variance Estimates Figure : GP Variance Estimates account for input errors. Resources \u00b6 Papers \u00b6 Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Important Papers \u00b6","title":"GPs and Uncertain Inputs through the Ages"},{"location":"appendix/gps/3_input_error/#gps-and-uncertain-inputs-through-the-ages","text":"","title":"GPs and Uncertain Inputs through the Ages"},{"location":"appendix/gps/3_input_error/#summary","text":"Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution","title":"Summary"},{"location":"appendix/gps/3_input_error/#standard-gp-formulation","text":"Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood","title":"Standard GP Formulation"},{"location":"appendix/gps/3_input_error/#stochastic-test-points","text":"This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance.","title":"Stochastic Test Points"},{"location":"appendix/gps/3_input_error/#setup","text":"Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step.","title":"Setup"},{"location":"appendix/gps/3_input_error/#gp-predictive-distribution","text":"So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation.","title":"GP Predictive Distribution"},{"location":"appendix/gps/3_input_error/#numerical-integration","text":"The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ).","title":"Numerical Integration"},{"location":"appendix/gps/3_input_error/#approximate-gaussian-distribution","text":"Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations.","title":"Approximate Gaussian Distribution"},{"location":"appendix/gps/3_input_error/#stochastic-measurements","text":"A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this.","title":"Stochastic Measurements"},{"location":"appendix/gps/3_input_error/#noisy-input-gp-nigp","text":"We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings.","title":"Noisy-Input GP (NIGP)"},{"location":"appendix/gps/3_input_error/#expected-derivative","text":"Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances .","title":"Expected Derivative"},{"location":"appendix/gps/3_input_error/#moment-matching","text":"The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion.","title":"Moment Matching"},{"location":"appendix/gps/3_input_error/#variational-strategies","text":"What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ).","title":"Variational Strategies"},{"location":"appendix/gps/3_input_error/#my-method-marriage-of-two-strategies","text":"I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below.","title":"My Method - Marriage of Two Strategies"},{"location":"appendix/gps/3_input_error/#model","text":"Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic.","title":"Model"},{"location":"appendix/gps/3_input_error/#inference","text":"So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter","title":"Inference"},{"location":"appendix/gps/3_input_error/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"appendix/gps/3_input_error/#moment-matching_1","text":"In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function.","title":"Moment Matching"},{"location":"appendix/gps/3_input_error/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"appendix/gps/3_input_error/#propagation-of-variances","text":"Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) +","title":"Propagation of Variances"},{"location":"appendix/gps/3_input_error/#nigp-propagating-variances","text":"Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation","title":"NIGP - Propagating Variances"},{"location":"appendix/gps/3_input_error/#real-results-with-variance-estimates","text":"Figure : Absolute Error From a GP Model Figure : Standard GP Variance Estimates Figure : GP Variance Estimates account for input errors.","title":"Real Results with Variance Estimates"},{"location":"appendix/gps/3_input_error/#resources","text":"","title":"Resources"},{"location":"appendix/gps/3_input_error/#papers","text":"","title":"Papers"},{"location":"appendix/gps/3_input_error/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Thesis Explain"},{"location":"appendix/gps/3_input_error/#important-papers","text":"","title":"Important Papers"},{"location":"appendix/gps/3_taylor_expansion/","text":"Uncertainty of GPs: Taylor Expansion \u00b6 In this document, I will be showing how we can use the Taylor Expansion approach to the posterior of Gaussian process algorithm GP Model \u00b6 We have a standard GP model. $$ \\begin{aligned} y &= f(x) + \\epsilon_y\\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\ \\end{aligned} $$ GP Prior : p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) Gaussian Likelihood : p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) Posterior : f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} Taylor Expansion \u00b6 Let's assume we have inputs with an additive noise term \\epsilon_x \\epsilon_x and let's assume that it is Gaussian distributed. We can write some expressions which are very similar to the GP model equations specified above: $$ \\begin{aligned} y &= f(x) + \\epsilon_y \\ x &= \\mu_x + \\epsilon_x \\ \\epsilon_y &\\sim \\mathcal{N} (0, \\sigma_y^2) \\ \\epsilon_x &\\sim \\mathcal{N} (0, \\Sigma_x) \\end{aligned} $$ This is the transformation of a Gaussian random variable x x through another r.v. y y where we have some additive noise \\epsilon_y \\epsilon_y . The biggest difference is that the GP model assumes that x x is deterministic whereas we assume here that x x is a random variable itself. Because we know that integrating out the x x 's is quite difficult to do in practice (because of the nonlinear Kernel functions), we can make an approximation of f(\\cdot) f(\\cdot) via the Taylor expansion. We can take the a 2 nd order Taylor expansion of f f to be: $$ \\begin{aligned} f(x) &\\approx f(\\mu_x + \\epsilon_x) \\ &\\approx f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x + \\sum_{i}\\frac{1}{2} \\epsilon_x^\\top \\nabla^2 f(\\mu_x) \\epsilon_x \\end{aligned} $$ where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Now the question is: where to put use the Taylor expansion within the GP model? There are two options: the model or the posterior. We will outline the two approaches below. Approximate the Model \u00b6 Approximate The Posterior \u00b6 We can compute the expectation \\mathbb{E}[\\cdot] \\mathbb{E}[\\cdot] and variance \\mathbb{V}[\\cdot] \\mathbb{V}[\\cdot] of this Taylor expansion to come up with an approximate mean and variance function for our posterior. Expectation \u00b6 This calculation is straight-forward because we are taking the expected value of a mean function f(\\mu_x) f(\\mu_x) , the derivative of a mean function f(\\mu_x) f(\\mu_x) and a Gaussian distribution noise term \\epsilon_x \\epsilon_x with mean 0. $$ \\begin{aligned} \\mathbb{E}[f(x)] &\\approx \\mathbb{E}[f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x] \\ &= f(\\mu_x) + \\nabla f(\\mu_x) \\mathbb{\\epsilon_x} \\ &= f(\\mu_x) \\end{aligned} $$ Variance \u00b6 The variance term is a bit more complex. $$ \\begin{aligned} \\mathbb{E}\\left[(f(x) - \\mathbb{E}[f(x)])^\\top(f(x) - \\mathbb{E}[f(x)])\\right] &\\approx \\mathbb{E}\\left[(f(x) - f(\\mu_x))^\\top(f(x) - f(\\mu_x))\\right] \\ &\\approx \\mathbb{E} \\left[ \\left(f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x \\right) \\left( f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x\\right)^\\top\\right] \\ &= \\mathbb{E} \\left[ \\left(\\nabla f(\\mu_x): \\epsilon_x \\right)^\\top \\left( \\nabla f(\\mu_x): \\epsilon_x \\right) \\right] \\ &= \\nabla f(\\mu_x) \\mathbb{E}[\\epsilon_x\\epsilon_x^\\top]\\nabla f(\\mu_x) \\ &= \\nabla f(\\mu_x): \\Sigma_x :\\nabla f(\\mu_x) \\end{aligned} $$ I: Additive Noise Model ( x,f x,f ) \u00b6 This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top . Other GP Methods \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. II: Non-Additive Noise Model \u00b6 III: Quadratic Approximation \u00b6 Parallels to the Kalman Filter \u00b6 The Kalman Filter (KF) community use this exact formulation to motivate the Extended Kalman Filter (EKF) algorithm and some variants. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Uncertainty of GPs: Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#uncertainty-of-gps-taylor-expansion","text":"In this document, I will be showing how we can use the Taylor Expansion approach to the posterior of Gaussian process algorithm","title":"Uncertainty of GPs: Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#gp-model","text":"We have a standard GP model. $$ \\begin{aligned} y &= f(x) + \\epsilon_y\\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\ \\end{aligned} $$ GP Prior : p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) Gaussian Likelihood : p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) Posterior : f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned}","title":"GP Model"},{"location":"appendix/gps/3_taylor_expansion/#taylor-expansion","text":"Let's assume we have inputs with an additive noise term \\epsilon_x \\epsilon_x and let's assume that it is Gaussian distributed. We can write some expressions which are very similar to the GP model equations specified above: $$ \\begin{aligned} y &= f(x) + \\epsilon_y \\ x &= \\mu_x + \\epsilon_x \\ \\epsilon_y &\\sim \\mathcal{N} (0, \\sigma_y^2) \\ \\epsilon_x &\\sim \\mathcal{N} (0, \\Sigma_x) \\end{aligned} $$ This is the transformation of a Gaussian random variable x x through another r.v. y y where we have some additive noise \\epsilon_y \\epsilon_y . The biggest difference is that the GP model assumes that x x is deterministic whereas we assume here that x x is a random variable itself. Because we know that integrating out the x x 's is quite difficult to do in practice (because of the nonlinear Kernel functions), we can make an approximation of f(\\cdot) f(\\cdot) via the Taylor expansion. We can take the a 2 nd order Taylor expansion of f f to be: $$ \\begin{aligned} f(x) &\\approx f(\\mu_x + \\epsilon_x) \\ &\\approx f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x + \\sum_{i}\\frac{1}{2} \\epsilon_x^\\top \\nabla^2 f(\\mu_x) \\epsilon_x \\end{aligned} $$ where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Now the question is: where to put use the Taylor expansion within the GP model? There are two options: the model or the posterior. We will outline the two approaches below.","title":"Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#approximate-the-model","text":"","title":"Approximate the Model"},{"location":"appendix/gps/3_taylor_expansion/#approximate-the-posterior","text":"We can compute the expectation \\mathbb{E}[\\cdot] \\mathbb{E}[\\cdot] and variance \\mathbb{V}[\\cdot] \\mathbb{V}[\\cdot] of this Taylor expansion to come up with an approximate mean and variance function for our posterior.","title":"Approximate The Posterior"},{"location":"appendix/gps/3_taylor_expansion/#expectation","text":"This calculation is straight-forward because we are taking the expected value of a mean function f(\\mu_x) f(\\mu_x) , the derivative of a mean function f(\\mu_x) f(\\mu_x) and a Gaussian distribution noise term \\epsilon_x \\epsilon_x with mean 0. $$ \\begin{aligned} \\mathbb{E}[f(x)] &\\approx \\mathbb{E}[f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x] \\ &= f(\\mu_x) + \\nabla f(\\mu_x) \\mathbb{\\epsilon_x} \\ &= f(\\mu_x) \\end{aligned} $$","title":"Expectation"},{"location":"appendix/gps/3_taylor_expansion/#variance","text":"The variance term is a bit more complex. $$ \\begin{aligned} \\mathbb{E}\\left[(f(x) - \\mathbb{E}[f(x)])^\\top(f(x) - \\mathbb{E}[f(x)])\\right] &\\approx \\mathbb{E}\\left[(f(x) - f(\\mu_x))^\\top(f(x) - f(\\mu_x))\\right] \\ &\\approx \\mathbb{E} \\left[ \\left(f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x \\right) \\left( f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x\\right)^\\top\\right] \\ &= \\mathbb{E} \\left[ \\left(\\nabla f(\\mu_x): \\epsilon_x \\right)^\\top \\left( \\nabla f(\\mu_x): \\epsilon_x \\right) \\right] \\ &= \\nabla f(\\mu_x) \\mathbb{E}[\\epsilon_x\\epsilon_x^\\top]\\nabla f(\\mu_x) \\ &= \\nabla f(\\mu_x): \\Sigma_x :\\nabla f(\\mu_x) \\end{aligned} $$","title":"Variance"},{"location":"appendix/gps/3_taylor_expansion/#i-additive-noise-model-xfxf","text":"This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP} {-1}K_{*} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"I: Additive Noise Model (x,fx,f)"},{"location":"appendix/gps/3_taylor_expansion/#other-gp-methods","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{ z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{ } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz} {-1}SK_{zz} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Other GP Methods"},{"location":"appendix/gps/3_taylor_expansion/#ii-non-additive-noise-model","text":"","title":"II: Non-Additive Noise Model"},{"location":"appendix/gps/3_taylor_expansion/#iii-quadratic-approximation","text":"","title":"III: Quadratic Approximation"},{"location":"appendix/gps/3_taylor_expansion/#parallels-to-the-kalman-filter","text":"The Kalman Filter (KF) community use this exact formulation to motivate the Extended Kalman Filter (EKF) algorithm and some variants. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Parallels to the Kalman Filter"},{"location":"appendix/gps/4_variational/","text":"Variational Gaussian Processes \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Extended and Unscented Gaussian Processes - Steinburg & Bonilla (NIPS, 2014) Model \u00b6 Recall, we are give some data \\mathcal{D}\\in \\left\\{x_i, y_i \\right\\}^{N}_{i=1} \\mathcal{D}\\in \\left\\{x_i, y_i \\right\\}^{N}_{i=1} of vector valued input and output pairs. We are interested in finding some latent function f f that describes the relationship between \\mathbf{x} \\mathbf{x} and y y . We put a likelihood on this function an say it comes from a Gaussian process. So, from a Bayesian perspective, we have: p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) We typically solve this problem using maximum likelihood as our noise model is a Gaussian likelihood, and so the marginal log-likelihood can be computed in closed form. However, in the case that our likelihood is not Gaussian or if we want to marginalize over some uncertain inputs, then this posterior no longer tractable. So one strategy is to use variational inference to approximate the posterior distribution. Variational Inference \u00b6 Reference : Linearized and Unscented GP - Steinburg & Bonilla We can approximate the posterior distribution p(f|y) p(f|y) with variational inference. We introduce a variational distribution that is approximately Gaussian. We can introduce the Evidence Lower Bound (ELBO) term on the log-marginal likelihood \\mathcal{L} = \\mathbb{E}_{q(f)}\\left[ \\log p(y|f) \\right] - \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] \\mathcal{L} = \\mathbb{E}_{q(f)}\\left[ \\log p(y|f) \\right] - \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] TODO Do the full derivation of the ELBO from the KLD perspective. $$ \\mathcal{L} = \\text{D}_\\text{KL}[ || q()] $$ We assume that the posterior can be approximated with a Gaussian distribution q(f)=\\mathcal{N}(f|m,S) q(f)=\\mathcal{N}(f|m,S) . Because of this assumption, we have a tractable KL-Divergence term. \\mathbf{E}_{q(f)}\\left[ \\log p(y|f) \\right] = -\\frac{1}{2} \\left[D\\log 2\\pi + \\log|\\Sigma| + \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\right] \\mathbf{E}_{q(f)}\\left[ \\log p(y|f) \\right] = -\\frac{1}{2} \\left[D\\log 2\\pi + \\log|\\Sigma| + \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\right] \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] = \\frac{1}{2} \\left[\\text{Tr}(K^{-1}S) + (\\mu - m)^\\top K^{-1} (\\mu - m) - \\log|S| + \\log|K|-D \\right] \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] = \\frac{1}{2} \\left[\\text{Tr}(K^{-1}S) + (\\mu - m)^\\top K^{-1} (\\mu - m) - \\log|S| + \\log|K|-D \\right] Do the derivation of this term. GPFlow/Pyro package for reference. \\begin{aligned} \\int_{q(f)} \\log p(y|f) q(f)df &= \\mathbf{E}_{q(f)}\\left[ p(y|f) \\right] \\\\ &= - \\frac{1}{2N}\\sum_{i=1}^{N} \\log 2\\pi + \\log \\sigma^2 + \\left( (Y-m)^2 + S\\right) \\end{aligned} \\begin{aligned} \\int_{q(f)} \\log p(y|f) q(f)df &= \\mathbf{E}_{q(f)}\\left[ p(y|f) \\right] \\\\ &= - \\frac{1}{2N}\\sum_{i=1}^{N} \\log 2\\pi + \\log \\sigma^2 + \\left( (Y-m)^2 + S\\right) \\end{aligned} \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. VGP Model Posterior Distribution: p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF <span><span class=\"MathJax_Preview\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF</span><script type=\"math/tex\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Variational GP Model w/ Prior Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Sparse Model \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Variational GPs"},{"location":"appendix/gps/4_variational/#variational-gaussian-processes","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Extended and Unscented Gaussian Processes - Steinburg & Bonilla (NIPS, 2014)","title":"Variational Gaussian Processes"},{"location":"appendix/gps/4_variational/#model","text":"Recall, we are give some data \\mathcal{D}\\in \\left\\{x_i, y_i \\right\\}^{N}_{i=1} \\mathcal{D}\\in \\left\\{x_i, y_i \\right\\}^{N}_{i=1} of vector valued input and output pairs. We are interested in finding some latent function f f that describes the relationship between \\mathbf{x} \\mathbf{x} and y y . We put a likelihood on this function an say it comes from a Gaussian process. So, from a Bayesian perspective, we have: p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} p(f|X,y) = \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) We typically solve this problem using maximum likelihood as our noise model is a Gaussian likelihood, and so the marginal log-likelihood can be computed in closed form. However, in the case that our likelihood is not Gaussian or if we want to marginalize over some uncertain inputs, then this posterior no longer tractable. So one strategy is to use variational inference to approximate the posterior distribution.","title":"Model"},{"location":"appendix/gps/4_variational/#variational-inference","text":"Reference : Linearized and Unscented GP - Steinburg & Bonilla We can approximate the posterior distribution p(f|y) p(f|y) with variational inference. We introduce a variational distribution that is approximately Gaussian. We can introduce the Evidence Lower Bound (ELBO) term on the log-marginal likelihood \\mathcal{L} = \\mathbb{E}_{q(f)}\\left[ \\log p(y|f) \\right] - \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] \\mathcal{L} = \\mathbb{E}_{q(f)}\\left[ \\log p(y|f) \\right] - \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] TODO Do the full derivation of the ELBO from the KLD perspective. $$ \\mathcal{L} = \\text{D}_\\text{KL}[ || q()] $$ We assume that the posterior can be approximated with a Gaussian distribution q(f)=\\mathcal{N}(f|m,S) q(f)=\\mathcal{N}(f|m,S) . Because of this assumption, we have a tractable KL-Divergence term. \\mathbf{E}_{q(f)}\\left[ \\log p(y|f) \\right] = -\\frac{1}{2} \\left[D\\log 2\\pi + \\log|\\Sigma| + \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\right] \\mathbf{E}_{q(f)}\\left[ \\log p(y|f) \\right] = -\\frac{1}{2} \\left[D\\log 2\\pi + \\log|\\Sigma| + \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\right] \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] = \\frac{1}{2} \\left[\\text{Tr}(K^{-1}S) + (\\mu - m)^\\top K^{-1} (\\mu - m) - \\log|S| + \\log|K|-D \\right] \\text{D}_\\text{KL}\\left[ q(f) || p(f) \\right] = \\frac{1}{2} \\left[\\text{Tr}(K^{-1}S) + (\\mu - m)^\\top K^{-1} (\\mu - m) - \\log|S| + \\log|K|-D \\right] Do the derivation of this term. GPFlow/Pyro package for reference. \\begin{aligned} \\int_{q(f)} \\log p(y|f) q(f)df &= \\mathbf{E}_{q(f)}\\left[ p(y|f) \\right] \\\\ &= - \\frac{1}{2N}\\sum_{i=1}^{N} \\log 2\\pi + \\log \\sigma^2 + \\left( (Y-m)^2 + S\\right) \\end{aligned} \\begin{aligned} \\int_{q(f)} \\log p(y|f) q(f)df &= \\mathbf{E}_{q(f)}\\left[ p(y|f) \\right] \\\\ &= - \\frac{1}{2N}\\sum_{i=1}^{N} \\log 2\\pi + \\log \\sigma^2 + \\left( (Y-m)^2 + S\\right) \\end{aligned} \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right] \\mathbb{E}_{q(f)}\\left[ (y - g(f))^\\top \\Sigma^{-1} (y -g(f))\\right]","title":"Variational Inference"},{"location":"appendix/gps/4_variational/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. VGP Model Posterior Distribution: p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF <span><span class=\"MathJax_Preview\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF</span><script type=\"math/tex\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Variational GP Model w/ Prior Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Posterior Approximations"},{"location":"appendix/gps/4_variational/#sparse-model","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior.","title":"Sparse Model"},{"location":"appendix/gps/4_variational/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"appendix/gps/4_variational/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"appendix/gps/4_variational/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"appendix/gps/4_variational/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"appendix/gps/4_variational/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"appendix/gps/4_variational/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"appendix/gps/4_variational/#resources","text":"","title":"Resources"},{"location":"appendix/gps/4_variational/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"appendix/gps/4_variational/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"appendix/gps/4_variational/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"appendix/gps/gps_and_it/","text":"Gaussian Dists. and GPs \u00b6 In this summary, I will be exploring what makes Gaussian distributions so special as well as the relation they have with IT measures. I'll also look at some extensions to Gaussian Processes (GPs) and what connection they have have with IT measures. GPs, Entropy, Residuals \u00b6 Easy to compute stuff out of a GP (which is a joint multivariate Gaussian with covariance K) would be: 1) Differential) entropy from the GP: \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} where K_x is the kernel matrix (a covariance after all in feature space). Wiki | GP Book (see e.g. A.5, eq. A.20, etc) 2) And then I remembered that the LS error estimate could be bounded, and since a GP is after all LS regression in feature space, maybe we could check if the formula is right: MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) https://en.wikipedia.org/wiki/Conditional_entropy that is, MSE obtained with the GP is lower bounded by that conditional entropy estimate from RBIG. Some building blocks to start with connecting dots... :) Jesus Intuition said that \"the bigger the conditional entropy, the bigger the residual uncertainty is -> the bigger the MSE should be (no matter the model)\" Cool wiki formula: MSE >= exp(conditional) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) where: Conditional Variance: \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} Unread Stuff : Conditional Entropy in the Context of GPs - Stack Entripy of a GP (Log(det(Cov))) - stack Get full covariance matrix and find its entropy - stack GPs and Causality \u00b6 Intro to Causal Inference with GPs - blog I | blog II","title":"Gaussian Dists. and GPs"},{"location":"appendix/gps/gps_and_it/#gaussian-dists-and-gps","text":"In this summary, I will be exploring what makes Gaussian distributions so special as well as the relation they have with IT measures. I'll also look at some extensions to Gaussian Processes (GPs) and what connection they have have with IT measures.","title":"Gaussian Dists. and GPs"},{"location":"appendix/gps/gps_and_it/#gps-entropy-residuals","text":"Easy to compute stuff out of a GP (which is a joint multivariate Gaussian with covariance K) would be: 1) Differential) entropy from the GP: \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} where K_x is the kernel matrix (a covariance after all in feature space). Wiki | GP Book (see e.g. A.5, eq. A.20, etc) 2) And then I remembered that the LS error estimate could be bounded, and since a GP is after all LS regression in feature space, maybe we could check if the formula is right: MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) https://en.wikipedia.org/wiki/Conditional_entropy that is, MSE obtained with the GP is lower bounded by that conditional entropy estimate from RBIG. Some building blocks to start with connecting dots... :) Jesus Intuition said that \"the bigger the conditional entropy, the bigger the residual uncertainty is -> the bigger the MSE should be (no matter the model)\" Cool wiki formula: MSE >= exp(conditional) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) where: Conditional Variance: \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} Unread Stuff : Conditional Entropy in the Context of GPs - Stack Entripy of a GP (Log(det(Cov))) - stack Get full covariance matrix and find its entropy - stack","title":"GPs, Entropy, Residuals"},{"location":"appendix/gps/gps_and_it/#gps-and-causality","text":"Intro to Causal Inference with GPs - blog I | blog II","title":"GPs and Causality"},{"location":"appendix/gps/sota/","text":"SOTA GPs \u00b6 Literature \u00b6 Interpretable Deep Gaussian Processes with Moments - Lu et. al. (2019) - arxiv Doubly Sparse Variational Gaussian processes - Adam et. al. (2020) - axriv Monotonic GP Flows - Ustyuzhaninov & Kazlauskaite et. al. (2020) - arxiv Sparse Orthogonal Variational Inference for Gaussian Processes - Shi et. al. (2020) - arxiv A Framework forInterdomain and Multioutput Gaussian Processes - Van der Wilk et. al. (2020) - arxiv Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2020) - arxiv","title":"SOTA GPs"},{"location":"appendix/gps/sota/#sota-gps","text":"","title":"SOTA GPs"},{"location":"appendix/gps/sota/#literature","text":"Interpretable Deep Gaussian Processes with Moments - Lu et. al. (2019) - arxiv Doubly Sparse Variational Gaussian processes - Adam et. al. (2020) - axriv Monotonic GP Flows - Ustyuzhaninov & Kazlauskaite et. al. (2020) - arxiv Sparse Orthogonal Variational Inference for Gaussian Processes - Shi et. al. (2020) - arxiv A Framework forInterdomain and Multioutput Gaussian Processes - Van der Wilk et. al. (2020) - arxiv Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2020) - arxiv","title":"Literature"},{"location":"appendix/information/entropy/","text":"Entropy \u00b6 Intuition Formulas Code - Step-by-Step Code - Refactored Estimating Entropy Histogram Kernel Density Estimation KNN Approximation Single Variable Multivariate Relative Entropy (KL-Divergence) Intuition \u00b6 Expected uncertainty. H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} Lower bound on the number of bits needed to represent a RV, e.g. a RV that has a unform distribution over 32 outcomes. Lower bound on the average length of the shortest description of X X Self-Information Formulas \u00b6 H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} And we can estimate this empirically by: H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i where p_i = P(\\mathbf{X}) p_i = P(\\mathbf{X}) . Code - Step-by-Step \u00b6 # 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from scipy which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. Code - Refactored \u00b6 # 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above base = 2 H = entropy ( counts , base = base ) Estimating Entropy \u00b6 Histogram \u00b6 import numpy as np from scipy import stats # data s1 = np . random . normal ( 10 , 10 , 1_000 ) # construct histogram hist_pdf , hist_bins = np . histogram ( data , bins = 50 , range = (), density = True ) # calculate the entropy H_data = stats . entropy ( hist_pdf , base = 2 ) Kernel Density Estimation \u00b6 KNN Approximation \u00b6 Single Variable \u00b6 H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) Multivariate \u00b6 H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) Relative Entropy (KL-Divergence) \u00b6 Measure of distance between two distributions D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx aka expected log-likelihood ratio measure of inefficiency of assuming that the distribution is q q when we know the true distribution is p p .","title":"Entropy"},{"location":"appendix/information/entropy/#entropy","text":"Intuition Formulas Code - Step-by-Step Code - Refactored Estimating Entropy Histogram Kernel Density Estimation KNN Approximation Single Variable Multivariate Relative Entropy (KL-Divergence)","title":"Entropy"},{"location":"appendix/information/entropy/#intuition","text":"Expected uncertainty. H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} Lower bound on the number of bits needed to represent a RV, e.g. a RV that has a unform distribution over 32 outcomes. Lower bound on the average length of the shortest description of X X Self-Information","title":"Intuition"},{"location":"appendix/information/entropy/#formulas","text":"H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} And we can estimate this empirically by: H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i where p_i = P(\\mathbf{X}) p_i = P(\\mathbf{X}) .","title":"Formulas"},{"location":"appendix/information/entropy/#code-step-by-step","text":"# 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from scipy which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function.","title":"Code - Step-by-Step"},{"location":"appendix/information/entropy/#code-refactored","text":"# 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above base = 2 H = entropy ( counts , base = base )","title":"Code - Refactored"},{"location":"appendix/information/entropy/#estimating-entropy","text":"","title":"Estimating Entropy"},{"location":"appendix/information/entropy/#histogram","text":"import numpy as np from scipy import stats # data s1 = np . random . normal ( 10 , 10 , 1_000 ) # construct histogram hist_pdf , hist_bins = np . histogram ( data , bins = 50 , range = (), density = True ) # calculate the entropy H_data = stats . entropy ( hist_pdf , base = 2 )","title":"Histogram"},{"location":"appendix/information/entropy/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"appendix/information/entropy/#knn-approximation","text":"","title":"KNN Approximation"},{"location":"appendix/information/entropy/#single-variable","text":"H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right)","title":"Single Variable"},{"location":"appendix/information/entropy/#multivariate","text":"H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right)","title":"Multivariate"},{"location":"appendix/information/entropy/#relative-entropy-kl-divergence","text":"Measure of distance between two distributions D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx aka expected log-likelihood ratio measure of inefficiency of assuming that the distribution is q q when we know the true distribution is p p .","title":"Relative Entropy (KL-Divergence)"},{"location":"appendix/information/expf/","text":"Exponential Family of Solutions \u00b6","title":"Exponential Family of Solutions"},{"location":"appendix/information/expf/#exponential-family-of-solutions","text":"","title":"Exponential Family of Solutions"},{"location":"appendix/information/gaussian/","text":"Gaussian Distribution \u00b6 f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) Entropy \u00b6 The closed-form solution for entropy is: h(X) = \\frac{1}{2}\\log (2\\pi e\\sigma^2) h(X) = \\frac{1}{2}\\log (2\\pi e\\sigma^2) Derivation \\begin{aligned} h(X) &= - \\int_\\mathcal{X} f(X) \\log f(X) dx \\\\ &= - \\int_\\mathcal{X} f(X) \\log \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) \\right)dx \\\\ &= - \\int_\\mathcal{X} f(X) \\left[ -\\frac{1}{2}\\log (2\\pi \\sigma^2) - \\frac{x^2}{2\\sigma^2}\\log e \\right]dx \\\\ &= \\frac{1}{2} \\log (2\\pi\\sigma^2) + \\frac{\\sigma^2}{2\\sigma^2}\\log e \\\\ &= \\frac{1}{2} \\log (2\\pi e \\sigma^2) \\end{aligned} \\begin{aligned} h(X) &= - \\int_\\mathcal{X} f(X) \\log f(X) dx \\\\ &= - \\int_\\mathcal{X} f(X) \\log \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) \\right)dx \\\\ &= - \\int_\\mathcal{X} f(X) \\left[ -\\frac{1}{2}\\log (2\\pi \\sigma^2) - \\frac{x^2}{2\\sigma^2}\\log e \\right]dx \\\\ &= \\frac{1}{2} \\log (2\\pi\\sigma^2) + \\frac{\\sigma^2}{2\\sigma^2}\\log e \\\\ &= \\frac{1}{2} \\log (2\\pi e \\sigma^2) \\end{aligned} Code From Scratch def entropy_gauss ( sigma : float ) -> float : return np . log ( 2 * np . pi * np . e * sigma ** 2 ) Refactored from scipy import stats H_g = stats . norm ( scale = sigma ) . entropy ()","title":"Gaussian Distribution"},{"location":"appendix/information/gaussian/#gaussian-distribution","text":"f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right)","title":"Gaussian Distribution"},{"location":"appendix/information/gaussian/#entropy","text":"The closed-form solution for entropy is: h(X) = \\frac{1}{2}\\log (2\\pi e\\sigma^2) h(X) = \\frac{1}{2}\\log (2\\pi e\\sigma^2) Derivation \\begin{aligned} h(X) &= - \\int_\\mathcal{X} f(X) \\log f(X) dx \\\\ &= - \\int_\\mathcal{X} f(X) \\log \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) \\right)dx \\\\ &= - \\int_\\mathcal{X} f(X) \\left[ -\\frac{1}{2}\\log (2\\pi \\sigma^2) - \\frac{x^2}{2\\sigma^2}\\log e \\right]dx \\\\ &= \\frac{1}{2} \\log (2\\pi\\sigma^2) + \\frac{\\sigma^2}{2\\sigma^2}\\log e \\\\ &= \\frac{1}{2} \\log (2\\pi e \\sigma^2) \\end{aligned} \\begin{aligned} h(X) &= - \\int_\\mathcal{X} f(X) \\log f(X) dx \\\\ &= - \\int_\\mathcal{X} f(X) \\log \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( \\frac{-x^2}{2\\sigma^2} \\right) \\right)dx \\\\ &= - \\int_\\mathcal{X} f(X) \\left[ -\\frac{1}{2}\\log (2\\pi \\sigma^2) - \\frac{x^2}{2\\sigma^2}\\log e \\right]dx \\\\ &= \\frac{1}{2} \\log (2\\pi\\sigma^2) + \\frac{\\sigma^2}{2\\sigma^2}\\log e \\\\ &= \\frac{1}{2} \\log (2\\pi e \\sigma^2) \\end{aligned} Code From Scratch def entropy_gauss ( sigma : float ) -> float : return np . log ( 2 * np . pi * np . e * sigma ** 2 ) Refactored from scipy import stats H_g = stats . norm ( scale = sigma ) . entropy ()","title":"Entropy"},{"location":"appendix/information/info/","text":"Information \u00b6 Formulation \u00b6 I(X) = - \\log \\frac{1}{p(X)} I(X) = - \\log \\frac{1}{p(X)} Units \u00b6 Base Units Conversion Approximate 2 bits 1 bit = 1 bit 1 bit = 1 bit e nats 1 bit = \\log_e 2 \\log_e 2 1 bit \\approx \\approx 0.693 nats 10 bans 1 bit = \\log_{10}2 \\log_{10}2 1 bit \\approx \\approx 0.301 bans","title":"Information"},{"location":"appendix/information/info/#information","text":"","title":"Information"},{"location":"appendix/information/info/#formulation","text":"I(X) = - \\log \\frac{1}{p(X)} I(X) = - \\log \\frac{1}{p(X)}","title":"Formulation"},{"location":"appendix/information/info/#units","text":"Base Units Conversion Approximate 2 bits 1 bit = 1 bit 1 bit = 1 bit e nats 1 bit = \\log_e 2 \\log_e 2 1 bit \\approx \\approx 0.693 nats 10 bans 1 bit = \\log_{10}2 \\log_{10}2 1 bit \\approx \\approx 0.301 bans","title":"Units"},{"location":"appendix/information/information/","text":"Information \u00b6 Empirical Estimation \u00b6 def information ( p ): return - np . log2 ( p )","title":"Information"},{"location":"appendix/information/information/#information","text":"","title":"Information"},{"location":"appendix/information/information/#empirical-estimation","text":"def information ( p ): return - np . log2 ( p )","title":"Empirical Estimation"},{"location":"appendix/information/information_bottleneck/","text":"Information Bottleneck \u00b6 Blog \u00b6 Steps Towards Understanding Deep Learning: The Information Bottleneck Connection (Part 1) - blog On the information bottleneck theory of deep learning - Adrian Colyer (2017) - blog Discussions \u00b6 What is the status of the \"Information Bottleneck Theory of Deep Learning\"? - reddit On the information bottleneck theory of deep learning - reddit Papers \u00b6 The HSIC Bottleneck: Deep Learning without Back-Propagation - Kurt Ma et al (2019) - [ arxiv ] | paper | code Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle - Amjad & Geiger (2019) - arxiv | paper","title":"Information Bottleneck"},{"location":"appendix/information/information_bottleneck/#information-bottleneck","text":"","title":"Information Bottleneck"},{"location":"appendix/information/information_bottleneck/#blog","text":"Steps Towards Understanding Deep Learning: The Information Bottleneck Connection (Part 1) - blog On the information bottleneck theory of deep learning - Adrian Colyer (2017) - blog","title":"Blog"},{"location":"appendix/information/information_bottleneck/#discussions","text":"What is the status of the \"Information Bottleneck Theory of Deep Learning\"? - reddit On the information bottleneck theory of deep learning - reddit","title":"Discussions"},{"location":"appendix/information/information_bottleneck/#papers","text":"The HSIC Bottleneck: Deep Learning without Back-Propagation - Kurt Ma et al (2019) - [ arxiv ] | paper | code Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle - Amjad & Geiger (2019) - arxiv | paper","title":"Papers"},{"location":"appendix/information/it_estimators/","text":"Information Theory Measures Estimators \u00b6 Information theory measures are potentially very useful in many applications ranging from Earth science to neuroscience. Measures like entropy enable us to summarise the uncertainty of a dataset, total correlation enable us to summarise the redundancy of the data, and mutual information enable to calculate the similarities between two or more datasets. However, all of these measures require us to estimate the probabilidy density function (PDF) and thus we need to create information theory estimators (ITEs). Each measure needs a probability distribution function p(X) p(X) and this is a difficult task especially with large high-dimensional datasets. In this post, I will be outlining a few key methods that are used a lot in the literature as well ones that I am familiar with. As a side note, I would like to stress that most conventional IT estimators work on 1D or 2D variables. Rarely do we see estimators designed to work for datasets with a high number of samples and/or a high number of dimensions. The estimator that our lab uses (RBIG) is designed exactly for that. Convential Estimators \u00b6 Now lets consider the conventional estimators of the measures considered in this work: H, T, I and DKL. While plenty of methods focus on the estimation of the above magnitudes for one dimensional or two-dimensional variables [?], there are few methods that deal with variables of arbitrary dimensions. Here we focus our comparative in the relatively wide family of methods present in this recent toolbox 1 which address the general multivariate case. Specifically, the family of methods in [7] is based on the following literature [8]\u2013[18], which tackles the estimation problems according to different strategies. Gaussian Assumption \u00b6 The simplest ITM estimator is to assume that the data distributions are Gaussian. With this Gaussian assumption, the aforementioned ITMs are straightforward to calculate as the close-form solution just involves the approximated covariance matrix of the distribution(s). This assumption is very common to use for many scientific fields as datasets with a high number of samples tend to have a mean that is Gaussian distributed. However, this is not true for the tails of the distribution. In addition, the Gaussian distribution is in the exponential family of distributions which have attractive properties of mathematical convenience. The Sharma-Mittal entropy yields an analytical expression for the entropy of a Gaussian distribution \\cite{Nielsen_2011} which also includes the derivative of the log-Normalizer of the distribution which acts as a corrective term yielding better estimates of the distribution. Omitting the error due to noise, the experiments will in the following section will illustrate that the Gaussian assumption should perform well on simple distributions but should perform worse for distributions which are characterized by their tails like the T-Student or the Pareto distribution. k-Nearest Neighbours Estimator \u00b6 The next family of methods which are very commonly used are those with binning strategies. These binning methods include algorithms like the ridged histogram estimation, the smooth kernel density estimator (KDE), and the adaptive k-Nearest Neighbours (kNN) estimator \\cite{Goria05}. These methods are non-parametric which allows them to be more flexible and should be able to capture more complex distributions. However, these methods are model-based and hence are sensitive to the parameters chosen and there is no intuitive method to choose the appropriate parameters. It all depends on the data given. We chose the most robust method, the kNN estimator, for our experiments which is adaptive in nature due to the neighbour structure from a distance matrix. The kNN algorithms is known to have problems at scale with added problems of high dimensionality so we also included the scaled version which uses partition trees \\cite{Stowell09} which sacrifices accuracy for speed. Von Mises Expansion \u00b6 Algorithms \u00b6 KnnK \u00b6 This is the most common method to estimate multivariate entropy used by the many different communities is the k-nearestest neighbours (kNN) estimator which was originally proposed by Kozachenko & Leonenko (1987); inspired by Dobrushin (1958). Asymptotically unbiased Weakly consistent Non-parametric strategy which can heavily depend on the number of neighbours k k chosen. There has been some work done on this in ( citation ) but this issue is still an open problem for the community. This method works by taking the distance from the k^{th} k^{th} sample fromDeviations of this method can be seen in common machine learning packages such as sklearn to estimate mutual information as well in large scale global causal graphs ( E : Jakob's paper). This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). In relation to the other standard methods of density estimation, it is the most robust in higher dimensions due to its adaptive-like binning. A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - Paper Nearest neighbor estimates of entropy - Singh et. al. (2003) - paper A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - paper KDP \u00b6 This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume. Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - Paper expF \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper vME \u00b6 This estimates the Shannon differential entropy (H) using the von Mises expansion. Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - Paper Ensemble \u00b6 Estimates the entropy from the average entropy estimations on groups of samples This is a simple implementation with the freedom to choose the estimator estimate_H . # split into groups for igroup in batches : H += estimate_H ( igroup ) H /= len ( batches ) High-dimensional mutual information estimation for image registration - Kybic (2004) - Paper Potential New Experiments \u00b6 Voronoi \u00b6 Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities. A new class of entropy estimators for multi-dimensional densities - Miller (2003) - Paper","title":"Information Theory Measures Estimators"},{"location":"appendix/information/it_estimators/#information-theory-measures-estimators","text":"Information theory measures are potentially very useful in many applications ranging from Earth science to neuroscience. Measures like entropy enable us to summarise the uncertainty of a dataset, total correlation enable us to summarise the redundancy of the data, and mutual information enable to calculate the similarities between two or more datasets. However, all of these measures require us to estimate the probabilidy density function (PDF) and thus we need to create information theory estimators (ITEs). Each measure needs a probability distribution function p(X) p(X) and this is a difficult task especially with large high-dimensional datasets. In this post, I will be outlining a few key methods that are used a lot in the literature as well ones that I am familiar with. As a side note, I would like to stress that most conventional IT estimators work on 1D or 2D variables. Rarely do we see estimators designed to work for datasets with a high number of samples and/or a high number of dimensions. The estimator that our lab uses (RBIG) is designed exactly for that.","title":"Information Theory Measures Estimators"},{"location":"appendix/information/it_estimators/#convential-estimators","text":"Now lets consider the conventional estimators of the measures considered in this work: H, T, I and DKL. While plenty of methods focus on the estimation of the above magnitudes for one dimensional or two-dimensional variables [?], there are few methods that deal with variables of arbitrary dimensions. Here we focus our comparative in the relatively wide family of methods present in this recent toolbox 1 which address the general multivariate case. Specifically, the family of methods in [7] is based on the following literature [8]\u2013[18], which tackles the estimation problems according to different strategies.","title":"Convential Estimators"},{"location":"appendix/information/it_estimators/#gaussian-assumption","text":"The simplest ITM estimator is to assume that the data distributions are Gaussian. With this Gaussian assumption, the aforementioned ITMs are straightforward to calculate as the close-form solution just involves the approximated covariance matrix of the distribution(s). This assumption is very common to use for many scientific fields as datasets with a high number of samples tend to have a mean that is Gaussian distributed. However, this is not true for the tails of the distribution. In addition, the Gaussian distribution is in the exponential family of distributions which have attractive properties of mathematical convenience. The Sharma-Mittal entropy yields an analytical expression for the entropy of a Gaussian distribution \\cite{Nielsen_2011} which also includes the derivative of the log-Normalizer of the distribution which acts as a corrective term yielding better estimates of the distribution. Omitting the error due to noise, the experiments will in the following section will illustrate that the Gaussian assumption should perform well on simple distributions but should perform worse for distributions which are characterized by their tails like the T-Student or the Pareto distribution.","title":"Gaussian Assumption"},{"location":"appendix/information/it_estimators/#k-nearest-neighbours-estimator","text":"The next family of methods which are very commonly used are those with binning strategies. These binning methods include algorithms like the ridged histogram estimation, the smooth kernel density estimator (KDE), and the adaptive k-Nearest Neighbours (kNN) estimator \\cite{Goria05}. These methods are non-parametric which allows them to be more flexible and should be able to capture more complex distributions. However, these methods are model-based and hence are sensitive to the parameters chosen and there is no intuitive method to choose the appropriate parameters. It all depends on the data given. We chose the most robust method, the kNN estimator, for our experiments which is adaptive in nature due to the neighbour structure from a distance matrix. The kNN algorithms is known to have problems at scale with added problems of high dimensionality so we also included the scaled version which uses partition trees \\cite{Stowell09} which sacrifices accuracy for speed.","title":"k-Nearest Neighbours Estimator"},{"location":"appendix/information/it_estimators/#von-mises-expansion","text":"","title":"Von Mises Expansion"},{"location":"appendix/information/it_estimators/#algorithms","text":"","title":"Algorithms"},{"location":"appendix/information/it_estimators/#knnk","text":"This is the most common method to estimate multivariate entropy used by the many different communities is the k-nearestest neighbours (kNN) estimator which was originally proposed by Kozachenko & Leonenko (1987); inspired by Dobrushin (1958). Asymptotically unbiased Weakly consistent Non-parametric strategy which can heavily depend on the number of neighbours k k chosen. There has been some work done on this in ( citation ) but this issue is still an open problem for the community. This method works by taking the distance from the k^{th} k^{th} sample fromDeviations of this method can be seen in common machine learning packages such as sklearn to estimate mutual information as well in large scale global causal graphs ( E : Jakob's paper). This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). In relation to the other standard methods of density estimation, it is the most robust in higher dimensions due to its adaptive-like binning. A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - Paper Nearest neighbor estimates of entropy - Singh et. al. (2003) - paper A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - paper","title":"KnnK"},{"location":"appendix/information/it_estimators/#kdp","text":"This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume. Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - Paper","title":"KDP"},{"location":"appendix/information/it_estimators/#expf","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper","title":"expF"},{"location":"appendix/information/it_estimators/#vme","text":"This estimates the Shannon differential entropy (H) using the von Mises expansion. Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - Paper","title":"vME"},{"location":"appendix/information/it_estimators/#ensemble","text":"Estimates the entropy from the average entropy estimations on groups of samples This is a simple implementation with the freedom to choose the estimator estimate_H . # split into groups for igroup in batches : H += estimate_H ( igroup ) H /= len ( batches ) High-dimensional mutual information estimation for image registration - Kybic (2004) - Paper","title":"Ensemble"},{"location":"appendix/information/it_estimators/#potential-new-experiments","text":"","title":"Potential New Experiments"},{"location":"appendix/information/it_estimators/#voronoi","text":"Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities. A new class of entropy estimators for multi-dimensional densities - Miller (2003) - Paper","title":"Voronoi"},{"location":"appendix/information/it_formulas/","text":"Information Theory Measures \u00b6 References \u00b6 Lecture Notes I - PDF Video Introduction - Youtube Entropy (Shannon) \u00b6 One Random Variable \u00b6 If we have a discrete random variable X with p.m.f. p_x(x) p_x(x) , the entropy is: H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] This measures the expected uncertainty in X X . The entropy is basically how much information we learn on average from one instance of the r.v. X X . The standard definition of Entropy can be written as: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{Q(y)}{P(x)}dx\\\\ &=\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{P(x)}{Q(y)}dx \\end{aligned}$$ and the discrete version: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)}\\\\ &=\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)} \\end{aligned}$$ If we want the viewpoint in terms of expectations, we can do a bit of rearranging to get: $$\\begin{aligned} D_{KLD} &= \\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)}\\\\ &= \\sum_{x\\in\\mathcal{X}} P(x) \\log P(x)- \\sum_{-\\infty}^{\\infty}P(x)\\log Q(y)dx \\\\ &= \\sum_{x\\in\\mathcal{X}} P(x)\\left[\\log P(x) - \\log Q(y) \\right] \\\\ &= \\mathbb{E}_x\\left[ \\log P(x) - \\log Q(y) \\right] \\end{aligned}$$ #### Code - Step-by-Step 1. Obtain all of the possible occurrences of the outcomes. values , counts = np . unique ( labels , return_counts = True ) 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from `scipy` which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. 2. Use Scipy Function H = entropy ( counts , base = base ) Two Random Variables \u00b6 If we have two random variables X, Y X, Y jointly distributed according to the p.m.f. p(x,y) p(x,y) , we can come up with two more quantities for entropy. Joint Entropy \u00b6 This is given by: H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] Definition : how much uncertainty we have between two r.v.s X,Y X,Y . Conditional Entropy \u00b6 This is given by: H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] Definition : how much uncertainty remains about the r.v. X X when we know the value of Y Y . Properties of Entropic Quantities \u00b6 Non-Negativity : H(X) \\geq 0 H(X) \\geq 0 , unless X X is deterministic (i.e. no randomness). Chain Rule : You can decompose the joint entropy measure: H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) where X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} . So the result is: H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) Monotonicity : Conditioning always reduces entropy. Information never hurts . H(X|Y) \\leq H(X) H(X|Y) \\leq H(X) Negentropy \u00b6 It is simply entropy but we restrict the comparison to a Gaussian. Let's say that we have Z Z which comes from a normal distribution z\\sim\\mathcal{N}(0, \\mathbb{I}) z\\sim\\mathcal{N}(0, \\mathbb{I}) . We can write the same standard KLD formulation but with the Entropy (Renyi) \u00b6 Above we looked at Shannon entropy which is a special case of Renyi's Entropy measure. But the generalized entropy formula actually is a generalization on entropy. Below is the given formula. H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) Mutual Information \u00b6 Definition : The mutual information (MI) between two discreet r.v.s X,Y X,Y jointly distributed according to p(x,y) p(x,y) is given by: I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia Total Correlation (Multi-Information) \u00b6 In general, the formula for Total Correlation (TC) between two random variables is as follows: TC(X,Y) = H(X) + H(Y) - H(X,Y) TC(X,Y) = H(X) + H(Y) - H(X,Y) Note : This is the same as the equation for mutual information between two random variables, I(X;Y)=H(X)+H(Y)-H(X,Y) I(X;Y)=H(X)+H(Y)-H(X,Y) . This makes sense because for a Venn Diagram between two r.v.s will only have one part that intersects. This is different for the multivariate case where the number of r.v.s is greater than 2. Let's have D D random variables for X = \\{ X_1, X_2, \\ldots, X_D\\} X = \\{ X_1, X_2, \\ldots, X_D\\} . The TC is: TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) In this case, D D can be a feature for X X . Now, let's say we would like to get the difference in total correlation between two random variables, \\Delta \\Delta TC. \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) Note : There is a special case in RBIG where the two random variables are simply rotations of one another. So each feature will have a difference in entropy but the total overall dataset will not. So our function would be reduced to: \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) which is overall much easier to solve. Cross Entropy (Log-Loss Function) \u00b6 Let P(\\cdot) P(\\cdot) be the true distribution and Q(\\cdot) Q(\\cdot) be the predicted distribution. We can define the cross entropy as: H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) This can be thought of the measure in information length. Note : The original cross-entropy uses \\log_2(\\cdot) \\log_2(\\cdot) but in a supervised setting, we can use \\log_{10} \\log_{10} because if we use log rules, we get the following relation \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} . Kullback-Leibler Divergence (KL) \u00b6 Furthermore, the KL divergence is the difference between the cross-entropy and the entropy. D_{KL}(P||Q) = H(P, Q) - H(P) D_{KL}(P||Q) = H(P, Q) - H(P) So this is how far away our predictions are from our actual distribution. Conditional Information Theory Measures \u00b6 Conditional Entropy \u00b6 Conditional Mutual Information \u00b6 Definition : Let X,Y,Z X,Y,Z be jointly distributed according to some p.m.f. p(x,y,z) p(x,y,z) . The conditional mutual information X,Y X,Y given Z Z is: I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) + H(Y) - H(X,Y) I(X;Y|Z) = H(X) + H(Y) - H(X,Y)","title":"Information Theory Measures"},{"location":"appendix/information/it_formulas/#information-theory-measures","text":"","title":"Information Theory Measures"},{"location":"appendix/information/it_formulas/#references","text":"Lecture Notes I - PDF Video Introduction - Youtube","title":"References"},{"location":"appendix/information/it_formulas/#entropy-shannon","text":"","title":"Entropy (Shannon)"},{"location":"appendix/information/it_formulas/#one-random-variable","text":"If we have a discrete random variable X with p.m.f. p_x(x) p_x(x) , the entropy is: H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] This measures the expected uncertainty in X X . The entropy is basically how much information we learn on average from one instance of the r.v. X X . The standard definition of Entropy can be written as: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{Q(y)}{P(x)}dx\\\\ &=\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{P(x)}{Q(y)}dx \\end{aligned}$$ and the discrete version: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)}\\\\ &=\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)} \\end{aligned}$$ If we want the viewpoint in terms of expectations, we can do a bit of rearranging to get: $$\\begin{aligned} D_{KLD} &= \\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)}\\\\ &= \\sum_{x\\in\\mathcal{X}} P(x) \\log P(x)- \\sum_{-\\infty}^{\\infty}P(x)\\log Q(y)dx \\\\ &= \\sum_{x\\in\\mathcal{X}} P(x)\\left[\\log P(x) - \\log Q(y) \\right] \\\\ &= \\mathbb{E}_x\\left[ \\log P(x) - \\log Q(y) \\right] \\end{aligned}$$ #### Code - Step-by-Step 1. Obtain all of the possible occurrences of the outcomes. values , counts = np . unique ( labels , return_counts = True ) 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from `scipy` which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. 2. Use Scipy Function H = entropy ( counts , base = base )","title":"One Random Variable"},{"location":"appendix/information/it_formulas/#two-random-variables","text":"If we have two random variables X, Y X, Y jointly distributed according to the p.m.f. p(x,y) p(x,y) , we can come up with two more quantities for entropy.","title":"Two Random Variables"},{"location":"appendix/information/it_formulas/#joint-entropy","text":"This is given by: H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] Definition : how much uncertainty we have between two r.v.s X,Y X,Y .","title":"Joint Entropy"},{"location":"appendix/information/it_formulas/#conditional-entropy","text":"This is given by: H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] Definition : how much uncertainty remains about the r.v. X X when we know the value of Y Y .","title":"Conditional Entropy"},{"location":"appendix/information/it_formulas/#properties-of-entropic-quantities","text":"Non-Negativity : H(X) \\geq 0 H(X) \\geq 0 , unless X X is deterministic (i.e. no randomness). Chain Rule : You can decompose the joint entropy measure: H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) where X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} . So the result is: H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) Monotonicity : Conditioning always reduces entropy. Information never hurts . H(X|Y) \\leq H(X) H(X|Y) \\leq H(X)","title":"Properties of Entropic Quantities"},{"location":"appendix/information/it_formulas/#negentropy","text":"It is simply entropy but we restrict the comparison to a Gaussian. Let's say that we have Z Z which comes from a normal distribution z\\sim\\mathcal{N}(0, \\mathbb{I}) z\\sim\\mathcal{N}(0, \\mathbb{I}) . We can write the same standard KLD formulation but with the","title":"Negentropy"},{"location":"appendix/information/it_formulas/#entropy-renyi","text":"Above we looked at Shannon entropy which is a special case of Renyi's Entropy measure. But the generalized entropy formula actually is a generalization on entropy. Below is the given formula. H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x)","title":"Entropy (Renyi)"},{"location":"appendix/information/it_formulas/#mutual-information","text":"Definition : The mutual information (MI) between two discreet r.v.s X,Y X,Y jointly distributed according to p(x,y) p(x,y) is given by: I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia","title":"Mutual Information"},{"location":"appendix/information/it_formulas/#total-correlation-multi-information","text":"In general, the formula for Total Correlation (TC) between two random variables is as follows: TC(X,Y) = H(X) + H(Y) - H(X,Y) TC(X,Y) = H(X) + H(Y) - H(X,Y) Note : This is the same as the equation for mutual information between two random variables, I(X;Y)=H(X)+H(Y)-H(X,Y) I(X;Y)=H(X)+H(Y)-H(X,Y) . This makes sense because for a Venn Diagram between two r.v.s will only have one part that intersects. This is different for the multivariate case where the number of r.v.s is greater than 2. Let's have D D random variables for X = \\{ X_1, X_2, \\ldots, X_D\\} X = \\{ X_1, X_2, \\ldots, X_D\\} . The TC is: TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) In this case, D D can be a feature for X X . Now, let's say we would like to get the difference in total correlation between two random variables, \\Delta \\Delta TC. \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) Note : There is a special case in RBIG where the two random variables are simply rotations of one another. So each feature will have a difference in entropy but the total overall dataset will not. So our function would be reduced to: \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) which is overall much easier to solve.","title":"Total Correlation (Multi-Information)"},{"location":"appendix/information/it_formulas/#cross-entropy-log-loss-function","text":"Let P(\\cdot) P(\\cdot) be the true distribution and Q(\\cdot) Q(\\cdot) be the predicted distribution. We can define the cross entropy as: H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) This can be thought of the measure in information length. Note : The original cross-entropy uses \\log_2(\\cdot) \\log_2(\\cdot) but in a supervised setting, we can use \\log_{10} \\log_{10} because if we use log rules, we get the following relation \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} .","title":"Cross Entropy (Log-Loss Function)"},{"location":"appendix/information/it_formulas/#kullback-leibler-divergence-kl","text":"Furthermore, the KL divergence is the difference between the cross-entropy and the entropy. D_{KL}(P||Q) = H(P, Q) - H(P) D_{KL}(P||Q) = H(P, Q) - H(P) So this is how far away our predictions are from our actual distribution.","title":"Kullback-Leibler Divergence (KL)"},{"location":"appendix/information/it_formulas/#conditional-information-theory-measures","text":"","title":"Conditional Information Theory Measures"},{"location":"appendix/information/it_formulas/#conditional-entropy_1","text":"","title":"Conditional Entropy"},{"location":"appendix/information/it_formulas/#conditional-mutual-information","text":"Definition : Let X,Y,Z X,Y,Z be jointly distributed according to some p.m.f. p(x,y,z) p(x,y,z) . The conditional mutual information X,Y X,Y given Z Z is: I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) + H(Y) - H(X,Y) I(X;Y|Z) = H(X) + H(Y) - H(X,Y)","title":"Conditional Mutual Information"},{"location":"appendix/information/kld/","text":"KLD \u00b6 Invariant Under Transformations \u00b6 \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{Y}p(y)\\log \\frac{p(y)}{q(y)}dy \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{Y}p(y)\\log \\frac{p(y)}{q(y)}dy Let's make a transformation on p(y) p(y) using some nonlinear function f() f() . So that leaves us with y = f(x) y = f(x) . So let's apply the change of variables formula to get the probability of y y after the transformation. p(y)dy=p(x)dx = p(x)\\left|\\frac{dx}{dy} \\right| p(y)dy=p(x)dx = p(x)\\left|\\frac{dx}{dy} \\right| Remember, we defined our function as y=f(x) y=f(x) so technically we don't have access to the probability of y y . Only the probability of x x . So we cannot take the derivative in terms of y. But we can take the derivative in terms of x x . So let's rewrite the function: p(y) = p(x) \\left| \\frac{dy}{dx} \\right|^{-1} p(y) = p(x) \\left| \\frac{dy}{dx} \\right|^{-1} Now, let's plug in this formula into our KLD formulation. \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)}dy \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)}dy We still have two terms that need to go: dy dy and q(y) q(y) . For the intergration, we can simply multiple by 1 to get dy\\frac{dx}{dx} dy\\frac{dx}{dx} and then with a bit of rearranging we get: \\frac{dy}{dx}dx \\frac{dy}{dx}dx . I'm also going to change the notation as well to get \\left| \\frac{dy}{dx} \\right|dx \\left| \\frac{dy}{dx} \\right|dx . And plugging this in our formula gives us: \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)} \\left| \\frac{dy}{dx} \\right|dx \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)} \\left| \\frac{dy}{dx} \\right|dx Now, we still have the distribution q(y) q(y) .","title":"KLD"},{"location":"appendix/information/kld/#kld","text":"","title":"KLD"},{"location":"appendix/information/kld/#invariant-under-transformations","text":"\\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{Y}p(y)\\log \\frac{p(y)}{q(y)}dy \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{Y}p(y)\\log \\frac{p(y)}{q(y)}dy Let's make a transformation on p(y) p(y) using some nonlinear function f() f() . So that leaves us with y = f(x) y = f(x) . So let's apply the change of variables formula to get the probability of y y after the transformation. p(y)dy=p(x)dx = p(x)\\left|\\frac{dx}{dy} \\right| p(y)dy=p(x)dx = p(x)\\left|\\frac{dx}{dy} \\right| Remember, we defined our function as y=f(x) y=f(x) so technically we don't have access to the probability of y y . Only the probability of x x . So we cannot take the derivative in terms of y. But we can take the derivative in terms of x x . So let's rewrite the function: p(y) = p(x) \\left| \\frac{dy}{dx} \\right|^{-1} p(y) = p(x) \\left| \\frac{dy}{dx} \\right|^{-1} Now, let's plug in this formula into our KLD formulation. \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)}dy \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)}dy We still have two terms that need to go: dy dy and q(y) q(y) . For the intergration, we can simply multiple by 1 to get dy\\frac{dx}{dx} dy\\frac{dx}{dx} and then with a bit of rearranging we get: \\frac{dy}{dx}dx \\frac{dy}{dx}dx . I'm also going to change the notation as well to get \\left| \\frac{dy}{dx} \\right|dx \\left| \\frac{dy}{dx} \\right|dx . And plugging this in our formula gives us: \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)} \\left| \\frac{dy}{dx} \\right|dx \\text{D}_\\text{KL}\\left[p(y)||q(y) \\right] = \\int_\\mathcal{y=?}p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\log \\frac{p(x) \\left| \\frac{dy}{dx} \\right|^{-1}}{q(y)} \\left| \\frac{dy}{dx} \\right|dx Now, we still have the distribution q(y) q(y) .","title":"Invariant Under Transformations"},{"location":"appendix/information/knn/","text":"K-Nearest Neighbors Estimator \u00b6 The full entropy expression: \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} where: * \\psi \\psi - the digamma function. * c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} * \\Gamma \\Gamma - is the gamma function * \\epsilon(i) \\epsilon(i) is the distance to the i^{th} i^{th} sample to its k^{th} k^{th} neighbour.","title":"K-Nearest Neighbors Estimator"},{"location":"appendix/information/knn/#k-nearest-neighbors-estimator","text":"The full entropy expression: \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} where: * \\psi \\psi - the digamma function. * c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} * \\Gamma \\Gamma - is the gamma function * \\epsilon(i) \\epsilon(i) is the distance to the i^{th} i^{th} sample to its k^{th} k^{th} neighbour.","title":"K-Nearest Neighbors Estimator"},{"location":"appendix/information/main/","text":"Main \u00b6 Resources \u00b6 Prezi - Entropy and Mutual Info > Good formulas, good explanations, Gaussian stuff","title":"Main"},{"location":"appendix/information/main/#main","text":"","title":"Main"},{"location":"appendix/information/main/#resources","text":"Prezi - Entropy and Mutual Info > Good formulas, good explanations, Gaussian stuff","title":"Resources"},{"location":"appendix/information/mi/","text":"Mutual Information \u00b6 How much information one random variable says about another random variable. Intiution Full Definition Code Supplementary Information Intuition Supplementary Intiution \u00b6 Measure of the amount of information that one RV contains about another RV Reduction in the uncertainty of one rv due to knowledge of another The intersection of information in X with information in Y Full Definition \u00b6 I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia Code \u00b6 We need a PDF estimation... Normalize counts to probability values pxy = bin_counts / float ( np . sum ( bin_counts )) Get the marginal distributions px = np . sum ( pxy , axis = 1 ) # marginal for x over y py = np . sum ( pxy , axis = 0 ) # marginal for y over x Joint Probability Supplementary \u00b6 Information \u00b6 Intuition \u00b6 Things that don't normally happen, happen. Supplementary \u00b6 MI w. Numpy Predictions and Correlations in Complex Data","title":"Mutual Information"},{"location":"appendix/information/mi/#mutual-information","text":"How much information one random variable says about another random variable. Intiution Full Definition Code Supplementary Information Intuition Supplementary","title":"Mutual Information"},{"location":"appendix/information/mi/#intiution","text":"Measure of the amount of information that one RV contains about another RV Reduction in the uncertainty of one rv due to knowledge of another The intersection of information in X with information in Y","title":"Intiution"},{"location":"appendix/information/mi/#full-definition","text":"I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia","title":"Full Definition"},{"location":"appendix/information/mi/#code","text":"We need a PDF estimation... Normalize counts to probability values pxy = bin_counts / float ( np . sum ( bin_counts )) Get the marginal distributions px = np . sum ( pxy , axis = 1 ) # marginal for x over y py = np . sum ( pxy , axis = 0 ) # marginal for y over x Joint Probability","title":"Code"},{"location":"appendix/information/mi/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/information/mi/#information","text":"","title":"Information"},{"location":"appendix/information/mi/#intuition","text":"Things that don't normally happen, happen.","title":"Intuition"},{"location":"appendix/information/mi/#supplementary_1","text":"MI w. Numpy Predictions and Correlations in Complex Data","title":"Supplementary"},{"location":"appendix/information/pdf_est/","text":"Probability Density Function Estimators \u00b6 Resources \u00b6 Histogram Approximations Sebastian Blog - 1 | 2 Non-Parametric k-nearest neighbor entropy estimator - pdf","title":"Probability Density Function Estimators"},{"location":"appendix/information/pdf_est/#probability-density-function-estimators","text":"","title":"Probability Density Function Estimators"},{"location":"appendix/information/pdf_est/#resources","text":"Histogram Approximations Sebastian Blog - 1 | 2 Non-Parametric k-nearest neighbor entropy estimator - pdf","title":"Resources"},{"location":"appendix/information/variation/","text":"Variation of Information \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Correlation Example Root Mean Squared Example Taylor Diagram Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram Example Data \u00b6 We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, \u00a9 linear dataset with an outlier. Standard Methods \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$ Example \u00b6 If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics. Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Example \u00b6 An easier number to interpret. But it will not distinguish the datasets. Root Mean Squared \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} Example \u00b6 Taylor Diagram \u00b6 The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 We see that the points are on top of each other. Makes sense seeing as how all of the other measures were also equivalent. Information Theory \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) Example \u00b6 Now we finally see some differences between the distributions. Normalized Mutual Information \u00b6 The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore. Variation of Information \u00b6 This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this... RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison. VI-Based Diagram \u00b6 This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"Variation of Information"},{"location":"appendix/information/variation/#variation-of-information","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Correlation Example Root Mean Squared Example Taylor Diagram Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram","title":"Variation of Information"},{"location":"appendix/information/variation/#example-data","text":"We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, \u00a9 linear dataset with an outlier.","title":"Example Data"},{"location":"appendix/information/variation/#standard-methods","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared","title":"Standard Methods"},{"location":"appendix/information/variation/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$","title":"Covariance"},{"location":"appendix/information/variation/#example","text":"If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics.","title":"Example"},{"location":"appendix/information/variation/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Correlation"},{"location":"appendix/information/variation/#example_1","text":"An easier number to interpret. But it will not distinguish the datasets.","title":"Example"},{"location":"appendix/information/variation/#root-mean-squared","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2}","title":"Root Mean Squared"},{"location":"appendix/information/variation/#example_2","text":"","title":"Example"},{"location":"appendix/information/variation/#taylor-diagram","text":"The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"Taylor Diagram"},{"location":"appendix/information/variation/#example_3","text":"We see that the points are on top of each other. Makes sense seeing as how all of the other measures were also equivalent.","title":"Example"},{"location":"appendix/information/variation/#information-theory","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Information Theory"},{"location":"appendix/information/variation/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform.","title":"Entropy"},{"location":"appendix/information/variation/#mutual-information","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y)","title":"Mutual Information"},{"location":"appendix/information/variation/#example_4","text":"Now we finally see some differences between the distributions.","title":"Example"},{"location":"appendix/information/variation/#normalized-mutual-information","text":"The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore.","title":"Normalized Mutual Information"},{"location":"appendix/information/variation/#variation-of-information_1","text":"This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this...","title":"Variation of Information"},{"location":"appendix/information/variation/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"RVI-Based Diagram"},{"location":"appendix/information/variation/#example_5","text":"The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison.","title":"Example"},{"location":"appendix/information/variation/#vi-based-diagram","text":"This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"VI-Based Diagram"},{"location":"appendix/kernels/distances/","text":"Other Distances \u00b6 Standard Distances Haversine Distances * The Performance Impact of Vectorized Operations - [blog](https://blog.godatadriven.com/the-performance-impact-of-vectorized-operations) * Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - [blog](https://nathanrooy.github.io/posts/2016-09-07/haversine-with-python/) * RBF Formaulas - [code](https://github.com/JeremyLinux/PyTorch-Radial-Basis-Function-Layer/blob/master/Torch%20RBF/torch_rbf.py)","title":"Other Distances"},{"location":"appendix/kernels/distances/#other-distances","text":"Standard Distances Haversine Distances * The Performance Impact of Vectorized Operations - [blog](https://blog.godatadriven.com/the-performance-impact-of-vectorized-operations) * Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - [blog](https://nathanrooy.github.io/posts/2016-09-07/haversine-with-python/) * RBF Formaulas - [code](https://github.com/JeremyLinux/PyTorch-Radial-Basis-Function-Layer/blob/master/Torch%20RBF/torch_rbf.py)","title":"Other Distances"},{"location":"appendix/kernels/kernel/","text":"Kernel Measures of Similarity \u00b6 Notation \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} K \\in \\mathbb{R}^{N \\times N} K \\in \\mathbb{R}^{N \\times N} is a kernel matrix. K_\\mathbf{x} K_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} K_\\mathbf{y} K_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} K_\\mathbf{xy} K_\\mathbf{xy} is the cross kernel matrix for the r.v. \\mathcal{X,Y} \\mathcal{X,Y} \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Observations \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} can have different number of dimensions \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} must have different number of samples Feature Map \u00b6 We have a function \\varphi(X) \\varphi(X) to map \\mathcal{X} \\mathcal{X} to some feature space \\mathcal{F} \\mathcal{F} . \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N Function Class \u00b6 Reproducing Kernel Hilbert Space \\mathcal{H} \\mathcal{H} with kernel k k . Evaluation functionals f(x) = \\langle k(x,\\cdot), f \\rangle f(x) = \\langle k(x,\\cdot), f \\rangle We can compute means via linearity \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} And empirically \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} Kernels \u00b6 This allows us to not have to explicitly calculate \\varphi(X) \\varphi(X) . We just need an algorithm that calculates the dot product between them. \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') Reproducing Kernel Hilbert Space Notation \u00b6 Reproducing Property \\langle f, k(x,\\cdot) \\rangle = f(x) \\langle f, k(x,\\cdot) \\rangle = f(x) Equivalence between \\phi(x) \\phi(x) and k(x,\\cdot) k(x,\\cdot) . \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') Probabilities in Feature Space: The Mean Trick \u00b6 Mean Embedding \u00b6 \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k Maximum Mean Discrepency (MMD) \u00b6 \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} Hilbert-Schmidt Independence Criterion (HSIC) \u00b6 \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Given \\mathbb{P} \\mathbb{P} a Borel probability measure on \\mathcal{X} \\mathcal{X} , we can define a feature map \\mu_P \\in \\mathcal{F} \\mu_P \\in \\mathcal{F} . \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] Given a positive definite kernel k(x,x') k(x,x') , we can define the expectation of the cross kernel as: \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} for x \\sim P x \\sim P and q \\sim Q q \\sim Q . We can use the mean trick to define the following: \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} Covariance Measures \u00b6 Uncentered Kernel \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} <span><span class=\"MathJax_Preview\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}</span><script type=\"math/tex\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} Centered Kernel \u00b6 Hilbert-Schmidt Independence Criterion (HSIC) \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Maximum Mean Discrepency (MMD) \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Source Kernel Matrix Inversion \u00b6 Sherman-Morrison-Woodbury \u00b6 (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} Matrix Sketch (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} Kernel Approximation \u00b6 Random Fourier Features \u00b6 K \\approx ZZ^\\top K \\approx ZZ^\\top Nystrom Approximation \u00b6 K \\approx C W^\\dagger C^\\top K \\approx C W^\\dagger C^\\top According to ... the Nystroem approximation works better when you want features that are data dependent. The RFF method assumes a basis function and it is irrelevant to the data. It's merely projecting the data into the independent basis. The Nystroem approximation forms the basis through the data itself. Resources A Practical Guide to Randomized Matrix Computations with MATLAB Implementations - Shusen Wang (2015) - axriv Structured Kernel Interpolation \u00b6 \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} Correlation Measures \u00b6 Uncentered Kernel \u00b6 Kernel Alignment (KA) \u00b6 \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Kernel Alignment Uncentered Kernel \u00b6 Centered Kernel Alignment (cKA) \u00b6 \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} <span><span class=\"MathJax_Preview\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}}</span><script type=\"math/tex\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Centered Kernel Alignment Supplementary \u00b6 Ideas \u00b6 What happens when? HS Norm of Noisy Matrix HS Norm of PCA components","title":"Kernel Measures of Similarity"},{"location":"appendix/kernels/kernel/#kernel-measures-of-similarity","text":"Notation \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} K \\in \\mathbb{R}^{N \\times N} K \\in \\mathbb{R}^{N \\times N} is a kernel matrix. K_\\mathbf{x} K_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} K_\\mathbf{y} K_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} K_\\mathbf{xy} K_\\mathbf{xy} is the cross kernel matrix for the r.v. \\mathcal{X,Y} \\mathcal{X,Y} \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Observations \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} can have different number of dimensions \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} must have different number of samples","title":"Kernel Measures of Similarity"},{"location":"appendix/kernels/kernel/#feature-map","text":"We have a function \\varphi(X) \\varphi(X) to map \\mathcal{X} \\mathcal{X} to some feature space \\mathcal{F} \\mathcal{F} . \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N","title":"Feature Map"},{"location":"appendix/kernels/kernel/#function-class","text":"Reproducing Kernel Hilbert Space \\mathcal{H} \\mathcal{H} with kernel k k . Evaluation functionals f(x) = \\langle k(x,\\cdot), f \\rangle f(x) = \\langle k(x,\\cdot), f \\rangle We can compute means via linearity \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} And empirically \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned}","title":"Function Class"},{"location":"appendix/kernels/kernel/#kernels","text":"This allows us to not have to explicitly calculate \\varphi(X) \\varphi(X) . We just need an algorithm that calculates the dot product between them. \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X')","title":"Kernels"},{"location":"appendix/kernels/kernel/#reproducing-kernel-hilbert-space-notation","text":"Reproducing Property \\langle f, k(x,\\cdot) \\rangle = f(x) \\langle f, k(x,\\cdot) \\rangle = f(x) Equivalence between \\phi(x) \\phi(x) and k(x,\\cdot) k(x,\\cdot) . \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x')","title":"Reproducing Kernel Hilbert Space Notation"},{"location":"appendix/kernels/kernel/#probabilities-in-feature-space-the-mean-trick","text":"","title":"Probabilities in Feature Space: The Mean Trick"},{"location":"appendix/kernels/kernel/#mean-embedding","text":"\\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k","title":"Mean Embedding"},{"location":"appendix/kernels/kernel/#maximum-mean-discrepency-mmd","text":"\\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k}","title":"Maximum Mean Discrepency (MMD)"},{"location":"appendix/kernels/kernel/#hilbert-schmidt-independence-criterion-hsic","text":"\\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Given \\mathbb{P} \\mathbb{P} a Borel probability measure on \\mathcal{X} \\mathcal{X} , we can define a feature map \\mu_P \\in \\mathcal{F} \\mu_P \\in \\mathcal{F} . \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] Given a positive definite kernel k(x,x') k(x,x') , we can define the expectation of the cross kernel as: \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} for x \\sim P x \\sim P and q \\sim Q q \\sim Q . We can use the mean trick to define the following: \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F}","title":"Hilbert-Schmidt Independence Criterion (HSIC)"},{"location":"appendix/kernels/kernel/#covariance-measures","text":"","title":"Covariance Measures"},{"location":"appendix/kernels/kernel/#uncentered-kernel","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} <span><span class=\"MathJax_Preview\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}</span><script type=\"math/tex\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#centered-kernel","text":"","title":"Centered Kernel"},{"location":"appendix/kernels/kernel/#hilbert-schmidt-independence-criterion-hsic_1","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}","title":"Hilbert-Schmidt Independence Criterion (HSIC)"},{"location":"appendix/kernels/kernel/#maximum-mean-discrepency-mmd_1","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Source","title":"Maximum Mean Discrepency (MMD)"},{"location":"appendix/kernels/kernel/#kernel-matrix-inversion","text":"","title":"Kernel Matrix Inversion"},{"location":"appendix/kernels/kernel/#sherman-morrison-woodbury","text":"(A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} Matrix Sketch (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top}","title":"Sherman-Morrison-Woodbury"},{"location":"appendix/kernels/kernel/#kernel-approximation","text":"","title":"Kernel Approximation"},{"location":"appendix/kernels/kernel/#random-fourier-features","text":"K \\approx ZZ^\\top K \\approx ZZ^\\top","title":"Random Fourier Features"},{"location":"appendix/kernels/kernel/#nystrom-approximation","text":"K \\approx C W^\\dagger C^\\top K \\approx C W^\\dagger C^\\top According to ... the Nystroem approximation works better when you want features that are data dependent. The RFF method assumes a basis function and it is irrelevant to the data. It's merely projecting the data into the independent basis. The Nystroem approximation forms the basis through the data itself. Resources A Practical Guide to Randomized Matrix Computations with MATLAB Implementations - Shusen Wang (2015) - axriv","title":"Nystrom Approximation"},{"location":"appendix/kernels/kernel/#structured-kernel-interpolation","text":"\\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned}","title":"Structured Kernel Interpolation"},{"location":"appendix/kernels/kernel/#correlation-measures","text":"","title":"Correlation Measures"},{"location":"appendix/kernels/kernel/#uncentered-kernel_1","text":"","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#kernel-alignment-ka","text":"\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Kernel Alignment","title":"Kernel Alignment (KA)"},{"location":"appendix/kernels/kernel/#uncentered-kernel_2","text":"","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#centered-kernel-alignment-cka","text":"\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} <span><span class=\"MathJax_Preview\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}}</span><script type=\"math/tex\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Centered Kernel Alignment","title":"Centered Kernel Alignment (cKA)"},{"location":"appendix/kernels/kernel/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/kernels/kernel/#ideas","text":"What happens when? HS Norm of Noisy Matrix HS Norm of PCA components","title":"Ideas"},{"location":"appendix/kernels/density/kde/","text":"Kernel Density Estimation \u00b6 Resources \u00b6 KDEPy Literature Viz Demo of KDE A Tutorial on KDE and Recent Advances - arxiv (2017) KDE From Scratch - w Julia In Depth KDE - Jake KDE Tutorial KDE: How to compute gaussian KDE w. Python Statsmodels Tutorial Software \u00b6 kdepy StatsModels Numba Implementation KDE Numba Wrapper for Scipy pyqt - KDE Wrapper","title":"Kernel Density Estimation"},{"location":"appendix/kernels/density/kde/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"appendix/kernels/density/kde/#resources","text":"KDEPy Literature Viz Demo of KDE A Tutorial on KDE and Recent Advances - arxiv (2017) KDE From Scratch - w Julia In Depth KDE - Jake KDE Tutorial KDE: How to compute gaussian KDE w. Python Statsmodels Tutorial","title":"Resources"},{"location":"appendix/kernels/density/kde/#software","text":"kdepy StatsModels Numba Implementation KDE Numba Wrapper for Scipy pyqt - KDE Wrapper","title":"Software"},{"location":"appendix/kernels/derivatives/literature/","text":"Literature \u00b6 Paper I \u00b6 Linear Operators and Stochastic Partial Differential Equations in GPR - Simo S\u00e4rkk\u00e4 - PDF Expresses derivatives of GPs as operators Demo Colab Notebook He looks at ths special case where we have a GP with a mean function zero and a covariance matrix K K defined as: $$ \\mathbb{E}[f(\\mathbf{x})f^\\top(\\mathbf{x'})] = K_{ff}(\\mathbf{x,x'}) $$ So in GP terminology: $$ f(\\mathbf(x)) \\sim \\mathcal{GP}(\\mathbf{0}, K_{ff}(\\mathbf{x,x'})) $$ We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. Let's define the notation for the derivative of a kernel matrix. Let g(\\cdot) g(\\cdot) be the derivative operator on a function f(\\cdot) f(\\cdot) . So: $$ g(\\mathbf{x}) = \\mathcal{L}_x f(\\mathbf{x}) $$ So now, we want to define the cross operators between the derivative g(\\cdot) g(\\cdot) and the function f(\\cdot) f(\\cdot) . Example : He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) . This operator: acts on a scalar GP f(x) f(x) a scalar input x x a covariance function k_{ff}(x,x') k_{ff}(x,x') outputs a scalar value y y We can get the following transformations: $$ \\begin{aligned} K_{gf}(\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x}) = \\mathcal{L}_xK (\\mathbf{x,x'}) \\ K_{fg}(\\mathbf{x,x'}) &= f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} {x'} = K (\\mathbf{x,x'})\\mathcal{L} {x'} \\ K (\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} = \\mathcal{L} xK (\\mathbf{x,x'})\\mathcal{L}_{x'}^\\top \\ \\end{aligned} $$ Example : The Cross-Covariance term K_{fg}(\\mathbf{x,x'}) K_{fg}(\\mathbf{x,x'}) We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'}","title":"Overview"},{"location":"appendix/kernels/derivatives/literature/#literature","text":"","title":"Literature"},{"location":"appendix/kernels/derivatives/literature/#paper-i","text":"Linear Operators and Stochastic Partial Differential Equations in GPR - Simo S\u00e4rkk\u00e4 - PDF Expresses derivatives of GPs as operators Demo Colab Notebook He looks at ths special case where we have a GP with a mean function zero and a covariance matrix K K defined as: $$ \\mathbb{E}[f(\\mathbf{x})f^\\top(\\mathbf{x'})] = K_{ff}(\\mathbf{x,x'}) $$ So in GP terminology: $$ f(\\mathbf(x)) \\sim \\mathcal{GP}(\\mathbf{0}, K_{ff}(\\mathbf{x,x'})) $$ We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. Let's define the notation for the derivative of a kernel matrix. Let g(\\cdot) g(\\cdot) be the derivative operator on a function f(\\cdot) f(\\cdot) . So: $$ g(\\mathbf{x}) = \\mathcal{L}_x f(\\mathbf{x}) $$ So now, we want to define the cross operators between the derivative g(\\cdot) g(\\cdot) and the function f(\\cdot) f(\\cdot) . Example : He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) \\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x}) . This operator: acts on a scalar GP f(x) f(x) a scalar input x x a covariance function k_{ff}(x,x') k_{ff}(x,x') outputs a scalar value y y We can get the following transformations: $$ \\begin{aligned} K_{gf}(\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x}) = \\mathcal{L}_xK (\\mathbf{x,x'}) \\ K_{fg}(\\mathbf{x,x'}) &= f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} {x'} = K (\\mathbf{x,x'})\\mathcal{L} {x'} \\ K (\\mathbf{x,x'}) &= \\mathcal{L} x f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L} = \\mathcal{L} xK (\\mathbf{x,x'})\\mathcal{L}_{x'}^\\top \\ \\end{aligned} $$ Example : The Cross-Covariance term K_{fg}(\\mathbf{x,x'}) K_{fg}(\\mathbf{x,x'}) We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'}","title":"Paper I"},{"location":"appendix/kernels/similarity/kernels_it/","text":"Kernels and Information Measures \u00b6 This post will be based off of the paper from the following papers: Measures of Entropy from Data Using Infinitely Divisible Kernels - Giraldo et. al. (2014) Multivariate Extension of Matrix-based Renyi's \\alpha \\alpha -order Entropy Functional - Yu et. al. (2018) Short Overview \u00b6 The following IT measures are possible with the scheme mentioned above: Entropy Joint Entropy Conditional Entropy Mutual Information Kernel Matrices \u00b6 \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} Entropy \u00b6 \\alpha=1 \\alpha=1 \u00b6 In this case, we can show that for kernel matrices, the Renyi entropy formulation becomes the eigenvalue decomposition of the kernel matrix. $$ \\begin{aligned} H_1(x) &= \\log \\int_\\mathcal{X}f^1(x) \\cdot dx \\ \\end{aligned}$$ \\alpha=2 \\alpha=2 \u00b6 In this case, we will have the Kernel Density Estimation procedure. H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} Note : We have to use the convolution theorem for Gaussian functions. Source | Practically \u00b6 We can calculate this above formulation by simply multiplying the kernel matrix K_x K_x by the vector 1_N 1_N . \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N where \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} . The quantity \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N is known as the information potential , V V . Cross Information Potential RKHS \u00b6 \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) Cross Information Potential \\mathcal{V} \\mathcal{V} Joint Entropy \u00b6 This formula uses the above entropy formulation. To incorporate both r.v.'s X,Y X,Y , we construct two kernel matrices A,B A,B respectively S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) Note : * The trace is there for normalization. * The matrices A,B A,B have to be the same size (due to the Hadamard product). Multivariate \u00b6 This extends to multiple variables. Let's say we have L variables, then we can calculate the joint entropy like so: S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) Conditional Entropy \u00b6 This formula respects the traditional formula for conditional entropy; the joint entropy of r.v. X,Y X,Y minus the entropy of r.v. Y Y , ( H(X|Y) = H(X,Y) - H(Y) H(X|Y) = H(X,Y) - H(Y) ). Assume we have the kernel matrix for r.v. X X as A A and the kernel matrix for r.v. Y Y as B B . The following formula shows how this is calculated using kernel functions. S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) Mutual Information \u00b6 The classic Shannon definition is the sum of the marginal entropies mines the intersection between the r.v.'s X,Y X,Y , i.e. MI(X;Y)=H(X)+H(Y)-H(X,Y) MI(X;Y)=H(X)+H(Y)-H(X,Y) . The following formula shows the MI with kernels: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) The definition is the exactly the same and utilizes the entropy and joint entropy formulations above. Multivariate \u00b6 This can be extended to multiple variables. Let's use the same example for multi-variate solutions. Let's assume B B is univariate but A A is multivariate, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . We can write the MI as: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) Total Correlation \u00b6 This is a measure of redundancy for multivariate data. It is basically the entropy of each of the marginals minus the joint entropy of the multivariate distribution. Let's assume we have A A as a multivarate distribution, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . Thus we can write this distribution using Kernel matrices: T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right) T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right)","title":"Kernels and Information Measures"},{"location":"appendix/kernels/similarity/kernels_it/#kernels-and-information-measures","text":"This post will be based off of the paper from the following papers: Measures of Entropy from Data Using Infinitely Divisible Kernels - Giraldo et. al. (2014) Multivariate Extension of Matrix-based Renyi's \\alpha \\alpha -order Entropy Functional - Yu et. al. (2018)","title":"Kernels and Information Measures"},{"location":"appendix/kernels/similarity/kernels_it/#short-overview","text":"The following IT measures are possible with the scheme mentioned above: Entropy Joint Entropy Conditional Entropy Mutual Information","title":"Short Overview"},{"location":"appendix/kernels/similarity/kernels_it/#kernel-matrices","text":"\\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned}","title":"Kernel Matrices"},{"location":"appendix/kernels/similarity/kernels_it/#entropy","text":"","title":"Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#alpha1alpha1","text":"In this case, we can show that for kernel matrices, the Renyi entropy formulation becomes the eigenvalue decomposition of the kernel matrix. $$ \\begin{aligned} H_1(x) &= \\log \\int_\\mathcal{X}f^1(x) \\cdot dx \\ \\end{aligned}$$","title":"\\alpha=1\\alpha=1"},{"location":"appendix/kernels/similarity/kernels_it/#alpha2alpha2","text":"In this case, we will have the Kernel Density Estimation procedure. H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} Note : We have to use the convolution theorem for Gaussian functions. Source |","title":"\\alpha=2\\alpha=2"},{"location":"appendix/kernels/similarity/kernels_it/#practically","text":"We can calculate this above formulation by simply multiplying the kernel matrix K_x K_x by the vector 1_N 1_N . \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N where \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} . The quantity \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N is known as the information potential , V V .","title":"Practically"},{"location":"appendix/kernels/similarity/kernels_it/#cross-information-potential-rkhs","text":"\\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) Cross Information Potential \\mathcal{V} \\mathcal{V}","title":"Cross Information Potential RKHS"},{"location":"appendix/kernels/similarity/kernels_it/#joint-entropy","text":"This formula uses the above entropy formulation. To incorporate both r.v.'s X,Y X,Y , we construct two kernel matrices A,B A,B respectively S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) Note : * The trace is there for normalization. * The matrices A,B A,B have to be the same size (due to the Hadamard product).","title":"Joint Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#multivariate","text":"This extends to multiple variables. Let's say we have L variables, then we can calculate the joint entropy like so: S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right)","title":"Multivariate"},{"location":"appendix/kernels/similarity/kernels_it/#conditional-entropy","text":"This formula respects the traditional formula for conditional entropy; the joint entropy of r.v. X,Y X,Y minus the entropy of r.v. Y Y , ( H(X|Y) = H(X,Y) - H(Y) H(X|Y) = H(X,Y) - H(Y) ). Assume we have the kernel matrix for r.v. X X as A A and the kernel matrix for r.v. Y Y as B B . The following formula shows how this is calculated using kernel functions. S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B)","title":"Conditional Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#mutual-information","text":"The classic Shannon definition is the sum of the marginal entropies mines the intersection between the r.v.'s X,Y X,Y , i.e. MI(X;Y)=H(X)+H(Y)-H(X,Y) MI(X;Y)=H(X)+H(Y)-H(X,Y) . The following formula shows the MI with kernels: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) The definition is the exactly the same and utilizes the entropy and joint entropy formulations above.","title":"Mutual Information"},{"location":"appendix/kernels/similarity/kernels_it/#multivariate_1","text":"This can be extended to multiple variables. Let's use the same example for multi-variate solutions. Let's assume B B is univariate but A A is multivariate, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . We can write the MI as: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right)","title":"Multivariate"},{"location":"appendix/kernels/similarity/kernels_it/#total-correlation","text":"This is a measure of redundancy for multivariate data. It is basically the entropy of each of the marginals minus the joint entropy of the multivariate distribution. Let's assume we have A A as a multivarate distribution, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . Thus we can write this distribution using Kernel matrices: T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right) T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right)","title":"Total Correlation"},{"location":"appendix/kernels/similarity/similarity/","text":"Similarity Measures \u00b6 Covariance \u00b6 C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) Alternative Formulations $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ Measures Dependence Unbounded, (-\\infty,\\infty) (-\\infty,\\infty) Isotropic scaling Units depend on inputs Empirical Covariance \u00b6 This shows the joint variation of all pairs of random variables. C_{xy} = X^\\top X C_{xy} = X^\\top X Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Empirical Cross-Covariance \u00b6 This is the covariance between different datasets C_{xy} = X^\\top Y C_{xy} = X^\\top Y Code c_xy = X . T @ Y Linear Kernel \u00b6 This measures the covariance between samples. K_{xx} = X X^\\top K_{xx} = X X^\\top Code K_xy = X @ X . T A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data. Summarizing \u00b6 The only thing in the literature where I've seen this Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured Correlation \u00b6","title":"Similarity Measures"},{"location":"appendix/kernels/similarity/similarity/#similarity-measures","text":"","title":"Similarity Measures"},{"location":"appendix/kernels/similarity/similarity/#covariance","text":"C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) Alternative Formulations $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ Measures Dependence Unbounded, (-\\infty,\\infty) (-\\infty,\\infty) Isotropic scaling Units depend on inputs","title":"Covariance"},{"location":"appendix/kernels/similarity/similarity/#empirical-covariance","text":"This shows the joint variation of all pairs of random variables. C_{xy} = X^\\top X C_{xy} = X^\\top X Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features!","title":"Empirical Covariance"},{"location":"appendix/kernels/similarity/similarity/#empirical-cross-covariance","text":"This is the covariance between different datasets C_{xy} = X^\\top Y C_{xy} = X^\\top Y Code c_xy = X . T @ Y","title":"Empirical Cross-Covariance"},{"location":"appendix/kernels/similarity/similarity/#linear-kernel","text":"This measures the covariance between samples. K_{xx} = X X^\\top K_{xx} = X X^\\top Code K_xy = X @ X . T A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data.","title":"Linear Kernel"},{"location":"appendix/kernels/similarity/similarity/#summarizing","text":"The only thing in the literature where I've seen this Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured","title":"Summarizing"},{"location":"appendix/kernels/similarity/similarity/#correlation","text":"","title":"Correlation"},{"location":"appendix/math/bisection/","text":"","title":"Bisection"},{"location":"appendix/math/orthogonal/","text":"Resources \u00b6 Code \u00b6 INN 4 Inverse Problems A good module to walk-through how it works. Features Orthogonal w. SVD and HouseHolder Permuations","title":"Orthogonal"},{"location":"appendix/math/orthogonal/#resources","text":"","title":"Resources"},{"location":"appendix/math/orthogonal/#code","text":"INN 4 Inverse Problems A good module to walk-through how it works. Features Orthogonal w. SVD and HouseHolder Permuations","title":"Code"},{"location":"appendix/ml/mutlioutput/","text":"Multi-Output / Multi-Task learning \u00b6 So in this setting, we assume that we have a feature vector \\mathbf{x} \\in \\mathbb{R}^D \\mathbf{x} \\in \\mathbb{R}^D and an output vector \\mathbf{y} \\in \\mathbb{R}^P \\mathbf{y} \\in \\mathbb{R}^P . Talk from Andrei Karpathy \u00b6 Architectures \u00b6 One Model per Task \u00b6 We use one model per task. So we have P P independent f(\\cdot) f(\\cdot) 's for each output \\mathbf{y} \\mathbf{y} . F = \\left[f_1, f_2, \\ldots, f_p \\right] F = \\left[f_1, f_2, \\ldots, f_p \\right] Pros Decoupled Functionality Easier to manage Cons Expensive to calculate at test time No feature sharing Potential overfitting with data concernts One Model for all \u00b6 Pros Cheaper at test time Tasks help capacity Cons Fully coupled functionality expensive to fine-tine because you need to retrain the entire model every time Tasks fight for capacity One task might be easier to train whereas another might not be. If there is no dedicated loss function that addresses this (correctly) then you don't discriminate between tasks very well. Loss Functions \u00b6 When we minimize something, we need a loss function. If we have multiple tasks, then we need to ensure we have a loss function that best describes our needs. This is another static consideration. \\mathcal{L}(\\theta) = \\min_\\theta \\alpha l_1 + \\beta l_2 + \\ldots + \\zeta l_P \\mathcal{L}(\\theta) = \\min_\\theta \\alpha l_1 + \\beta l_2 + \\ldots + \\zeta l_P where l l is a loss term for a single output/task p p . Note : This method works well for 2 problems but for p>2 p>2 this doesn't work as well. Some examples where we can derive loss functions Scales We can have a loss function that considers different scales, e.g. if we have a regression and a classification task. Then we need to scale each of the tasks appropriately. If I have a regression task and a 10-classification tasks, how do you scale this appropriately? Importance We may have specific goals or specific problems that we want to do better at than others. For this, we need to add some sort of weight in order to focus on that problem. Convergence Some tasks may be easier or harder. The easier tasks will probably converge faster whereas the harder tasks will not. So how do you propose early stopping criteria? It becomes quite difficult when you have multiple tasks which may require multiple stopping criteria. More/Less Data Some tasks will have more data which might increase the convergence time and/or hinder the training of other tasks. So you need to adjust your loss function accordingly. More Noise More noise means more regularization. But you need to know the amount of noise a priori and need to adjust accordingly. Training Dynamics \u00b6 Sampling \u00b6 Do we do within-task sampling and across-task sampling? If we know that two tasks are very sensitive to certain features, it makes sense for them to use only those features. Amortized ? Team Management If you have a giant network and you want to fine-tune certain bits and pieces of your network, then it becomes difficult to allocate resources and allow people to only change certain parts. For example, if each person tries to change one little section, then this may lead to really complex networks that are no longer able to reproduce the best results. In a Transfer Learning , we definitely need to consider the final product. Literature \u00b6 Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation - Liu et. al. (2019) - arxiv | PyTorch This paper searches the space of neural network for the best neural network using brute force search techniques. Which Tasks Should Be Learned Together in Multi-task Learning? - Standley et. al. (2019) - arxiv","title":"Multi-Output"},{"location":"appendix/ml/mutlioutput/#multi-output-multi-task-learning","text":"So in this setting, we assume that we have a feature vector \\mathbf{x} \\in \\mathbb{R}^D \\mathbf{x} \\in \\mathbb{R}^D and an output vector \\mathbf{y} \\in \\mathbb{R}^P \\mathbf{y} \\in \\mathbb{R}^P .","title":"Multi-Output / Multi-Task learning"},{"location":"appendix/ml/mutlioutput/#talk-from-andrei-karpathy","text":"","title":"Talk from Andrei Karpathy"},{"location":"appendix/ml/mutlioutput/#architectures","text":"","title":"Architectures"},{"location":"appendix/ml/mutlioutput/#one-model-per-task","text":"We use one model per task. So we have P P independent f(\\cdot) f(\\cdot) 's for each output \\mathbf{y} \\mathbf{y} . F = \\left[f_1, f_2, \\ldots, f_p \\right] F = \\left[f_1, f_2, \\ldots, f_p \\right] Pros Decoupled Functionality Easier to manage Cons Expensive to calculate at test time No feature sharing Potential overfitting with data concernts","title":"One Model per Task"},{"location":"appendix/ml/mutlioutput/#one-model-for-all","text":"Pros Cheaper at test time Tasks help capacity Cons Fully coupled functionality expensive to fine-tine because you need to retrain the entire model every time Tasks fight for capacity One task might be easier to train whereas another might not be. If there is no dedicated loss function that addresses this (correctly) then you don't discriminate between tasks very well.","title":"One Model for all"},{"location":"appendix/ml/mutlioutput/#loss-functions","text":"When we minimize something, we need a loss function. If we have multiple tasks, then we need to ensure we have a loss function that best describes our needs. This is another static consideration. \\mathcal{L}(\\theta) = \\min_\\theta \\alpha l_1 + \\beta l_2 + \\ldots + \\zeta l_P \\mathcal{L}(\\theta) = \\min_\\theta \\alpha l_1 + \\beta l_2 + \\ldots + \\zeta l_P where l l is a loss term for a single output/task p p . Note : This method works well for 2 problems but for p>2 p>2 this doesn't work as well. Some examples where we can derive loss functions Scales We can have a loss function that considers different scales, e.g. if we have a regression and a classification task. Then we need to scale each of the tasks appropriately. If I have a regression task and a 10-classification tasks, how do you scale this appropriately? Importance We may have specific goals or specific problems that we want to do better at than others. For this, we need to add some sort of weight in order to focus on that problem. Convergence Some tasks may be easier or harder. The easier tasks will probably converge faster whereas the harder tasks will not. So how do you propose early stopping criteria? It becomes quite difficult when you have multiple tasks which may require multiple stopping criteria. More/Less Data Some tasks will have more data which might increase the convergence time and/or hinder the training of other tasks. So you need to adjust your loss function accordingly. More Noise More noise means more regularization. But you need to know the amount of noise a priori and need to adjust accordingly.","title":"Loss Functions"},{"location":"appendix/ml/mutlioutput/#training-dynamics","text":"","title":"Training Dynamics"},{"location":"appendix/ml/mutlioutput/#sampling","text":"Do we do within-task sampling and across-task sampling? If we know that two tasks are very sensitive to certain features, it makes sense for them to use only those features. Amortized ? Team Management If you have a giant network and you want to fine-tune certain bits and pieces of your network, then it becomes difficult to allocate resources and allow people to only change certain parts. For example, if each person tries to change one little section, then this may lead to really complex networks that are no longer able to reproduce the best results. In a Transfer Learning , we definitely need to consider the final product.","title":"Sampling"},{"location":"appendix/ml/mutlioutput/#literature","text":"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation - Liu et. al. (2019) - arxiv | PyTorch This paper searches the space of neural network for the best neural network using brute force search techniques. Which Tasks Should Be Learned Together in Multi-task Learning? - Standley et. al. (2019) - arxiv","title":"Literature"},{"location":"appendix/neural/rbig/","text":"Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6 Testing \u00b6 Here we have a few tests that we can do: DCT components are orthogonal PCA works Eigenvalues are descending PCA + Whitening Works Eigenvalues are descending Data is white [all(abs(np.cov(pca.transform(data))< 1e-6>)] Test the non-symmetric version (?) def test_dct ( self ): # Initialize Linear Transformation Class and DCT components dct = LinearTransformation ( basis = 'DCT' , conv = 16 ) # Make sure the DCT basis is orthogonal self . assertTrue ( all ( abs ( dct . W , dct . W . T ) - np . eye ( 256 )) < 1e-10 ) # The Jacobian should be zero X_rand = np . random . randn ( 16 , 10 ) self . assertTrue ( all ( abs ( dct . logjacobian ( X_rand ), dct . W . T ) - np . eye ( 256 ) < 1e-10 )) def test_pca ( self ): # Get Test Data X_rand = np . random . randn ( 16 , 256 ) covr = np . cov ( X_rand ) data = np . linalg . cholesky ( covr ) @ np . random . randn ( 16 , 10000 ) # Initialize Linear Transformation with PCA pca = LinearTransformation ( basis = 'PCA' ) # Make sure eigenvalues descend # # Make sure data is white def test_pca_whitening ( self ): Supplementary \u00b6 Boundary Issues \u00b6 PDF Estimation under arbitrary transformation \u00b6 Let \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d be a r.v. with a PDF, \\mathcal P_x (\\mathbf x) \\mathcal P_x (\\mathbf x) . Given some bijective, differentiable transform \\mathbf x \\mathbf x and \\mathbf y \\mathbf y using \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d , \\mathbf y = \\mathcal G(\\mathbf x) \\mathbf y = \\mathcal G(\\mathbf x) , we can use the change of variables formula to calculate the determinant: \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| In the case of Gaussianization, we can calculate \\mathcal P (\\mathbf x) \\mathcal P (\\mathbf x) if the Jacobian is known since Iterative Gaussianization Transform is Invertible \u00b6 Given a Gaussianization transform: \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) by simple manipulation, the inversion transform is: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) Note : If \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal, then \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} . So we can simplify our transformation like so: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) iff \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal or (orthonormal vectors). Note 2 : to ensure that \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is invertible, we need to be sure that the PDF support is connected. So the domain is continuous and there are no disjoint spaces ( ??? ). References \u00b6 Algorithm Multivariate Gaussianization for Data Proceessing - Prezi Nonlineear Extraction of 'IC' of elliptically symmetric densities using radial Gaussianization - Lyu et. al. (2008) - Code Real-NVP Implementation - PyTorch Normalizing Flows with PyTorch Radial Gaussianization - Python PyTorch GDN Good RBIG Implementations - Transforms | Models Histogram Estimation Scipy Use Scipy Function HistogramUniverateDensity","title":"Rbig"},{"location":"appendix/neural/rbig/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"appendix/neural/rbig/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"appendix/neural/rbig/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"appendix/neural/rbig/#jacobian","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"appendix/neural/rbig/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"appendix/neural/rbig/#testing","text":"Here we have a few tests that we can do: DCT components are orthogonal PCA works Eigenvalues are descending PCA + Whitening Works Eigenvalues are descending Data is white [all(abs(np.cov(pca.transform(data))< 1e-6>)] Test the non-symmetric version (?) def test_dct ( self ): # Initialize Linear Transformation Class and DCT components dct = LinearTransformation ( basis = 'DCT' , conv = 16 ) # Make sure the DCT basis is orthogonal self . assertTrue ( all ( abs ( dct . W , dct . W . T ) - np . eye ( 256 )) < 1e-10 ) # The Jacobian should be zero X_rand = np . random . randn ( 16 , 10 ) self . assertTrue ( all ( abs ( dct . logjacobian ( X_rand ), dct . W . T ) - np . eye ( 256 ) < 1e-10 )) def test_pca ( self ): # Get Test Data X_rand = np . random . randn ( 16 , 256 ) covr = np . cov ( X_rand ) data = np . linalg . cholesky ( covr ) @ np . random . randn ( 16 , 10000 ) # Initialize Linear Transformation with PCA pca = LinearTransformation ( basis = 'PCA' ) # Make sure eigenvalues descend # # Make sure data is white def test_pca_whitening ( self ):","title":"Testing"},{"location":"appendix/neural/rbig/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/neural/rbig/#boundary-issues","text":"","title":"Boundary Issues"},{"location":"appendix/neural/rbig/#pdf-estimation-under-arbitrary-transformation","text":"Let \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d be a r.v. with a PDF, \\mathcal P_x (\\mathbf x) \\mathcal P_x (\\mathbf x) . Given some bijective, differentiable transform \\mathbf x \\mathbf x and \\mathbf y \\mathbf y using \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d , \\mathbf y = \\mathcal G(\\mathbf x) \\mathbf y = \\mathcal G(\\mathbf x) , we can use the change of variables formula to calculate the determinant: \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| In the case of Gaussianization, we can calculate \\mathcal P (\\mathbf x) \\mathcal P (\\mathbf x) if the Jacobian is known since","title":"PDF Estimation under arbitrary transformation"},{"location":"appendix/neural/rbig/#iterative-gaussianization-transform-is-invertible","text":"Given a Gaussianization transform: \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) by simple manipulation, the inversion transform is: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) Note : If \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal, then \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} . So we can simplify our transformation like so: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) iff \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal or (orthonormal vectors). Note 2 : to ensure that \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is invertible, we need to be sure that the PDF support is connected. So the domain is continuous and there are no disjoint spaces ( ??? ).","title":"Iterative Gaussianization Transform is Invertible"},{"location":"appendix/neural/rbig/#references","text":"Algorithm Multivariate Gaussianization for Data Proceessing - Prezi Nonlineear Extraction of 'IC' of elliptically symmetric densities using radial Gaussianization - Lyu et. al. (2008) - Code Real-NVP Implementation - PyTorch Normalizing Flows with PyTorch Radial Gaussianization - Python PyTorch GDN Good RBIG Implementations - Transforms | Models Histogram Estimation Scipy Use Scipy Function HistogramUniverateDensity","title":"References"},{"location":"appendix/similarity/","text":"Similarity? \u00b6 What is similarity? \u00b6 In words: Overview of Methods \u00b6 Family Example Methods Linear \\rho V \\rho V -Coefficient Distance-Based Energy Distance, MMD Shannon Entropy D _\\text{KL} _\\text{KL} All Linear methods are just regression - notebook Method Property Linear Uniform Distance Adaptive Kernels Adaptive, Smoothing Information Distribution Contents \u00b6 Linear Methods \u00b6 In this document, I will go over the main linear ways that we can estimate similarity. They include the classic measures such as covariance and Pearson's correlation coefficient \\rho \\rho . I will also go over how this can be extended to multivariate datasets with measures such as the congruence coefficient and the \\rho \\rho V-Coefficient; multi-dimensional extensions to the original \\rho \\rho . Theory: Document Lab: Colab Notebook Distance-Based \u00b6 This is a non-linear extension to the linear method which attempts to estimate similarity by distance formulas. This gives us a lot of flexibility with the distance metric that we choose (as long as they satisfy some key conditions). In the document, I will go over some of the common ones and how we go from linear to distance-metrics. Theory: Document Kernel Methods \u00b6 The kernel methods acts as a non-linear extension to the linear method and it generalizes the distance-based methods. These utilize kernel functions which act as non-linear functions which operate on the linear method and/or the distance metric. This adds an even higher level of flexibility. This is good or bad depending upon your perspective. There are two main candidates: the HSIC/Kernel Alignment and the Maximum Mean Discrepency (MMD) methods. They are related but the way the methods are calculated vary as will be explained above. Theory (HSIC): Document Theory (MMD): Document Variation of Information \u00b6 The final method that we investigate is a different approach than the methods listed about: estimating the PDF of the dataset and then calculating Information theoretic measures. Variation of Information Visualization \u00b6 This is something that I didn't believe was so important until I had to explain my results. It was very clear it my head but I ended up showing a lot of different graphs and it was difficult for everyone to grasp the essence. Taylor Diagrams were originally used for univariate comparisons to show the Pearson correlation coefficient, the norm of the vector and the RMSE all in one plot. I show that this can be extended to the multivariate measures such as the \\rho \\rho V-Coefficient, the Kernel Alignment Method and the Variation of Information methods. Taylor Diagram Resources \u00b6 Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions - Sung-Huyk Cha - International Journal of Mathematical Models and Methods in Applied Sciences, (...) Multivariate Statistics Summary and Comparison of Techniques - Presenation","title":"Overview"},{"location":"appendix/similarity/#similarity","text":"","title":"Similarity?"},{"location":"appendix/similarity/#what-is-similarity","text":"In words:","title":"What is similarity?"},{"location":"appendix/similarity/#overview-of-methods","text":"Family Example Methods Linear \\rho V \\rho V -Coefficient Distance-Based Energy Distance, MMD Shannon Entropy D _\\text{KL} _\\text{KL} All Linear methods are just regression - notebook Method Property Linear Uniform Distance Adaptive Kernels Adaptive, Smoothing Information Distribution","title":"Overview of Methods"},{"location":"appendix/similarity/#contents","text":"","title":"Contents"},{"location":"appendix/similarity/#linear-methods","text":"In this document, I will go over the main linear ways that we can estimate similarity. They include the classic measures such as covariance and Pearson's correlation coefficient \\rho \\rho . I will also go over how this can be extended to multivariate datasets with measures such as the congruence coefficient and the \\rho \\rho V-Coefficient; multi-dimensional extensions to the original \\rho \\rho . Theory: Document Lab: Colab Notebook","title":"Linear Methods"},{"location":"appendix/similarity/#distance-based","text":"This is a non-linear extension to the linear method which attempts to estimate similarity by distance formulas. This gives us a lot of flexibility with the distance metric that we choose (as long as they satisfy some key conditions). In the document, I will go over some of the common ones and how we go from linear to distance-metrics. Theory: Document","title":"Distance-Based"},{"location":"appendix/similarity/#kernel-methods","text":"The kernel methods acts as a non-linear extension to the linear method and it generalizes the distance-based methods. These utilize kernel functions which act as non-linear functions which operate on the linear method and/or the distance metric. This adds an even higher level of flexibility. This is good or bad depending upon your perspective. There are two main candidates: the HSIC/Kernel Alignment and the Maximum Mean Discrepency (MMD) methods. They are related but the way the methods are calculated vary as will be explained above. Theory (HSIC): Document Theory (MMD): Document","title":"Kernel Methods"},{"location":"appendix/similarity/#variation-of-information","text":"The final method that we investigate is a different approach than the methods listed about: estimating the PDF of the dataset and then calculating Information theoretic measures. Variation of Information","title":"Variation of Information"},{"location":"appendix/similarity/#visualization","text":"This is something that I didn't believe was so important until I had to explain my results. It was very clear it my head but I ended up showing a lot of different graphs and it was difficult for everyone to grasp the essence. Taylor Diagrams were originally used for univariate comparisons to show the Pearson correlation coefficient, the norm of the vector and the RMSE all in one plot. I show that this can be extended to the multivariate measures such as the \\rho \\rho V-Coefficient, the Kernel Alignment Method and the Variation of Information methods. Taylor Diagram","title":"Visualization"},{"location":"appendix/similarity/#resources","text":"Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions - Sung-Huyk Cha - International Journal of Mathematical Models and Methods in Applied Sciences, (...) Multivariate Statistics Summary and Comparison of Techniques - Presenation","title":"Resources"},{"location":"appendix/similarity/definition/","text":"What is Similarity? \u00b6 On Paper \u00b6 Mathematical Conditions \u00b6 Isotropic Scaling \u00b6 Orthogonal Transformations \u00b6 Invertible Transformations \u00b6","title":"What is Similarity?"},{"location":"appendix/similarity/definition/#what-is-similarity","text":"","title":"What is Similarity?"},{"location":"appendix/similarity/definition/#on-paper","text":"","title":"On Paper"},{"location":"appendix/similarity/definition/#mathematical-conditions","text":"","title":"Mathematical Conditions"},{"location":"appendix/similarity/definition/#isotropic-scaling","text":"","title":"Isotropic Scaling"},{"location":"appendix/similarity/definition/#orthogonal-transformations","text":"","title":"Orthogonal Transformations"},{"location":"appendix/similarity/definition/#invertible-transformations","text":"","title":"Invertible Transformations"},{"location":"appendix/similarity/distance/","text":"Distances \u00b6 Euclidean vs. Cosine Similarity \u00b6 Euclidean Distance \u00b6 Figure II : The triangle showing the cosine similarity and it's relationship to the euclidean distance. d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle def euclidean_distance ( x , y ): return np . sum (( x - y ) ** 2 ) Cosine Distance \u00b6 This is useful as a metric for measuring distance when the magnitude of the vectors don't matter. Otherwise the dimensionality could really affect the notion of \"similarity\". This is a measure that looks for 'directional similarity', not just magnitude. Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code : def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim Relationship \u00b6 The squared Euclidean distance is proportional to the cosine distance. Proof : Let's write down the full form. ||\\mathbf{x-y}||_2^2 = (\\mathbf{x-y})^\\top (\\mathbf{x-y}) ||\\mathbf{x-y}||_2^2 = (\\mathbf{x-y})^\\top (\\mathbf{x-y}) Now we can expand this even further: ||\\mathbf{x-y}||_2^2 = \\mathbf{x^\\top x} + \\mathbf{y^\\top y} - 2 \\mathbf{x^\\top y} ||\\mathbf{x-y}||_2^2 = \\mathbf{x^\\top x} + \\mathbf{y^\\top y} - 2 \\mathbf{x^\\top y} If the following is true: ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 , then we can simplify this expression: ||\\mathbf{x-y}||_2^2 = 2 - 2 \\mathbf{x^\\top y} ||\\mathbf{x-y}||_2^2 = 2 - 2 \\mathbf{x^\\top y} Keep in mind, now we can subsitude the cosine similarity expression in this formulation because \\mathbf{x^\\top y} = \\langle \\mathbf{x,y} \\rangle \\mathbf{x^\\top y} = \\langle \\mathbf{x,y} \\rangle . We will add a constant term \\frac{||\\mathbf{x}||_2||\\mathbf{y}||_2}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\frac{||\\mathbf{x}||_2||\\mathbf{y}||_2}{||\\mathbf{x}||_2||\\mathbf{y}||_2} to make it absolutely clear: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\frac{\\langle \\mathbf{x,y} \\rangle}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\frac{\\langle \\mathbf{x,y} \\rangle}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\\\ \\end{aligned} Part of this formulation is exactly the cosine similarity formula: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} and we already said that ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 so we can remove the constant term that we just added: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} So now we see that the squared euclidean distance is proportional to the cosine similarity if we have \\mathcal{l}^2 \\mathcal{l}^2 -normalized inputs \\mathbf{x,y} \\mathbf{x,y} . While this is an option on scikit-learn , I don't think it's the standard. We typically standardize the data (remove the mean, divide by the variance). I don't see people normalizing their data. Idea Check to see if this relationship is true (or approximately true) for univariate and multivariate datasets. Sources : Euclidean vs. Cosine Distance - Chris Emmery StackOverFlow Distance Correlation \u00b6 This is a measure of dependence between two rvs. It can also detect non-linear relationships between two rvs. \\text{D}_{\\rho}(X,Y) = \\frac{\\text{D}_{cov}(X,Y)}{\\sqrt{\\text{D}_\\text{var}(X)}\\sqrt{\\text{D}_\\text{var}(Y)}} \\text{D}_{\\rho}(X,Y) = \\frac{\\text{D}_{cov}(X,Y)}{\\sqrt{\\text{D}_\\text{var}(X)}\\sqrt{\\text{D}_\\text{var}(Y)}} where: \\text{D}_{cov}(X,X)= \\frac{1}{N^2}\\sum_{i}\\sum_{j}A_{i,j}B_{i,j} \\text{D}_{cov}(X,X)= \\frac{1}{N^2}\\sum_{i}\\sum_{j}A_{i,j}B_{i,j} \\text{D}_\\text{var}^2(X)=\\text{D}_{cov}(X,X)=\\frac{1}{N}\\sum_{i,j}A^2_{i,j} \\text{D}_\\text{var}^2(X)=\\text{D}_{cov}(X,X)=\\frac{1}{N}\\sum_{i,j}A^2_{i,j} $$$$ Code : Insert your favorite distance calculation d(X,Y) d(X,Y) # distance covariance d_cov = np . sum ( D_x * D_y ) # distance correlation d_corr = np . sum ( D_x * D_y ) / np . linalg . norm ( D_x ) / np . linalg . norm ( D_y ) Tip : In a paper that noone understands , Sejdinovic et. al. (2013) found that the HSIC method generalizes the distance covariance measure except that it has a non-linear transformation, i.e. a kernel function. So by extension, the Centered Kernel Alignment method (the normalized version of HSIC) generalizes the distance correlation method. This arxiv paper does a much better job at explaining the similarities between the two methods. Also, see this masters thesis [ PDF ] which does the proofs for the equivalence of HSIC and the distance covariance. Energy Distance \u00b6 This is the statistical distance between probability distribtions. d^2(\\mathbb{P,Q}) = 2 \\mathbb{E}\\left[ ||X-Y|| \\right] - \\mathbb{E}\\left[ ||X-X'|| \\right] - \\mathbb{E}\\left[ ||Y-Y'|| \\right] d^2(\\mathbb{P,Q}) = 2 \\mathbb{E}\\left[ ||X-Y|| \\right] - \\mathbb{E}\\left[ ||X-X'|| \\right] - \\mathbb{E}\\left[ ||Y-Y'|| \\right] if we relax the constraints a bit so that we are in metric spaces, we get: d^2(\\mathbb{\\mu,\\nu}) = 2 \\mathbb{E}\\left[ d(X, Y) \\right] - \\mathbb{E}\\left[ d(X, X') \\right] - \\mathbb{E}\\left[ d(Y, Y') \\right] d^2(\\mathbb{\\mu,\\nu}) = 2 \\mathbb{E}\\left[ d(X, Y) \\right] - \\mathbb{E}\\left[ d(X, X') \\right] - \\mathbb{E}\\left[ d(Y, Y') \\right] Code : Insert your favorite distance calculation d(X,Y) d(X,Y) d_xx = pairwise_distances ( X ) d_yy = pairwise_distances ( Y ) d_xy = pairwise_distances ( X , Y ) # calculate the mean of everything energy_dist = 2 * np . mean ( d_xy ) - np . mean ( d_xx ) - np . mean ( d_yy ) Tip : See this masters thesis [ PDF ] which does the proofs for the equivalence of MMD and the energy distance. Other Distances \u00b6 I've just focused on the common distance metrics that capture important attributes. There are some examples like the Manhattan distance which ... and the Haversine function which is geocentric. In addition, instead of calculating the entire distance matrix, we can only look at the k k -Nearest Neighbors which enforces sparsity. Standard Distances Haversine Distances References \u00b6 Software - Distance Correlation and related E-Statistics in Python Euclidean Distance Matrices: Essential Theory, Algorithms and Applications - arxiv Fast Computations \u00b6 The Performance Impact of Vectorized Operations - blog Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - blog RBF Formaulas - code","title":"Distances"},{"location":"appendix/similarity/distance/#distances","text":"","title":"Distances"},{"location":"appendix/similarity/distance/#euclidean-vs-cosine-similarity","text":"","title":"Euclidean vs. Cosine Similarity"},{"location":"appendix/similarity/distance/#euclidean-distance","text":"Figure II : The triangle showing the cosine similarity and it's relationship to the euclidean distance. d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle def euclidean_distance ( x , y ): return np . sum (( x - y ) ** 2 )","title":"Euclidean Distance"},{"location":"appendix/similarity/distance/#cosine-distance","text":"This is useful as a metric for measuring distance when the magnitude of the vectors don't matter. Otherwise the dimensionality could really affect the notion of \"similarity\". This is a measure that looks for 'directional similarity', not just magnitude. Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code : def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim","title":"Cosine Distance"},{"location":"appendix/similarity/distance/#relationship","text":"The squared Euclidean distance is proportional to the cosine distance. Proof : Let's write down the full form. ||\\mathbf{x-y}||_2^2 = (\\mathbf{x-y})^\\top (\\mathbf{x-y}) ||\\mathbf{x-y}||_2^2 = (\\mathbf{x-y})^\\top (\\mathbf{x-y}) Now we can expand this even further: ||\\mathbf{x-y}||_2^2 = \\mathbf{x^\\top x} + \\mathbf{y^\\top y} - 2 \\mathbf{x^\\top y} ||\\mathbf{x-y}||_2^2 = \\mathbf{x^\\top x} + \\mathbf{y^\\top y} - 2 \\mathbf{x^\\top y} If the following is true: ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 , then we can simplify this expression: ||\\mathbf{x-y}||_2^2 = 2 - 2 \\mathbf{x^\\top y} ||\\mathbf{x-y}||_2^2 = 2 - 2 \\mathbf{x^\\top y} Keep in mind, now we can subsitude the cosine similarity expression in this formulation because \\mathbf{x^\\top y} = \\langle \\mathbf{x,y} \\rangle \\mathbf{x^\\top y} = \\langle \\mathbf{x,y} \\rangle . We will add a constant term \\frac{||\\mathbf{x}||_2||\\mathbf{y}||_2}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\frac{||\\mathbf{x}||_2||\\mathbf{y}||_2}{||\\mathbf{x}||_2||\\mathbf{y}||_2} to make it absolutely clear: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\frac{\\langle \\mathbf{x,y} \\rangle}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\frac{\\langle \\mathbf{x,y} \\rangle}{||\\mathbf{x}||_2||\\mathbf{y}||_2} \\\\ \\end{aligned} Part of this formulation is exactly the cosine similarity formula: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 ||\\mathbf{x}||_2||\\mathbf{y}||_2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} and we already said that ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 ||\\mathbf{x}||_2 = ||\\mathbf{y}||_2 = 1 so we can remove the constant term that we just added: \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} \\begin{aligned} ||\\mathbf{x-y}||_2^2 &= 2 - 2 \\cos \\angle (\\mathbf{x,y}) \\\\ \\end{aligned} So now we see that the squared euclidean distance is proportional to the cosine similarity if we have \\mathcal{l}^2 \\mathcal{l}^2 -normalized inputs \\mathbf{x,y} \\mathbf{x,y} . While this is an option on scikit-learn , I don't think it's the standard. We typically standardize the data (remove the mean, divide by the variance). I don't see people normalizing their data. Idea Check to see if this relationship is true (or approximately true) for univariate and multivariate datasets. Sources : Euclidean vs. Cosine Distance - Chris Emmery StackOverFlow","title":"Relationship"},{"location":"appendix/similarity/distance/#distance-correlation","text":"This is a measure of dependence between two rvs. It can also detect non-linear relationships between two rvs. \\text{D}_{\\rho}(X,Y) = \\frac{\\text{D}_{cov}(X,Y)}{\\sqrt{\\text{D}_\\text{var}(X)}\\sqrt{\\text{D}_\\text{var}(Y)}} \\text{D}_{\\rho}(X,Y) = \\frac{\\text{D}_{cov}(X,Y)}{\\sqrt{\\text{D}_\\text{var}(X)}\\sqrt{\\text{D}_\\text{var}(Y)}} where: \\text{D}_{cov}(X,X)= \\frac{1}{N^2}\\sum_{i}\\sum_{j}A_{i,j}B_{i,j} \\text{D}_{cov}(X,X)= \\frac{1}{N^2}\\sum_{i}\\sum_{j}A_{i,j}B_{i,j} \\text{D}_\\text{var}^2(X)=\\text{D}_{cov}(X,X)=\\frac{1}{N}\\sum_{i,j}A^2_{i,j} \\text{D}_\\text{var}^2(X)=\\text{D}_{cov}(X,X)=\\frac{1}{N}\\sum_{i,j}A^2_{i,j} $$$$ Code : Insert your favorite distance calculation d(X,Y) d(X,Y) # distance covariance d_cov = np . sum ( D_x * D_y ) # distance correlation d_corr = np . sum ( D_x * D_y ) / np . linalg . norm ( D_x ) / np . linalg . norm ( D_y ) Tip : In a paper that noone understands , Sejdinovic et. al. (2013) found that the HSIC method generalizes the distance covariance measure except that it has a non-linear transformation, i.e. a kernel function. So by extension, the Centered Kernel Alignment method (the normalized version of HSIC) generalizes the distance correlation method. This arxiv paper does a much better job at explaining the similarities between the two methods. Also, see this masters thesis [ PDF ] which does the proofs for the equivalence of HSIC and the distance covariance.","title":"Distance Correlation"},{"location":"appendix/similarity/distance/#energy-distance","text":"This is the statistical distance between probability distribtions. d^2(\\mathbb{P,Q}) = 2 \\mathbb{E}\\left[ ||X-Y|| \\right] - \\mathbb{E}\\left[ ||X-X'|| \\right] - \\mathbb{E}\\left[ ||Y-Y'|| \\right] d^2(\\mathbb{P,Q}) = 2 \\mathbb{E}\\left[ ||X-Y|| \\right] - \\mathbb{E}\\left[ ||X-X'|| \\right] - \\mathbb{E}\\left[ ||Y-Y'|| \\right] if we relax the constraints a bit so that we are in metric spaces, we get: d^2(\\mathbb{\\mu,\\nu}) = 2 \\mathbb{E}\\left[ d(X, Y) \\right] - \\mathbb{E}\\left[ d(X, X') \\right] - \\mathbb{E}\\left[ d(Y, Y') \\right] d^2(\\mathbb{\\mu,\\nu}) = 2 \\mathbb{E}\\left[ d(X, Y) \\right] - \\mathbb{E}\\left[ d(X, X') \\right] - \\mathbb{E}\\left[ d(Y, Y') \\right] Code : Insert your favorite distance calculation d(X,Y) d(X,Y) d_xx = pairwise_distances ( X ) d_yy = pairwise_distances ( Y ) d_xy = pairwise_distances ( X , Y ) # calculate the mean of everything energy_dist = 2 * np . mean ( d_xy ) - np . mean ( d_xx ) - np . mean ( d_yy ) Tip : See this masters thesis [ PDF ] which does the proofs for the equivalence of MMD and the energy distance.","title":"Energy Distance"},{"location":"appendix/similarity/distance/#other-distances","text":"I've just focused on the common distance metrics that capture important attributes. There are some examples like the Manhattan distance which ... and the Haversine function which is geocentric. In addition, instead of calculating the entire distance matrix, we can only look at the k k -Nearest Neighbors which enforces sparsity. Standard Distances Haversine Distances","title":"Other Distances"},{"location":"appendix/similarity/distance/#references","text":"Software - Distance Correlation and related E-Statistics in Python Euclidean Distance Matrices: Essential Theory, Algorithms and Applications - arxiv","title":"References"},{"location":"appendix/similarity/distance/#fast-computations","text":"The Performance Impact of Vectorized Operations - blog Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - blog RBF Formaulas - code","title":"Fast Computations"},{"location":"appendix/similarity/hsic/","text":"HSIC \u00b6 We use the Hilbert-Schmidt Independence Criterion (HSIC) measure independence between two distributions. It involves constructing an appropriate kernel matrix for each dataset and then using the Frobenius Norm as a way to \"summarize\" the variability of the data. Often times the motivation for this method is lost in the notorious paper of Arthur Gretton (the creator of the method), but actually, this idea was developed long before him with ideas from a covariance matrix perspective. Below are my notes for how to get from a simple covariance matrix to the HSIC method and similar ones. Motivation \u00b6 A very common mechanism to measure the differences between datasets is to measure the variability. The easiest way is the measure the covariance between the two datasets. However, this is limited to datasets with linear relationships and with not many outliers. Anscombe's classic dataset is an example where we have datasets with the same mean and standard deviation. Case I Case II Case III Case IV This means measures like the covariance and correlation become useless because they will yield the same result. This requires us to have more robust methods that take into account the non-linear relationships that are clearly present within the data. That...or to do some really good preprocessing to make models easier. Recap - Summarizing Multivariate Information \u00b6 Let's have the two distributions \\mathcal{X} \\in \\mathbb{R}^{D_x} \\mathcal{X} \\in \\mathbb{R}^{D_x} and \\mathcal{Y} \\in \\mathbb{R}^{D_y} \\mathcal{Y} \\in \\mathbb{R}^{D_y} . Let's also assume that we can sample (x,y) (x,y) from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . We can capture the second order dependencies between X X and Y Y by constructing a covariance matrix in the feature space defined as: C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} We can use the Hilbert-Schmidt Norm (HS-Norm) as a statistic to effectively summarize content within this covariance matrix. It's defined as: ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] Note that this term is zero iff X X and Y Y are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value that summarizes the structure of our data. And also just like the correlation, we can also do a normalization scheme that allows us to have an interpretable scalar value. This is similar to the correlation coefficient except it can now be applied to multi-dimensional data. \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} Samples versus Features \u00b6 One interesting connection is that using the HS norm in the feature space is the sample thing as using it in the sample space. \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} Comparing Features is the same as comparing samples! Note : This is very similar to the dual versus sample space that is often mentioned in the kernel literature. So our equations before will change slightly in notation as we are constructing different matrices. But in the end, they will have the same output. This includes the correlation coefficient \\rho \\rho . \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} Kernel Trick \u00b6 So now, we have only had a linear dot-similarity in the sample space of \\mathcal{X} \\mathcal{X} and \\mathcal{Y} \\mathcal{Y} . This is good but we can easily extend this to a non-linear transformation where we add an additional function \\psi \\psi for each of the kernel functions. \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} Let's assume there exists a nonlinear mapping from our data space to the Hilbert space. So \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} and \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} . We also assume that there is a representation of this mapping via the dot product between the features of the data space; i.e. K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle and K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle . So now the data matrices are \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} and \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} . So we can take the kernelized version of the cross covariance mapping as defined for the covariance matrix: \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} Now after a bit of simplication, we end up with the HSIC-Norm: \\begin{aligned} \\text{HSIC}(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) &= \\text{Tr}(K_{\\mathbf{x}}K_{\\mathbf{y}}) \\\\ \\end{aligned} \\begin{aligned} \\text{HSIC}(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) &= \\text{Tr}(K_{\\mathbf{x}}K_{\\mathbf{y}}) \\\\ \\end{aligned} Details Proof \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} Full Expression Using the same argument as above, we can also define a cross covariance matrix of the form: C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right] C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right] where \\otimes \\otimes is the tensor product, \\mu_x, \\mu_y \\mu_x, \\mu_y are the expecations of the mappings \\mathbb{E}_x [\\phi (x)] \\mathbb{E}_x [\\phi (x)] , \\mathbb{E}_y[\\psi(y)] \\mathbb{E}_y[\\psi(y)] respectively. The HSIC is the cross-covariance operator described above and can be expressed in terms of kernels. \\begin{aligned} \\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) &= ||C_{xy}||_{\\mathcal{H}}^2 \\\\ &= \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] \\\\ &+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right] \\\\ &- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right] \\end{aligned} \\begin{aligned} \\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) &= ||C_{xy}||_{\\mathcal{H}}^2 \\\\ &= \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] \\\\ &+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right] \\\\ &- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right] \\end{aligned} where \\mathbb{E}_{xx'yy'} \\mathbb{E}_{xx'yy'} is the expectation over both (x,y) \\sim \\mathbb{P}_{xy} (x,y) \\sim \\mathbb{P}_{xy} and we assume that (x',y') (x',y') can be sampled independently from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) between two kernel matrics that correctly model your data. This boils down to computing the trace of the matrix multiplication of two matrices: tr(K_x^\\top K_y) tr(K_x^\\top K_y) . So in algorithmically that is hsic_score = np . trace ( K_x . T @ K_y ) Notice that this is a 3-part operation. So, of course, we can refactor this to be much easier. A faster way to do this is: hsic_score = np . sum ( K_x * K_y ) This can be orders of magnitude faster because it is a much cheaper operation to compute elementwise products than a sum. And for fun, we can even use the einsum notation. hsic_score = np . einsum ( \"ji,ij->\" , K_x , K_y ) Centering \u00b6 A very important but subtle point is that the method with kernels assumes that your data is centered in the kernel space. This isn't necessarily true. Fortunately it is easy to do so. HK_xH = \\tilde{K}_x HK_xH = \\tilde{K}_x where H H is your centering matrix. Normalizing your inputs does not equal centering your kernel matrix. Details Full Expression We assume that the kernel function \\psi(x_i) \\psi(x_i) has a zero mean like so: \\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r) \\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r) This holds if the covariance matrix is computed from \\psi(x_i) \\psi(x_i) . So the kernel matrix K_{ij}=\\psi(x_i)^\\top \\psi(x_j) K_{ij}=\\psi(x_i)^\\top \\psi(x_j) needs to be replaced with \\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s) \\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s) where \\tilde{K}_{ij} \\tilde{K}_{ij} is: \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} Code On a more practical note, this can be done easily by: H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top H = np . eye ( n_samples ) - ( 1 / n_samples ) * np . ones ( n_samples , n_samples ) Refactor There is also a function in the scikit-learn library which does it for you. from sklearn.preprocessing import KernelCenterer K_centered = KernelCenterer () . fit_transform ( K ) Correlation \u00b6 So above is the entire motivation behind HSIC as a non-linear covariance measure. But there is the obvious extension that we need to do: as a similarity measure (i.e. a correlation). HSIC suffers from the same issue as a covariance measure: it is difficult to interpret. HSIC's strongest factor is that it can be used for independence testing. However, as a similarity measure, it violates some key criteria that we need: invariant to scaling and interpretability (bounded between 0-1). Recall the HSIC formula: A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} Below is the HSIC term that is normalized by the norm of the \\text{cKA}(\\mathbf{xy})=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} \\text{cKA}(\\mathbf{xy})=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} This is known as Centered Kernel Alignment in the literature. They add a normalization term to deal with some of the shortcomings of the original Kernel Alignment algorithm which had some benefits e.g. a way to cancel out unbalanced class effects. In relation to HSICThe improvement over the original algorithm seems minor but there is a critical difference. Without the centering, the alignment does not correlate well to the performance of the learning machine. There is Original Kernel Alignment The original kernel alignment method had the normalization factor but the matrices were not centered. A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} The alignment can be seen as a similarity score based on the cosine of the angle. For arbitrary matrices, this score ranges between -1 and 1. But using positive semidefinite Gram matrices, the score is lower-bounded by 0. This method was introduced before the HSIC method was introduced by Gretton. However, because the kernel matrices were not centered, there were some serious problems when trying to use it for measuring similarity: You could literally get any number between 0-1 if with parameters. So for a simple 1D linear dataset which should have a correlation of 1, I could get any number between 0 and 1 if I just change the length scale slightly. The HSIC and the CKA were much more robust than this method so I would avoid it. Connections \u00b6 Maximum Mean Discrepency \u00b6 HSIC \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) = \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) = \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) where H H is the centering matrix H=I_n-\\frac{1}{n}1_n1_n^\\top H=I_n-\\frac{1}{n}1_n1_n^\\top . \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}_k(\\mathbb{P}) = \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) = \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Information Theory Measures \u00b6 Kullback-Leibler Divergence D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx Mutual Information I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) Future Outlook \u00b6 Advantages Sample Space - Nice for High Dimensional Problems w/ a low number of samples HSIC can estimate dependence between variables of different dimensions Very flexible: lots of ways to create kernel matices Disadvantages Computationally demanding for large scale problems Non-iid samples, e.g. speech or images Tuning Kernel parameters Why the HS norm? Resources \u00b6 Presentations HSIC, A Measure of Independence - Szabo (2018) Measuring Independence with Kernels - Gustau Literature Review \u00b6 An Overview of Kernel Alignment and its Applications - Wang et al (2012) - PDF This goes over the literature of the kernel alignment method as well as some applications it has been used it. Applications \u00b6 Kerneel Target Alignment Parameter: A New Modelability for Regression Tasks - Marcou et al (2016) - Paper Brain Activity Patterns - Paper Scaling - Paper Textbooks \u00b6 Kernel Methods for Digital Processing - Book","title":"HSIC"},{"location":"appendix/similarity/hsic/#hsic","text":"We use the Hilbert-Schmidt Independence Criterion (HSIC) measure independence between two distributions. It involves constructing an appropriate kernel matrix for each dataset and then using the Frobenius Norm as a way to \"summarize\" the variability of the data. Often times the motivation for this method is lost in the notorious paper of Arthur Gretton (the creator of the method), but actually, this idea was developed long before him with ideas from a covariance matrix perspective. Below are my notes for how to get from a simple covariance matrix to the HSIC method and similar ones.","title":"HSIC"},{"location":"appendix/similarity/hsic/#motivation","text":"A very common mechanism to measure the differences between datasets is to measure the variability. The easiest way is the measure the covariance between the two datasets. However, this is limited to datasets with linear relationships and with not many outliers. Anscombe's classic dataset is an example where we have datasets with the same mean and standard deviation. Case I Case II Case III Case IV This means measures like the covariance and correlation become useless because they will yield the same result. This requires us to have more robust methods that take into account the non-linear relationships that are clearly present within the data. That...or to do some really good preprocessing to make models easier.","title":"Motivation"},{"location":"appendix/similarity/hsic/#recap-summarizing-multivariate-information","text":"Let's have the two distributions \\mathcal{X} \\in \\mathbb{R}^{D_x} \\mathcal{X} \\in \\mathbb{R}^{D_x} and \\mathcal{Y} \\in \\mathbb{R}^{D_y} \\mathcal{Y} \\in \\mathbb{R}^{D_y} . Let's also assume that we can sample (x,y) (x,y) from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . We can capture the second order dependencies between X X and Y Y by constructing a covariance matrix in the feature space defined as: C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} We can use the Hilbert-Schmidt Norm (HS-Norm) as a statistic to effectively summarize content within this covariance matrix. It's defined as: ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] Note that this term is zero iff X X and Y Y are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value that summarizes the structure of our data. And also just like the correlation, we can also do a normalization scheme that allows us to have an interpretable scalar value. This is similar to the correlation coefficient except it can now be applied to multi-dimensional data. \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}}","title":"Recap - Summarizing Multivariate Information"},{"location":"appendix/similarity/hsic/#samples-versus-features","text":"One interesting connection is that using the HS norm in the feature space is the sample thing as using it in the sample space. \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} Comparing Features is the same as comparing samples! Note : This is very similar to the dual versus sample space that is often mentioned in the kernel literature. So our equations before will change slightly in notation as we are constructing different matrices. But in the end, they will have the same output. This includes the correlation coefficient \\rho \\rho . \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}}","title":"Samples versus Features"},{"location":"appendix/similarity/hsic/#kernel-trick","text":"So now, we have only had a linear dot-similarity in the sample space of \\mathcal{X} \\mathcal{X} and \\mathcal{Y} \\mathcal{Y} . This is good but we can easily extend this to a non-linear transformation where we add an additional function \\psi \\psi for each of the kernel functions. \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} Let's assume there exists a nonlinear mapping from our data space to the Hilbert space. So \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} and \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} . We also assume that there is a representation of this mapping via the dot product between the features of the data space; i.e. K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle and K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle . So now the data matrices are \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} and \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} . So we can take the kernelized version of the cross covariance mapping as defined for the covariance matrix: \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} Now after a bit of simplication, we end up with the HSIC-Norm: \\begin{aligned} \\text{HSIC}(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) &= \\text{Tr}(K_{\\mathbf{x}}K_{\\mathbf{y}}) \\\\ \\end{aligned} \\begin{aligned} \\text{HSIC}(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) &= \\text{Tr}(K_{\\mathbf{x}}K_{\\mathbf{y}}) \\\\ \\end{aligned} Details Proof \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} Full Expression Using the same argument as above, we can also define a cross covariance matrix of the form: C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right] C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right] where \\otimes \\otimes is the tensor product, \\mu_x, \\mu_y \\mu_x, \\mu_y are the expecations of the mappings \\mathbb{E}_x [\\phi (x)] \\mathbb{E}_x [\\phi (x)] , \\mathbb{E}_y[\\psi(y)] \\mathbb{E}_y[\\psi(y)] respectively. The HSIC is the cross-covariance operator described above and can be expressed in terms of kernels. \\begin{aligned} \\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) &= ||C_{xy}||_{\\mathcal{H}}^2 \\\\ &= \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] \\\\ &+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right] \\\\ &- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right] \\end{aligned} \\begin{aligned} \\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) &= ||C_{xy}||_{\\mathcal{H}}^2 \\\\ &= \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] \\\\ &+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right] \\\\ &- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right] \\end{aligned} where \\mathbb{E}_{xx'yy'} \\mathbb{E}_{xx'yy'} is the expectation over both (x,y) \\sim \\mathbb{P}_{xy} (x,y) \\sim \\mathbb{P}_{xy} and we assume that (x',y') (x',y') can be sampled independently from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) between two kernel matrics that correctly model your data. This boils down to computing the trace of the matrix multiplication of two matrices: tr(K_x^\\top K_y) tr(K_x^\\top K_y) . So in algorithmically that is hsic_score = np . trace ( K_x . T @ K_y ) Notice that this is a 3-part operation. So, of course, we can refactor this to be much easier. A faster way to do this is: hsic_score = np . sum ( K_x * K_y ) This can be orders of magnitude faster because it is a much cheaper operation to compute elementwise products than a sum. And for fun, we can even use the einsum notation. hsic_score = np . einsum ( \"ji,ij->\" , K_x , K_y )","title":"Kernel Trick"},{"location":"appendix/similarity/hsic/#centering","text":"A very important but subtle point is that the method with kernels assumes that your data is centered in the kernel space. This isn't necessarily true. Fortunately it is easy to do so. HK_xH = \\tilde{K}_x HK_xH = \\tilde{K}_x where H H is your centering matrix. Normalizing your inputs does not equal centering your kernel matrix. Details Full Expression We assume that the kernel function \\psi(x_i) \\psi(x_i) has a zero mean like so: \\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r) \\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r) This holds if the covariance matrix is computed from \\psi(x_i) \\psi(x_i) . So the kernel matrix K_{ij}=\\psi(x_i)^\\top \\psi(x_j) K_{ij}=\\psi(x_i)^\\top \\psi(x_j) needs to be replaced with \\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s) \\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s) where \\tilde{K}_{ij} \\tilde{K}_{ij} is: \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} Code On a more practical note, this can be done easily by: H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top H = np . eye ( n_samples ) - ( 1 / n_samples ) * np . ones ( n_samples , n_samples ) Refactor There is also a function in the scikit-learn library which does it for you. from sklearn.preprocessing import KernelCenterer K_centered = KernelCenterer () . fit_transform ( K )","title":"Centering"},{"location":"appendix/similarity/hsic/#correlation","text":"So above is the entire motivation behind HSIC as a non-linear covariance measure. But there is the obvious extension that we need to do: as a similarity measure (i.e. a correlation). HSIC suffers from the same issue as a covariance measure: it is difficult to interpret. HSIC's strongest factor is that it can be used for independence testing. However, as a similarity measure, it violates some key criteria that we need: invariant to scaling and interpretability (bounded between 0-1). Recall the HSIC formula: A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} Below is the HSIC term that is normalized by the norm of the \\text{cKA}(\\mathbf{xy})=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} \\text{cKA}(\\mathbf{xy})=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} This is known as Centered Kernel Alignment in the literature. They add a normalization term to deal with some of the shortcomings of the original Kernel Alignment algorithm which had some benefits e.g. a way to cancel out unbalanced class effects. In relation to HSICThe improvement over the original algorithm seems minor but there is a critical difference. Without the centering, the alignment does not correlate well to the performance of the learning machine. There is Original Kernel Alignment The original kernel alignment method had the normalization factor but the matrices were not centered. A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} The alignment can be seen as a similarity score based on the cosine of the angle. For arbitrary matrices, this score ranges between -1 and 1. But using positive semidefinite Gram matrices, the score is lower-bounded by 0. This method was introduced before the HSIC method was introduced by Gretton. However, because the kernel matrices were not centered, there were some serious problems when trying to use it for measuring similarity: You could literally get any number between 0-1 if with parameters. So for a simple 1D linear dataset which should have a correlation of 1, I could get any number between 0 and 1 if I just change the length scale slightly. The HSIC and the CKA were much more robust than this method so I would avoid it.","title":"Correlation"},{"location":"appendix/similarity/hsic/#connections","text":"","title":"Connections"},{"location":"appendix/similarity/hsic/#maximum-mean-discrepency","text":"HSIC \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) = \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) = \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) where H H is the centering matrix H=I_n-\\frac{1}{n}1_n1_n^\\top H=I_n-\\frac{1}{n}1_n1_n^\\top . \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}_k(\\mathbb{P}) = \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) = \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m)","title":"Maximum Mean Discrepency"},{"location":"appendix/similarity/hsic/#information-theory-measures","text":"Kullback-Leibler Divergence D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx Mutual Information I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right)","title":"Information Theory Measures"},{"location":"appendix/similarity/hsic/#future-outlook","text":"Advantages Sample Space - Nice for High Dimensional Problems w/ a low number of samples HSIC can estimate dependence between variables of different dimensions Very flexible: lots of ways to create kernel matices Disadvantages Computationally demanding for large scale problems Non-iid samples, e.g. speech or images Tuning Kernel parameters Why the HS norm?","title":"Future Outlook"},{"location":"appendix/similarity/hsic/#resources","text":"Presentations HSIC, A Measure of Independence - Szabo (2018) Measuring Independence with Kernels - Gustau","title":"Resources"},{"location":"appendix/similarity/hsic/#literature-review","text":"An Overview of Kernel Alignment and its Applications - Wang et al (2012) - PDF This goes over the literature of the kernel alignment method as well as some applications it has been used it.","title":"Literature Review"},{"location":"appendix/similarity/hsic/#applications","text":"Kerneel Target Alignment Parameter: A New Modelability for Regression Tasks - Marcou et al (2016) - Paper Brain Activity Patterns - Paper Scaling - Paper","title":"Applications"},{"location":"appendix/similarity/hsic/#textbooks","text":"Kernel Methods for Digital Processing - Book","title":"Textbooks"},{"location":"appendix/similarity/mmd/","text":"Maximum Mean Discrepancy (MMD) \u00b6 The Maximum Mean Discrepency (MMD) measurement is a distance measure between feature means. Idea \u00b6 This is done by taking the between dataset similarity of each of the datasets individually and then taking the cross-dataset similarity. Formulation \u00b6 \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} Proof \u00b6 \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} Kernel Trick \u00b6 Let k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} : \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} Source : Stackoverflow Empirical Estimate \u00b6 $$ \\begin{aligned} \\widehat{\\text{MMD}}^2 &= \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(x_i, x_j) + \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(y_i, y_j) - \\frac{2}{n^2} \\sum_{i,j}^N k(x_i, y_j) \\end{aligned} $$ Code \u00b6 # Term 1 c1 = 1 / ( m * ( m - 1 )) A = np . sum ( Kxx - np . diag ( np . diagonal ( Kxx ))) # Term II c2 = 1 / ( n * ( n - 1 )) B = np . sum ( Kyy - np . diag ( np . diagonal ( Kyy ))) # Term III c3 = 1 / ( m * n ) C = np . sum ( Kxy ) # estimate MMD mmd_est = c1 * A + c2 * B - 2 * c3 * C Sources Douglas Sutherland HSIC BottleNeck Eugene Belilovsky Equivalence \u00b6 Euclidean Distance \u00b6 Let's assume that \\mathbf{x,y} \\mathbf{x,y} come from two distributions, so \\mathbf{x} \\sim \\mathbb{P} \\mathbf{x} \\sim \\mathbb{P} and \\mathbf{x} \\sim \\mathbb{Q} \\mathbf{x} \\sim \\mathbb{Q} . We can write the MMD as norm of the difference between the means in feature spaces. \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F Empirical Estimation This is only good for Gaussian kernels. But we can empirically estimate this as: \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) where G is the Gaussian kernel with a standard deviation of \\sigma \\sigma . Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives - Principe KL-Divergence \u00b6 This has some alternative interpretation that is similar to the Kullback-Leibler Divergence. Remember, the MMD is the distance between the joint distribution P=\\mathbb{P}_{x,y} P=\\mathbb{P}_{x,y} and the product of the marginals Q=\\mathbb{P}_x\\mathbb{P}_y Q=\\mathbb{P}_x\\mathbb{P}_y . \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| This is similar to the KLD which has a similar interpretation in terms of the Mutual information: the difference between the joint distribution P(x,y) P(x,y) and the product of the marginal distributions p_x p_y p_x p_y . I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] Variation of Information \u00b6 In informaiton theory, we have a measure of variation of information (aka the shared information distance) which a simple linear expression involving mutual information. However, it is a valid distance metric that obeys the triangle inequality. \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) where H(X) H(X) is the entropy of \\mathcal{X} \\mathcal{X} and I(X,Y) I(X,Y) is the mutual information between \\mathcal{X,Y} \\mathcal{X,Y} . Properties \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) HSIC \u00b6 Similar to the KLD interpretation, this formulation is equivalent to the Hilbert-Schmidt Independence Criterion. If we think of the MMD distance between the joint distribution & the product of the marginals then we get the HSIC measure. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} which is the exact formulation for HSIC. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} where we have some equivalences. Proof \u00b6 First we need to do some equivalences. First the norm of two feature spaces \\varphi(\\cdot, \\cdot) \\varphi(\\cdot, \\cdot) is the same as the kernel of the cross product. \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} The second is the equivalence of the kernel of the cross-product of \\mathcal{X,Y} \\mathcal{X,Y} is equal to the multiplication of the respective kernels for \\mathcal{X,Y} \\mathcal{X,Y} . So, let's say we have a kernel k k on dataset \\mathcal{X} \\mathcal{X} in the feature space \\mathcal{F} \\mathcal{F} . We also have a kernel k k on dataset \\mathcal{Y} \\mathcal{Y} with feature space \\mathcal{G} \\mathcal{G} . The kernel k k on the \\mathcal{X,Y} \\mathcal{X,Y} pairs are similar. \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} Resources \u00b6 Arthur Grettons Lectures - lec 5 | lec 2 Notes on Mean Embedding What is RKHS The Maximum Mean Discrepancy for Training Generative Adversarial Networks From Zero to RKHS RKHS in ML Similarity Loss (TF2) - code MMD Smola","title":"MMD"},{"location":"appendix/similarity/mmd/#maximum-mean-discrepancy-mmd","text":"The Maximum Mean Discrepency (MMD) measurement is a distance measure between feature means.","title":"Maximum Mean Discrepancy (MMD)"},{"location":"appendix/similarity/mmd/#idea","text":"This is done by taking the between dataset similarity of each of the datasets individually and then taking the cross-dataset similarity.","title":"Idea"},{"location":"appendix/similarity/mmd/#formulation","text":"\\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned}","title":"Formulation"},{"location":"appendix/similarity/mmd/#proof","text":"\\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned}","title":"Proof"},{"location":"appendix/similarity/mmd/#kernel-trick","text":"Let k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} : \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} Source : Stackoverflow","title":"Kernel Trick"},{"location":"appendix/similarity/mmd/#empirical-estimate","text":"$$ \\begin{aligned} \\widehat{\\text{MMD}}^2 &= \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(x_i, x_j) + \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(y_i, y_j) - \\frac{2}{n^2} \\sum_{i,j}^N k(x_i, y_j) \\end{aligned} $$","title":"Empirical Estimate"},{"location":"appendix/similarity/mmd/#code","text":"# Term 1 c1 = 1 / ( m * ( m - 1 )) A = np . sum ( Kxx - np . diag ( np . diagonal ( Kxx ))) # Term II c2 = 1 / ( n * ( n - 1 )) B = np . sum ( Kyy - np . diag ( np . diagonal ( Kyy ))) # Term III c3 = 1 / ( m * n ) C = np . sum ( Kxy ) # estimate MMD mmd_est = c1 * A + c2 * B - 2 * c3 * C Sources Douglas Sutherland HSIC BottleNeck Eugene Belilovsky","title":"Code"},{"location":"appendix/similarity/mmd/#equivalence","text":"","title":"Equivalence"},{"location":"appendix/similarity/mmd/#euclidean-distance","text":"Let's assume that \\mathbf{x,y} \\mathbf{x,y} come from two distributions, so \\mathbf{x} \\sim \\mathbb{P} \\mathbf{x} \\sim \\mathbb{P} and \\mathbf{x} \\sim \\mathbb{Q} \\mathbf{x} \\sim \\mathbb{Q} . We can write the MMD as norm of the difference between the means in feature spaces. \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F Empirical Estimation This is only good for Gaussian kernels. But we can empirically estimate this as: \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) where G is the Gaussian kernel with a standard deviation of \\sigma \\sigma . Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives - Principe","title":"Euclidean Distance"},{"location":"appendix/similarity/mmd/#kl-divergence","text":"This has some alternative interpretation that is similar to the Kullback-Leibler Divergence. Remember, the MMD is the distance between the joint distribution P=\\mathbb{P}_{x,y} P=\\mathbb{P}_{x,y} and the product of the marginals Q=\\mathbb{P}_x\\mathbb{P}_y Q=\\mathbb{P}_x\\mathbb{P}_y . \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| This is similar to the KLD which has a similar interpretation in terms of the Mutual information: the difference between the joint distribution P(x,y) P(x,y) and the product of the marginal distributions p_x p_y p_x p_y . I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right]","title":"KL-Divergence"},{"location":"appendix/similarity/mmd/#variation-of-information","text":"In informaiton theory, we have a measure of variation of information (aka the shared information distance) which a simple linear expression involving mutual information. However, it is a valid distance metric that obeys the triangle inequality. \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) where H(X) H(X) is the entropy of \\mathcal{X} \\mathcal{X} and I(X,Y) I(X,Y) is the mutual information between \\mathcal{X,Y} \\mathcal{X,Y} . Properties \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z)","title":"Variation of Information"},{"location":"appendix/similarity/mmd/#hsic","text":"Similar to the KLD interpretation, this formulation is equivalent to the Hilbert-Schmidt Independence Criterion. If we think of the MMD distance between the joint distribution & the product of the marginals then we get the HSIC measure. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} which is the exact formulation for HSIC. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} where we have some equivalences.","title":"HSIC"},{"location":"appendix/similarity/mmd/#proof_1","text":"First we need to do some equivalences. First the norm of two feature spaces \\varphi(\\cdot, \\cdot) \\varphi(\\cdot, \\cdot) is the same as the kernel of the cross product. \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} The second is the equivalence of the kernel of the cross-product of \\mathcal{X,Y} \\mathcal{X,Y} is equal to the multiplication of the respective kernels for \\mathcal{X,Y} \\mathcal{X,Y} . So, let's say we have a kernel k k on dataset \\mathcal{X} \\mathcal{X} in the feature space \\mathcal{F} \\mathcal{F} . We also have a kernel k k on dataset \\mathcal{Y} \\mathcal{Y} with feature space \\mathcal{G} \\mathcal{G} . The kernel k k on the \\mathcal{X,Y} \\mathcal{X,Y} pairs are similar. \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned}","title":"Proof"},{"location":"appendix/similarity/mmd/#resources","text":"Arthur Grettons Lectures - lec 5 | lec 2 Notes on Mean Embedding What is RKHS The Maximum Mean Discrepancy for Training Generative Adversarial Networks From Zero to RKHS RKHS in ML Similarity Loss (TF2) - code MMD Smola","title":"Resources"},{"location":"appendix/similarity/rv/","text":"RV Coefficient \u00b6 Lab: Colab Notebook Notation \u00b6 \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma \\in \\mathbb{R}^{N \\times N} \\Sigma \\in \\mathbb{R}^{N \\times N} is a covariance matrix. \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} \\Sigma_\\mathbf{y} \\Sigma_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} is the population covariance matrix between \\mathcal{X,Y} \\mathcal{X,Y} tr(\\cdot) tr(\\cdot) - the trace operator ||\\cdot||_\\mathcal{F} ||\\cdot||_\\mathcal{F} - Frobenius Norm ||\\cdot||_\\mathcal{HS} ||\\cdot||_\\mathcal{HS} - Hilbert-Schmidt Norm \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Single Variables \u00b6 Let's consider a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. Mean, Expectation \u00b6 The first order measurement is the mean. This is the expected/average value that we would expect from a r.v.. This results in a scalar value Empirical Estimate \u00b6 \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i Variance \u00b6 The first measure we need to consider is the variance. This is a measure of spread. Empirical Estimate \u00b6 \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} Code We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. # remove mean from data X_mu = X . mean ( axis = 0 ) # ensure it is 1D var = ( X - X_mu [:, None ]) . T @ ( X - X_mu [:, None ]) Covariance \u00b6 The first measure we need to consider is the covariance. This can be used for a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . This results in a scalar value , \\mathbb{R} \\mathbb{R} . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\\\ \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\\\ \\end{aligned} This will result in a scalar value \\mathbb{R}^+ \\mathbb{R}^+ that ranges from (-\\infty, \\infty) (-\\infty, \\infty) . This number is affected by scale so we can different values depending upon the scale of our data, i.e. \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Empirical Estimate \u00b6 We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) Code c_xy = X . T @ Y Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} This results in a scalar value \\mathbb{R} \\mathbb{R} that lies in between [-1, 1] [-1, 1] . When \\rho=-1 \\rho=-1 , there is a negative correlation and when \\rho=1 \\rho=1 , there is a positive correlation. When \\rho=0 \\rho=0 there is no correlation. Empirical Estimate \u00b6 So the formulation is: \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Root Mean Squared Error \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} Multi-Dimensional \u00b6 For all of these measures, we have been under the assumption that \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} . However, we may have the case where we have multivariate datasets in \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} . In this case, we need methods that can handle multivariate inputs. Variance \u00b6 Self-Covariance \u00b6 So now we are considering the case when we have multidimensional vectors. If we think of a variable X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} which represents a set of samples with multiple features. First let's consider the variance for a multidimensional variable. This is also known as the covariance because we are actually finding the cross-covariance between itself. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} To simplify the notation, we can write this as: \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data. Empirical Estimation \u00b6 This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} Code c_xy = X . T @ X Cross-Covariance \u00b6 We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} This results in a scalar value which represents the similarity between the samples. There are some key observations of this measure. Empirical Estimation \u00b6 This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Root Mean Squared Vector Difference \u00b6 A diagram for evaluating multiple aspects of model performance insimulating vector fields - Xu et. al. (2016) Summarizing Multi-Dimensional Information \u00b6 Recall that we now have self-covariance matrices \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} and cross-covariance matrices \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} which are \\mathbb{R}^{D \\times D} \\mathbb{R}^{D \\times D} . This is very useful as it captures the structure of the overall data. However, if we want to summarize the statistics, then we need some methods to do so. The matrix norm, in particular the Frobenius Norm (aka the Hilbert-Schmidt Norm) to effectively summarize content within this covariance matrix. It's defined as: ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) Essentially this is a measure of the covariance matrix power or \"essence\" through its eigenvalue decomposition. Note that this term is zero iff \\mathbf{x,y} \\mathbf{x,y} are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value in \\mathbb{R} \\mathbb{R} that summarizes the structure of our data. Congruence Coefficient \u00b6 Tip This was a term introduced by Burt (1948) with the name \"unadjusted correlation\". It's a measure of similarity between two multivariate datasets. Later the term \"congruence coefficient\" was coined by Tucker (1951) and Harman (1976). In the context of matrices, let's take summarize the cross-covariance matrix and then normalize this value by the self-covariance matrices. This results in: \\varphi (\\mathbf{x,y}) = \\frac{\\text{Tr}\\left( XY^\\top\\right)}{||XX^\\top||_F \\; || YY^\\top||_F} \\varphi (\\mathbf{x,y}) = \\frac{\\text{Tr}\\left( XY^\\top\\right)}{||XX^\\top||_F \\; || YY^\\top||_F} This results in the Congruence-Coefficient ( \\varphi \\varphi ) which is analogous to the Pearson correlation coefficient \\rho \\rho as a measure of similarity but it's in the sample space not the feature space. We assume that the data is column centered (aka we have removed the mean from the features). HS-norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations still cannot be captured as this is still a linear method. \\rho \\rho V Coefficient \u00b6 A similarity measure between two squared symmetric matrices (positive semi-definite matrices) used to analyize multivariate datasets; the cosine between matrices. TIP This term was introduced by Escoufier (1973) and Robert & Escoufier (1976). We can also consider the case where the correlations can be measured between samples and not between features. So we can create cross product matrices: \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} and \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} . Just like the feature space, we can use the Hilbert-Schmidt (HS) norm, ||\\cdot||_{F} ||\\cdot||_{F} to measure proximity. \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} And like the above mentioned \\rho V \\rho V , we can also calculate a correlation measure using the sample space. \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ &= \\frac{\\text{Tr}\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{Tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{Tr}\\left( \\mathbf{yy}^\\top \\right)^2}} \\end{aligned} \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ &= \\frac{\\text{Tr}\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{Tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{Tr}\\left( \\mathbf{yy}^\\top \\right)^2}} \\end{aligned} Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: tr(C_{xy}^\\top C_{xy}) tr(C_{xy}^\\top C_{xy}) . So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the sum operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () Refactor There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro) Equivalence \u00b6 It turns out, for the linear case, when using the Frobenius norm to summarize the pairwise comparisons, comparing features is the same as comparing samples. For example, the norm of the covariance operator for the features and samples are equivalent: ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F We get the same for the \\rho V \\rho V case. \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle_F}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle_F}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} So what does this mean? Well, either method is fine. But you should probably choose one depending upon the computational resources available. For example, if you have more samples than features, then choose the feature space representation. On the other hand, if you have more features than samples, then choose the sample space representation. Linear Only This method only works for the linear case. There are some nonlinear transformations (called kernels) that one can use, but those will yield different values between feature space and sample space. Extensions \u00b6 Many frameworks is a generalization of this as they attempt to maximize these quantities with some sort of constraint. PCA - maximum variance CCA - ... Multivariate Regression - minimum MSE Linear Discrimination - ... Mutual Information \u00b6 There is a mutual information interpretation. This measurement only captures the 1 st and 2 nd order moments of the distribution. This is as if we were approximating as a Gaussian distribution which can be described by its first and second moments. The mutual information can be calculated directly if the cross covariance and the self-covariance matrices are known. I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_{xx}||C_{yy}||} \\right) I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_{xx}||C_{yy}||} \\right) As we showed above, the term inside the log is simply the Pearson correlation coefficient \\rho \\rho . I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) References \u00b6 RV Coefficient and Congruence Coefficient PDF - Abdi (2007) A great document that really breaks down the differences between the RV coefficient and the Congruence coefficient. Tucker's Congruence Coefficient as a Meaningful Index of Factor Similarity PDF - Lorenzo-Seva & Berge (2006) - Methodology More details relating to the Congruences coefficient and some reasoning as why one would use it. Measuring Multivariate Association and Beyond PDF - Josse & Holmes (2016) - Statistics Surveys An Excellent review for how we can get \\rho \\rho V-Coefficients and some of the modified versions. They also go into some other distance measures like the Graph, Mantel, Kernel and other. Average Distance of Random Points in a Unit Hypercube [ Blog ] - Martin Thoma A really nice blog post showing some empirical evidence for how these distance measures fail in high-dimensions. Supplementary \u00b6 Common Statistical Tests are Linear Models (or: How to Teach Stats) - Jonas Kristoffer Lindelov - notebook | rmarkdown Correlation vs Regression - Asim Jana - blog RealPython Numpy, SciPy and Pandas: Correlation with Python - blog Correlation and Lag for Signals - notebook Understanding the Covariance Matrix Numpy Vectorized method for computing covariance with population means Eric Marsden Modeling Correlations in Python Regression Analysis in Python Equivalences \u00b6 Using Summations \\rho V(X,Y) = \\frac{\\sum_{i,j} \\mathbf{x}_{i,j} \\mathbf{y}_{i,j}}{\\sqrt{\\sum_{i,j} \\mathbf{x}_{i,j}^2}\\sqrt{\\sum_{i,j} \\mathbf{y}_{i,j}^2}} \\rho V(X,Y) = \\frac{\\sum_{i,j} \\mathbf{x}_{i,j} \\mathbf{y}_{i,j}}{\\sqrt{\\sum_{i,j} \\mathbf{x}_{i,j}^2}\\sqrt{\\sum_{i,j} \\mathbf{y}_{i,j}^2}} Using Trace notation \\rho V(X,Y) = \\frac{\\text{Tr} \\left( XY^\\top\\right)}{\\text{Tr} \\left( XX^\\top\\right)\\text{Tr} \\left( YY^\\top\\right)} \\rho V(X,Y) = \\frac{\\text{Tr} \\left( XY^\\top\\right)}{\\text{Tr} \\left( XX^\\top\\right)\\text{Tr} \\left( YY^\\top\\right)}","title":"RV Coefficient"},{"location":"appendix/similarity/rv/#rv-coefficient","text":"Lab: Colab Notebook","title":"RV Coefficient"},{"location":"appendix/similarity/rv/#notation","text":"\\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma \\in \\mathbb{R}^{N \\times N} \\Sigma \\in \\mathbb{R}^{N \\times N} is a covariance matrix. \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} \\Sigma_\\mathbf{y} \\Sigma_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} is the population covariance matrix between \\mathcal{X,Y} \\mathcal{X,Y} tr(\\cdot) tr(\\cdot) - the trace operator ||\\cdot||_\\mathcal{F} ||\\cdot||_\\mathcal{F} - Frobenius Norm ||\\cdot||_\\mathcal{HS} ||\\cdot||_\\mathcal{HS} - Hilbert-Schmidt Norm \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix.","title":"Notation"},{"location":"appendix/similarity/rv/#single-variables","text":"Let's consider a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature.","title":"Single Variables"},{"location":"appendix/similarity/rv/#mean-expectation","text":"The first order measurement is the mean. This is the expected/average value that we would expect from a r.v.. This results in a scalar value","title":"Mean, Expectation"},{"location":"appendix/similarity/rv/#empirical-estimate","text":"\\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i","title":"Empirical Estimate"},{"location":"appendix/similarity/rv/#variance","text":"The first measure we need to consider is the variance. This is a measure of spread.","title":"Variance"},{"location":"appendix/similarity/rv/#empirical-estimate_1","text":"\\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} Code We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. # remove mean from data X_mu = X . mean ( axis = 0 ) # ensure it is 1D var = ( X - X_mu [:, None ]) . T @ ( X - X_mu [:, None ])","title":"Empirical Estimate"},{"location":"appendix/similarity/rv/#covariance","text":"The first measure we need to consider is the covariance. This can be used for a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . This results in a scalar value , \\mathbb{R} \\mathbb{R} . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\\\ \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\\\ \\end{aligned} This will result in a scalar value \\mathbb{R}^+ \\mathbb{R}^+ that ranges from (-\\infty, \\infty) (-\\infty, \\infty) . This number is affected by scale so we can different values depending upon the scale of our data, i.e. \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Covariance"},{"location":"appendix/similarity/rv/#empirical-estimate_2","text":"We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) Code c_xy = X . T @ Y","title":"Empirical Estimate"},{"location":"appendix/similarity/rv/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} This results in a scalar value \\mathbb{R} \\mathbb{R} that lies in between [-1, 1] [-1, 1] . When \\rho=-1 \\rho=-1 , there is a negative correlation and when \\rho=1 \\rho=1 , there is a positive correlation. When \\rho=0 \\rho=0 there is no correlation.","title":"Correlation"},{"location":"appendix/similarity/rv/#empirical-estimate_3","text":"So the formulation is: \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Empirical Estimate"},{"location":"appendix/similarity/rv/#root-mean-squared-error","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2}","title":"Root Mean Squared Error"},{"location":"appendix/similarity/rv/#multi-dimensional","text":"For all of these measures, we have been under the assumption that \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} . However, we may have the case where we have multivariate datasets in \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} . In this case, we need methods that can handle multivariate inputs.","title":"Multi-Dimensional"},{"location":"appendix/similarity/rv/#variance_1","text":"","title":"Variance"},{"location":"appendix/similarity/rv/#self-covariance","text":"So now we are considering the case when we have multidimensional vectors. If we think of a variable X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} which represents a set of samples with multiple features. First let's consider the variance for a multidimensional variable. This is also known as the covariance because we are actually finding the cross-covariance between itself. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} To simplify the notation, we can write this as: \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data.","title":"Self-Covariance"},{"location":"appendix/similarity/rv/#empirical-estimation","text":"This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} Code c_xy = X . T @ X","title":"Empirical Estimation"},{"location":"appendix/similarity/rv/#cross-covariance","text":"We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} This results in a scalar value which represents the similarity between the samples. There are some key observations of this measure.","title":"Cross-Covariance"},{"location":"appendix/similarity/rv/#empirical-estimation_1","text":"This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features!","title":"Empirical Estimation"},{"location":"appendix/similarity/rv/#root-mean-squared-vector-difference","text":"A diagram for evaluating multiple aspects of model performance insimulating vector fields - Xu et. al. (2016)","title":"Root Mean Squared Vector Difference"},{"location":"appendix/similarity/rv/#summarizing-multi-dimensional-information","text":"Recall that we now have self-covariance matrices \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} and cross-covariance matrices \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} which are \\mathbb{R}^{D \\times D} \\mathbb{R}^{D \\times D} . This is very useful as it captures the structure of the overall data. However, if we want to summarize the statistics, then we need some methods to do so. The matrix norm, in particular the Frobenius Norm (aka the Hilbert-Schmidt Norm) to effectively summarize content within this covariance matrix. It's defined as: ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) Essentially this is a measure of the covariance matrix power or \"essence\" through its eigenvalue decomposition. Note that this term is zero iff \\mathbf{x,y} \\mathbf{x,y} are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value in \\mathbb{R} \\mathbb{R} that summarizes the structure of our data.","title":"Summarizing Multi-Dimensional Information"},{"location":"appendix/similarity/rv/#congruence-coefficient","text":"Tip This was a term introduced by Burt (1948) with the name \"unadjusted correlation\". It's a measure of similarity between two multivariate datasets. Later the term \"congruence coefficient\" was coined by Tucker (1951) and Harman (1976). In the context of matrices, let's take summarize the cross-covariance matrix and then normalize this value by the self-covariance matrices. This results in: \\varphi (\\mathbf{x,y}) = \\frac{\\text{Tr}\\left( XY^\\top\\right)}{||XX^\\top||_F \\; || YY^\\top||_F} \\varphi (\\mathbf{x,y}) = \\frac{\\text{Tr}\\left( XY^\\top\\right)}{||XX^\\top||_F \\; || YY^\\top||_F} This results in the Congruence-Coefficient ( \\varphi \\varphi ) which is analogous to the Pearson correlation coefficient \\rho \\rho as a measure of similarity but it's in the sample space not the feature space. We assume that the data is column centered (aka we have removed the mean from the features). HS-norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations still cannot be captured as this is still a linear method.","title":"Congruence Coefficient"},{"location":"appendix/similarity/rv/#rhorhov-coefficient","text":"A similarity measure between two squared symmetric matrices (positive semi-definite matrices) used to analyize multivariate datasets; the cosine between matrices. TIP This term was introduced by Escoufier (1973) and Robert & Escoufier (1976). We can also consider the case where the correlations can be measured between samples and not between features. So we can create cross product matrices: \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} and \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} . Just like the feature space, we can use the Hilbert-Schmidt (HS) norm, ||\\cdot||_{F} ||\\cdot||_{F} to measure proximity. \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} And like the above mentioned \\rho V \\rho V , we can also calculate a correlation measure using the sample space. \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ &= \\frac{\\text{Tr}\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{Tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{Tr}\\left( \\mathbf{yy}^\\top \\right)^2}} \\end{aligned} \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ &= \\frac{\\text{Tr}\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{Tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{Tr}\\left( \\mathbf{yy}^\\top \\right)^2}} \\end{aligned} Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: tr(C_{xy}^\\top C_{xy}) tr(C_{xy}^\\top C_{xy}) . So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the sum operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () Refactor There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro)","title":"\\rho\\rhoV Coefficient"},{"location":"appendix/similarity/rv/#equivalence","text":"It turns out, for the linear case, when using the Frobenius norm to summarize the pairwise comparisons, comparing features is the same as comparing samples. For example, the norm of the covariance operator for the features and samples are equivalent: ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F We get the same for the \\rho V \\rho V case. \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle_F}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle_F}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} So what does this mean? Well, either method is fine. But you should probably choose one depending upon the computational resources available. For example, if you have more samples than features, then choose the feature space representation. On the other hand, if you have more features than samples, then choose the sample space representation. Linear Only This method only works for the linear case. There are some nonlinear transformations (called kernels) that one can use, but those will yield different values between feature space and sample space.","title":"Equivalence"},{"location":"appendix/similarity/rv/#extensions","text":"Many frameworks is a generalization of this as they attempt to maximize these quantities with some sort of constraint. PCA - maximum variance CCA - ... Multivariate Regression - minimum MSE Linear Discrimination - ...","title":"Extensions"},{"location":"appendix/similarity/rv/#mutual-information","text":"There is a mutual information interpretation. This measurement only captures the 1 st and 2 nd order moments of the distribution. This is as if we were approximating as a Gaussian distribution which can be described by its first and second moments. The mutual information can be calculated directly if the cross covariance and the self-covariance matrices are known. I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_{xx}||C_{yy}||} \\right) I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_{xx}||C_{yy}||} \\right) As we showed above, the term inside the log is simply the Pearson correlation coefficient \\rho \\rho . I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2)","title":"Mutual Information"},{"location":"appendix/similarity/rv/#references","text":"RV Coefficient and Congruence Coefficient PDF - Abdi (2007) A great document that really breaks down the differences between the RV coefficient and the Congruence coefficient. Tucker's Congruence Coefficient as a Meaningful Index of Factor Similarity PDF - Lorenzo-Seva & Berge (2006) - Methodology More details relating to the Congruences coefficient and some reasoning as why one would use it. Measuring Multivariate Association and Beyond PDF - Josse & Holmes (2016) - Statistics Surveys An Excellent review for how we can get \\rho \\rho V-Coefficients and some of the modified versions. They also go into some other distance measures like the Graph, Mantel, Kernel and other. Average Distance of Random Points in a Unit Hypercube [ Blog ] - Martin Thoma A really nice blog post showing some empirical evidence for how these distance measures fail in high-dimensions.","title":"References"},{"location":"appendix/similarity/rv/#supplementary","text":"Common Statistical Tests are Linear Models (or: How to Teach Stats) - Jonas Kristoffer Lindelov - notebook | rmarkdown Correlation vs Regression - Asim Jana - blog RealPython Numpy, SciPy and Pandas: Correlation with Python - blog Correlation and Lag for Signals - notebook Understanding the Covariance Matrix Numpy Vectorized method for computing covariance with population means Eric Marsden Modeling Correlations in Python Regression Analysis in Python","title":"Supplementary"},{"location":"appendix/similarity/rv/#equivalences","text":"Using Summations \\rho V(X,Y) = \\frac{\\sum_{i,j} \\mathbf{x}_{i,j} \\mathbf{y}_{i,j}}{\\sqrt{\\sum_{i,j} \\mathbf{x}_{i,j}^2}\\sqrt{\\sum_{i,j} \\mathbf{y}_{i,j}^2}} \\rho V(X,Y) = \\frac{\\sum_{i,j} \\mathbf{x}_{i,j} \\mathbf{y}_{i,j}}{\\sqrt{\\sum_{i,j} \\mathbf{x}_{i,j}^2}\\sqrt{\\sum_{i,j} \\mathbf{y}_{i,j}^2}} Using Trace notation \\rho V(X,Y) = \\frac{\\text{Tr} \\left( XY^\\top\\right)}{\\text{Tr} \\left( XX^\\top\\right)\\text{Tr} \\left( YY^\\top\\right)} \\rho V(X,Y) = \\frac{\\text{Tr} \\left( XY^\\top\\right)}{\\text{Tr} \\left( XX^\\top\\right)\\text{Tr} \\left( YY^\\top\\right)}","title":"Equivalences"},{"location":"appendix/similarity/taylor/","text":"Visualizing Similarities \u00b6 Motivation \u00b6 Visualizations: help find similarities between outputs stats are great, but visual uncertainty quantification Questions \u00b6 Which model is more similar to the reference/observations? Should we look at correlations across seasons or latitudes? Are there large discrepancies in the different outputs Current Ways \u00b6 Trend Plots often do not expose the comparison aspects... Scatter plots become impractical for many outputs Parallel Coordinate Plots are more practical, but only certain pairwise comparisons are possible Plots per ensemble - possible but it can be super cluttered Taylor Diagram - visualize several statistics simultaneously in a statistical metric space. Specific Statistics Mean, Variance, Correlation Box Plots (and variations) Cosine Similarity \u00b6 Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim Correlation \u00b6 There is a relationship between the cosine similarity and correlation coefficient \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} if \\rho(x,y) = 0 \\rho(x,y) = 0 , the spaces are orthogonal if \\rho(x,y) = 1 \\rho(x,y) = 1 , the spaces are equivalent Distances \u00b6 Law of cosines \u00b6 If we recall the law of cosines; an extension of the cosine angle formula but for all angles and sides of the triangle. c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta Notice that this forumala looks very similar to the euclidean distance formula shown above. If we do a simple rearrangement of the final term in the above equation to accommadate the correlation term \\rho(x,y) \\rho(x,y) , then we get d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} simplifying to: d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) This is actually equivalent to the law of cosines. Taylor Diagram \u00b6 The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. It simultaneously plots the standard deviation, the root mean square error and correlation between two variables. Statistics Metric Space \u00b6 They key is that it is possible to find a metric space for these quantities, based on the law of cosines. If you look closely, this identity looks like the cosine law of triangles.we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE as we have expressed above. \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) If we write out the full equation, we have the following: \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) The sides are as follows: a = \\sigma_{x} a = \\sigma_{x} - the standard deviation of x x b = \\sigma_{y} b = \\sigma_{y} - the standard deviation of y y \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example 1 - Intuition \u00b6 Figure III : An example Taylor diagram. Example 2 - Model Outputs \u00b6 Multi-Dimensional Data \u00b6 In the above examples, we assume that \\mathbf{x}, \\mathbf{y} \\mathbf{x}, \\mathbf{y} were both vectors of size \\mathbb{R}^{N \\times 1} \\mathbb{R}^{N \\times 1} . But what happens when we get datasets of size \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} ? Well, the above formulas can generalize using the inner product and the norm of the datasets. Figure I : A visual representation of the cosine similarity generalized to vectors. Distances \u00b6 We still get the same formulation as the above except now it is generalized to vectors. d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle Correlation \u00b6 Let \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} be the empirical covariance matrix between \\mathbf{x,y} \\mathbf{x,y} . \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} See the multidimensional section of this page for more details on the \\rho V \\rho V coefficient. So the same rules apply as done above, we can rewrite the law of cosines to encompass the multidimensional data inputs. d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) Sample Space \u00b6 Sometimes it's convenient to write the \\rho V (\\mathbf{x,y}) \\rho V (\\mathbf{x,y}) coefficient for the sample space for the data. So instead of calculating a cross-covariance matrix \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} , we calculate a self-similarity matrix for each of the datasets, e.g. \\mathbf{XX}^\\top = \\mathbf{W_x} \\mathbf{XX}^\\top = \\mathbf{W_x} and \\mathbf{YY}^\\top = \\mathbf{W_y} \\mathbf{YY}^\\top = \\mathbf{W_y} . This is a different and pairwise representation of the data. To measure the proximity between the two matrices, we can use the Frobenius norm (aka the Hilbert-Schmidt norm). This gives us: \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) Again, the two matrices will have differnt norms so we can renormalize them appropriately to obtain the \\rho V \\rho V coefficient: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} We have effectively computed the cosine angle between the two vectors in \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} cross-product of matrices. Again, we can repeat the same operations above in the law of cosines formulation. d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) Non-Linear Functions \u00b6 Let \\varphi(\\mathbf{X}) = \\mathbf{K_x} \\varphi(\\mathbf{X}) = \\mathbf{K_x} and \\varphi(\\mathbf{Y}) = \\mathbf{K_y} \\varphi(\\mathbf{Y}) = \\mathbf{K_y} . In the kernel community, this is known as the centered kernel alignment (cKA) \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} Rewriting this into the law of cosines: d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) References \u00b6 Le Traitement des Variables Vectorielles - Yves Escoufier (1973) Summarizing multiple aspects of model performance in a single diagram - Karl E. Taylor (2001) Taylor Diagram Primer - Karl E. Taylor (2005) The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) Measuring Multivariate Association and Beyond - Josse & Holmes - Statistics Surveys (2016)","title":"Taylor Diagram (1D Data)"},{"location":"appendix/similarity/taylor/#visualizing-similarities","text":"","title":"Visualizing Similarities"},{"location":"appendix/similarity/taylor/#motivation","text":"Visualizations: help find similarities between outputs stats are great, but visual uncertainty quantification","title":"Motivation"},{"location":"appendix/similarity/taylor/#questions","text":"Which model is more similar to the reference/observations? Should we look at correlations across seasons or latitudes? Are there large discrepancies in the different outputs","title":"Questions"},{"location":"appendix/similarity/taylor/#current-ways","text":"Trend Plots often do not expose the comparison aspects... Scatter plots become impractical for many outputs Parallel Coordinate Plots are more practical, but only certain pairwise comparisons are possible Plots per ensemble - possible but it can be super cluttered Taylor Diagram - visualize several statistics simultaneously in a statistical metric space. Specific Statistics Mean, Variance, Correlation Box Plots (and variations)","title":"Current Ways"},{"location":"appendix/similarity/taylor/#cosine-similarity","text":"Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim","title":"Cosine Similarity"},{"location":"appendix/similarity/taylor/#correlation","text":"There is a relationship between the cosine similarity and correlation coefficient \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} if \\rho(x,y) = 0 \\rho(x,y) = 0 , the spaces are orthogonal if \\rho(x,y) = 1 \\rho(x,y) = 1 , the spaces are equivalent","title":"Correlation"},{"location":"appendix/similarity/taylor/#distances","text":"","title":"Distances"},{"location":"appendix/similarity/taylor/#law-of-cosines","text":"If we recall the law of cosines; an extension of the cosine angle formula but for all angles and sides of the triangle. c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta Notice that this forumala looks very similar to the euclidean distance formula shown above. If we do a simple rearrangement of the final term in the above equation to accommadate the correlation term \\rho(x,y) \\rho(x,y) , then we get d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} simplifying to: d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) This is actually equivalent to the law of cosines.","title":"Law of cosines"},{"location":"appendix/similarity/taylor/#taylor-diagram","text":"The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. It simultaneously plots the standard deviation, the root mean square error and correlation between two variables.","title":"Taylor Diagram"},{"location":"appendix/similarity/taylor/#statistics-metric-space","text":"They key is that it is possible to find a metric space for these quantities, based on the law of cosines. If you look closely, this identity looks like the cosine law of triangles.we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE as we have expressed above. \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) If we write out the full equation, we have the following: \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) The sides are as follows: a = \\sigma_{x} a = \\sigma_{x} - the standard deviation of x x b = \\sigma_{y} b = \\sigma_{y} - the standard deviation of y y \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"Statistics Metric Space"},{"location":"appendix/similarity/taylor/#example-1-intuition","text":"Figure III : An example Taylor diagram.","title":"Example 1 - Intuition"},{"location":"appendix/similarity/taylor/#example-2-model-outputs","text":"","title":"Example 2 - Model Outputs"},{"location":"appendix/similarity/taylor/#multi-dimensional-data","text":"In the above examples, we assume that \\mathbf{x}, \\mathbf{y} \\mathbf{x}, \\mathbf{y} were both vectors of size \\mathbb{R}^{N \\times 1} \\mathbb{R}^{N \\times 1} . But what happens when we get datasets of size \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} ? Well, the above formulas can generalize using the inner product and the norm of the datasets. Figure I : A visual representation of the cosine similarity generalized to vectors.","title":"Multi-Dimensional Data"},{"location":"appendix/similarity/taylor/#distances_1","text":"We still get the same formulation as the above except now it is generalized to vectors. d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle","title":"Distances"},{"location":"appendix/similarity/taylor/#correlation_1","text":"Let \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} be the empirical covariance matrix between \\mathbf{x,y} \\mathbf{x,y} . \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} See the multidimensional section of this page for more details on the \\rho V \\rho V coefficient. So the same rules apply as done above, we can rewrite the law of cosines to encompass the multidimensional data inputs. d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y})","title":"Correlation"},{"location":"appendix/similarity/taylor/#sample-space","text":"Sometimes it's convenient to write the \\rho V (\\mathbf{x,y}) \\rho V (\\mathbf{x,y}) coefficient for the sample space for the data. So instead of calculating a cross-covariance matrix \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} , we calculate a self-similarity matrix for each of the datasets, e.g. \\mathbf{XX}^\\top = \\mathbf{W_x} \\mathbf{XX}^\\top = \\mathbf{W_x} and \\mathbf{YY}^\\top = \\mathbf{W_y} \\mathbf{YY}^\\top = \\mathbf{W_y} . This is a different and pairwise representation of the data. To measure the proximity between the two matrices, we can use the Frobenius norm (aka the Hilbert-Schmidt norm). This gives us: \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) Again, the two matrices will have differnt norms so we can renormalize them appropriately to obtain the \\rho V \\rho V coefficient: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} We have effectively computed the cosine angle between the two vectors in \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} cross-product of matrices. Again, we can repeat the same operations above in the law of cosines formulation. d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y})","title":"Sample Space"},{"location":"appendix/similarity/taylor/#non-linear-functions","text":"Let \\varphi(\\mathbf{X}) = \\mathbf{K_x} \\varphi(\\mathbf{X}) = \\mathbf{K_x} and \\varphi(\\mathbf{Y}) = \\mathbf{K_y} \\varphi(\\mathbf{Y}) = \\mathbf{K_y} . In the kernel community, this is known as the centered kernel alignment (cKA) \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} Rewriting this into the law of cosines: d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y})","title":"Non-Linear Functions"},{"location":"appendix/similarity/taylor/#references","text":"Le Traitement des Variables Vectorielles - Yves Escoufier (1973) Summarizing multiple aspects of model performance in a single diagram - Karl E. Taylor (2001) Taylor Diagram Primer - Karl E. Taylor (2005) The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) Measuring Multivariate Association and Beyond - Josse & Holmes - Statistics Surveys (2016)","title":"References"},{"location":"appendix/similarity/taylor_2d/","text":"Visualizing Similarities for 2D Data \u00b6","title":"Taylor Diagram (2D Data)"},{"location":"appendix/similarity/taylor_2d/#visualizing-similarities-for-2d-data","text":"","title":"Visualizing Similarities for 2D Data"},{"location":"appendix/similarity/vi/","text":"Variation of Information \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data \u00b6 We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Case I Case II Case III Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, \u00a9 linear dataset with an outlier. Standard Methods \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$ Example \u00b6 If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics. Example \u00b6 An easier number to interpret. But it will not distinguish the datasets. Example \u00b6 Information Theory \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) Example \u00b6 Now we finally see some differences between the distributions. Normalized Mutual Information \u00b6 The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore. Variation of Information \u00b6 This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this... RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Case I: Linear Methods Case II: Information Theoretic Relative comaprison. VI-Based Diagram \u00b6 This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range. Resources \u00b6 Comparing Clusterings by the Variation of Information - Meila (2003) - PDF Comparing Clusterings - An Information Based Distance - Meila (2007) - PDF Paper discussing the variation of information as a distance measure from a clustering perspective. Does all of the proofs that it is a legit distance measure. The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) - PDF | Prezi The VI measure in a Taylor diagram format. Uses the variation of information in addition to the slightly modified version.","title":"Variation of Information"},{"location":"appendix/similarity/vi/#variation-of-information","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information.","title":"Variation of Information"},{"location":"appendix/similarity/vi/#example-data","text":"We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Case I Case II Case III Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, \u00a9 linear dataset with an outlier.","title":"Example Data"},{"location":"appendix/similarity/vi/#standard-methods","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared","title":"Standard Methods"},{"location":"appendix/similarity/vi/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$","title":"Covariance"},{"location":"appendix/similarity/vi/#example","text":"If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics.","title":"Example"},{"location":"appendix/similarity/vi/#example_1","text":"An easier number to interpret. But it will not distinguish the datasets.","title":"Example"},{"location":"appendix/similarity/vi/#example_2","text":"","title":"Example"},{"location":"appendix/similarity/vi/#information-theory","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Information Theory"},{"location":"appendix/similarity/vi/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform.","title":"Entropy"},{"location":"appendix/similarity/vi/#mutual-information","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y)","title":"Mutual Information"},{"location":"appendix/similarity/vi/#example_3","text":"Now we finally see some differences between the distributions.","title":"Example"},{"location":"appendix/similarity/vi/#normalized-mutual-information","text":"The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore.","title":"Normalized Mutual Information"},{"location":"appendix/similarity/vi/#variation-of-information_1","text":"This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this...","title":"Variation of Information"},{"location":"appendix/similarity/vi/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"RVI-Based Diagram"},{"location":"appendix/similarity/vi/#example_4","text":"The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Case I: Linear Methods Case II: Information Theoretic Relative comaprison.","title":"Example"},{"location":"appendix/similarity/vi/#vi-based-diagram","text":"This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"VI-Based Diagram"},{"location":"appendix/similarity/vi/#resources","text":"Comparing Clusterings by the Variation of Information - Meila (2003) - PDF Comparing Clusterings - An Information Based Distance - Meila (2007) - PDF Paper discussing the variation of information as a distance measure from a clustering perspective. Does all of the proofs that it is a legit distance measure. The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) - PDF | Prezi The VI measure in a Taylor diagram format. Uses the variation of information in addition to the slightly modified version.","title":"Resources"},{"location":"blogs/","text":"My Blogs \u00b6 Drafts \u00b6 GPs from Scratch using Jax Accessing ESDCs Online Jax 4 Dummies - vmap | jit | grad Variation of Information Posted \u00b6 Uncertain GPs Through the Ages - 7 th July 2019","title":"My Blogs"},{"location":"blogs/#my-blogs","text":"","title":"My Blogs"},{"location":"blogs/#drafts","text":"GPs from Scratch using Jax Accessing ESDCs Online Jax 4 Dummies - vmap | jit | grad Variation of Information","title":"Drafts"},{"location":"blogs/#posted","text":"Uncertain GPs Through the Ages - 7 th July 2019","title":"Posted"},{"location":"blogs/ideas/concrete/","text":"Concrete Ideas \u00b6 My more concrete ideas for blog posts. Similarity Measures 4 Multi-Dimensional Data \u00b6 Methods Linear - \\rho \\rho V-Coefficient Distance: Energy Distance (Euclidean) Kernel: MMD Information: Variation of Information Visualization Taylor Diagrams Programming Concepts Sklearn Estimator - fit , score , DensityMixin XArray Data structures - spatial-temporal datasets Matplotlib Visualization - Taylor Diagram Figure from scratch Data Climate Models - CMIP6 Drought Indicators - ESDC Representing Uncertainty in Data w. GPs \u00b6 Models Linearized GP MCMC GP Unscented GP Variational Inference Programming Concepts Jax - functional, autograd, jit, vmap Numpyro - ppl Plotly Library Data ARGO Floats (nD) GPP Data (1D) \u00b6","title":"Concrete Ideas"},{"location":"blogs/ideas/concrete/#concrete-ideas","text":"My more concrete ideas for blog posts.","title":"Concrete Ideas"},{"location":"blogs/ideas/concrete/#similarity-measures-4-multi-dimensional-data","text":"Methods Linear - \\rho \\rho V-Coefficient Distance: Energy Distance (Euclidean) Kernel: MMD Information: Variation of Information Visualization Taylor Diagrams Programming Concepts Sklearn Estimator - fit , score , DensityMixin XArray Data structures - spatial-temporal datasets Matplotlib Visualization - Taylor Diagram Figure from scratch Data Climate Models - CMIP6 Drought Indicators - ESDC","title":"Similarity Measures 4 Multi-Dimensional Data"},{"location":"blogs/ideas/concrete/#representing-uncertainty-in-data-w-gps","text":"Models Linearized GP MCMC GP Unscented GP Variational Inference Programming Concepts Jax - functional, autograd, jit, vmap Numpyro - ppl Plotly Library Data ARGO Floats (nD) GPP Data (1D)","title":"Representing Uncertainty in Data w. GPs"},{"location":"blogs/ideas/concrete/#_1","text":"","title":""},{"location":"blogs/ideas/scratch/","text":"Blog Ideas \u00b6 I would like to do some blog posts for my academic blog. There are tons of things I would like to do but I don't really have time to do it all. So I would like to try a concept: combine scientific exploration with some programming concept exploration. For example, instead of doing a segment on Object-Oriented Programming and then a separate segment on Gaussian Process regression (GPR), I would combine the two. Programming \u00b6 Packages \u00b6 GPyTorch - DKL, Pyro GPFlow geopandas xarray Algorithms \u00b6 Optimized Kernel Ridge Regression (OKRR) Jax Optimized Kernel Entropy Components Analysis (OKECA) Jax Rotation-Based Iterative Gaussianization (RBIG) Gaussian Process Regression Variational Gaussian Process Regression Variational Inference Jax Bayesian Neural Networks Deep Kernel Learning GPyTorch - Gaussian Processes PyTorch - Neural Networks Data - Ocean Water Quality Gaussian Processes Exact Variational Sparse Sparse Variational Deep Packages \u00b6 Edward2 - Bayesian Layers (the future) Unsorted \u00b6 Bayesian Formulas + Plotly OOP + GPR from Scratch Kernel Functions (K, GPR, KRR) + Derivatives + AutoGrad VI + PyTorch RBIG + sklearn API IT Measures + ESDC + RBIG AD + ESDC + Feature Selection AD + ESDC + Feature Selection (Pt II) HyperLabelMe + TPOT + AutoSklearn Flask + Xarray Luigi + SLURM + Experiment Uncertainty + GPs Abstract Classes + Kernel Functions Work Deep Dive \u00b6 These notebooks will be directly related to my thesis and things that I am investigating actively. They should all include some results so that I can show off some of the actual applications. Output Normalized Methods I - Kernel Eigenmap Methods II - Kernel Eigenmap Projection Methods III - Manifold Alignment IV - Nearest Neighbours (Annoy, KDE Trees) V - Eigenvalue Decomposition Scaling (rSVD, Multigrid, Random Projections) VI - Out of Sampling (Nystrom, LLL, Var. Nystrom) Kernel Methods I - Kernel Functions II - Learning with Kernel Functions (Overview of Literature) III - Gradients and Sensitivity Analysis Gaussian Processes and Uncertainty I - GPs II - Sparse GPs III - Uncertain GPs (Literature, NIGP, My Work) IV - Variational Methods Deep Density Destructors I - Density Estimation II - RBIG III - GDN Concept Notebook \u00b6 These are notebooks that I decided to investigate because either I sucked at in the beginning or it was something I needed in order to advance to the next level of whatever I was doing related to my thesis. Bayesian Methods Variational Inference Information Theory Anomaly Detection Explorers Book \u00b6 Normalizing Flows Automatic Machine Learning Neural ODEs Lab Notebook \u00b6 Earth Science Data Cube + Xarray Naive AD Detection Dask Remote Computing xarray Shape Files Region Masks Large Scale ML - PCA, LR, KMeans, XGBoost Dask Jax Kernel methods kernels regression - krr, rff bayesian regression - gp, sgps classification - svm dependence estimation - hsic dimension reduction - okeca) Gaussianization flows","title":"Blog Ideas"},{"location":"blogs/ideas/scratch/#blog-ideas","text":"I would like to do some blog posts for my academic blog. There are tons of things I would like to do but I don't really have time to do it all. So I would like to try a concept: combine scientific exploration with some programming concept exploration. For example, instead of doing a segment on Object-Oriented Programming and then a separate segment on Gaussian Process regression (GPR), I would combine the two.","title":"Blog Ideas"},{"location":"blogs/ideas/scratch/#programming","text":"","title":"Programming"},{"location":"blogs/ideas/scratch/#packages","text":"GPyTorch - DKL, Pyro GPFlow geopandas xarray","title":"Packages"},{"location":"blogs/ideas/scratch/#algorithms","text":"Optimized Kernel Ridge Regression (OKRR) Jax Optimized Kernel Entropy Components Analysis (OKECA) Jax Rotation-Based Iterative Gaussianization (RBIG) Gaussian Process Regression Variational Gaussian Process Regression Variational Inference Jax Bayesian Neural Networks Deep Kernel Learning GPyTorch - Gaussian Processes PyTorch - Neural Networks Data - Ocean Water Quality Gaussian Processes Exact Variational Sparse Sparse Variational Deep","title":"Algorithms"},{"location":"blogs/ideas/scratch/#packages_1","text":"Edward2 - Bayesian Layers (the future)","title":"Packages"},{"location":"blogs/ideas/scratch/#unsorted","text":"Bayesian Formulas + Plotly OOP + GPR from Scratch Kernel Functions (K, GPR, KRR) + Derivatives + AutoGrad VI + PyTorch RBIG + sklearn API IT Measures + ESDC + RBIG AD + ESDC + Feature Selection AD + ESDC + Feature Selection (Pt II) HyperLabelMe + TPOT + AutoSklearn Flask + Xarray Luigi + SLURM + Experiment Uncertainty + GPs Abstract Classes + Kernel Functions","title":"Unsorted"},{"location":"blogs/ideas/scratch/#work-deep-dive","text":"These notebooks will be directly related to my thesis and things that I am investigating actively. They should all include some results so that I can show off some of the actual applications. Output Normalized Methods I - Kernel Eigenmap Methods II - Kernel Eigenmap Projection Methods III - Manifold Alignment IV - Nearest Neighbours (Annoy, KDE Trees) V - Eigenvalue Decomposition Scaling (rSVD, Multigrid, Random Projections) VI - Out of Sampling (Nystrom, LLL, Var. Nystrom) Kernel Methods I - Kernel Functions II - Learning with Kernel Functions (Overview of Literature) III - Gradients and Sensitivity Analysis Gaussian Processes and Uncertainty I - GPs II - Sparse GPs III - Uncertain GPs (Literature, NIGP, My Work) IV - Variational Methods Deep Density Destructors I - Density Estimation II - RBIG III - GDN","title":"Work Deep Dive"},{"location":"blogs/ideas/scratch/#concept-notebook","text":"These are notebooks that I decided to investigate because either I sucked at in the beginning or it was something I needed in order to advance to the next level of whatever I was doing related to my thesis. Bayesian Methods Variational Inference Information Theory Anomaly Detection","title":"Concept Notebook"},{"location":"blogs/ideas/scratch/#explorers-book","text":"Normalizing Flows Automatic Machine Learning Neural ODEs","title":"Explorers Book"},{"location":"blogs/ideas/scratch/#lab-notebook","text":"Earth Science Data Cube + Xarray Naive AD Detection Dask Remote Computing xarray Shape Files Region Masks Large Scale ML - PCA, LR, KMeans, XGBoost Dask Jax Kernel methods kernels regression - krr, rff bayesian regression - gp, sgps classification - svm dependence estimation - hsic dimension reduction - okeca) Gaussianization flows","title":"Lab Notebook"},{"location":"blogs/posted/uncertain_gps/","text":"GPs and Uncertain Inputs through the Ages \u00b6 Summary \u00b6 Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution Standard GP Formulation \u00b6 Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood Stochastic Test Points \u00b6 This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance. Setup \u00b6 Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step. GP Predictive Distribution \u00b6 So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation. Numerical Integration \u00b6 The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ). Approximate Gaussian Distribution \u00b6 Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations. Stochastic Measurements \u00b6 A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this. Noisy-Input GP (NIGP) \u00b6 We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings. Expected Derivative \u00b6 Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances . Moment Matching \u00b6 The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion. Variational Strategies \u00b6 What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ). My Method - Marriage of Two Strategies \u00b6 I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below. Model \u00b6 Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic. Inference \u00b6 So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter Supplementary Material \u00b6 Moment Matching \u00b6 In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function. Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Propagation of Variances \u00b6 Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) + NIGP - Propagating Variances \u00b6 Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation Real Results with Variance Estimates \u00b6 Absolute Error Figure : Absolute Error From a GP Model Exact GP These are the predictions using the exact GP and the predictive variances. Figure : Standard GP Variance Estimates Linearized GP Figure : GP Variance Estimates account for input errors. This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Resources \u00b6 Papers \u00b6 Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Important Papers \u00b6","title":"GPs and Uncertain Inputs through the Ages"},{"location":"blogs/posted/uncertain_gps/#gps-and-uncertain-inputs-through-the-ages","text":"","title":"GPs and Uncertain Inputs through the Ages"},{"location":"blogs/posted/uncertain_gps/#summary","text":"Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution","title":"Summary"},{"location":"blogs/posted/uncertain_gps/#standard-gp-formulation","text":"Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood","title":"Standard GP Formulation"},{"location":"blogs/posted/uncertain_gps/#stochastic-test-points","text":"This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance.","title":"Stochastic Test Points"},{"location":"blogs/posted/uncertain_gps/#setup","text":"Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step.","title":"Setup"},{"location":"blogs/posted/uncertain_gps/#gp-predictive-distribution","text":"So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation.","title":"GP Predictive Distribution"},{"location":"blogs/posted/uncertain_gps/#numerical-integration","text":"The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ).","title":"Numerical Integration"},{"location":"blogs/posted/uncertain_gps/#approximate-gaussian-distribution","text":"Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations.","title":"Approximate Gaussian Distribution"},{"location":"blogs/posted/uncertain_gps/#stochastic-measurements","text":"A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this.","title":"Stochastic Measurements"},{"location":"blogs/posted/uncertain_gps/#noisy-input-gp-nigp","text":"We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings.","title":"Noisy-Input GP (NIGP)"},{"location":"blogs/posted/uncertain_gps/#expected-derivative","text":"Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances .","title":"Expected Derivative"},{"location":"blogs/posted/uncertain_gps/#moment-matching","text":"The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion.","title":"Moment Matching"},{"location":"blogs/posted/uncertain_gps/#variational-strategies","text":"What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ).","title":"Variational Strategies"},{"location":"blogs/posted/uncertain_gps/#my-method-marriage-of-two-strategies","text":"I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below.","title":"My Method - Marriage of Two Strategies"},{"location":"blogs/posted/uncertain_gps/#model","text":"Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic.","title":"Model"},{"location":"blogs/posted/uncertain_gps/#inference","text":"So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter","title":"Inference"},{"location":"blogs/posted/uncertain_gps/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"blogs/posted/uncertain_gps/#moment-matching_1","text":"In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function.","title":"Moment Matching"},{"location":"blogs/posted/uncertain_gps/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"blogs/posted/uncertain_gps/#propagation-of-variances","text":"Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) +","title":"Propagation of Variances"},{"location":"blogs/posted/uncertain_gps/#nigp-propagating-variances","text":"Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation","title":"NIGP - Propagating Variances"},{"location":"blogs/posted/uncertain_gps/#real-results-with-variance-estimates","text":"Absolute Error Figure : Absolute Error From a GP Model Exact GP These are the predictions using the exact GP and the predictive variances. Figure : Standard GP Variance Estimates Linearized GP Figure : GP Variance Estimates account for input errors. This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Real Results with Variance Estimates"},{"location":"blogs/posted/uncertain_gps/#resources","text":"","title":"Resources"},{"location":"blogs/posted/uncertain_gps/#papers","text":"","title":"Papers"},{"location":"blogs/posted/uncertain_gps/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Thesis Explain"},{"location":"blogs/posted/uncertain_gps/#important-papers","text":"","title":"Important Papers"},{"location":"projects/","text":"Main Projects \u00b6 Similarity Measures \u00b6 Summary I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links : Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links : Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors Important Links : Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk PySim Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links : Github Repository Uncertainty Quantification \u00b6 Projects Input Uncertainty for Gaussian Processes Gaussianization Models in Eath Science Applications Gaussian Process Model Zoo RBIG 1.1 Python Package RBIG 2.0 Python Package Kernel Methods and Derivatives \u00b6 Projects Kernel Derivatives Applied to Earth Science Data Cubes Derivatives for Sensitivity Analysis in Gaussian Processes applied to Emulation Machine Learning for Ocean Applications \u00b6 Abstract You can visit the website here Projects ARGO Floats MultiOutput Models Ocean Water Types Regression Ocean Water Types Classification","title":"Overview"},{"location":"projects/#main-projects","text":"","title":"Main Projects"},{"location":"projects/#similarity-measures","text":"Summary I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links : Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links : Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors Important Links : Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk PySim Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links : Github Repository","title":"Similarity Measures"},{"location":"projects/#uncertainty-quantification","text":"Projects Input Uncertainty for Gaussian Processes Gaussianization Models in Eath Science Applications Gaussian Process Model Zoo RBIG 1.1 Python Package RBIG 2.0 Python Package","title":"Uncertainty Quantification"},{"location":"projects/#kernel-methods-and-derivatives","text":"Projects Kernel Derivatives Applied to Earth Science Data Cubes Derivatives for Sensitivity Analysis in Gaussian Processes applied to Emulation","title":"Kernel Methods and Derivatives"},{"location":"projects/#machine-learning-for-ocean-applications","text":"Abstract You can visit the website here Projects ARGO Floats MultiOutput Models Ocean Water Types Regression Ocean Water Types Classification","title":"Machine Learning for Ocean Applications"},{"location":"projects/ErrorGPs/","text":"Overview \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Repo: github.com/jejjohnson/uncertain_gps We will do a quick overview to show how we can account for input errors in Gaussian process regression models. Problem Statement \u00b6 Standard y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y where \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . Let \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} . Observe Noisy Estimates y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Observation Means only y = f(\\mu_\\mathbf{x}) + \\epsilon_y y = f(\\mu_\\mathbf{x}) + \\epsilon_y Posterior Predictions \u00b6 \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} Linearized Approximation \u00b6 Where we take the Taylor expansion of the predictive mean and variance function. The mean function stays the same: \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha but the predictive variance term gets changed slightly: \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} with the term in red being the derivative of the predictive mean function multiplied by the variance. Notes : * Assumes known variance * Assumes D\\times D D\\times D covariance matrix for multidimensional data * Quite inexpensive to implement * The 3 rd term (the 2 nd order component of the Taylor expansion) has been show to not make a huge difference egp_moment1 = jax . jfwd ( posterior , args_num = ( None , 0 )) egp_moment2 = jax . hessian ( posterior , args_num = ( None , 0 )) Moment-Matching \u00b6 Mean Predictions \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha where: q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] Variance Predictions \\nu_{eGP*}^2 \\nu_{eGP*}^2 Variational \u00b6 Assumes we have a variational distribution function \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] Other Resources \u00b6 Gaussian Process Model Zoo Datasets \u00b6 We use some toy datasets which including: \"near square sine wave\" f(x) = \\sin\\left(\\frac{\\pi}{c} \\cos\\left( 5 + \\frac{x}{2} \\right) \\right) f(x) = \\sin\\left(\\frac{\\pi}{c} \\cos\\left( 5 + \\frac{x}{2} \\right) \\right) The sigmoid curve f(x) = \\frac{1}{1+ \\exp(-x)} f(x) = \\frac{1}{1+ \\exp(-x)} Mauna Loa Ice Core Data | Data Portal $$ $$ Spatial IASI Data","title":"Overview"},{"location":"projects/ErrorGPs/#overview","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Repo: github.com/jejjohnson/uncertain_gps We will do a quick overview to show how we can account for input errors in Gaussian process regression models.","title":"Overview"},{"location":"projects/ErrorGPs/#problem-statement","text":"Standard y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y where \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . Let \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} . Observe Noisy Estimates y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Observation Means only y = f(\\mu_\\mathbf{x}) + \\epsilon_y y = f(\\mu_\\mathbf{x}) + \\epsilon_y","title":"Problem Statement"},{"location":"projects/ErrorGPs/#posterior-predictions","text":"\\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top}","title":"Posterior Predictions"},{"location":"projects/ErrorGPs/#linearized-approximation","text":"Where we take the Taylor expansion of the predictive mean and variance function. The mean function stays the same: \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha but the predictive variance term gets changed slightly: \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} with the term in red being the derivative of the predictive mean function multiplied by the variance. Notes : * Assumes known variance * Assumes D\\times D D\\times D covariance matrix for multidimensional data * Quite inexpensive to implement * The 3 rd term (the 2 nd order component of the Taylor expansion) has been show to not make a huge difference egp_moment1 = jax . jfwd ( posterior , args_num = ( None , 0 )) egp_moment2 = jax . hessian ( posterior , args_num = ( None , 0 ))","title":"Linearized Approximation"},{"location":"projects/ErrorGPs/#moment-matching","text":"Mean Predictions \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha where: q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] Variance Predictions \\nu_{eGP*}^2 \\nu_{eGP*}^2","title":"Moment-Matching"},{"location":"projects/ErrorGPs/#variational","text":"Assumes we have a variational distribution function \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right]","title":"Variational"},{"location":"projects/ErrorGPs/#other-resources","text":"Gaussian Process Model Zoo","title":"Other Resources"},{"location":"projects/ErrorGPs/#datasets","text":"We use some toy datasets which including: \"near square sine wave\" f(x) = \\sin\\left(\\frac{\\pi}{c} \\cos\\left( 5 + \\frac{x}{2} \\right) \\right) f(x) = \\sin\\left(\\frac{\\pi}{c} \\cos\\left( 5 + \\frac{x}{2} \\right) \\right) The sigmoid curve f(x) = \\frac{1}{1+ \\exp(-x)} f(x) = \\frac{1}{1+ \\exp(-x)} Mauna Loa Ice Core Data | Data Portal $$ $$ Spatial IASI Data","title":"Datasets"},{"location":"projects/ErrorGPs/approximate/","text":"Error Propagation in Gaussian Transformations \u00b6 We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x}) Change of Variables \u00b6 To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function. Conditional Gaussian Distribution \u00b6 Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) Taylor Expansions \u00b6 by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix). Joint Distribution \u00b6 So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} Joint Distribution for Additive Noise \u00b6 So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Gaussian Approximations"},{"location":"projects/ErrorGPs/approximate/#error-propagation-in-gaussian-transformations","text":"We're in the setting where we have some inputs that come from a distribution \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we have some outputs y\\in\\mathbb{R} y\\in\\mathbb{R} . Typically we have some function f(\\mathbf) f(\\mathbf) that maps the points f:\\mathbb{R}^D \\rightarrow \\mathbb{R} f:\\mathbb{R}^D \\rightarrow \\mathbb{R} . So like we do with GPs, we have the following function\" y=f(\\mathbf{x}) y=f(\\mathbf{x})","title":"Error Propagation in Gaussian Transformations"},{"location":"projects/ErrorGPs/approximate/#change-of-variables","text":"To have this full mapping, we would need to use the change of variables method because we are doing a transformation between of probability distributions. \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} \\begin{aligned} p(y) &= p(x)\\left| \\frac{dy}{dx} \\right|^{-1} \\\\ &= p(f^{-1}(y)|\\mu_x,\\Sigma_x)|\\nabla f(y)| \\end{aligned} This is very expensive and can be very difficult depending upon the function.","title":"Change of Variables"},{"location":"projects/ErrorGPs/approximate/#conditional-gaussian-distribution","text":"Alternatively, if we know that f() f() is described by a Gaussian distribution, then we can find the joint distribution between \\mathbf{x},y \\mathbf{x},y . This can be described as: \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} \\mathbf{x} \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mathbf{x}) \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Conditional Gaussian Distribution"},{"location":"projects/ErrorGPs/approximate/#taylor-expansions","text":"by using a Taylor series expansion. Let \\mu_x \\mu_x be the true inputs and we perturb these by some noise \\delta_x \\delta_x which is described by a normal distribution \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) \\delta_x \\sim \\mathcal{N}(0, \\Sigma_x) . So we can write an expression for the function f(\\mathbf{x}) f(\\mathbf{x}) . \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} \\begin{aligned} f(\\mathbf{x}) &= f(\\mu_x + \\delta_x) \\\\ &\\approx f(\\mu_x) + \\nabla_x f(\\mu_x)\\delta_x + \\frac{1}{2}\\sum_i \\delta_x^\\top \\nabla_{xx}^{(i)}f(\\mu_x)\\delta_x e_i + \\ldots \\end{aligned} where \\nabla_x f(\\mu_x) \\nabla_x f(\\mu_x) is the jacobian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x , \\nabla_{xx}f(\\mu_x) \\nabla_{xx}f(\\mu_x) is the hessian of f(\\cdot) f(\\cdot) evaluated at \\mu_x \\mu_x and e_i e_i is a ones vector (essentially the trace of a full matrix).","title":"Taylor Expansions"},{"location":"projects/ErrorGPs/approximate/#joint-distribution","text":"So, we want \\tilde{f}(\\mathbf{x}) \\tilde{f}(\\mathbf{x}) which is a joint distribution of \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ f(\\mathbf{x})\\end{bmatrix} . But as we said, this is difficult to compute so we do a Taylor approximation to this function \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} \\begin{bmatrix} \\mu_\\mathbf{x} \\\\ f(\\mu_\\mathbf{x} + \\delta_x) \\end{bmatrix} . But we still want the full joint distribution , ideally Gaussian. So that would mean we at least need the expectation and the covariance. \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right], \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] Derivation Mean Function The mean function is the easiest to derive. We can just take the expectation of the first two terms and we'll see why all the higher order terms disappear. \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} \\begin{aligned} \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\mathbb{E}_\\mathbf{x}\\left[ \\tilde{f}(\\mu_\\mathbf{x}) \\right] + \\mathbb{E}_\\mathbf{x}\\left[ \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}\\mathbb{E}_\\mathbf{x}\\left[ f(\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\\\ &= \\tilde{f}(\\mu_\\mathbf{x})\\\\ \\end{aligned} Covariance Function The covariance function is a bit harder. But again, the final expression is quite simple. \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} \\begin{aligned} \\mathbb{V}_\\mathbf{x}\\left[ \\tilde{f}(\\mathbf{x}) \\right] &= \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right] \\; \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x}) - \\mathbb{E}_\\mathbf{x}\\left[f(\\mathbf{x})\\right] \\right]^\\top \\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mathbf{x}) - f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} - f(\\mu_\\mathbf{x}) \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[f(\\mu_\\mathbf{x}) + \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}- f(\\mu_\\mathbf{x}) \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right] \\; \\mathbb{E}_\\mathbf{x} \\left[\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x} \\right]^\\top\\\\ &\\approx \\mathbb{E}_\\mathbf{x} \\left[\\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right) \\left(\\nabla_\\mathbf{x}f(\\mu_\\mathbf{x})\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\mathbb{E}_\\mathbf{x} \\left[\\left(\\epsilon_\\mathbf{x}\\right) \\left(\\epsilon_\\mathbf{x}\\right)^\\top \\right] \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ &\\approx \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\end{aligned} So now we can have a full expression for the joint distribution with the Taylor expansion. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{T} \\\\ C_\\mathcal{T}^\\top & \\Pi_\\mathcal{T} \\end{bmatrix} \\right) where: \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_\\mathbf{x})\\\\ C_T &= \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\Pi_T &= \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\Sigma_\\mathbf{x} \\nabla_\\mathbf{x}f(\\mu_\\mathbf{x}) \\\\ \\end{aligned}","title":"Joint Distribution"},{"location":"projects/ErrorGPs/approximate/#joint-distribution-for-additive-noise","text":"So in this setting, we are closer to what we typically use for a GP model. y = f(\\mathbf{x}) + \\epsilon y = f(\\mathbf{x}) + \\epsilon where we have noisy inputs with additive Gaussian noise \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\Sigma_\\mathbf{x}) and we assume additive Gaussian noise for the outputs \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 \\mathbf{I}) . We want a joint distribution of \\mathbf{x},y \\mathbf{x},y . So using the same sequences of steps as above, we actually get a very similar joint distribution, just with an additional term. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_{\\mathcal{GP}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_\\mathbf{x} & C_\\mathcal{eGP} \\\\ C_\\mathcal{eGP}^\\top & \\Pi_\\mathcal{eGP} \\end{bmatrix} \\right) where \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} \\begin{aligned} \\mu_y &= f(\\mu_x) \\\\ \\Pi_\\mathcal{eGP} &= \\nabla_x f(\\mu_x) \\: \\Sigma_x \\: \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\\\ C_\\mathcal{eGP} &= \\Sigma_x \\: \\nabla_x^\\top f(\\mu_x) \\end{aligned} So if we want to make predictions with our new model, we will have the final equation as: \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\\\ \\mu_\\mathcal{eGP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\sigma^2_\\mathcal{eGP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"Joint Distribution for Additive Noise"},{"location":"projects/ErrorGPs/Code/software/","text":"Software \u00b6 GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch. GPy \u00b6 This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems. My Model Zoo \u00b6 Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github GPFlow \u00b6 This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting. Pyro \u00b6 This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research. My Model Zoo \u00b6 Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab GPyTorch \u00b6 This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications. Summary \u00b6 Algorithms Implemented \u00b6 Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Software"},{"location":"projects/ErrorGPs/Code/software/#software","text":"GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch.","title":"Software"},{"location":"projects/ErrorGPs/Code/software/#gpy","text":"This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems.","title":"GPy"},{"location":"projects/ErrorGPs/Code/software/#my-model-zoo","text":"Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github","title":"My Model Zoo"},{"location":"projects/ErrorGPs/Code/software/#gpflow","text":"This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting.","title":"GPFlow"},{"location":"projects/ErrorGPs/Code/software/#pyro","text":"This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research.","title":"Pyro"},{"location":"projects/ErrorGPs/Code/software/#my-model-zoo_1","text":"Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab","title":"My Model Zoo"},{"location":"projects/ErrorGPs/Code/software/#gpytorch","text":"This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications.","title":"GPyTorch"},{"location":"projects/ErrorGPs/Code/software/#summary","text":"","title":"Summary"},{"location":"projects/ErrorGPs/Code/software/#algorithms-implemented","text":"Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Algorithms Implemented"},{"location":"projects/ErrorGPs/Overview/basics/","text":"Basics \u00b6 Data \u00b6 Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise. Model \u00b6 Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} We specify a mean function, m m and a covariance function k k . Likelihood , \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } which describes the dataset Marginal Likelihood , \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} . Posterior \u00b6 First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F}) Deterministic Inputs \u00b6 In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) Probabilistic Inputs \u00b6 In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} Variational GP Models \u00b6 Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Sparse GP Models \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Basics"},{"location":"projects/ErrorGPs/Overview/basics/#basics","text":"","title":"Basics"},{"location":"projects/ErrorGPs/Overview/basics/#data","text":"Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise.","title":"Data"},{"location":"projects/ErrorGPs/Overview/basics/#model","text":"Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} We specify a mean function, m m and a covariance function k k . Likelihood , \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } which describes the dataset Marginal Likelihood , \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} .","title":"Model"},{"location":"projects/ErrorGPs/Overview/basics/#posterior","text":"First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F})","title":"Posterior"},{"location":"projects/ErrorGPs/Overview/basics/#deterministic-inputs","text":"In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I})","title":"Deterministic Inputs"},{"location":"projects/ErrorGPs/Overview/basics/#probabilistic-inputs","text":"In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned}","title":"Probabilistic Inputs"},{"location":"projects/ErrorGPs/Overview/basics/#variational-gp-models","text":"Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right)","title":"Variational GP Models"},{"location":"projects/ErrorGPs/Overview/basics/#sparse-gp-models","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Sparse GP Models"},{"location":"projects/ErrorGPs/Overview/literature/","text":"Literature Review \u00b6 Motivation \u00b6 This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs. Algorithms \u00b6 Error-In-Variables Regression \u00b6 This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ] Monte Carlo Sampling \u00b6 So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this. Taylor Expansion \u00b6 Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ] Moment Matching \u00b6 This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end) Covariance Functions \u00b6 Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code Iterative \u00b6 Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR Linearized (Unscented) Approximation \u00b6 This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach. Heteroscedastic Likelihood Models \u00b6 Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code Latent Variable Models \u00b6 Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016) Latent Covariates \u00b6 Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) Variational Strategies \u00b6 Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Appendix \u00b6 Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Literature \u00b6 Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel Toolboxes \u00b6 Emukit Connecting Concepts \u00b6 Moment Matching \u00b6 Derivatives of GPs \u00b6 Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018) Extended Kalman Filter \u00b6 This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code Uncertain Inputs in other ML fields \u00b6 Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation Key Equations \u00b6 Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Literature Review"},{"location":"projects/ErrorGPs/Overview/literature/#literature-review","text":"","title":"Literature Review"},{"location":"projects/ErrorGPs/Overview/literature/#motivation","text":"This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs.","title":"Motivation"},{"location":"projects/ErrorGPs/Overview/literature/#algorithms","text":"","title":"Algorithms"},{"location":"projects/ErrorGPs/Overview/literature/#error-in-variables-regression","text":"This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ]","title":"Error-In-Variables Regression"},{"location":"projects/ErrorGPs/Overview/literature/#monte-carlo-sampling","text":"So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this.","title":"Monte Carlo Sampling"},{"location":"projects/ErrorGPs/Overview/literature/#taylor-expansion","text":"Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ]","title":"Taylor Expansion"},{"location":"projects/ErrorGPs/Overview/literature/#moment-matching","text":"This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end)","title":"Moment Matching"},{"location":"projects/ErrorGPs/Overview/literature/#covariance-functions","text":"Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code","title":"Covariance Functions"},{"location":"projects/ErrorGPs/Overview/literature/#iterative","text":"Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR","title":"Iterative"},{"location":"projects/ErrorGPs/Overview/literature/#linearized-unscented-approximation","text":"This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach.","title":"Linearized (Unscented) Approximation"},{"location":"projects/ErrorGPs/Overview/literature/#heteroscedastic-likelihood-models","text":"Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code","title":"Heteroscedastic Likelihood Models"},{"location":"projects/ErrorGPs/Overview/literature/#latent-variable-models","text":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016)","title":"Latent Variable Models"},{"location":"projects/ErrorGPs/Overview/literature/#latent-covariates","text":"Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019)","title":"Latent Covariates"},{"location":"projects/ErrorGPs/Overview/literature/#variational-strategies","text":"Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Overview/literature/#appendix","text":"","title":"Appendix"},{"location":"projects/ErrorGPs/Overview/literature/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"projects/ErrorGPs/Overview/literature/#literature","text":"Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel","title":"Literature"},{"location":"projects/ErrorGPs/Overview/literature/#toolboxes","text":"Emukit","title":"Toolboxes"},{"location":"projects/ErrorGPs/Overview/literature/#connecting-concepts","text":"","title":"Connecting Concepts"},{"location":"projects/ErrorGPs/Overview/literature/#moment-matching_1","text":"","title":"Moment Matching"},{"location":"projects/ErrorGPs/Overview/literature/#derivatives-of-gps","text":"Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018)","title":"Derivatives of GPs"},{"location":"projects/ErrorGPs/Overview/literature/#extended-kalman-filter","text":"This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code","title":"Extended Kalman Filter"},{"location":"projects/ErrorGPs/Overview/literature/#uncertain-inputs-in-other-ml-fields","text":"Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation","title":"Uncertain Inputs in other ML fields"},{"location":"projects/ErrorGPs/Overview/literature/#key-equations","text":"Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Key Equations"},{"location":"projects/ErrorGPs/Overview/next/","text":"Next Steps \u00b6 So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen: 1. Apply these algorithms to different problems (other than dynamical systems) \u00b6 It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it? 2. Improve the Kernel Expectation Calculations \u00b6 So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there. 3. Think about the problem differently \u00b6 An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too! 4. Think about pragmatic solutions \u00b6 Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free. 5. Figure Out how to extend it to Deep GPs \u00b6 So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"Next Steps"},{"location":"projects/ErrorGPs/Overview/next/#next-steps","text":"So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen:","title":"Next Steps"},{"location":"projects/ErrorGPs/Overview/next/#1-apply-these-algorithms-to-different-problems-other-than-dynamical-systems","text":"It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it?","title":"1. Apply these algorithms to different problems (other than dynamical systems)"},{"location":"projects/ErrorGPs/Overview/next/#2-improve-the-kernel-expectation-calculations","text":"So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there.","title":"2. Improve the Kernel Expectation Calculations"},{"location":"projects/ErrorGPs/Overview/next/#3-think-about-the-problem-differently","text":"An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too!","title":"3. Think about the problem differently"},{"location":"projects/ErrorGPs/Overview/next/#4-think-about-pragmatic-solutions","text":"Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free.","title":"4. Think about pragmatic solutions"},{"location":"projects/ErrorGPs/Overview/next/#5-figure-out-how-to-extend-it-to-deep-gps","text":"So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"5. Figure Out how to extend it to Deep GPs"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#error-propagation","text":"","title":"Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"projects/ErrorGPs/Taylor%20Expansion/scratch/","text":"","title":"Scratch"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/","text":"Linearized GP \u00b6 Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Examples \u00b6 1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation. Sparse GPs \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. Literature \u00b6 Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Linearized GP"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#linearized-gp","text":"Recall the GP formulation: y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) y=f(\\mathbf{x}) + \\epsilon_y, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2\\mathbf{I}) Recall the posterior formulas: \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu_\\mathcal{GP}(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\sigma_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} In the case where we have uncertain inputs \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) , this function needs to be modified in order to accommodate the uncertainty The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the posterior distribution wrt the inputs. Mean Function \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Linearized GP"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#taylor-approximation","text":"Taking complete expectations can be very expensive because we need to take the expectation wrt to the inputs through nonlinear terms such as the kernel functions and their inverses. So, we will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\sigma^2_\\text{GP}(\\mathbf{x_*})= \\sigma^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\sigma^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Note To see more about error propagation and the relation to the mean and variance, see here . So expanding these equations gives us the following: \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\sigma^2_\\text{GP}(\\mu_{x_*}) + \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_{\\mu_{GP_*}} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} + {\\color{red}{\\nabla_{\\mu_{GP_*}}\\Sigma_x\\nabla_{\\mu_{GP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Taylor Approximation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#examples","text":"1D Demo Exact GP 1st Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top}} 2nd Order Taylor \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\color{red}{\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} Differences Here, we see a plot for the differences between the two GPs. Satellite Data Absolute Error Exact GP These are the predictions using the exact GP and the predictive variances. Linearized GP This is an example where we used the Taylor expanded GP. In this example, we only did the first order approximation.","title":"Examples"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#sparse-gps","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} So the new predictive functions will be: \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\\\ \\nu^2_{SGP} &= K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} \\begin{aligned} \\mu_\\text{eSGP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\sigma_{eSGP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + K_{**} - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + {\\color{red}{\\nabla_{\\mu_{SGP_*}}\\Sigma_x\\nabla_{\\mu_{SGP_*}}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\sigma^2_{SGP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\}} \\end{aligned} As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Sparse GPs"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#literature","text":"Theory Bayesian Filtering and Smoothing - Smio Sarkka ()- Book Modelling and Control of Dynamic Systems Using GP Models - Jus Kocijan () - Book Applied to Gaussian Processes Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"projects/ErrorGPs/Variational/vi/","text":"Variational Strategies \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. Variational GP Model with Latent Inputs \u00b6 Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Variational/vi/#variational-strategies","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error.","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Variational/vi/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more.","title":"Posterior Approximations"},{"location":"projects/ErrorGPs/Variational/vi/#variational-gp-model-with-latent-inputs","text":"Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Variational GP Model with Latent Inputs"},{"location":"projects/ErrorGPs/Variational/vi/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"projects/ErrorGPs/Variational/vi/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"projects/ErrorGPs/Variational/vi/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"projects/ErrorGPs/Variational/vi/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"projects/ErrorGPs/Variational/vi/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"projects/ErrorGPs/Variational/vi/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"projects/ErrorGPs/Variational/vi/#resources","text":"","title":"Resources"},{"location":"projects/ErrorGPs/Variational/vi/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"projects/ErrorGPs/Variational/vi/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"projects/ErrorGPs/Variational/vi/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/","text":"Code Packages ! pip install pyro - ppl Imports import os import matplotlib.pyplot as plt plt . style . use ([ 'seaborn-darkgrid' , 'seaborn-notebook' ]) import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 ) Plot Utils # note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 ) Data \u00b6 Code n_samples = 500 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () Code X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda () Variational GP Regression \u00b6 Model \u00b6 # initialize the inducing inputs (kmeans) n_inducing = 20. Xu = kmeans2 ( X . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vsgp = gp . models . VariationalSparseGP ( X , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vsgp . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vsgp . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( vsgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vsgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational Sparse GP' , fontsize = 20 ) plt . show () Uncertain VSGP \u00b6 Method 0 - Standard Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) Losses \u00b6 plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figure plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Uncertain VSGP, Standard Prior' , fontsize = 20 ) plt . show () Method III - Bayesian Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # gplvm.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites)) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Bayesian Prior' , fontsize = 20 ) plt . show () Method I - Strong Conservative Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.05 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Strong Conservative Prior' , fontsize = 20 ) plt . show () Method II - Strong Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} Model \u00b6 # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() ) Inference \u00b6 # the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) Losses \u00b6 plt . plot ( losses ); Predictions \u00b6 X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, strong prior' , fontsize = 20 ) plt . show ()","title":"eSVGP 1D Demo"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#data","text":"Code n_samples = 500 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () Code X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda ()","title":"Data"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#variational-gp-regression","text":"","title":"Variational GP Regression"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#model","text":"# initialize the inducing inputs (kmeans) n_inducing = 20. Xu = kmeans2 ( X . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vsgp = gp . models . VariationalSparseGP ( X , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vsgp . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#inference","text":"# the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vsgp . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( vsgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer )","title":"Inference"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#losses","text":"plt . plot ( losses );","title":"Losses"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#predictions","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vsgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational Sparse GP' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#uncertain-vsgp","text":"","title":"Uncertain VSGP"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#method-0-standard-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m, S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter.","title":"Method 0 - Standard Prior"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#model_1","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#inference_1","text":"# the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites))","title":"Inference"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#losses_1","text":"plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figure plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Uncertain VSGP, Standard Prior' , fontsize = 20 ) plt . show ()","title":"Losses"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#method-iii-bayesian-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter.","title":"Method III - Bayesian Prior"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#model_2","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # gplvm.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#inference_2","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer ) /usr/local/lib/python3.6/dist-packages/pyro/infer/trace_mean_field_elbo.py:32: UserWarning: Failed to verify mean field restriction on the guide. To eliminate this warning, ensure model and guide sites occur in the same order. Model sites: u XGuide sites: X u \"Guide sites:\\n \" + \"\\n \".join(guide_sites))","title":"Inference"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#losses_2","text":"plt . plot ( losses );","title":"Losses"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#predictions_1","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Figures plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#method-i-strong-conservative-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned}","title":"Method I - Strong Conservative Prior"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#model_3","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.05 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#inference_3","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer )","title":"Inference"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#losses_3","text":"plt . plot ( losses );","title":"Losses"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#predictions_2","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, Strong Conservative Prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#method-ii-strong-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\end{aligned}","title":"Method II - Strong Prior"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#model_4","text":"# make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # ================================== # Inducing Points # ================================== n_inducing = 20. # initialize the inducing inputs (kmeans) Xu = kmeans2 ( Xmu . cpu () . numpy (), int ( n_inducing ), minit = \"points\" )[ 0 ] Xu = torch . tensor ( Xu ) # create parameter Xu = Parameter ( Xu . clone (), requires_grad = True ) Xu = Xu . cuda () # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability gplvm = gp . models . VariationalSparseGP ( Xmu , y , kernel , Xu = Xu , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () gplvm . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters gplvm . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = False ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) gplvm . X_loc = X_var_loc gplvm . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA gplvm . cuda () VariationalSparseGP( (kernel): RBF() (likelihood): Gaussian() )","title":"Model"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#inference_4","text":"# the way we setup inference is similar to above optimizer = torch . optim . Adam ( gplvm . parameters (), lr = 0.01 ) num_steps = 5_000 losses = gp . util . train ( gplvm , num_steps = num_steps , optimizer = optimizer )","title":"Inference"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#losses_4","text":"plt . plot ( losses );","title":"Losses"},{"location":"projects/ErrorGPs/Variational/egp_pyro_svgp/egp_pyro_svgp/#predictions_3","text":"X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = gplvm ( X_test , full_cov = True ) std = cov . diag () . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points plt . scatter ( vsgp . Xu . cpu () . detach () . numpy (), - 0.75 * torch . ones ( int ( n_inducing )) . cpu () . numpy (), color = 'g' , marker = '*' , s = 200 , label = 'Inducing Inputs' ) # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Bayesian GPLVM, strong prior' , fontsize = 20 ) plt . show ()","title":"Predictions"},{"location":"projects/ErrorGPs/Variational/egp_pyro_vgp/egp_pyro_vgp/","text":"#@title Package Install ! pip install pyro - ppl Collecting pyro-ppl \u001b[?25l Downloading https://files.pythonhosted.org/packages/c0/77/4db4946f6b5bf0601869c7b7594def42a7197729167484e1779fff5ca0d6/pyro_ppl-1.3.1-py3-none-any.whl (520kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 522kB 6.4MB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1) Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.3) Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.38.0) Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0) Collecting pyro-api>=0.1.1 Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl Installing collected packages: pyro-api, pyro-ppl Successfully installed pyro-api-0.1.1 pyro-ppl-1.3.1 #@title Import Packages import os import time import torch from torch.nn import Parameter import pyro import pyro.contrib.gp as gp import pyro.distributions as dist from pyro.nn import PyroSample , PyroParam from scipy.cluster.vq import kmeans2 smoke_test = ( 'CI' in os . environ ) # ignore; used to check code integrity in the Pyro repo # assert pyro.__version__.startswith('0.5.1') pyro . enable_validation ( True ) # can help with debugging pyro . set_rng_seed ( 0 ) # import matplotlib.pyplot as plt # plt.style.use(['seaborn-darkgrid', 'seaborn-notebook']) import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm #@title Plot Utils # note that this helper function does three different things: # (i) plots the observed data; # (ii) plots the predictions from the learned GP after conditioning on data; # (iii) plots samples from the GP prior (with no conditioning on observed data) def plot ( plot_observed_data = False , plot_predictions = False , n_prior_samples = 0 , model = None , kernel = None , n_test = 500 ): plt . figure ( figsize = ( 12 , 6 )) if plot_observed_data : plt . plot ( X . numpy (), y . numpy (), 'kx' ) if plot_predictions : Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs # compute predictive mean and variance with torch . no_grad (): if type ( model ) == gp . models . VariationalSparseGP : mean , cov = model ( Xtest , full_cov = True ) else : try : mean , cov = model ( Xtest , full_cov = True , noiseless = False ) except : mean , cov = model ( Xtest ) sd = cov . diag () . sqrt () # standard deviation at each input point x plt . plot ( Xtest . numpy (), mean . numpy (), 'r' , lw = 2 ) # plot the mean plt . fill_between ( Xtest . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * sd ) . numpy (), ( mean + 2.0 * sd ) . numpy (), color = 'C0' , alpha = 0.3 ) if n_prior_samples > 0 : # plot samples from the GP prior Xtest = torch . linspace ( - 0.5 , 5.5 , n_test ) # test inputs noise = ( model . noise if type ( model ) != gp . models . VariationalSparseGP else model . likelihood . variance ) cov = kernel . forward ( Xtest ) + noise . expand ( n_test ) . diag () samples = dist . MultivariateNormal ( torch . zeros ( n_test ), covariance_matrix = cov ) \\ . sample ( sample_shape = ( n_prior_samples ,)) plt . plot ( Xtest . numpy (), samples . numpy () . T , lw = 2 , alpha = 0.4 ) plt . xlim ( - 0.5 , 5.5 ) #@title Data n_samples = 100 t_samples = 1_000 x_var = 0.1 y_var = 0.05 X_mu = dist . Uniform ( 0.0 , 5.0 ) . sample ( sample_shape = ( n_samples ,)) X_test = torch . linspace ( - 0.05 , 5.05 , t_samples ) y_mu = - 0.5 * torch . sin ( 1.6 * X_mu ) plt . figure () plt . scatter ( X_mu . numpy (), y_mu . numpy ()) plt . title ( 'Clean Dataset' ) plt . xlabel ( '$\\mu_x$' , fontsize = 20 ) plt . ylabel ( '$y$' , fontsize = 20 ) plt . show () #@title Plot Noisy Data X = X_mu + dist . Normal ( 0.0 , x_var ) . sample ( sample_shape = ( n_samples ,)) y = y_mu + dist . Normal ( 0.0 , y_var ) . sample ( sample_shape = ( n_samples ,)) plt . figure () plt . scatter ( X . numpy (), y . numpy ()) plt . title ( 'Noisy Dataset' ) plt . xlabel ( '$X = \\mu_x + \\epsilon_x$' , fontsize = 20 ) plt . ylabel ( '$y + \\epsilon_y$' , fontsize = 20 ) plt . show () X = X . cuda () y = y . cuda () X_test = X_test . cuda () Variational GP Regression \u00b6 #@title Model # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vgp = gp . models . VariationalGP ( X , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( vgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 29.33 secs #@title Losses plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Plots plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' , zorder = 2 ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' , zorder = 2 ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP' , fontsize = 20 ) plt . show () So virtually no error bars. There have been reports that error bars in regression datasets is a problem. But this is a bit ridiculous. VGP w. Uncertain Inputs \u00b6 Method 0 - Standard Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # evgp.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 34.96 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Standard Prior' , fontsize = 20 ) plt . show () Method III - Bayesian Prior \u00b6 In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 35.74 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"eVGP 1D Demo"},{"location":"projects/ErrorGPs/Variational/egp_pyro_vgp/egp_pyro_vgp/#variational-gp-regression","text":"#@title Model # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability vgp = gp . models . VariationalGP ( X , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) vgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( vgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( vgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 29.33 secs #@title Losses plt . plot ( losses ); #@title Predictions X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = vgp ( X_test , full_cov = True ) std = cov . diag () . sqrt () #@title Plots plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' , zorder = 2 ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' , zorder = 2 ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP' , fontsize = 20 ) plt . show () So virtually no error bars. There have been reports that error bars in regression datasets is a problem. But this is a bit ridiculous.","title":"Variational GP Regression"},{"location":"projects/ErrorGPs/Variational/egp_pyro_vgp/egp_pyro_vgp/#vgp-w-uncertain-inputs","text":"","title":"VGP w. Uncertain Inputs"},{"location":"projects/ErrorGPs/Variational/egp_pyro_vgp/egp_pyro_vgp/#method-0-standard-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\mathbf{I})\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # create priors mu_x, sigma_x X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( 0.1 * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () # set prior distribution for p(X) as N(Xmu, diag(0.1)) evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # evgp.set_constraint(\"X_scale\", dist.constraints.positive) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above elbo = pyro . infer . TraceMeanField_ELBO () loss_fn = elbo . differentiable_loss optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 34.96 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Standard Prior' , fontsize = 20 ) plt . show ()","title":"Method 0 - Standard Prior"},{"location":"projects/ErrorGPs/Variational/egp_pyro_vgp/egp_pyro_vgp/#method-iii-bayesian-prior","text":"In this method I will be imposing the following constraints: \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} \\begin{aligned} p(\\mathbf{X}) &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\\\\ q(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{m,S}) \\end{aligned} where \\mathbf{S} \\mathbf{S} is a free parameter. #@title Model # make X a latent variable Xmu = Parameter ( X . clone (), requires_grad = False ) # initialize the kernel and model kernel = gp . kernels . RBF ( input_dim = 1 ) likelihood = gp . likelihoods . Gaussian () # we increase the jitter for better numerical stability evgp = gp . models . VariationalGP ( Xmu , y , kernel , likelihood = likelihood , whiten = True , jitter = 1e-3 ) # ============================== # Prior Distribution, p(X) # ============================== # set prior distribution to X to be N(Xmu,I) X_prior_mean = Parameter ( Xmu . clone (), requires_grad = False ) . cuda () X_prior_std = Parameter ( x_var * torch . ones ( Xmu . size ()), requires_grad = False ) . cuda () evgp . X = PyroSample ( dist . Normal ( # Normal Distribution X_prior_mean , # Prior Mean X_prior_std # Prior Variance ) . to_event ()) # ============================== # Variational Distribution, q(X) # ============================== # create guide, i.e. variational parameters evgp . autoguide ( \"X\" , dist . Normal ) # create priors for variational parameters X_var_loc = Parameter ( Xmu . clone (), requires_grad = True ) . cuda () X_var_scale = Parameter ( x_var * torch . ones (( Xmu . shape [ 0 ])), requires_grad = True ) . cuda () # set quide (variational params) to be N(mu_q, sigma_q) evgp . X_loc = X_var_loc evgp . X_scale = PyroParam ( X_var_scale , dist . constraints . positive ) # Convert to CUDA evgp . cuda () VariationalGP( (kernel): RBF() (likelihood): Gaussian() ) #@title Inference # the way we setup inference is similar to above optimizer = torch . optim . Adam ( evgp . parameters (), lr = 0.01 ) num_steps = 2_000 t0 = time . time () losses = gp . util . train ( evgp , num_steps = num_steps , loss_fn = loss_fn , optimizer = optimizer ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 : .2f } secs\" ) Time Taken: 35.74 secs #@title Losses plt . plot ( losses ); #@title Predictive Mean, Var X_plot = torch . sort ( X )[ 0 ] with torch . no_grad (): mean , cov = evgp ( X_test , full_cov = False ) std = cov . sqrt () plt . figure () # Training Data plt . scatter ( X . cpu () . numpy (), y . cpu () . numpy (), color = 'k' , label = 'Training Data' ) # Test Data plt . plot ( X_test . cpu () . numpy (), mean . cpu () . numpy (), color = 'r' , linewidth = 6 , label = 'Predictive Mean' ) # plot the mean # # Inducing Points # plt.scatter(vsgp.Xu.cpu().detach().numpy(), -0.75 * torch.ones(int(n_inducing)).cpu().numpy(), color='g', marker='*', s=200, label='Inducing Inputs') # Confidence Intervals plt . fill_between ( X_test . cpu () . numpy (), # plot the two-sigma uncertainty about the mean ( mean - 2.0 * std ) . cpu () . numpy (), ( mean + 2.0 * std ) . cpu () . numpy (), color = 'C0' , alpha = 0.3 , label = '2 Stddev' ) plt . legend ( fontsize = 10 ) plt . title ( 'Variational GP, Bayesian Prior' , fontsize = 20 ) plt . show ()","title":"Method III - Bayesian Prior"},{"location":"projects/ErrorGPs/monte_carlo/demo/","text":"MCMC eGP \u00b6 TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error Experiment \u00b6 Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test GP Model \u00b6 #@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise Experiment \u00b6 @dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248 Predictions \u00b6 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] GP Model - Uncertain Inputs \u00b6 def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] Results \u00b6 Exact Known Input Error Prior","title":"MCMC eGP"},{"location":"projects/ErrorGPs/monte_carlo/demo/#mcmc-egp","text":"TLDR I did a quick experiment where I look at how we can impact the error bars when doing a fully Bayesian GP (i.e. GP with MCMC inference). I have 3 cases where I use no prior on the inputs, where I use a modest prior on the inputs, and one where I use the exact known prior on the inputs. The results are definitely different than what I'm used to because I actually trained the GP knowing the priors. The error bars were reduced which I guess makes sense. TODO : Do the MCMC where we approximate the posterior when we trained the GP with uncertain inputs. Posterior Approximation Training Exact Prior Known Input Error","title":"MCMC eGP"},{"location":"projects/ErrorGPs/monte_carlo/demo/#experiment","text":"Code Blocks Install ! pip install jax jaxlib numpyro Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.62) Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.42) Collecting numpyro \u001b[?25l Downloading https://files.pythonhosted.org/packages/b8/58/54e914bb6d8ee9196f8dbf28b81057fea81871fc171dbee03b790336d0c5/numpyro-0.2.4-py3-none-any.whl (159kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.5MB/s \u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.2.1) Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.3) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from numpyro) (4.38.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.12.0) \u001b[31mERROR: numpyro 0.2.4 has requirement jax==0.1.57, but you'll have jax 0.1.62 which is incompatible.\u001b[0m \u001b[31mERROR: numpyro 0.2.4 has requirement jaxlib==0.1.37, but you'll have jaxlib 0.1.42 which is incompatible.\u001b[0m Installing collected packages: numpyro Successfully installed numpyro-0.2.4 Imports #@title packages import time import numpy as onp from dataclasses import dataclass import jax from jax import vmap import jax.numpy as np import jax.random as random import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC , NUTS import matplotlib import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline Data #@title Data def get_data ( N = 30 , sigma_inputs = 0.15 , sigma_obs = 0.15 , N_test = 400 ): onp . random . seed ( 0 ) X = np . linspace ( - 10 , 10 , N ) # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X) Y = np . sin ( 1.0 * np . pi / 1.6 * np . cos ( 5 + . 5 * X )) Y += sigma_obs * onp . random . randn ( N ) X += sigma_inputs * onp . random . randn ( N ) Y -= np . mean ( Y ) Y /= np . std ( Y ) assert X . shape == ( N ,) assert Y . shape == ( N ,) X_test = np . linspace ( - 11 , 11 , N_test ) X_test += sigma_inputs * onp . random . randn ( N_test ) return X , Y , X_test","title":"Experiment"},{"location":"projects/ErrorGPs/monte_carlo/demo/#gp-model","text":"#@title GP Model # squared exponential kernel with diagonal noise term def kernel ( X , Z , var , length , noise , jitter = 1.0e-6 , include_noise = True ): deltaXsq = np . power (( X [:, None ] - Z ) / length , 2.0 ) k = var * np . exp ( - 0.5 * deltaXsq ) if include_noise : k += ( noise + jitter ) * np . eye ( X . shape [ 0 ]) return k def model ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15 * np.ones((Xmu.shape[0],)))) X = Xmu # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) # helper function for doing hmc inference def run_inference ( model , args , rng_key , X , Y ): start = time . time () kernel = NUTS ( model ) mcmc = MCMC ( kernel , args . num_warmup , args . num_samples , num_chains = args . num_chains , progress_bar = True ) mcmc . run ( rng_key , X , Y ) mcmc . print_summary () print ( ' \\n MCMC elapsed time:' , time . time () - start ) return mcmc . get_samples () # do GP prediction for a given set of hyperparameters. this makes use of the well-known # formula for gaussian process predictions def predict ( rng_key , X , Y , X_test , var , length , noise ): # compute kernels between train and test data, etc. k_pp = kernel ( X_test , X_test , var , length , noise , include_noise = True ) k_pX = kernel ( X_test , X , var , length , noise , include_noise = False ) k_XX = kernel ( X , X , var , length , noise , include_noise = True ) K_xx_inv = np . linalg . inv ( k_XX ) K = k_pp - np . matmul ( k_pX , np . matmul ( K_xx_inv , np . transpose ( k_pX ))) sigma_noise = np . sqrt ( np . clip ( np . diag ( K ), a_min = 0. )) * jax . random . normal ( rng_key , X_test . shape [: 1 ]) mean = np . matmul ( k_pX , np . matmul ( K_xx_inv , Y )) # we return both the mean function and a sample from the posterior predictive for the # given set of hyperparameters return mean , mean + sigma_noise","title":"GP Model"},{"location":"projects/ErrorGPs/monte_carlo/demo/#experiment_1","text":"@dataclass class args : num_data = 60 num_warmup = 100 num_chains = 1 num_samples = 1_000 device = 'cpu' sigma_inputs = 0.3 sigma_obs = 0.05 numpyro . set_platform ( args . device ) X , Y , X_test = get_data ( args . num_data , sigma_inputs = args . sigma_inputs , sigma_obs = args . sigma_obs ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( model , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:11<00:00, 96.81it/s, 7 steps of size 6.48e-01. acc. prob=0.94] mean std median 5.0% 95.0% n_eff r_hat kernel_length 1.97 0.23 1.97 1.58 2.34 650.87 1.00 kernel_noise 0.04 0.01 0.04 0.02 0.05 637.46 1.00 kernel_var 1.15 0.65 0.98 0.34 1.97 563.69 1.00 Number of divergences: 0 MCMC elapsed time: 14.073462963104248","title":"Experiment"},{"location":"projects/ErrorGPs/monte_carlo/demo/#predictions","text":"# do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"Predictions"},{"location":"projects/ErrorGPs/monte_carlo/demo/#gp-model-uncertain-inputs","text":"def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) X = numpyro . sample ( \"X\" , dist . Normal ( Xmu , 0.3 ), ) # X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:19<00:00, 56.73it/s, 15 steps of size 2.14e-01. acc. prob=0.93] mean std median 5.0% 95.0% n_eff r_hat X[0] -10.01 0.25 -9.98 -10.40 -9.59 597.56 1.00 X[1] -9.91 0.27 -9.92 -10.30 -9.45 887.43 1.00 X[2] -9.65 0.27 -9.67 -10.03 -9.16 475.77 1.00 X[3] -9.40 0.25 -9.41 -9.85 -9.04 937.07 1.00 X[4] -8.83 0.23 -8.83 -9.27 -8.51 759.05 1.00 X[5] -8.33 0.19 -8.31 -8.61 -8.01 463.77 1.00 X[6] -8.31 0.19 -8.30 -8.60 -8.01 556.40 1.00 X[7] -7.77 0.13 -7.77 -7.98 -7.57 554.29 1.00 X[8] -7.42 0.12 -7.42 -7.62 -7.23 426.67 1.00 X[9] -7.03 0.11 -7.03 -7.21 -6.84 363.45 1.00 X[10] -6.60 0.12 -6.61 -6.79 -6.41 370.03 1.00 X[11] -6.29 0.13 -6.29 -6.51 -6.10 423.12 1.00 X[12] -5.80 0.16 -5.80 -6.05 -5.54 461.56 1.00 X[13] -5.45 0.18 -5.46 -5.74 -5.15 665.71 1.00 X[14] -5.00 0.25 -5.02 -5.41 -4.61 1038.74 1.00 X[15] -4.97 0.23 -4.98 -5.32 -4.59 782.89 1.00 X[16] -4.92 0.32 -4.93 -5.42 -4.40 1370.20 1.00 X[17] -4.41 0.29 -4.41 -4.87 -3.91 1293.31 1.00 X[18] -4.00 0.31 -3.99 -4.49 -3.50 1020.15 1.00 X[19] -3.57 0.28 -3.55 -4.06 -3.14 686.66 1.00 X[20] -3.42 0.36 -3.40 -3.96 -2.79 798.33 1.00 X[21] -2.57 0.36 -2.57 -3.12 -1.96 760.66 1.00 X[22] -2.34 0.28 -2.32 -2.82 -1.92 800.18 1.00 X[23] -2.68 0.27 -2.66 -3.07 -2.18 779.41 1.00 X[24] -1.61 0.16 -1.61 -1.89 -1.38 410.05 1.01 X[25] -1.65 0.16 -1.65 -1.91 -1.38 460.06 1.00 X[26] -1.16 0.13 -1.16 -1.36 -0.96 352.57 1.01 X[27] -0.81 0.12 -0.80 -0.98 -0.59 363.98 1.01 X[28] -0.32 0.12 -0.33 -0.51 -0.13 388.07 1.01 X[29] 0.11 0.12 0.10 -0.10 0.29 393.42 1.02 X[30] 0.41 0.13 0.40 0.18 0.62 534.79 1.01 X[31] 0.93 0.19 0.92 0.63 1.24 897.67 1.00 X[32] 1.08 0.21 1.07 0.75 1.42 398.94 1.00 X[33] 1.39 0.33 1.36 0.87 1.90 443.17 1.00 X[34] 1.67 0.28 1.67 1.25 2.13 822.54 1.00 X[35] 2.07 0.30 2.07 1.60 2.55 735.84 1.00 X[36] 2.18 0.30 2.18 1.69 2.65 546.85 1.00 X[37] 3.10 0.31 3.10 2.63 3.64 1153.64 1.00 X[38] 2.92 0.28 2.93 2.46 3.35 1498.27 1.00 X[39] 3.33 0.30 3.34 2.82 3.79 1393.60 1.00 X[40] 4.02 0.26 4.03 3.64 4.48 743.00 1.00 X[41] 3.50 0.31 3.50 3.01 3.99 895.25 1.00 X[42] 3.93 0.33 3.96 3.41 4.47 667.17 1.00 X[43] 4.27 0.19 4.27 3.96 4.60 626.95 1.00 X[44] 4.89 0.16 4.88 4.62 5.14 340.23 1.00 X[45] 5.36 0.13 5.36 5.17 5.58 373.83 1.00 X[46] 5.78 0.12 5.78 5.58 5.96 307.39 1.00 X[47] 6.05 0.12 6.06 5.86 6.24 329.60 1.00 X[48] 6.65 0.13 6.64 6.45 6.86 343.38 1.00 X[49] 6.94 0.15 6.94 6.68 7.18 385.53 1.00 X[50] 7.56 0.26 7.53 7.18 8.04 551.86 1.00 X[51] 7.61 0.26 7.59 7.17 8.03 524.45 1.00 X[52] 7.63 0.21 7.63 7.25 7.95 1001.54 1.00 X[53] 8.40 0.30 8.38 7.91 8.89 576.53 1.00 X[54] 8.28 0.33 8.32 7.69 8.78 1210.06 1.00 X[55] 8.95 0.26 8.95 8.50 9.34 1332.43 1.00 X[56] 9.25 0.27 9.25 8.77 9.65 1931.84 1.00 X[57] 9.27 0.27 9.28 8.82 9.69 1255.06 1.00 X[58] 9.89 0.28 9.91 9.46 10.36 613.94 1.00 X[59] 10.22 0.28 10.20 9.78 10.66 925.47 1.00 kernel_length 1.95 0.19 1.96 1.63 2.23 440.62 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 222.75 1.00 kernel_var 1.18 0.64 1.02 0.39 2.01 423.49 1.01 Number of divergences: 0 MCMC elapsed time: 23.19466996192932 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')] def emodel ( Xmu , Y ): # set uninformative log-normal priors on our three kernel hyperparameters var = numpyro . sample ( \"kernel_var\" , dist . LogNormal ( 0.0 , 10.0 )) noise = numpyro . sample ( \"kernel_noise\" , dist . LogNormal ( 0.0 , 10.0 )) length = numpyro . sample ( \"kernel_length\" , dist . LogNormal ( 0.0 , 10.0 )) # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.15), ) Xstd = numpyro . sample ( \"Xstd\" , dist . Normal ( 0.0 , 0.3 ), sample_shape = ( Xmu . shape [ 0 ],)) X = Xmu + Xstd # X = numpyro.sample(\"X\", dist.Normal(Xmu, 0.3 * np.ones(Xmu.shape[-1])), ) # compute kernel k = kernel ( X , X , var , length , noise ) # sample Y according to the standard gaussian process formula numpyro . sample ( \"Y\" , dist . MultivariateNormal ( loc = np . zeros ( X . shape [ 0 ]), covariance_matrix = k ), obs = Y ) rng_key , rng_key_predict = random . split ( random . PRNGKey ( 0 )) # Run inference scheme samples = run_inference ( emodel , args , rng_key , X , Y , ) sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1100/1100 [00:17<00:00, 62.81it/s, 15 steps of size 2.65e-01. acc. prob=0.89] mean std median 5.0% 95.0% n_eff r_hat Xstd[0] 0.20 0.26 0.22 -0.23 0.62 792.72 1.00 Xstd[1] -0.15 0.26 -0.15 -0.63 0.22 951.20 1.00 Xstd[2] -0.09 0.26 -0.11 -0.50 0.37 832.81 1.00 Xstd[3] 0.10 0.24 0.10 -0.28 0.49 982.06 1.00 Xstd[4] -0.25 0.22 -0.26 -0.63 0.09 934.13 1.00 Xstd[5] 0.09 0.19 0.11 -0.19 0.42 531.47 1.00 Xstd[6] 0.13 0.18 0.15 -0.18 0.43 452.94 1.00 Xstd[7] -0.28 0.13 -0.28 -0.49 -0.06 495.33 1.00 Xstd[8] 0.14 0.13 0.15 -0.05 0.35 365.29 1.00 Xstd[9] -0.10 0.12 -0.10 -0.30 0.09 306.55 1.00 Xstd[10] -0.21 0.12 -0.20 -0.42 -0.02 304.69 1.00 Xstd[11] -0.05 0.13 -0.05 -0.26 0.18 284.53 1.00 Xstd[12] -0.21 0.17 -0.21 -0.49 0.05 415.29 1.00 Xstd[13] 0.52 0.19 0.50 0.20 0.81 540.49 1.00 Xstd[14] 0.12 0.24 0.11 -0.29 0.47 898.49 1.01 Xstd[15] 0.15 0.23 0.14 -0.23 0.52 1165.26 1.00 Xstd[16] -0.08 0.33 -0.10 -0.60 0.44 904.75 1.00 Xstd[17] -0.00 0.29 -0.00 -0.53 0.43 1652.78 1.00 Xstd[18] -0.01 0.32 0.00 -0.49 0.54 1462.54 1.00 Xstd[19] -0.02 0.28 -0.01 -0.47 0.47 903.68 1.00 Xstd[20] 0.14 0.36 0.17 -0.47 0.66 905.66 1.00 Xstd[21] 0.05 0.36 0.06 -0.54 0.61 648.84 1.00 Xstd[22] 0.04 0.30 0.07 -0.44 0.51 1011.25 1.00 Xstd[23] -0.01 0.27 0.00 -0.43 0.45 1237.85 1.00 Xstd[24] -0.20 0.16 -0.20 -0.45 0.06 419.10 1.00 Xstd[25] -0.69 0.16 -0.68 -0.98 -0.46 379.36 1.00 Xstd[26] -0.33 0.13 -0.33 -0.54 -0.12 320.10 1.00 Xstd[27] 0.09 0.13 0.09 -0.10 0.30 245.93 1.01 Xstd[28] 0.50 0.13 0.50 0.30 0.71 253.37 1.01 Xstd[29] -0.04 0.13 -0.04 -0.24 0.18 259.11 1.01 Xstd[30] 0.36 0.14 0.36 0.13 0.57 296.81 1.01 Xstd[31] 0.07 0.19 0.06 -0.23 0.37 539.63 1.00 Xstd[32] 0.18 0.21 0.17 -0.18 0.50 868.61 1.00 Xstd[33] -0.09 0.31 -0.14 -0.54 0.45 551.52 1.00 Xstd[34] 0.04 0.27 0.04 -0.35 0.53 1343.46 1.00 Xstd[35] -0.01 0.29 -0.01 -0.48 0.51 1573.42 1.00 Xstd[36] -0.04 0.29 -0.04 -0.51 0.44 1578.87 1.00 Xstd[37] 0.02 0.31 0.03 -0.48 0.53 2398.18 1.00 Xstd[38] -0.00 0.29 -0.00 -0.45 0.47 1411.13 1.00 Xstd[39] -0.01 0.30 -0.01 -0.49 0.48 2119.89 1.00 Xstd[40] -0.11 0.25 -0.10 -0.49 0.33 537.76 1.00 Xstd[41] 0.00 0.30 0.01 -0.48 0.51 934.64 1.00 Xstd[42] 0.09 0.32 0.12 -0.49 0.55 1000.19 1.00 Xstd[43] -0.61 0.20 -0.60 -0.92 -0.28 716.21 1.00 Xstd[44] 0.31 0.15 0.32 0.08 0.57 487.44 1.00 Xstd[45] -0.49 0.12 -0.48 -0.68 -0.28 426.20 1.00 Xstd[46] 0.30 0.12 0.30 0.11 0.49 383.15 1.00 Xstd[47] 0.34 0.12 0.34 0.15 0.53 329.32 1.00 Xstd[48] -0.21 0.13 -0.22 -0.41 -0.01 383.74 1.00 Xstd[49] -0.12 0.15 -0.13 -0.37 0.10 392.93 1.00 Xstd[50] 0.03 0.25 0.01 -0.37 0.40 668.25 1.00 Xstd[51] 0.05 0.25 0.03 -0.37 0.46 928.56 1.00 Xstd[52] 0.26 0.22 0.24 -0.12 0.60 776.53 1.00 Xstd[53] -0.14 0.33 -0.17 -0.62 0.48 672.39 1.00 Xstd[54] 0.06 0.32 0.08 -0.49 0.54 1436.10 1.00 Xstd[55] 0.06 0.26 0.05 -0.35 0.49 1649.02 1.00 Xstd[56] -0.02 0.29 -0.02 -0.47 0.44 1683.10 1.00 Xstd[57] -0.01 0.27 -0.01 -0.42 0.48 1384.29 1.00 Xstd[58] 0.05 0.28 0.06 -0.37 0.54 1061.70 1.00 Xstd[59] -0.06 0.26 -0.08 -0.46 0.39 1705.82 1.00 kernel_length 1.93 0.21 1.94 1.57 2.26 224.70 1.01 kernel_noise 0.00 0.00 0.00 0.00 0.01 262.87 1.00 kernel_var 1.15 0.62 1.00 0.41 1.95 344.59 1.00 Number of divergences: 0 MCMC elapsed time: 19.7586088180542 # do prediction vmap_args = ( random . split ( rng_key_predict , args . num_samples * args . num_chains ), samples [ 'kernel_var' ], samples [ 'kernel_length' ], samples [ 'kernel_noise' ]) means , predictions = vmap ( lambda rng_key , var , length , noise : predict ( rng_key , X , Y , X_test , var , length , noise ))( * vmap_args ) mean_prediction = onp . mean ( means , axis = 0 ) percentiles = onp . percentile ( predictions , [ 5.0 , 95.0 ], axis = 0 ) # make plots fig , ax = plt . subplots ( 1 , 1 ) # plot training data ax . plot ( X , Y , 'kx' ) # plot 90% confidence level of predictions ax . fill_between ( X_test , percentiles [ 0 , :], percentiles [ 1 , :], color = 'lightblue' ) # plot mean prediction ax . plot ( X_test , mean_prediction , 'blue' , ls = 'solid' , lw = 2.0 ) ax . set ( xlabel = \"X\" , ylabel = \"Y\" , title = \"Mean predictions with 90% CI\" ) [Text(0, 0.5, 'Y'), Text(0.5, 0, 'X'), Text(0.5, 1.0, 'Mean predictions with 90% CI')]","title":"GP Model - Uncertain Inputs"},{"location":"projects/ErrorGPs/monte_carlo/demo/#results","text":"Exact Known Input Error Prior","title":"Results"},{"location":"projects/similarity/","text":"Similarity Measures \u00b6 Description I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation \u00b6 In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons \u00b6 In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors \u00b6 Summary In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. In particular, I investigate the Important Links Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk Software: PySim \u00b6 Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links Github Repository","title":"Overview"},{"location":"projects/similarity/#similarity-measures","text":"Description I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed.","title":"Similarity Measures"},{"location":"projects/similarity/#kernel-parameter-estimation","text":"In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links Main Project Page LaTeX Doc FFT Talk","title":"Kernel Parameter Estimation"},{"location":"projects/similarity/#information-measures-for-climate-model-comparisons","text":"In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links Main Projecct Page LaTeX Doc FFT Talk","title":"Information Measures for Climate Model Comparisons"},{"location":"projects/similarity/#information-measures-for-drought-factors","text":"Summary In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. In particular, I investigate the Important Links Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk","title":"Information Measures for Drought Factors"},{"location":"projects/similarity/#software-pysim","text":"Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links Github Repository","title":"Software: PySim"},{"location":"resources/","text":"My Resources \u00b6 Python \u00b6 I use python as my primary programming language and I have tried to really make use of what the community has to offer to help me with my tasks. I do things from scratch in the beginning but then I heavily refactor and prefer to use popular, up-to-date libraries with well-tested code. I mostly work with machine learning Standard Stack Integrated Development Environments Earth Science Stack Deep Learning Software Good Programming Practices Gaussian Processes \u00b6 I have worked extensively with Gaussian processes (GPs) and I have also done quite a lot of research on the literature and software side of most modern GPs.I have a detailed webpage where I outline some of the most modern Gaussian processs. Webpage: jejjohnson.github.io/gp_model_zoo I also work a lot with Gaussian process regression and how they deal with uncertainty. In this setting, I have looked specifically at what happens when we want to propagate uncertainty from the inputs through the GP function through training and/or predictions. I have had some success with this but the literature was very scattered when I first started. So I decided to Webpage: jejjohnson.github.io/uncertain_gps","title":"My Resources"},{"location":"resources/#my-resources","text":"","title":"My Resources"},{"location":"resources/#python","text":"I use python as my primary programming language and I have tried to really make use of what the community has to offer to help me with my tasks. I do things from scratch in the beginning but then I heavily refactor and prefer to use popular, up-to-date libraries with well-tested code. I mostly work with machine learning Standard Stack Integrated Development Environments Earth Science Stack Deep Learning Software Good Programming Practices","title":"Python"},{"location":"resources/#gaussian-processes","text":"I have worked extensively with Gaussian processes (GPs) and I have also done quite a lot of research on the literature and software side of most modern GPs.I have a detailed webpage where I outline some of the most modern Gaussian processs. Webpage: jejjohnson.github.io/gp_model_zoo I also work a lot with Gaussian process regression and how they deal with uncertainty. In this setting, I have looked specifically at what happens when we want to propagate uncertainty from the inputs through the GP function through training and/or predictions. I have had some success with this but the literature was very scattered when I first started. So I decided to Webpage: jejjohnson.github.io/uncertain_gps","title":"Gaussian Processes"},{"location":"resources/dl_overview/","text":"Software \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? Anatomy of good DL software Convergence of the Libraries So what to choose? List of Software Core Packages TensorFlow (TF) PyTorch Other Packages What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So what to choose? \u00b6 There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health. List of Software \u00b6 There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning. Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Software"},{"location":"resources/dl_overview/#software","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? Anatomy of good DL software Convergence of the Libraries So what to choose? List of Software Core Packages TensorFlow (TF) PyTorch Other Packages","title":"Software"},{"location":"resources/dl_overview/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"resources/dl_overview/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"resources/dl_overview/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.","title":"Convergence of the Libraries"},{"location":"resources/dl_overview/#so-what-to-choose","text":"There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"So what to choose?"},{"location":"resources/dl_overview/#list-of-software","text":"There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning.","title":"List of Software"},{"location":"resources/dl_overview/#core-packages","text":"","title":"Core Packages"},{"location":"resources/dl_overview/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"resources/dl_overview/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"resources/dl_overview/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"resources/mkdocs/","text":"MKDocs Tips n Tricks \u00b6 Deployment \u00b6 If you have a heavy website, it helps to just reload what you have edited. The reason I do this is because markdown doesn't support pymdown formats so it's difficult to see what you're actually doing if you want fancy formats. So use the following command to only reload the page you are currently editing. mkdocs serve --dirtyreload Todo I want to see if this works on a remote server. In theory, I should be able to run the following command: mkdocs serve --dirtyreload --dev-addr 3009 Then I would be able to ssh into the server: ssh erc -L 3009 :localhost:3009 Or I could use the built-in tunneling with VSCode. Extensions \u00b6 Admonition \u00b6 This is great, it allows me to add little blocks of things I find interesting. Note with title !!! note \"Title\" without title !!! note \"\" Collapsible !!! - open by default ??? - collapsible Defaults \u00b6 Note Button Can use note or seealso Abstract Button Can use abstract,summary,tldr Info Button Can use info,todo Fire Button tip,hint,important Checkmark Button success, check, done Question Mark question,help,faq Warning Sign warning,caution,attention X Mark failure,fail,missing Danger sign danger, error Bug sign bug Numbered List bug Quotation Marks quote, cite What Now? \u00b6 Question How does this work if I want to design my own symbol? Todo Design my own symbol and add it to the research notebook! CodeHilite \u00b6 Very simple, we have codeblocks with highlights. Using Python: import numpy as np and using Bash: echo \"Times are changing\" Colocate Different bits of Code \u00b6 I can also collapse them! TensorFlow import tensorflow as tf PyTorch import torch Line Numbers \u00b6 We can also enable line numbers with the linenums: true extension. import numpy as np import matplotlib.pyplot as plt Highlight Line Numbers \u00b6 import numpy as np import matplotlib.pyplot as plt from scipy import * # Don't do this!! Meta Data \u00b6 --- title: PyTest Tricks description: The deep learning python stack authors: - J. Emmanuel Johnson path: docs/snippets/testing source: pytest.md ---","title":"MKDocs Tips"},{"location":"resources/mkdocs/#mkdocs-tips-n-tricks","text":"","title":"MKDocs Tips n Tricks"},{"location":"resources/mkdocs/#deployment","text":"If you have a heavy website, it helps to just reload what you have edited. The reason I do this is because markdown doesn't support pymdown formats so it's difficult to see what you're actually doing if you want fancy formats. So use the following command to only reload the page you are currently editing. mkdocs serve --dirtyreload Todo I want to see if this works on a remote server. In theory, I should be able to run the following command: mkdocs serve --dirtyreload --dev-addr 3009 Then I would be able to ssh into the server: ssh erc -L 3009 :localhost:3009 Or I could use the built-in tunneling with VSCode.","title":"Deployment"},{"location":"resources/mkdocs/#extensions","text":"","title":"Extensions"},{"location":"resources/mkdocs/#admonition","text":"This is great, it allows me to add little blocks of things I find interesting. Note with title !!! note \"Title\" without title !!! note \"\" Collapsible !!! - open by default ??? - collapsible","title":"Admonition"},{"location":"resources/mkdocs/#defaults","text":"Note Button Can use note or seealso Abstract Button Can use abstract,summary,tldr Info Button Can use info,todo Fire Button tip,hint,important Checkmark Button success, check, done Question Mark question,help,faq Warning Sign warning,caution,attention X Mark failure,fail,missing Danger sign danger, error Bug sign bug Numbered List bug Quotation Marks quote, cite","title":"Defaults"},{"location":"resources/mkdocs/#what-now","text":"Question How does this work if I want to design my own symbol? Todo Design my own symbol and add it to the research notebook!","title":"What Now?"},{"location":"resources/mkdocs/#codehilite","text":"Very simple, we have codeblocks with highlights. Using Python: import numpy as np and using Bash: echo \"Times are changing\"","title":"CodeHilite"},{"location":"resources/mkdocs/#colocate-different-bits-of-code","text":"I can also collapse them! TensorFlow import tensorflow as tf PyTorch import torch","title":"Colocate Different bits of Code"},{"location":"resources/mkdocs/#line-numbers","text":"We can also enable line numbers with the linenums: true extension. import numpy as np import matplotlib.pyplot as plt","title":"Line Numbers"},{"location":"resources/mkdocs/#highlight-line-numbers","text":"import numpy as np import matplotlib.pyplot as plt from scipy import * # Don't do this!!","title":"Highlight Line Numbers"},{"location":"resources/mkdocs/#meta-data","text":"--- title: PyTest Tricks description: The deep learning python stack authors: - J. Emmanuel Johnson path: docs/snippets/testing source: pytest.md ---","title":"Meta Data"},{"location":"resources/my_workflow/","text":"My WorkFlow \u00b6 0. Ideas - Markdown \u00b6 1. Prototyping - JupyterLab \u00b6 2. Good - VSCode \u00b6 3. Theory & Empirical Documentation - Docsify \u00b6","title":"My WorkFlow"},{"location":"resources/my_workflow/#my-workflow","text":"","title":"My WorkFlow"},{"location":"resources/my_workflow/#0-ideas-markdown","text":"","title":"0. Ideas - Markdown"},{"location":"resources/my_workflow/#1-prototyping-jupyterlab","text":"","title":"1. Prototyping - JupyterLab"},{"location":"resources/my_workflow/#2-good-vscode","text":"","title":"2. Good - VSCode"},{"location":"resources/my_workflow/#3-theory-empirical-documentation-docsify","text":"","title":"3. Theory &amp; Empirical Documentation - Docsify"},{"location":"resources/reproducibility/","text":"Reproducibility \u00b6 My Philosophy \u00b6 Documentation \u00b6 Packages \u00b6 PyTorch Lightning \u00b6 Kedro (Quantum Black Labs) \u00b6","title":"Reproducibility"},{"location":"resources/reproducibility/#reproducibility","text":"","title":"Reproducibility"},{"location":"resources/reproducibility/#my-philosophy","text":"","title":"My Philosophy"},{"location":"resources/reproducibility/#documentation","text":"","title":"Documentation"},{"location":"resources/reproducibility/#packages","text":"","title":"Packages"},{"location":"resources/reproducibility/#pytorch-lightning","text":"","title":"PyTorch Lightning"},{"location":"resources/reproducibility/#kedro-quantum-black-labs","text":"","title":"Kedro (Quantum Black Labs)"},{"location":"resources/machine_learning/","text":"Machine Learning \u00b6 Made With ML - Newsletter","title":"Machine Learning"},{"location":"resources/machine_learning/#machine-learning","text":"Made With ML - Newsletter","title":"Machine Learning"},{"location":"resources/machine_learning/bayesian/","text":"Bayesian ML \u00b6 Resources \u00b6 Bayesian Data Analysis Course at Alto | Python Demos","title":"Bayesian ML"},{"location":"resources/machine_learning/bayesian/#bayesian-ml","text":"","title":"Bayesian ML"},{"location":"resources/machine_learning/bayesian/#resources","text":"Bayesian Data Analysis Course at Alto | Python Demos","title":"Resources"},{"location":"resources/machine_learning/books/","text":"Recommended Books \u00b6 Deep Learning \u00b6 Bayesian \u00b6 A Student's Guide to Bayesian Statitics - Lambert | Solutions Bayesian Data Analysis - Gelman Statistical Rethinking - McElreath Statistical Thinking for the 21 st Century - Online Book - Russell A Poldrack Bayes' Rule: A Tutorial Introduction to Bayes Analysis - Stone Bayesian Methods for Hackers - Davidson-Pilon Bayesian Statistics for Beginners: Step-by-Step Approach - Donovan","title":"Recommended Books"},{"location":"resources/machine_learning/books/#recommended-books","text":"","title":"Recommended Books"},{"location":"resources/machine_learning/books/#deep-learning","text":"","title":"Deep Learning"},{"location":"resources/machine_learning/books/#bayesian","text":"A Student's Guide to Bayesian Statitics - Lambert | Solutions Bayesian Data Analysis - Gelman Statistical Rethinking - McElreath Statistical Thinking for the 21 st Century - Online Book - Russell A Poldrack Bayes' Rule: A Tutorial Introduction to Bayes Analysis - Stone Bayesian Methods for Hackers - Davidson-Pilon Bayesian Statistics for Beginners: Step-by-Step Approach - Donovan","title":"Bayesian"},{"location":"resources/machine_learning/class_notes/","text":"Class Notes \u00b6 ECE1513H: Introduction to Machine Learning Has a nice Jax Tutorial","title":"Class Notes"},{"location":"resources/machine_learning/class_notes/#class-notes","text":"ECE1513H: Introduction to Machine Learning Has a nice Jax Tutorial","title":"Class Notes"},{"location":"resources/machine_learning/deep_learning/","text":"Deep Learning \u00b6 Batch Normalization \u00b6 Why Batch Norm Causes Exploding Gradients - Kyle Luther Books \u00b6 Dive Into Deep Learning Code \u00b6 MadewithML - Lessons","title":"Deep Learning"},{"location":"resources/machine_learning/deep_learning/#deep-learning","text":"","title":"Deep Learning"},{"location":"resources/machine_learning/deep_learning/#batch-normalization","text":"Why Batch Norm Causes Exploding Gradients - Kyle Luther","title":"Batch Normalization"},{"location":"resources/machine_learning/deep_learning/#books","text":"Dive Into Deep Learning","title":"Books"},{"location":"resources/machine_learning/deep_learning/#code","text":"MadewithML - Lessons","title":"Code"},{"location":"resources/machine_learning/interpretability/","text":"Interpretable Machine Learning \u00b6 Packages \u00b6 interpret","title":"Interpretable Machine Learning"},{"location":"resources/machine_learning/interpretability/#interpretable-machine-learning","text":"","title":"Interpretable Machine Learning"},{"location":"resources/machine_learning/interpretability/#packages","text":"interpret","title":"Packages"},{"location":"resources/machine_learning/nns/","text":"Neural Networks \u00b6 From Scratch \u00b6 Neural Networks in 100 lines of pure Python - Julian Eisenschlos Micrograd - Karpathy","title":"Neural Networks"},{"location":"resources/machine_learning/nns/#neural-networks","text":"","title":"Neural Networks"},{"location":"resources/machine_learning/nns/#from-scratch","text":"Neural Networks in 100 lines of pure Python - Julian Eisenschlos Micrograd - Karpathy","title":"From Scratch"},{"location":"resources/machine_learning/physics/","text":"Physic-Informed Machine Learning \u00b6 Deep Learning \u00b6 General Overview","title":"Physic-Informed Machine Learning"},{"location":"resources/machine_learning/physics/#physic-informed-machine-learning","text":"","title":"Physic-Informed Machine Learning"},{"location":"resources/machine_learning/physics/#deep-learning","text":"General Overview","title":"Deep Learning"},{"location":"resources/machine_learning/statistics/","text":"Statistics \u00b6 Common Statistical Tests are Linear Models - George Ho (Jonas Kristoffer Lindelov - Python Port)","title":"Statistics"},{"location":"resources/machine_learning/statistics/#statistics","text":"Common Statistical Tests are Linear Models - George Ho (Jonas Kristoffer Lindelov - Python Port)","title":"Statistics"},{"location":"resources/math/bisection/","text":"Bisection Method \u00b6 Resources \u00b6 Bisection Method from Scratch - github","title":"Bisection Method"},{"location":"resources/math/bisection/#bisection-method","text":"","title":"Bisection Method"},{"location":"resources/math/bisection/#resources","text":"Bisection Method from Scratch - github","title":"Resources"},{"location":"resources/python/ides/","text":"Integraded Development Environment (IDE) \u00b6 So this is something that MATLAB and R users don't have to deal with: which IDE should I use ? I'm sorry but there is not correct answer, there are many and it all depends on what is your objective. I will list a few suggestions below and then you can decide for yourself which one you need to choose. Coding Interactively ( Recommended ) Full Featured Python Editors 1. Jack of All Trades ( Recommended ) 2. Python Extremist 3. The Hacker 4. Former MATLAB users Coding Interactively ( Recommended ) \u00b6 So first and foremost, I am going to suggest that you use JupyterLab . It is an interactive environment to allow you to program step-by-step. You can visualize tables of data, run code and also visualize figures. The part I mentioned is mainly the 'Jupyter Notebook'. The lab features much more. It has a terminal, a text editor and you can also visualize the directories without leaving the notebook environment. It's the ultimate program-as-you-go type of environment. Most people use it as default and I personally always start coding in this environment right away when I want to work on something. So hop onboard! Note : A lot of just stay with Jupyter Notebooks only for whatever reason they prefer. JupyterLab includes jupyter notebook. So you can always switch to the traditional notebook environment whenever you wish. You just need to switch to tree view by changing the URL in your browser from http://localhost:8888/lab to http://localhost:8888/tree . Tutorials Blog - JupyterLab: Evolution of the Jupyter Notebook > A nice walkthrough of the lab interface. Blog - Working Efficiently with JupyterLab Notebooks > I highly recommend you look through this as it will advise you how to use best programming practices. A lot of people have spent hours on their code with an 'apparent bug' when in reality they just ran the notebooks out of order or something like that. Working with SLURM Blog - Running Jupyter Notebooks Remotely with Slurm Blog - Running Jupyter Lab Remotely JupyterLab Extension Jumping Feet First Into Code If you just want to try something that's relatively not so complicated, then I highly recommend you just using a google colaboratory notebook . It's a Jupyter Notebook(-like) interface, most python packages that you will need are already installed, you have access to CPUs, RAM, GPUs and even TPUs. I use it all the time when I want to just test something out or try something new. Full Featured Python Editors \u00b6 The editors above are special cases. But if you want a full featured Python editor then I would highly suggest the ones I list below. You may not think you need one, especially those dedicated users of JupyterLab. But I can assure you that if you want to make good reproducible code. 1. Jack of All Trades ( Recommended ) \u00b6 The most popular IDE as of now would be Visual Studio Code (VSCode). I really like it because it is hackable like Sublime or Atom but it also just works . I rarely run into IDE-breaking changes. It's also fairly lightweight and supports remote computing via ssh out of the box! Backed by Microsoft, it is definitely my most recommended IDE for almost any open-source programming language. It may support all languages but the Python specifics are great. They even have some support Jupyter Notebooks and it comes shipped with the Anaconda distribution . 2. Python Extremist \u00b6 Probably the best IDE out there that is Python specific is PyCharm . It's backed by JetBrains and they have been around for a while. Source : 9 Reasons You Should Be Using PyCharm (2015) - Michael Kennedy 3. The Hacker \u00b6 I would suggest that you learn VIM and hack your way into creating an IDE for yourself. I'm not a proficient VIM user but I can definitely see the benefits if you would like to be \"one with your terminal\". In edition, you will be able to edit anywhere as long as you have access to a terminal. I often say Python programmers (that don't use the out-of-the-box Anaconda GUI) have more abilities with the terminal that say MATLAB or SPSS users simply because python users typically spend quite a large amount of time trying to configure things. Especially when we want to use stuff remotely. But VIM users...are a different breed altogether. So needless to say the learning curve is huge. But...the rewards will be Jordi level heights. Source : Vim as a Python IDE, or Python IDE as Vim (2013) - Dmitry Filippov Tutorials Blog: Real Python - VIM and Python: A Match Made in Heaven 4. Former MATLAB users \u00b6 If you're coming from MATLAB and want to minimize the amount of changes you do during the transition, then I recommend that you start with Spyder . It is the most complete environment available to date for people who like working with everything at their fingertips in front of them; i.e. script, terminal, variable explorer, etc. It was put on hiatus for a while but it is now sponsored by QuantSight and NumFOCUS which means it has a proper team of devs. Source : Spyder Blog Remote Development So there are ways to utilize remote kernels (i.e. python environments in other locations). I personally haven't tested any way to do it but I'll like some resources below. Tutorials Spyder Docs - Connect to an external kernel Blog - Connecting Spyder IDE to a remote IPython kernel (2019) Blog - How to connect your Spyder IDE to an external ipython kernel with SSH PuTTY tunnel (2019)","title":"Python IDEs"},{"location":"resources/python/ides/#integraded-development-environment-ide","text":"So this is something that MATLAB and R users don't have to deal with: which IDE should I use ? I'm sorry but there is not correct answer, there are many and it all depends on what is your objective. I will list a few suggestions below and then you can decide for yourself which one you need to choose. Coding Interactively ( Recommended ) Full Featured Python Editors 1. Jack of All Trades ( Recommended ) 2. Python Extremist 3. The Hacker 4. Former MATLAB users","title":"Integraded Development Environment (IDE)"},{"location":"resources/python/ides/#coding-interactively-recommended","text":"So first and foremost, I am going to suggest that you use JupyterLab . It is an interactive environment to allow you to program step-by-step. You can visualize tables of data, run code and also visualize figures. The part I mentioned is mainly the 'Jupyter Notebook'. The lab features much more. It has a terminal, a text editor and you can also visualize the directories without leaving the notebook environment. It's the ultimate program-as-you-go type of environment. Most people use it as default and I personally always start coding in this environment right away when I want to work on something. So hop onboard! Note : A lot of just stay with Jupyter Notebooks only for whatever reason they prefer. JupyterLab includes jupyter notebook. So you can always switch to the traditional notebook environment whenever you wish. You just need to switch to tree view by changing the URL in your browser from http://localhost:8888/lab to http://localhost:8888/tree . Tutorials Blog - JupyterLab: Evolution of the Jupyter Notebook > A nice walkthrough of the lab interface. Blog - Working Efficiently with JupyterLab Notebooks > I highly recommend you look through this as it will advise you how to use best programming practices. A lot of people have spent hours on their code with an 'apparent bug' when in reality they just ran the notebooks out of order or something like that. Working with SLURM Blog - Running Jupyter Notebooks Remotely with Slurm Blog - Running Jupyter Lab Remotely JupyterLab Extension Jumping Feet First Into Code If you just want to try something that's relatively not so complicated, then I highly recommend you just using a google colaboratory notebook . It's a Jupyter Notebook(-like) interface, most python packages that you will need are already installed, you have access to CPUs, RAM, GPUs and even TPUs. I use it all the time when I want to just test something out or try something new.","title":"Coding Interactively (Recommended)"},{"location":"resources/python/ides/#full-featured-python-editors","text":"The editors above are special cases. But if you want a full featured Python editor then I would highly suggest the ones I list below. You may not think you need one, especially those dedicated users of JupyterLab. But I can assure you that if you want to make good reproducible code.","title":"Full Featured Python Editors"},{"location":"resources/python/ides/#1-jack-of-all-trades-recommended","text":"The most popular IDE as of now would be Visual Studio Code (VSCode). I really like it because it is hackable like Sublime or Atom but it also just works . I rarely run into IDE-breaking changes. It's also fairly lightweight and supports remote computing via ssh out of the box! Backed by Microsoft, it is definitely my most recommended IDE for almost any open-source programming language. It may support all languages but the Python specifics are great. They even have some support Jupyter Notebooks and it comes shipped with the Anaconda distribution .","title":"1. Jack of All Trades (Recommended)"},{"location":"resources/python/ides/#2-python-extremist","text":"Probably the best IDE out there that is Python specific is PyCharm . It's backed by JetBrains and they have been around for a while. Source : 9 Reasons You Should Be Using PyCharm (2015) - Michael Kennedy","title":"2. Python Extremist"},{"location":"resources/python/ides/#3-the-hacker","text":"I would suggest that you learn VIM and hack your way into creating an IDE for yourself. I'm not a proficient VIM user but I can definitely see the benefits if you would like to be \"one with your terminal\". In edition, you will be able to edit anywhere as long as you have access to a terminal. I often say Python programmers (that don't use the out-of-the-box Anaconda GUI) have more abilities with the terminal that say MATLAB or SPSS users simply because python users typically spend quite a large amount of time trying to configure things. Especially when we want to use stuff remotely. But VIM users...are a different breed altogether. So needless to say the learning curve is huge. But...the rewards will be Jordi level heights. Source : Vim as a Python IDE, or Python IDE as Vim (2013) - Dmitry Filippov Tutorials Blog: Real Python - VIM and Python: A Match Made in Heaven","title":"3. The Hacker"},{"location":"resources/python/ides/#4-former-matlab-users","text":"If you're coming from MATLAB and want to minimize the amount of changes you do during the transition, then I recommend that you start with Spyder . It is the most complete environment available to date for people who like working with everything at their fingertips in front of them; i.e. script, terminal, variable explorer, etc. It was put on hiatus for a while but it is now sponsored by QuantSight and NumFOCUS which means it has a proper team of devs. Source : Spyder Blog Remote Development So there are ways to utilize remote kernels (i.e. python environments in other locations). I personally haven't tested any way to do it but I'll like some resources below. Tutorials Spyder Docs - Connect to an external kernel Blog - Connecting Spyder IDE to a remote IPython kernel (2019) Blog - How to connect your Spyder IDE to an external ipython kernel with SSH PuTTY tunnel (2019)","title":"4. Former MATLAB users"},{"location":"resources/python/tidbits/","text":"Interesting Tidbits \u00b6 Einsum Notation \u00b6 Understanding PyTorch Einsum Einsum is all you need Einstein Summation in Numpy Software Engineering \u00b6 Awesome SWE 4 ML Testing \u00b6 Annotating Code Tests and Selectively Running Tests - Blog XArray \u00b6 XBatcher Saving and Loading Models \u00b6 sklearn \u00b6 Sklearn Docs ONNX sklearn-onnx Deploying an Sklearn model with onnx and fast-api - blog Deployment","title":"Interesting Tidbits"},{"location":"resources/python/tidbits/#interesting-tidbits","text":"","title":"Interesting Tidbits"},{"location":"resources/python/tidbits/#einsum-notation","text":"Understanding PyTorch Einsum Einsum is all you need Einstein Summation in Numpy","title":"Einsum Notation"},{"location":"resources/python/tidbits/#software-engineering","text":"Awesome SWE 4 ML","title":"Software Engineering"},{"location":"resources/python/tidbits/#testing","text":"Annotating Code Tests and Selectively Running Tests - Blog","title":"Testing"},{"location":"resources/python/tidbits/#xarray","text":"XBatcher","title":"XArray"},{"location":"resources/python/tidbits/#saving-and-loading-models","text":"","title":"Saving and Loading Models"},{"location":"resources/python/tidbits/#sklearn","text":"Sklearn Docs ONNX sklearn-onnx Deploying an Sklearn model with onnx and fast-api - blog Deployment","title":"sklearn"},{"location":"resources/python/code/einsum/","text":"Einsum \u00b6 Really Fascinating Tutorials \u00b6 Einsum Extensions: A Wish List for Readable Matrix Ops - Madison May","title":"Einsum"},{"location":"resources/python/code/einsum/#einsum","text":"Really Fascinating","title":"Einsum"},{"location":"resources/python/code/einsum/#tutorials","text":"Einsum Extensions: A Wish List for Readable Matrix Ops - Madison May","title":"Tutorials"},{"location":"resources/python/code/large_scale/","text":"Large Scale \u00b6 Tutorials \u00b6 Small Big Data \u00b6 These are tutorials to see how we can Small Big Data: Using Numpy and Pandas when your data doesn't fit - Itamar Turner-Trauring - (PyCon 2020) Dask - Youtube Page | Matthew Playlist Packages \u00b6 Vaex \u00b6 This is an out-of-core dataframes library for Python, ML and visualization. This is sort of like pandas with some built-in functions. It also plays well with scikit-learn. Train 1 Billion Samples with Vaex and Scikit-Learn","title":"Large Scale"},{"location":"resources/python/code/large_scale/#large-scale","text":"","title":"Large Scale"},{"location":"resources/python/code/large_scale/#tutorials","text":"","title":"Tutorials"},{"location":"resources/python/code/large_scale/#small-big-data","text":"These are tutorials to see how we can Small Big Data: Using Numpy and Pandas when your data doesn't fit - Itamar Turner-Trauring - (PyCon 2020) Dask - Youtube Page | Matthew Playlist","title":"Small Big Data"},{"location":"resources/python/code/large_scale/#packages","text":"","title":"Packages"},{"location":"resources/python/code/large_scale/#vaex","text":"This is an out-of-core dataframes library for Python, ML and visualization. This is sort of like pandas with some built-in functions. It also plays well with scikit-learn. Train 1 Billion Samples with Vaex and Scikit-Learn","title":"Vaex"},{"location":"resources/python/good_code/","text":"Tutorials \u00b6 Overviews \u00b6 Python 102 for scientific computing and data analysis Good overview. Includes things like organization, testing, documentation, logging, cli, and optimization HyperModern Python Part I - Setup Part II - Testing Part III - Linting Part IV - Typing Part V - Documentation Part VI - CI/CD Python for HPC: Community Materials Creeds \u00b6 Better Scientifc Software (BBSW)","title":"Tutorials"},{"location":"resources/python/good_code/#tutorials","text":"","title":"Tutorials"},{"location":"resources/python/good_code/#overviews","text":"Python 102 for scientific computing and data analysis Good overview. Includes things like organization, testing, documentation, logging, cli, and optimization HyperModern Python Part I - Setup Part II - Testing Part III - Linting Part IV - Typing Part V - Documentation Part VI - CI/CD Python for HPC: Community Materials","title":"Overviews"},{"location":"resources/python/good_code/#creeds","text":"Better Scientifc Software (BBSW)","title":"Creeds"},{"location":"resources/python/good_code/documentation/","text":"Documentation \u00b6 Tutorials \u00b6 Documentation with Sphinx - Betterscientificsoftware Templates \u00b6 scikit-learn Function Documentation Template sklearn-template | documentation Sphinx Gallery Template","title":"Documentation"},{"location":"resources/python/good_code/documentation/#documentation","text":"","title":"Documentation"},{"location":"resources/python/good_code/documentation/#tutorials","text":"Documentation with Sphinx - Betterscientificsoftware","title":"Tutorials"},{"location":"resources/python/good_code/documentation/#templates","text":"scikit-learn Function Documentation Template sklearn-template | documentation Sphinx Gallery Template","title":"Templates"},{"location":"resources/python/good_code/experiments/","text":"Experiments \u00b6 Releasing Research Code Hydra \u00b6 Minimal Example - github Comet \u00b6 PyTorch Lightning \u00b6 Using Optuna to Optimize PyTorch Lightning HyperParameters","title":"Experiments"},{"location":"resources/python/good_code/experiments/#experiments","text":"Releasing Research Code","title":"Experiments"},{"location":"resources/python/good_code/experiments/#hydra","text":"Minimal Example - github","title":"Hydra"},{"location":"resources/python/good_code/experiments/#comet","text":"","title":"Comet"},{"location":"resources/python/good_code/experiments/#pytorch-lightning","text":"Using Optuna to Optimize PyTorch Lightning HyperParameters","title":"PyTorch Lightning"},{"location":"resources/python/good_code/git/","text":"Git \u00b6 Git Workflow \u00b6 A Git workflow with branches - Karink Nudson The simplest and clearest git workflow guide you'll find. Start here. A Successful Git Branching Model - Vincent Driessen Contribution Guide \u00b6 Using VSCode for First Contributions - wiki","title":"Git"},{"location":"resources/python/good_code/git/#git","text":"","title":"Git"},{"location":"resources/python/good_code/git/#git-workflow","text":"A Git workflow with branches - Karink Nudson The simplest and clearest git workflow guide you'll find. Start here. A Successful Git Branching Model - Vincent Driessen","title":"Git Workflow"},{"location":"resources/python/good_code/git/#contribution-guide","text":"Using VSCode for First Contributions - wiki","title":"Contribution Guide"},{"location":"resources/python/good_code/good_code/","text":"Good Code \u00b6 Logging \u00b6 This is something I often use whenever I am in the process of building software and I think there are some key things that need to be documented. It is often much better than print statements. I often do this when I'm not really sure if what I did is correct in the process. It's really important to log. Especially when you're doing server computing and you need a history of what was going on. My Style * INFO - General stuff of where I am at in the program so I can follow the control of flow * DEBUG - Typically more about sizes/shapes of my matrices or possibly in the checks * WARNING - Where things could go wrong but I have ignored this part due to some reason. Tutorials * Python Logging: A Stroll Through the Source Code - [RealPython](https://realpython.com/python-logging-source-code/) * Python Logging Cheat Sheet - [gist](https://gist.github.com/jonepl/dd5dc90a5bc1b86b2fc2b3a244af7fc6) * The Hitchhikers Guide to Python: Logging - [blog](https://docs.python-guide.org/writing/logging/) * Good Logging Practice in Python - [blog](https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/) * Logging CookBook - [Python Docs](https://docs.python.org/3/howto/logging-cookbook.html) * Corey Schafer * Logging Basics - Logging to Files, Setting Levels, and Formating - [Youtube](https://www.youtube.com/watch?v=-ARI4Cz-awo&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=2&t=0s) * Logging Advanced: Loggers, Handlers, and Formatters - [Youtube](https://www.youtube.com/watch?v=jxmzY9soFXg&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=4&t=0s) Testing \u00b6 Something that we all should do but don't always do. It's important for the long run but it seems annoying for the short game. But overall, you cannot go wrong with tests; you just can't. My Style * Package - PyTest * IDE - VSCode Tutorials * Testing Python Applications with PyTest - [Blog](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest) * Getting Started with Testing in Python - [RealPython](https://realpython.com/python-testing/) * Testing Your Python Code with PyTest - [SciPy 2019](https://www.youtube.com/watch?v=LX2ksGYXJ80) * Learn PyTest in 60 Minutes: Python Unit Testing Framework - [Youtube](https://www.youtube.com/watch?v=bbp_849-RZ4) * Python Testing in VSCode - [VSCode Docs](https://code.visualstudio.com/docs/python/testing) * Eric Ma * Testing Data Science Code - [YouTube](https://www.youtube.com/watch?v=fmVbtHMHEZc) * Best Testing Practices - [PyCon 2017](https://www.youtube.com/watch?v=yACtdj1_IxE) Repository Organization \u00b6 Packaging \u00b6 Source : * PyPA * How to Package Your Python Code Type Checking \u00b6 Continuous Integration \u00b6 WorkFlow \u00b6 JupyterLab - Prototyping, Remote Computing VSCode - Package Management, Remote Computing Remote Computing - SSH (JLab, VSCode)","title":"Good Code"},{"location":"resources/python/good_code/good_code/#good-code","text":"","title":"Good Code"},{"location":"resources/python/good_code/good_code/#logging","text":"This is something I often use whenever I am in the process of building software and I think there are some key things that need to be documented. It is often much better than print statements. I often do this when I'm not really sure if what I did is correct in the process. It's really important to log. Especially when you're doing server computing and you need a history of what was going on. My Style * INFO - General stuff of where I am at in the program so I can follow the control of flow * DEBUG - Typically more about sizes/shapes of my matrices or possibly in the checks * WARNING - Where things could go wrong but I have ignored this part due to some reason. Tutorials * Python Logging: A Stroll Through the Source Code - [RealPython](https://realpython.com/python-logging-source-code/) * Python Logging Cheat Sheet - [gist](https://gist.github.com/jonepl/dd5dc90a5bc1b86b2fc2b3a244af7fc6) * The Hitchhikers Guide to Python: Logging - [blog](https://docs.python-guide.org/writing/logging/) * Good Logging Practice in Python - [blog](https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/) * Logging CookBook - [Python Docs](https://docs.python.org/3/howto/logging-cookbook.html) * Corey Schafer * Logging Basics - Logging to Files, Setting Levels, and Formating - [Youtube](https://www.youtube.com/watch?v=-ARI4Cz-awo&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=2&t=0s) * Logging Advanced: Loggers, Handlers, and Formatters - [Youtube](https://www.youtube.com/watch?v=jxmzY9soFXg&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=4&t=0s)","title":"Logging"},{"location":"resources/python/good_code/good_code/#testing","text":"Something that we all should do but don't always do. It's important for the long run but it seems annoying for the short game. But overall, you cannot go wrong with tests; you just can't. My Style * Package - PyTest * IDE - VSCode Tutorials * Testing Python Applications with PyTest - [Blog](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest) * Getting Started with Testing in Python - [RealPython](https://realpython.com/python-testing/) * Testing Your Python Code with PyTest - [SciPy 2019](https://www.youtube.com/watch?v=LX2ksGYXJ80) * Learn PyTest in 60 Minutes: Python Unit Testing Framework - [Youtube](https://www.youtube.com/watch?v=bbp_849-RZ4) * Python Testing in VSCode - [VSCode Docs](https://code.visualstudio.com/docs/python/testing) * Eric Ma * Testing Data Science Code - [YouTube](https://www.youtube.com/watch?v=fmVbtHMHEZc) * Best Testing Practices - [PyCon 2017](https://www.youtube.com/watch?v=yACtdj1_IxE)","title":"Testing"},{"location":"resources/python/good_code/good_code/#repository-organization","text":"","title":"Repository Organization"},{"location":"resources/python/good_code/good_code/#packaging","text":"Source : * PyPA * How to Package Your Python Code","title":"Packaging"},{"location":"resources/python/good_code/good_code/#type-checking","text":"","title":"Type Checking"},{"location":"resources/python/good_code/good_code/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"resources/python/good_code/good_code/#workflow","text":"JupyterLab - Prototyping, Remote Computing VSCode - Package Management, Remote Computing Remote Computing - SSH (JLab, VSCode)","title":"WorkFlow"},{"location":"resources/python/good_code/makefiles/","text":"MakeFiles \u00b6 Good Documentation \u00b6 CookierCutter A good example of a make file. Includes how one can use pdoc and mkdocs at the same time. Well Documented MakeFiles - Svenska A Really good post about how one can make good documentation for make files.","title":"MakeFiles"},{"location":"resources/python/good_code/makefiles/#makefiles","text":"","title":"MakeFiles"},{"location":"resources/python/good_code/makefiles/#good-documentation","text":"CookierCutter A good example of a make file. Includes how one can use pdoc and mkdocs at the same time. Well Documented MakeFiles - Svenska A Really good post about how one can make good documentation for make files.","title":"Good Documentation"},{"location":"resources/python/good_code/pip/","text":"Pip \u00b6 Personal Pip Package \u00b6 Make Your Project Pippable - Maria Antoniak Probably the clearest and simplest introduction you'll find. Start with this if you don't know what to do. Python: Create a pip installable package More details.","title":"Pip"},{"location":"resources/python/good_code/pip/#pip","text":"","title":"Pip"},{"location":"resources/python/good_code/pip/#personal-pip-package","text":"Make Your Project Pippable - Maria Antoniak Probably the clearest and simplest introduction you'll find. Start with this if you don't know what to do. Python: Create a pip installable package More details.","title":"Personal Pip Package"},{"location":"resources/python/good_code/pythonic/","text":"Resources \u00b6 The Little Book of Python Anti-Patterns A really good book to show some succinct pythonic ways of coding.","title":"Pythonic"},{"location":"resources/python/good_code/pythonic/#resources","text":"The Little Book of Python Anti-Patterns A really good book to show some succinct pythonic ways of coding.","title":"Resources"},{"location":"resources/python/good_code/typing/","text":"","title":"Typing"},{"location":"resources/python/packages/geospatial/","text":"Earth-Sci Stuff \u00b6 Compilation \u00b6 Essential Packages for GeoSpatial Analysis in Python - James Brennan Visualization \u00b6 Visualizing 2 Billion Pixel Rasters w. DataShader and XArray","title":"Earth-Sci Stuff"},{"location":"resources/python/packages/geospatial/#earth-sci-stuff","text":"","title":"Earth-Sci Stuff"},{"location":"resources/python/packages/geospatial/#compilation","text":"Essential Packages for GeoSpatial Analysis in Python - James Brennan","title":"Compilation"},{"location":"resources/python/packages/geospatial/#visualization","text":"Visualizing 2 Billion Pixel Rasters w. DataShader and XArray","title":"Visualization"},{"location":"resources/python/packages/jax/","text":"Jax \u00b6 Tutorials \u00b6 A First Look at Jax - Madison May Getting Started with Jax (MLPs, CNNs & RNNs) - Robert Tjarko Lange Flax * Flax: Google's First Open Source Approach to Flexibility in Machine Learning - Hackernoon Haiku * Finetuning Transformers with Jax + Haiku - Madison May Interesting Algorithms \u00b6 Kernel Least Squares Gaussian Processes","title":"Jax"},{"location":"resources/python/packages/jax/#jax","text":"","title":"Jax"},{"location":"resources/python/packages/jax/#tutorials","text":"A First Look at Jax - Madison May Getting Started with Jax (MLPs, CNNs & RNNs) - Robert Tjarko Lange Flax * Flax: Google's First Open Source Approach to Flexibility in Machine Learning - Hackernoon Haiku * Finetuning Transformers with Jax + Haiku - Madison May","title":"Tutorials"},{"location":"resources/python/packages/jax/#interesting-algorithms","text":"Kernel Least Squares Gaussian Processes","title":"Interesting Algorithms"},{"location":"resources/python/packages/logging/","text":"","title":"Logging"},{"location":"resources/python/packages/pandas/","text":"Pandas \u00b6 Resources \u00b6 Pandas Vault - A Collection of useful pandas scripts.","title":"Pandas"},{"location":"resources/python/packages/pandas/#pandas","text":"","title":"Pandas"},{"location":"resources/python/packages/pandas/#resources","text":"Pandas Vault - A Collection of useful pandas scripts.","title":"Resources"},{"location":"resources/python/packages/parallel/","text":"Parallel Programming \u00b6 Blogs \u00b6 Every Python Programmer Should Know the Not-So-Secret ThreadPool Gives a good overview of threading, asyncio and multiprocessing. Makes a case for mp . Uses the pool.map function.","title":"Parallel Programming"},{"location":"resources/python/packages/parallel/#parallel-programming","text":"","title":"Parallel Programming"},{"location":"resources/python/packages/parallel/#blogs","text":"Every Python Programmer Should Know the Not-So-Secret ThreadPool Gives a good overview of threading, asyncio and multiprocessing. Makes a case for mp . Uses the pool.map function.","title":"Blogs"},{"location":"resources/python/packages/pathlib/","text":"PathLib \u00b6 This browser does not support PDFs. Please download the PDF to view it: Download PDF . CheatSheet - Chris","title":"PathLib"},{"location":"resources/python/packages/pathlib/#pathlib","text":"This browser does not support PDFs. Please download the PDF to view it: Download PDF . CheatSheet - Chris","title":"PathLib"},{"location":"resources/python/packages/plotting/","text":"Plotting Libraries \u00b6 List \u00b6 mpld3 pygal Bokeh HoloViews Plotly Lists \u00b6 5 Python Libraries for Creating Interactive Plots - Melissa Bierly","title":"Plotting Libraries"},{"location":"resources/python/packages/plotting/#plotting-libraries","text":"","title":"Plotting Libraries"},{"location":"resources/python/packages/plotting/#list","text":"mpld3 pygal Bokeh HoloViews Plotly","title":"List"},{"location":"resources/python/packages/plotting/#lists","text":"5 Python Libraries for Creating Interactive Plots - Melissa Bierly","title":"Lists"},{"location":"resources/python/packages/pytorch/","text":"PyTorch \u00b6 Basics \u00b6 Effective PyTorch Add-Ons \u00b6 Skorch","title":"PyTorch"},{"location":"resources/python/packages/pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"resources/python/packages/pytorch/#basics","text":"Effective PyTorch","title":"Basics"},{"location":"resources/python/packages/pytorch/#add-ons","text":"Skorch","title":"Add-Ons"},{"location":"resources/python/packages/sklearn/","text":"Scikit-Learn \u00b6 Customizing \u00b6 Developing scikit-learn estimators","title":"Scikit-Learn"},{"location":"resources/python/packages/sklearn/#scikit-learn","text":"","title":"Scikit-Learn"},{"location":"resources/python/packages/sklearn/#customizing","text":"Developing scikit-learn estimators","title":"Customizing"},{"location":"resources/python/packages/sql/","text":"SQL \u00b6 I'm not good at SQL but I would like to learn. SQLite \u00b6 Fast subsets of large datasets with Pandas and SQLite - blog Teaches you a minimal way to replace large csv files within a SQL database","title":"SQL"},{"location":"resources/python/packages/sql/#sql","text":"I'm not good at SQL but I would like to learn.","title":"SQL"},{"location":"resources/python/packages/sql/#sqlite","text":"Fast subsets of large datasets with Pandas and SQLite - blog Teaches you a minimal way to replace large csv files within a SQL database","title":"SQLite"},{"location":"resources/python/remote_computing/","text":"Overview \u00b6","title":"Overview"},{"location":"resources/python/remote_computing/#overview","text":"","title":"Overview"},{"location":"resources/python/remote_computing/jupyter_slurm/","text":"JupyterLab + Slurm \u00b6 Resources : Yale Center For Research and Computing Guide Running Jupyter Notebooks Interactively with SLURM - Alexander Lab @WHOI --port-retries = 0 --ip = '*' --NotebookApp.shutdown_no_activity_timeout = 600","title":"JupyterLab + Slurm"},{"location":"resources/python/remote_computing/jupyter_slurm/#jupyterlab-slurm","text":"Resources : Yale Center For Research and Computing Guide Running Jupyter Notebooks Interactively with SLURM - Alexander Lab @WHOI --port-retries = 0 --ip = '*' --NotebookApp.shutdown_no_activity_timeout = 600","title":"JupyterLab + Slurm"},{"location":"resources/python/remote_computing/vscode/","text":"VSCode \u00b6 Extensions \u00b6 LiveServer - Local Development Server to enable live reload features for static and dynamic pages LiveShare Draw.io Code Spell Checker GitHub Extension TODO highlight Remote Containers Better Comments Path Intellisense Path Autocomplete Settings Sync Code Time Excel Viewer Remote - SSH","title":"VSCode"},{"location":"resources/python/remote_computing/vscode/#vscode","text":"","title":"VSCode"},{"location":"resources/python/remote_computing/vscode/#extensions","text":"LiveServer - Local Development Server to enable live reload features for static and dynamic pages LiveShare Draw.io Code Spell Checker GitHub Extension TODO highlight Remote Containers Better Comments Path Intellisense Path Autocomplete Settings Sync Code Time Excel Viewer Remote - SSH","title":"Extensions"},{"location":"resources/python/software_stacks/dl_software/","text":"Deep Learning Software for Python \u00b6 Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Deep Learning"},{"location":"resources/python/software_stacks/dl_software/#deep-learning-software-for-python","text":"","title":"Deep Learning Software for Python"},{"location":"resources/python/software_stacks/dl_software/#core-packages","text":"","title":"Core Packages"},{"location":"resources/python/software_stacks/dl_software/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"resources/python/software_stacks/dl_software/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"resources/python/software_stacks/dl_software/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"resources/python/software_stacks/earthsci/","text":"Earth Science Tools \u00b6 These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing. DataStructures \u00b6 Xarray \u00b6 Source : Xarray Data Structure documentation This image says a lot: is the default package for handling spatial-temporal-variable datasets. This alone has helped me contain data where I care about the meta-data as well. Numpy Arrays are great but they are limited in their retention of meta-data. In addition, it has many features that allow you to work with it from a numpy array perspective and even better from a pandas perspective. It makes the whole ordeal a lot easier. GeoPandas \u00b6 If you don't work with arrays and you prefer to use shapefiles, then I suggest using GeoPandas to store this data. In the end, it is exactly like Pandas but it has some special routines that cater to working with Earth Sci data. I am no expert and I have really only used the plotting routines and the shape extract routines. But in the end, as a data structure, this would be an easy go to with a relatively low learning curve if you're already familiar with Pandas. Manipulating ShapeFiles \u00b6 RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase. Visualization \u00b6 Built-In \u00b6 These are packages where the routine is built-in as a side option and not a fully-fledged packaged dedicated to this. Not saying that the built-in functionality isn't extensive, but typically it might use another framework to do some simple routines. xarray The xarray docs have a few examples for how one can plot. geopandas This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy A package that handles all of the projections needed for better visualizations of the globe. Works well with matplotlib, geopandas, and xarray. Tutorials Maps in Scientific Python > A great tutorial by Rabernat Dedicated Plotting Libraries \u00b6 folium This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . geoviews kepler.gl pyviz Other Useful Utilities \u00b6 Regridding: xESMF \u00b6 Ever had one dataset in xarray that had one lat,lon resolution and then you have another dataset with a different lat,lon resolution? Well, you can use this package to easily move from one coordinate grid reference to another. It removes a lot of the difficulty and it is relatively fast for small-medium sized datasets. Geometries: rioxarray \u00b6 A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else. Rasterio \u00b6","title":"Earth Science"},{"location":"resources/python/software_stacks/earthsci/#earth-science-tools","text":"These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing.","title":"Earth Science Tools"},{"location":"resources/python/software_stacks/earthsci/#datastructures","text":"","title":"DataStructures"},{"location":"resources/python/software_stacks/earthsci/#xarray","text":"Source : Xarray Data Structure documentation This image says a lot: is the default package for handling spatial-temporal-variable datasets. This alone has helped me contain data where I care about the meta-data as well. Numpy Arrays are great but they are limited in their retention of meta-data. In addition, it has many features that allow you to work with it from a numpy array perspective and even better from a pandas perspective. It makes the whole ordeal a lot easier.","title":"Xarray"},{"location":"resources/python/software_stacks/earthsci/#geopandas","text":"If you don't work with arrays and you prefer to use shapefiles, then I suggest using GeoPandas to store this data. In the end, it is exactly like Pandas but it has some special routines that cater to working with Earth Sci data. I am no expert and I have really only used the plotting routines and the shape extract routines. But in the end, as a data structure, this would be an easy go to with a relatively low learning curve if you're already familiar with Pandas.","title":"GeoPandas"},{"location":"resources/python/software_stacks/earthsci/#manipulating-shapefiles","text":"RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase.","title":"Manipulating ShapeFiles"},{"location":"resources/python/software_stacks/earthsci/#visualization","text":"","title":"Visualization"},{"location":"resources/python/software_stacks/earthsci/#built-in","text":"These are packages where the routine is built-in as a side option and not a fully-fledged packaged dedicated to this. Not saying that the built-in functionality isn't extensive, but typically it might use another framework to do some simple routines. xarray The xarray docs have a few examples for how one can plot. geopandas This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy A package that handles all of the projections needed for better visualizations of the globe. Works well with matplotlib, geopandas, and xarray. Tutorials Maps in Scientific Python > A great tutorial by Rabernat","title":"Built-In"},{"location":"resources/python/software_stacks/earthsci/#dedicated-plotting-libraries","text":"folium This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . geoviews kepler.gl pyviz","title":"Dedicated Plotting Libraries"},{"location":"resources/python/software_stacks/earthsci/#other-useful-utilities","text":"","title":"Other Useful Utilities"},{"location":"resources/python/software_stacks/earthsci/#regridding-xesmf","text":"Ever had one dataset in xarray that had one lat,lon resolution and then you have another dataset with a different lat,lon resolution? Well, you can use this package to easily move from one coordinate grid reference to another. It removes a lot of the difficulty and it is relatively fast for small-medium sized datasets.","title":"Regridding: xESMF"},{"location":"resources/python/software_stacks/earthsci/#geometries-rioxarray","text":"A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Geometries: rioxarray"},{"location":"resources/python/software_stacks/earthsci/#rasterio","text":"","title":"Rasterio"},{"location":"resources/python/software_stacks/python_stack/","text":"Python Packages \u00b6 Specialized Stack \u00b6 scikit-image Statsmodels xarray Automatic Differentiation Stack \u00b6 These programs are mainly focused on automatic differentiation (a.k.a. AutoGrad). Each package is backed by some big company (e.g. Google, Facebook, Microsoft or Amazon).There are many packages nowadays, each with their pros and cons, but I will recommend the most popular. In the beginning, static graphs (define first then run after) was the standard but nowadays the dynamic method (define/run as you go) is more standard due to its popularity amongst the research community. So the differences between many of the libraries are starting to converge. Please go to this webpage for a more detailed overview of the SOTA deep learning packages in python. Kernel Methods \u00b6 So I haven't found any designated kernel methods library ( open market?! ) but there are packages that have kernel methods within them. There are many packages that have GPs. scikit-learn This library has some of the standard kernel functions and kernel methods such as KPCA , SVMs , KRR and some kernel approximation schemes such as the nystrom method and RFF. Note : they do not have the kernel approximation schemes actually integrated into the KRR algorithm ( open market?! ). For that, you can see my implementation . Visualization Stack \u00b6 Geospatial Processing Stack \u00b6 Geopandas This would be the easiest package to use when we need to deal with shape files. It also follows the pandas syntax closes with added plotting capabilities. Rasterio Shapely rioxarray A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Python Ecosystem"},{"location":"resources/python/software_stacks/python_stack/#python-packages","text":"","title":"Python Packages"},{"location":"resources/python/software_stacks/python_stack/#specialized-stack","text":"scikit-image Statsmodels xarray","title":"Specialized Stack"},{"location":"resources/python/software_stacks/python_stack/#automatic-differentiation-stack","text":"These programs are mainly focused on automatic differentiation (a.k.a. AutoGrad). Each package is backed by some big company (e.g. Google, Facebook, Microsoft or Amazon).There are many packages nowadays, each with their pros and cons, but I will recommend the most popular. In the beginning, static graphs (define first then run after) was the standard but nowadays the dynamic method (define/run as you go) is more standard due to its popularity amongst the research community. So the differences between many of the libraries are starting to converge. Please go to this webpage for a more detailed overview of the SOTA deep learning packages in python.","title":"Automatic Differentiation Stack"},{"location":"resources/python/software_stacks/python_stack/#kernel-methods","text":"So I haven't found any designated kernel methods library ( open market?! ) but there are packages that have kernel methods within them. There are many packages that have GPs. scikit-learn This library has some of the standard kernel functions and kernel methods such as KPCA , SVMs , KRR and some kernel approximation schemes such as the nystrom method and RFF. Note : they do not have the kernel approximation schemes actually integrated into the KRR algorithm ( open market?! ). For that, you can see my implementation .","title":"Kernel Methods"},{"location":"resources/python/software_stacks/python_stack/#visualization-stack","text":"","title":"Visualization Stack"},{"location":"resources/python/software_stacks/python_stack/#geospatial-processing-stack","text":"Geopandas This would be the easiest package to use when we need to deal with shape files. It also follows the pandas syntax closes with added plotting capabilities. Rasterio Shapely rioxarray A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Geospatial Processing Stack"},{"location":"resources/python/software_stacks/standard_stack/","text":"Standard Python Stack \u00b6 For the most part, you'll see this at the top of everyone's scientific computing notebook/script: import numpy as np import pandas as pd from scipy.stats import norm from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt This is the impact of these libraries on the python community. By far, the most mature, the most robust and have the most documentation. In principle, you should be able to do almost any kind of scientific computing from start to finish with these libraries. Below I list them and I give the most useful resources that I have use (and still use). Just remember, when in doubt, stackoverflow is your best friend. Numpy \u00b6 The standard library for utilizing the most fundamental data structure for scientific computing: the array. It also has many linear algebra routines that have been optimized in C/C++ behind the scences. Often times doing things in raw python can get a massive speed up by doing things with numpy. The must have package for everyones python stack. It also has some of the best documentation for python packages. Tutorials Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 2 - Intro to Numpy Intro to Numpy A Visual Guide to Numpy From Python to Numpy 100 Exercises in Numpy Broadcasting Tutorial Einsum - I | II Scipy \u00b6 The other defacto library for doing scientific computing. This package is quite heavily linked with numpy but it does have it's own suite of routines. This library also has linear algebra routines including sparse matrices. But it also has signal processing, probability and statistics and optimization functions. Another library with some of the best documentation. Resources Documentation Scipy Lecture Notes Scikit-Learn \u00b6 This is the de facto library for machine learning. It will have all of the standard algorithms for machine learning. They also have a lot of utilities that can be useful when preprocessing or training your algorithms. The API (the famous .fit() , .predict() , and .transform() ) is great and has been adopted in many other machine learning packages. There is also no other tutorial that comes close to being helpful as the documentation. Resources Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 5 - Machine Learning Creating your own estimator in scikit-learn - Daniel Hnyk (2015) Pandas \u00b6 The also most fundamental package that's often used in the preprocessing chain is the fundamental datastructure known as a pandas table. Taking inspiration from R, this features data with meaningful meta data attached to it as columns or as rows. It also allows entries other than floats and ints. The best part is that is has reallt fast routines to do data manipulation and advanced calculations. This is the hardest package to get accustomed to compared to the previous packages but with a little time and effort, you can easily become one of the most effective data scientists around. The documentation is excellent but I don't find it as friendly as the other packages. Because it has a bit of a learning curve and there are likely more than 5 ways to do what you want to do, you'll find a lot of tutorials online. Resources Documentation Chris Albon Snippets Python Data Science Handbook - Jake Vanderplas - Chapter 3 - Intro to Pandas Greg Reda 3 part Tutorial I - Intro to Pandas Structures II - Working with DataFrames III - Using Pandas with the MovieLens Dataset Tom Augspurger - 7-Part Tutorial - Modern Pandas Chris Fonnesneck Class - Advanced Statistical Computing Class Matplotlib \u00b6 This is the default plotting library for python. Most people have a love-hate relationship with this library. I find it very easy to plot things but it gets difficult when I need to modify something down to the T. Like pandas, I think this library has a bit of a learning curve if you really want to make kick-ass plots. The documentation is good but the problem is I don't understand the data structures or language for creating plots; creating plotting libraries is quite difficult for this very reason. This is a great library but prepared to be a bit frustrated at times down the road. Resources Matplotlib Gallery Anatomy of Matplotlib An Inquiry Into Matplotlib's Figures Python Plotting with Matplotlib - Real Python Python Data Science Handbook - Jake Vanderplas - Chapter 4 - Visualization with Matplotlib Creating Publication-Quality Figures with Matplotlib Matplotlib Cheatsheet","title":"Standard"},{"location":"resources/python/software_stacks/standard_stack/#standard-python-stack","text":"For the most part, you'll see this at the top of everyone's scientific computing notebook/script: import numpy as np import pandas as pd from scipy.stats import norm from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt This is the impact of these libraries on the python community. By far, the most mature, the most robust and have the most documentation. In principle, you should be able to do almost any kind of scientific computing from start to finish with these libraries. Below I list them and I give the most useful resources that I have use (and still use). Just remember, when in doubt, stackoverflow is your best friend.","title":"Standard Python Stack"},{"location":"resources/python/software_stacks/standard_stack/#numpy","text":"The standard library for utilizing the most fundamental data structure for scientific computing: the array. It also has many linear algebra routines that have been optimized in C/C++ behind the scences. Often times doing things in raw python can get a massive speed up by doing things with numpy. The must have package for everyones python stack. It also has some of the best documentation for python packages. Tutorials Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 2 - Intro to Numpy Intro to Numpy A Visual Guide to Numpy From Python to Numpy 100 Exercises in Numpy Broadcasting Tutorial Einsum - I | II","title":"Numpy"},{"location":"resources/python/software_stacks/standard_stack/#scipy","text":"The other defacto library for doing scientific computing. This package is quite heavily linked with numpy but it does have it's own suite of routines. This library also has linear algebra routines including sparse matrices. But it also has signal processing, probability and statistics and optimization functions. Another library with some of the best documentation. Resources Documentation Scipy Lecture Notes","title":"Scipy"},{"location":"resources/python/software_stacks/standard_stack/#scikit-learn","text":"This is the de facto library for machine learning. It will have all of the standard algorithms for machine learning. They also have a lot of utilities that can be useful when preprocessing or training your algorithms. The API (the famous .fit() , .predict() , and .transform() ) is great and has been adopted in many other machine learning packages. There is also no other tutorial that comes close to being helpful as the documentation. Resources Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 5 - Machine Learning Creating your own estimator in scikit-learn - Daniel Hnyk (2015)","title":"Scikit-Learn"},{"location":"resources/python/software_stacks/standard_stack/#pandas","text":"The also most fundamental package that's often used in the preprocessing chain is the fundamental datastructure known as a pandas table. Taking inspiration from R, this features data with meaningful meta data attached to it as columns or as rows. It also allows entries other than floats and ints. The best part is that is has reallt fast routines to do data manipulation and advanced calculations. This is the hardest package to get accustomed to compared to the previous packages but with a little time and effort, you can easily become one of the most effective data scientists around. The documentation is excellent but I don't find it as friendly as the other packages. Because it has a bit of a learning curve and there are likely more than 5 ways to do what you want to do, you'll find a lot of tutorials online. Resources Documentation Chris Albon Snippets Python Data Science Handbook - Jake Vanderplas - Chapter 3 - Intro to Pandas Greg Reda 3 part Tutorial I - Intro to Pandas Structures II - Working with DataFrames III - Using Pandas with the MovieLens Dataset Tom Augspurger - 7-Part Tutorial - Modern Pandas Chris Fonnesneck Class - Advanced Statistical Computing Class","title":"Pandas"},{"location":"resources/python/software_stacks/standard_stack/#matplotlib","text":"This is the default plotting library for python. Most people have a love-hate relationship with this library. I find it very easy to plot things but it gets difficult when I need to modify something down to the T. Like pandas, I think this library has a bit of a learning curve if you really want to make kick-ass plots. The documentation is good but the problem is I don't understand the data structures or language for creating plots; creating plotting libraries is quite difficult for this very reason. This is a great library but prepared to be a bit frustrated at times down the road. Resources Matplotlib Gallery Anatomy of Matplotlib An Inquiry Into Matplotlib's Figures Python Plotting with Matplotlib - Real Python Python Data Science Handbook - Jake Vanderplas - Chapter 4 - Visualization with Matplotlib Creating Publication-Quality Figures with Matplotlib Matplotlib Cheatsheet","title":"Matplotlib"},{"location":"resources/tips/","text":"Cheat Sheets \u00b6","title":"Cheat Sheets"},{"location":"resources/tips/#cheat-sheets","text":"","title":"Cheat Sheets"},{"location":"resources/tips/colab/","text":"Colab \u00b6 Installing Packages from Conda-Forge \u00b6 Example : Cartopy # get package then extract !wget https://anaconda.org/conda-forge/cartopy/0.16.0/download/linux-64/cartopy-0.16.0-py36h81b52dc_2.tar.bz2 !tar xvjf cartopy-0.16.0-py36h81b52dc_2.tar.bz2 !cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/ # install dependencies !pip install shapely pyshp !apt install libproj-dev libgeos-dev # finally import cartopy","title":"Colab"},{"location":"resources/tips/colab/#colab","text":"","title":"Colab"},{"location":"resources/tips/colab/#installing-packages-from-conda-forge","text":"Example : Cartopy # get package then extract !wget https://anaconda.org/conda-forge/cartopy/0.16.0/download/linux-64/cartopy-0.16.0-py36h81b52dc_2.tar.bz2 !tar xvjf cartopy-0.16.0-py36h81b52dc_2.tar.bz2 !cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/ # install dependencies !pip install shapely pyshp !apt install libproj-dev libgeos-dev # finally import cartopy","title":"Installing Packages from Conda-Forge"},{"location":"resources/tips/conda/","text":"Conda \u00b6","title":"Conda"},{"location":"resources/tips/conda/#conda","text":"","title":"Conda"},{"location":"resources/tips/github/","text":"DOI for a Repository - GitHub","title":"Github"},{"location":"resources/tips/markdown/","text":"Markdown \u00b6 References \u00b6 Tutorial.md Generating Markdown Tables from Excel Files","title":"Markdown"},{"location":"resources/tips/markdown/#markdown","text":"","title":"Markdown"},{"location":"resources/tips/markdown/#references","text":"Tutorial.md Generating Markdown Tables from Excel Files","title":"References"},{"location":"resources/tips/sklearn/","text":"Scikit-Learn \u00b6","title":"Scikit-Learn"},{"location":"resources/tips/sklearn/#scikit-learn","text":"","title":"Scikit-Learn"},{"location":"snippets/","text":"My Snippets \u00b6 I like hoarding links and I needed somewhere to put them. Bash Using Args Loops Subsequent Scripts Optimized RBF Kernel Euclidean Distance Group Sum Deep Learning \u00b6 PyTorch Device Handling Histograms Interpolation/Biscection RBF Kernel Large Scale Kernels Annotated Loops (tqdm) Multiple Kernel Matrices Tensor2Numpy Good Programming \u00b6 Testing Packaging PyTest Test Types setup.py Scaling \u00b6 Multiprocessing Parallel Step Data Structures \u00b6 Dictionarys Lists To List Unpacking List Earth Science \u00b6 Geodesic to Cartesian Coords Visualization \u00b6 Matplotlib colorbars legends logscale Nice defaults Writing \u00b6 Latex Markdown PseudoCode Figures Tables","title":"My Snippets"},{"location":"snippets/#my-snippets","text":"I like hoarding links and I needed somewhere to put them. Bash Using Args Loops Subsequent Scripts Optimized RBF Kernel Euclidean Distance Group Sum","title":"My Snippets"},{"location":"snippets/#deep-learning","text":"PyTorch Device Handling Histograms Interpolation/Biscection RBF Kernel Large Scale Kernels Annotated Loops (tqdm) Multiple Kernel Matrices Tensor2Numpy","title":"Deep Learning"},{"location":"snippets/#good-programming","text":"Testing Packaging PyTest Test Types setup.py","title":"Good Programming"},{"location":"snippets/#scaling","text":"Multiprocessing Parallel Step","title":"Scaling"},{"location":"snippets/#data-structures","text":"Dictionarys Lists To List Unpacking List","title":"Data Structures"},{"location":"snippets/#earth-science","text":"Geodesic to Cartesian Coords","title":"Earth Science"},{"location":"snippets/#visualization","text":"Matplotlib colorbars legends logscale Nice defaults","title":"Visualization"},{"location":"snippets/#writing","text":"Latex Markdown PseudoCode Figures Tables","title":"Writing"},{"location":"snippets/bash/args/","text":"Arguments in Scripts \u00b6 #!/bin/bash ### Print total arguments and their values echo \"Total Arguments:\" $# echo \"All Arguments values:\" $@ ### Command arguments can be accessed as echo \"First->\" $1 echo \"Second->\" $2 # You can also access all arguments in an array and use them in a script. args =( \" $@ \" ) echo \"First->\" ${ args [0] } echo \"Second->\" ${ args [1] }","title":"Arguments in Scripts"},{"location":"snippets/bash/args/#arguments-in-scripts","text":"#!/bin/bash ### Print total arguments and their values echo \"Total Arguments:\" $# echo \"All Arguments values:\" $@ ### Command arguments can be accessed as echo \"First->\" $1 echo \"Second->\" $2 # You can also access all arguments in an array and use them in a script. args =( \" $@ \" ) echo \"First->\" ${ args [0] } echo \"Second->\" ${ args [1] }","title":"Arguments in Scripts"},{"location":"snippets/bash/loops/","text":"Loops \u00b6 For Loops \u00b6 #!/usr/bin/env bash for i in 1 2 3 4 5 do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10 } do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10..2 } do echo \"Doing $i now\" done Source","title":"Loops"},{"location":"snippets/bash/loops/#loops","text":"","title":"Loops"},{"location":"snippets/bash/loops/#for-loops","text":"#!/usr/bin/env bash for i in 1 2 3 4 5 do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10 } do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10..2 } do echo \"Doing $i now\" done Source","title":"For Loops"},{"location":"snippets/bash/make_args/","text":"Makefile Arguments \u00b6 run: python manage.py runserver --host $( HOST ) --port $( PORT ) make run HOST:localhost PORT=8000","title":"Makefile Arguments"},{"location":"snippets/bash/make_args/#makefile-arguments","text":"run: python manage.py runserver --host $( HOST ) --port $( PORT ) make run HOST:localhost PORT=8000","title":"Makefile Arguments"},{"location":"snippets/bash/run_scripts/","text":"Running Subsequent Scripts \u00b6 Case I: Run Script 1, Wait, Run Script 2 \u00b6 For this, we want to wait for [script1.py](http://script1.py) to finish successfully, then we run script2.py . #!/usr/bin/env bash python script1.py && python script2.py Case II: Run Script 1, Wait, Run Script 2 IFF Script 1 has failed \u00b6 This is the case where we want to run [script2.py](http://script2.py) but IFF [script1.py](http://script1.py) has finished #!/usr/bin/env bash python script1.py || python script2.py Case III: Run script1 AND script 2 \u00b6 We want to run both scripts concurrently as background processes #!/usr/bin/env bash python script1.py & python script2.py","title":"Running Subsequent Scripts"},{"location":"snippets/bash/run_scripts/#running-subsequent-scripts","text":"","title":"Running Subsequent Scripts"},{"location":"snippets/bash/run_scripts/#case-i-run-script-1-wait-run-script-2","text":"For this, we want to wait for [script1.py](http://script1.py) to finish successfully, then we run script2.py . #!/usr/bin/env bash python script1.py && python script2.py","title":"Case I: Run Script 1, Wait, Run Script 2"},{"location":"snippets/bash/run_scripts/#case-ii-run-script-1-wait-run-script-2-iff-script-1-has-failed","text":"This is the case where we want to run [script2.py](http://script2.py) but IFF [script1.py](http://script1.py) has finished #!/usr/bin/env bash python script1.py || python script2.py","title":"Case II: Run Script 1, Wait, Run Script 2 IFF Script 1 has failed"},{"location":"snippets/bash/run_scripts/#case-iii-run-script1-and-script-2","text":"We want to run both scripts concurrently as background processes #!/usr/bin/env bash python script1.py & python script2.py","title":"Case III: Run script1 AND script 2"},{"location":"snippets/bayesian/sklearn_egp/","text":"Error in GPs in Sklearn \u00b6 Example in AstroML = Example | Example II","title":"Error in GPs in Sklearn"},{"location":"snippets/bayesian/sklearn_egp/#error-in-gps-in-sklearn","text":"Example in AstroML = Example | Example II","title":"Error in GPs in Sklearn"},{"location":"snippets/earth/cart_geo/","text":"Cartesian Coordinates 2 Geocoordinates \u00b6 Geocoordinates 2 Cartesian Coordinates \u00b6 import pandas as pd import numpy as np def geo_2_cartesian ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transforms geo coordinates (lat, lon) to cartesian coordinates (x, y, z). Parameters ---------- df : pd.DataFrame A dataframe with the geo coordinates values. The columns need to have the following ['lat', 'lon] Returns ------- df : pd.DataFrame A dataframe with the converted values. Example ------- >> df = geo_2_cartesian(df) \"\"\" cols = df . columns . tolist () if \"lat\" not in cols or \"lon\" not in cols : print ( \"lat,lon columns not present in df.\" ) return df # approximate earth radius earth_radius = 6371 # transform from degrees to radians # df = df.apply(lambda x: np.deg2rad(x) if x.name in ['lat', 'lat'] else x) df [ \"lat\" ] = np . deg2rad ( df [ \"lat\" ]) df [ \"lon\" ] = np . deg2rad ( df [ \"lon\" ]) # From Geo coords to cartesian coords df [ \"x\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . cos ( df [ \"lon\" ]) df [ \"y\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . sin ( df [ \"lon\" ]) df [ \"z\" ] = earth_radius * np . sin ( df [ \"lat\" ]) # drop original lat,lon columns df = df . drop ([ \"lat\" , \"lon\" ], axis = 1 ) return df Cartesian Coordinates 2 Geocoordinates \u00b6 def cartesian_2_geo ( x : np . ndarray , y : np . ndarray , z : np . ndarray ): R = 6371 # radius of the earth lat = np . degrees ( np . arcsin ( z / R )) lon = np . degrees ( np . arctan2 ( y , x )) return lat , lon Source : Blog | Stackoverflow","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/earth/cart_geo/#cartesian-coordinates-2-geocoordinates","text":"","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/earth/cart_geo/#geocoordinates-2-cartesian-coordinates","text":"import pandas as pd import numpy as np def geo_2_cartesian ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transforms geo coordinates (lat, lon) to cartesian coordinates (x, y, z). Parameters ---------- df : pd.DataFrame A dataframe with the geo coordinates values. The columns need to have the following ['lat', 'lon] Returns ------- df : pd.DataFrame A dataframe with the converted values. Example ------- >> df = geo_2_cartesian(df) \"\"\" cols = df . columns . tolist () if \"lat\" not in cols or \"lon\" not in cols : print ( \"lat,lon columns not present in df.\" ) return df # approximate earth radius earth_radius = 6371 # transform from degrees to radians # df = df.apply(lambda x: np.deg2rad(x) if x.name in ['lat', 'lat'] else x) df [ \"lat\" ] = np . deg2rad ( df [ \"lat\" ]) df [ \"lon\" ] = np . deg2rad ( df [ \"lon\" ]) # From Geo coords to cartesian coords df [ \"x\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . cos ( df [ \"lon\" ]) df [ \"y\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . sin ( df [ \"lon\" ]) df [ \"z\" ] = earth_radius * np . sin ( df [ \"lat\" ]) # drop original lat,lon columns df = df . drop ([ \"lat\" , \"lon\" ], axis = 1 ) return df","title":"Geocoordinates 2 Cartesian Coordinates"},{"location":"snippets/earth/cart_geo/#cartesian-coordinates-2-geocoordinates_1","text":"def cartesian_2_geo ( x : np . ndarray , y : np . ndarray , z : np . ndarray ): R = 6371 # radius of the earth lat = np . degrees ( np . arcsin ( z / R )) lon = np . degrees ( np . arctan2 ( y , x )) return lat , lon Source : Blog | Stackoverflow","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/earth/xarray/","text":"XArray \u00b6 Seasonal Grouping \u00b6 Extract Time Series (from Location) \u00b6 time_series = data . isel ( x = 1000 , y = 1000 ) . to_pandas () . dropna () Mean Across Multiple Dimensions \u00b6 data . mean ( dim = [ 'lat' , 'lon' ])","title":"XArray"},{"location":"snippets/earth/xarray/#xarray","text":"","title":"XArray"},{"location":"snippets/earth/xarray/#seasonal-grouping","text":"","title":"Seasonal Grouping"},{"location":"snippets/earth/xarray/#extract-time-series-from-location","text":"time_series = data . isel ( x = 1000 , y = 1000 ) . to_pandas () . dropna ()","title":"Extract Time Series (from Location)"},{"location":"snippets/earth/xarray/#mean-across-multiple-dimensions","text":"data . mean ( dim = [ 'lat' , 'lon' ])","title":"Mean Across Multiple Dimensions"},{"location":"snippets/latex/pseudocode/","text":"PseudoCode \u00b6 Source : How to Write Algorithm Pseudo Code in LaTeX - jdhao's blog","title":"PseudoCode"},{"location":"snippets/latex/pseudocode/#pseudocode","text":"Source : How to Write Algorithm Pseudo Code in LaTeX - jdhao's blog","title":"PseudoCode"},{"location":"snippets/latex/tables/","text":"Tables \u00b6 Professional Looking Tables \u00b6 Creating A Professional Table in LaTeX with booktabs - jdhao's blog","title":"Tables"},{"location":"snippets/latex/tables/#tables","text":"","title":"Tables"},{"location":"snippets/latex/tables/#professional-looking-tables","text":"Creating A Professional Table in LaTeX with booktabs - jdhao's blog","title":"Professional Looking Tables"},{"location":"snippets/markdown/figures/","text":"Figures \u00b6 Resizing \u00b6 Markdown Lingo \u00b6 Original Image or you can use relative value to the page or column width HTML \u00b6 Label References \u00b6 In Fig.\\ref{fig-my-great-img}, I show a great image.","title":"Figures"},{"location":"snippets/markdown/figures/#figures","text":"","title":"Figures"},{"location":"snippets/markdown/figures/#resizing","text":"","title":"Resizing"},{"location":"snippets/markdown/figures/#markdown-lingo","text":"Original Image or you can use relative value to the page or column width","title":"Markdown Lingo"},{"location":"snippets/markdown/figures/#html","text":"","title":"HTML"},{"location":"snippets/markdown/figures/#label-references","text":"In Fig.\\ref{fig-my-great-img}, I show a great image.","title":"Label References"},{"location":"snippets/ml/anomaly/","text":"Anomaly Detection \u00b6 # define threshold instances = 4 # 4% # calculate densityes densities = model . score_samples ( X ) # calculate density threshold density_threshold = np . percentile ( densities , instances ) # reproduce the anomalies anomalies = X [ densities < densities_threshold ] For more examples, see the pyOD documentation. In particular: predict_proba - predict the probability of a sample being an outlier. predict - predict if a sample is an outlier or not.","title":"Anomaly Detection"},{"location":"snippets/ml/anomaly/#anomaly-detection","text":"# define threshold instances = 4 # 4% # calculate densityes densities = model . score_samples ( X ) # calculate density threshold density_threshold = np . percentile ( densities , instances ) # reproduce the anomalies anomalies = X [ densities < densities_threshold ] For more examples, see the pyOD documentation. In particular: predict_proba - predict the probability of a sample being an outlier. predict - predict if a sample is an outlier or not.","title":"Anomaly Detection"},{"location":"snippets/my_configs/mkdocs/","text":"MkDocs \u00b6 # Project information site_name: Research Journal site_description: My Personal Research Journal site_author: J. Emmanuel Johnson site_url: https://jejjohnson.github.io/research_journal # Repository repo_name: jejjohnson/research_journal repo_url: https://github.com/jejjohnson/research_journal # Configuration theme: name: material language: en palette: primary: black accent: gray font: text: source code pro code: source code pro plugins: - search - mknotebooks: execute: false write_markdown: true timeout: 600 # Copyright copyright: Copyright & copy ; 2020 J. Emmanuel Johnson markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.codehilite: guess_lang: false - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:pymdownx.emoji.twemoji emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.highlight: linenums_style: pymdownx-inline - pymdownx.inlinehilite - pymdownx.keys # - pymdownx.magiclink: # repo_url_shorthand: true # user: squidfunk # repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde extra_javascript: - javascripts/extra.js - https://polyfill.io/v3/polyfill.min.js?features = es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js # - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML extra: # disqus: XHR39t5kZv social: # - type: 'envelope' # link: 'http://www.shortwhale.com/ericmjl' - icon: fontawesome/brands/github link: 'https://github.com/jejjohnson' - icon: fontawesome/brands/twitter link: 'https://twitter.com/jejjohnson' - icon: fontawesome/brands/linkedin link: 'https://linkedin.com/in/jejjohnson' - icon: fontawesome/solid/globe link: 'https://jejjohnson.netlify.com'","title":"MkDocs"},{"location":"snippets/my_configs/mkdocs/#mkdocs","text":"# Project information site_name: Research Journal site_description: My Personal Research Journal site_author: J. Emmanuel Johnson site_url: https://jejjohnson.github.io/research_journal # Repository repo_name: jejjohnson/research_journal repo_url: https://github.com/jejjohnson/research_journal # Configuration theme: name: material language: en palette: primary: black accent: gray font: text: source code pro code: source code pro plugins: - search - mknotebooks: execute: false write_markdown: true timeout: 600 # Copyright copyright: Copyright & copy ; 2020 J. Emmanuel Johnson markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.codehilite: guess_lang: false - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:pymdownx.emoji.twemoji emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.highlight: linenums_style: pymdownx-inline - pymdownx.inlinehilite - pymdownx.keys # - pymdownx.magiclink: # repo_url_shorthand: true # user: squidfunk # repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde extra_javascript: - javascripts/extra.js - https://polyfill.io/v3/polyfill.min.js?features = es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js # - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML extra: # disqus: XHR39t5kZv social: # - type: 'envelope' # link: 'http://www.shortwhale.com/ericmjl' - icon: fontawesome/brands/github link: 'https://github.com/jejjohnson' - icon: fontawesome/brands/twitter link: 'https://twitter.com/jejjohnson' - icon: fontawesome/brands/linkedin link: 'https://linkedin.com/in/jejjohnson' - icon: fontawesome/solid/globe link: 'https://jejjohnson.netlify.com'","title":"MkDocs"},{"location":"snippets/my_configs/conda/env_jax/","text":"","title":"Env jax"},{"location":"snippets/my_configs/jlab/installsh/","text":"Install JupyterLab Stuff \u00b6 # install pyviz extension jupyter labextension install @pyviz/jupyterlab_pyviz # install ipywidgets extension jupyter labextension install @jupyter-widgets/jupyterlab-manager","title":"Install JupyterLab Stuff"},{"location":"snippets/my_configs/jlab/installsh/#install-jupyterlab-stuff","text":"# install pyviz extension jupyter labextension install @pyviz/jupyterlab_pyviz # install ipywidgets extension jupyter labextension install @jupyter-widgets/jupyterlab-manager","title":"Install JupyterLab Stuff"},{"location":"snippets/my_configs/makefile/make/","text":"format: isort -rc -y . black -l 79 .","title":"Make"},{"location":"snippets/my_configs/setups/setup_py/","text":"","title":"Setup py"},{"location":"snippets/numpy/bisection/","text":"","title":"Bisection"},{"location":"snippets/numpy/euclidean/","text":"Efficient Euclidean Distance Calculation - Numpy Einsum \u00b6 This script uses numpy's einsum function to calculate the euclidean distance. Resources: import numpy as np def euclidean_distance_einsum ( X , Y ): \"\"\"Efficiently calculates the euclidean distance between two vectors using Numpys einsum function. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) Returns ------- D : array, (n_samples, n_samples) \"\"\" XX = np . einsum ( 'ij,ij->i' , X , X )[:, np . newaxis ] YY = np . einsum ( 'ij,ij->i' , Y , Y ) # XY = 2 * np.einsum('ij,kj->ik', X, Y) XY = 2 * np . dot ( X , Y . T ) return XX + YY - XY An alternative way per stackoverflow would be to do it in one shot. Sources How to calculate euclidean distance between pair of rows of a numpy array Calculate Distance between numpy arrays einsum and distance calculations How can the Euclidean distance be calculated with NumPy? Using Python numpy einsum to obtain dot product between 2 Matrices High-Performance computation in Python | NumPy","title":"Efficient Euclidean Distance Calculation - Numpy Einsum"},{"location":"snippets/numpy/euclidean/#efficient-euclidean-distance-calculation-numpy-einsum","text":"This script uses numpy's einsum function to calculate the euclidean distance. Resources: import numpy as np def euclidean_distance_einsum ( X , Y ): \"\"\"Efficiently calculates the euclidean distance between two vectors using Numpys einsum function. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) Returns ------- D : array, (n_samples, n_samples) \"\"\" XX = np . einsum ( 'ij,ij->i' , X , X )[:, np . newaxis ] YY = np . einsum ( 'ij,ij->i' , Y , Y ) # XY = 2 * np.einsum('ij,kj->ik', X, Y) XY = 2 * np . dot ( X , Y . T ) return XX + YY - XY An alternative way per stackoverflow would be to do it in one shot. Sources How to calculate euclidean distance between pair of rows of a numpy array Calculate Distance between numpy arrays einsum and distance calculations How can the Euclidean distance be calculated with NumPy? Using Python numpy einsum to obtain dot product between 2 Matrices High-Performance computation in Python | NumPy","title":"Efficient Euclidean Distance Calculation - Numpy Einsum"},{"location":"snippets/numpy/group_sum/","text":"Add every n values in array \u00b6 This is the case where you want to add every 3. So for example: a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] We want the following if we add every 3 values: b = [ 1 + 2 + 3 , 4 + 5 + 6 , 7 + 8 + 9 ] b = [ 6 , 14 , 24 ] We can do this by reshaping the array Make sure it's at minimum, a 2D array. a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) . reshape ( - 1 , 1 ) print ( a . shape ) (9,1) Reshape via a chunksize chunk_size = 3 a = a . reshape ( - 1 , chunk_size , a . shape [ 1 ]) print ( a . shape ) (3, 3, 1) Sum the middle column. a = a . sum ( 1 ) print ( a . shape ) print ( a . squeeze ()) (3, 1) [ 6 15 24] Note : the size of a has to be divisible by the chunk-size. So in our case 9/3=3 so we're good. But this wouldn't work for 10/3 because we have some remainder. Source : StackOverFlow In One Shot \u00b6 We can do this in one shot using the numpy built-in function: import numpy as np chunk_size = 3 # 1d arrays a = np . ones (( 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size )) # n-d arrays a = np . ones (( 9 , 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size ), axis = 0 )","title":"Add every n values in array"},{"location":"snippets/numpy/group_sum/#add-every-n-values-in-array","text":"This is the case where you want to add every 3. So for example: a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] We want the following if we add every 3 values: b = [ 1 + 2 + 3 , 4 + 5 + 6 , 7 + 8 + 9 ] b = [ 6 , 14 , 24 ] We can do this by reshaping the array Make sure it's at minimum, a 2D array. a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) . reshape ( - 1 , 1 ) print ( a . shape ) (9,1) Reshape via a chunksize chunk_size = 3 a = a . reshape ( - 1 , chunk_size , a . shape [ 1 ]) print ( a . shape ) (3, 3, 1) Sum the middle column. a = a . sum ( 1 ) print ( a . shape ) print ( a . squeeze ()) (3, 1) [ 6 15 24] Note : the size of a has to be divisible by the chunk-size. So in our case 9/3=3 so we're good. But this wouldn't work for 10/3 because we have some remainder. Source : StackOverFlow","title":"Add every n values in array"},{"location":"snippets/numpy/group_sum/#in-one-shot","text":"We can do this in one shot using the numpy built-in function: import numpy as np chunk_size = 3 # 1d arrays a = np . ones (( 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size )) # n-d arrays a = np . ones (( 9 , 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size ), axis = 0 )","title":"In One Shot"},{"location":"snippets/numpy/rbf_kernel/","text":"Optimized RBF kernel using numexpr \u00b6 A fast implementation of the RBF Kernel using numexpr. Using the fact that: ||x-y|| 2 = ||x|| 2 + ||y|| 2 - 2 * x T * y Resources Fast Implementation - StackOverFlow import numpy as np import numexpr as ne from sklearn.metrics import euclidean_distances from sklearn.check def rbf_kernel_ne ( X , Y = None , length_scale = 1.0 , signal_variance = 1.0 ): \"\"\"This function calculates the RBF kernel. It has been optimized using some advice found online. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) length_scale : float, default: 1.0 signal_variance : float, default: 1.0 Returns ------- K : array, (n_samples x d_dimensions) Resources --------- StackOverFlow: https://goo.gl/FXbgkj \"\"\" X_norm = np . einsum ( 'ij,ij->i' , X , X ) if Y is not None : Y_norm = np . einsum ( 'ij,ij->i' , Y , Y ) else : Y = X Y_norm = X_norm K = ne . evaluate ( 'v * exp(-g * (A + B - 2 * C))' , { 'A' : X_norm [:, None ], 'B' : Y_norm [ None , :], 'C' : np . dot ( X , Y . T ), 'g' : 1 / ( 2 * length_scale ** 2 ), 'v' : signal_variance }) return K def rbf_kernel ( X , Y = None , signal_variance = 1.0 , length_scale = 1.0 ): \"\"\" Compute the rbf (gaussian) kernel between X and Y:: K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the :ref:`User Guide <rbf_kernel>`. Parameters ---------- X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features Returns ------- kernel_matrix : array of shape (n_samples_X, n_samples_Y) \"\"\" X , Y = check_pairwise_arrays ( X , Y ) X /= length_scale Y /= length_scale K = - 0.5 * euclidean_distances ( X , Y , squared = True ) np . exp ( K , K ) # exponentiate K in-place K *= signal_variance # multiply by signal_variance return K","title":"Optimized RBF kernel using `numexpr`"},{"location":"snippets/numpy/rbf_kernel/#optimized-rbf-kernel-using-numexpr","text":"A fast implementation of the RBF Kernel using numexpr. Using the fact that: ||x-y|| 2 = ||x|| 2 + ||y|| 2 - 2 * x T * y Resources Fast Implementation - StackOverFlow import numpy as np import numexpr as ne from sklearn.metrics import euclidean_distances from sklearn.check def rbf_kernel_ne ( X , Y = None , length_scale = 1.0 , signal_variance = 1.0 ): \"\"\"This function calculates the RBF kernel. It has been optimized using some advice found online. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) length_scale : float, default: 1.0 signal_variance : float, default: 1.0 Returns ------- K : array, (n_samples x d_dimensions) Resources --------- StackOverFlow: https://goo.gl/FXbgkj \"\"\" X_norm = np . einsum ( 'ij,ij->i' , X , X ) if Y is not None : Y_norm = np . einsum ( 'ij,ij->i' , Y , Y ) else : Y = X Y_norm = X_norm K = ne . evaluate ( 'v * exp(-g * (A + B - 2 * C))' , { 'A' : X_norm [:, None ], 'B' : Y_norm [ None , :], 'C' : np . dot ( X , Y . T ), 'g' : 1 / ( 2 * length_scale ** 2 ), 'v' : signal_variance }) return K def rbf_kernel ( X , Y = None , signal_variance = 1.0 , length_scale = 1.0 ): \"\"\" Compute the rbf (gaussian) kernel between X and Y:: K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the :ref:`User Guide <rbf_kernel>`. Parameters ---------- X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features Returns ------- kernel_matrix : array of shape (n_samples_X, n_samples_Y) \"\"\" X , Y = check_pairwise_arrays ( X , Y ) X /= length_scale Y /= length_scale K = - 0.5 * euclidean_distances ( X , Y , squared = True ) np . exp ( K , K ) # exponentiate K in-place K *= signal_variance # multiply by signal_variance return K","title":"Optimized RBF kernel using numexpr"},{"location":"snippets/parallel/dask_embarass/","text":"Embarrassingly Parallel Workloads \u00b6 Embarrassingly Parallel Workloads An Excellent tutorial about how we can use Dask to do embarrassingly parallel workloads.","title":"Embarrassingly Parallel Workloads"},{"location":"snippets/parallel/dask_embarass/#embarrassingly-parallel-workloads","text":"Embarrassingly Parallel Workloads An Excellent tutorial about how we can use Dask to do embarrassingly parallel workloads.","title":"Embarrassingly Parallel Workloads"},{"location":"snippets/parallel/hyperparams/","text":"Hyperparameter Optimization \u00b6 Hyperparameter optimization with Dask Scale sklearn for Small Data Problems","title":"Hyperparameter Optimization"},{"location":"snippets/parallel/hyperparams/#hyperparameter-optimization","text":"Hyperparameter optimization with Dask Scale sklearn for Small Data Problems","title":"Hyperparameter Optimization"},{"location":"snippets/parallel/large_predict/","text":"Batch Prediction w. PyTorch Score and Predict Large Datasets","title":"Large predict"},{"location":"snippets/parallel/mp_module/","text":"Parallel Processing \u00b6 Multiprocessing Module \u00b6 Useful for doing \"embarrassingly parallel\" tasks. You do need to control how it works (e.g. the number of jobs, what to loop over). I use it because it's quite easy but there are more advanced ways to do parallization below. === info \"Snippet\" ```python results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) ``` === info \"Full Function\" ```python from joblib import Parallel, delayed from typing import Callable, Iterable def run_parallel_step( exp_step: Callable, parameters: Iterable, n_jobs: int = 2, verbose: int = 1, **kwargs ) -> List: \"\"\"Helper function to run experimental loops in parallel Parameters ---------- exp_step : Callable a callable function which does each experimental step parameters : Iterable, an iterable (List, Dict, etc) of parameters to be looped through n_jobs : int, default=2 the number of cores to use verbose : int, default=1 the amount of information to display in the Returns ------- results : List list of the results from the function Examples -------- Example 1 - No keyword arguments >>> parameters = [1, 10, 100] >>> def step(x): return x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1 ) >>> results [1, 100, 10000] Example II: Keyword arguments >>> parameters = [1, 10, 100] >>> def step(x, a=1.0): return a * x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1, a=10 ) >>> results [100, 10000, 1000000] \"\"\" # loop through parameters results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) return results ``` Dask \u00b6","title":"Parallel Processing"},{"location":"snippets/parallel/mp_module/#parallel-processing","text":"","title":"Parallel Processing"},{"location":"snippets/parallel/mp_module/#multiprocessing-module","text":"Useful for doing \"embarrassingly parallel\" tasks. You do need to control how it works (e.g. the number of jobs, what to loop over). I use it because it's quite easy but there are more advanced ways to do parallization below. === info \"Snippet\" ```python results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) ``` === info \"Full Function\" ```python from joblib import Parallel, delayed from typing import Callable, Iterable def run_parallel_step( exp_step: Callable, parameters: Iterable, n_jobs: int = 2, verbose: int = 1, **kwargs ) -> List: \"\"\"Helper function to run experimental loops in parallel Parameters ---------- exp_step : Callable a callable function which does each experimental step parameters : Iterable, an iterable (List, Dict, etc) of parameters to be looped through n_jobs : int, default=2 the number of cores to use verbose : int, default=1 the amount of information to display in the Returns ------- results : List list of the results from the function Examples -------- Example 1 - No keyword arguments >>> parameters = [1, 10, 100] >>> def step(x): return x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1 ) >>> results [1, 100, 10000] Example II: Keyword arguments >>> parameters = [1, 10, 100] >>> def step(x, a=1.0): return a * x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1, a=10 ) >>> results [100, 10000, 1000000] \"\"\" # loop through parameters results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) return results ```","title":"Multiprocessing Module"},{"location":"snippets/parallel/mp_module/#dask","text":"","title":"Dask"},{"location":"snippets/parallel/pipelines/","text":"ETL Pipelines with Prefect ETL w. Prefect | ETL Flow","title":"Pipelines"},{"location":"snippets/python/conda/","text":"Conda \u00b6","title":"Conda"},{"location":"snippets/python/conda/#conda","text":"","title":"Conda"},{"location":"snippets/python/conda_envs/","text":"My Typical Conda Environments \u00b6","title":"My Typical Conda Environments"},{"location":"snippets/python/conda_envs/#my-typical-conda-environments","text":"","title":"My Typical Conda Environments"},{"location":"snippets/python/dict_2_list/","text":"Dictionaries to Lists \u00b6 Full Function \"\"\" NOTE! Need the mypy (typing) package >>> !pip install mypy \"\"\" import itertools from typing import Dict , List def dict_product ( dicts : Dict ) -> List [ Dict ]: \"\"\"Returns the product of a dictionary with lists Parameters ---------- dicts : Dict, a dictionary where each key has a list of inputs Returns ------- prod : List[Dict] the list of dictionary products Example ------- >>> parameters = { \"samples\": [100, 1_000, 10_000], \"dimensions\": [2, 3, 10, 100, 1_000] } >>> parameters = list(dict_product(parameters)) >>> parameters [{'samples': 100, 'dimensions': 2}, {'samples': 100, 'dimensions': 3}, {'samples': 1000, 'dimensions': 2}, {'samples': 1000, 'dimensions': 3}, {'samples': 10000, 'dimensions': 2}, {'samples': 10000, 'dimensions': 3}] \"\"\" return ( dict ( zip ( dicts . keys (), x )) for x in itertools . product ( * dicts . values ())) Resources : Using itertools.product with dictionaries - Stephan Tulkens Using itertools.product instead of nested for loops - Stephan Tulkens","title":"Dictionaries to Lists"},{"location":"snippets/python/dict_2_list/#dictionaries-to-lists","text":"Full Function \"\"\" NOTE! Need the mypy (typing) package >>> !pip install mypy \"\"\" import itertools from typing import Dict , List def dict_product ( dicts : Dict ) -> List [ Dict ]: \"\"\"Returns the product of a dictionary with lists Parameters ---------- dicts : Dict, a dictionary where each key has a list of inputs Returns ------- prod : List[Dict] the list of dictionary products Example ------- >>> parameters = { \"samples\": [100, 1_000, 10_000], \"dimensions\": [2, 3, 10, 100, 1_000] } >>> parameters = list(dict_product(parameters)) >>> parameters [{'samples': 100, 'dimensions': 2}, {'samples': 100, 'dimensions': 3}, {'samples': 1000, 'dimensions': 2}, {'samples': 1000, 'dimensions': 3}, {'samples': 10000, 'dimensions': 2}, {'samples': 10000, 'dimensions': 3}] \"\"\" return ( dict ( zip ( dicts . keys (), x )) for x in itertools . product ( * dicts . values ())) Resources : Using itertools.product with dictionaries - Stephan Tulkens Using itertools.product instead of nested for loops - Stephan Tulkens","title":"Dictionaries to Lists"},{"location":"snippets/python/lists_tricks/","text":"Tricks with Lists \u00b6 Unpacking a List of tuples \u00b6 I've found this useful in situations where I need to do parallel processing and my output is a tuple of 2 elements. So for example, let's have the following. list_of_tuples = ( '1' , 'a' ), ( '2' , 'b' ), ( '3' , 'c' ), ( '4' , 'd' ) Now I would like to unpack this into 2 lists. I can do this like so: list1 , list2 = zip ( * list_of_tuples ) So if I print the lists, I will get the following: list1 , list2 ('1', '2', '3', '4'), ('a', 'b', 'c', 'd')","title":"Tricks with Lists"},{"location":"snippets/python/lists_tricks/#tricks-with-lists","text":"","title":"Tricks with Lists"},{"location":"snippets/python/lists_tricks/#unpacking-a-list-of-tuples","text":"I've found this useful in situations where I need to do parallel processing and my output is a tuple of 2 elements. So for example, let's have the following. list_of_tuples = ( '1' , 'a' ), ( '2' , 'b' ), ( '3' , 'c' ), ( '4' , 'd' ) Now I would like to unpack this into 2 lists. I can do this like so: list1 , list2 = zip ( * list_of_tuples ) So if I print the lists, I will get the following: list1 , list2 ('1', '2', '3', '4'), ('a', 'b', 'c', 'd')","title":"Unpacking a List of tuples"},{"location":"snippets/python/named_tuples/","text":"Named Tuples \u00b6 I've recently grown a liking to namedtuple 's. It's a nice way to have custom containers. from collections import namedtuple Instance of \u00b6 your_tuple = namedtuple ( 'your_tuple' , [ 'entry1' , 'entry2' ]) isinstance ( a , your_tuple )","title":"Named Tuples"},{"location":"snippets/python/named_tuples/#named-tuples","text":"I've recently grown a liking to namedtuple 's. It's a nice way to have custom containers. from collections import namedtuple","title":"Named Tuples"},{"location":"snippets/python/named_tuples/#instance-of","text":"your_tuple = namedtuple ( 'your_tuple' , [ 'entry1' , 'entry2' ]) isinstance ( a , your_tuple )","title":"Instance of"},{"location":"snippets/python/paths/","text":"Paths \u00b6 This is so annoying to have to do a lot of the time. Here is a nice package to deal with that: pyprojroot .","title":"Paths"},{"location":"snippets/python/paths/#paths","text":"This is so annoying to have to do a lot of the time. Here is a nice package to deal with that: pyprojroot .","title":"Paths"},{"location":"snippets/python/setup/","text":"My Setup File \u00b6 #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pipenv install twine --dev import io import os import sys from shutil import rmtree from setuptools import Command , find_packages , setup # Package meta-data. NAME = \"rbig\" DESCRIPTION = \"Gaussianization Flows in Python.\" URL = \"https://github.com/jejjohnson/rbig\" EMAIL = \"jemanjohnson34@gmail.com\" AUTHOR = \"J. Emmanuel Johnson\" REQUIRES_PYTHON = \">=3.8.0\" VERSION = \"1.1.0\" # Keywords KEYWORDS = [ \"machine learning python scikit-learn gaussianization\" ] # What packages are required for this module to be executed? REQUIRED = [ \"numpy\" , \"scipy\" , \"scikit-learn\" ,] # What packages are optional? EXTRAS = { \"dev\" : [ \"flake8\" , \"pylint\" , # checkers \"black\" , \"isort\" , # formatters \"mypy\" , # Type checking ], \"notebooks\" : [ \"ipykernel\" # Notebooks ], \"examples\" : [ \"matplotlib\" , \"seaborn\" # almost always have plots ], \"tests\" : [ \"pytest\" ], # test library } # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os . path . abspath ( os . path . dirname ( __file__ )) # Import the README and use it as the long-description. # Note: this will only work if 'README.md' is present in your MANIFEST.in file! try : with io . open ( os . path . join ( here , \"README.md\" ), encoding = \"utf-8\" ) as f : long_description = \" \\n \" + f . read () except FileNotFoundError : long_description = DESCRIPTION # Load the package's __version__.py module as a dictionary. about = {} if not VERSION : project_slug = NAME . lower () . replace ( \"-\" , \"_\" ) . replace ( \" \" , \"_\" ) with open ( os . path . join ( here , project_slug , \"__version__.py\" )) as f : exec ( f . read (), about ) else : about [ \"__version__\" ] = VERSION class UploadCommand ( Command ): \"\"\"Support setup.py upload.\"\"\" description = \"Build and publish the package.\" user_options = [] @staticmethod def status ( s ): \"\"\"Prints things in bold.\"\"\" print ( \" \\033 [1m {0} \\033 [0m\" . format ( s )) def initialize_options ( self ): pass def finalize_options ( self ): pass def run ( self ): try : self . status ( \"Removing previous builds\u2026\" ) rmtree ( os . path . join ( here , \"dist\" )) except OSError : pass self . status ( \"Building Source and Wheel (universal) distribution\u2026\" ) os . system ( \" {0} setup.py sdist bdist_wheel --universal\" . format ( sys . executable )) self . status ( \"Uploading the package to PyPI via Twine\u2026\" ) os . system ( \"twine upload dist/*\" ) self . status ( \"Pushing git tags\u2026\" ) os . system ( \"git tag v {0} \" . format ( about [ \"__version__\" ])) os . system ( \"git push --tags\" ) sys . exit () # Where the magic happens: setup ( name = NAME , version = about [ \"__version__\" ], description = DESCRIPTION , long_description = long_description , long_description_content_type = \"text/markdown\" , author = AUTHOR , author_email = EMAIL , python_requires = REQUIRES_PYTHON , url = URL , packages = find_packages ( exclude = [ \"tests\" , \"*.tests\" , \"*.tests.*\" , \"tests.*\" ]), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, setup_requires = [ \"setuptools-yaml\" ], metadata_yaml = \"environment.yml\" , install_requires = REQUIRED , extras_require = EXTRAS , include_package_data = True , license = \"MIT\" , keywords = KEYWORDS , classifiers = [ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers \"Development Status :: 3 - Alpha\" , \"License :: OSI Approved :: MIT License\" , \"Intended Audience :: Science/Research\" , \"Programming Language :: Python\" , \"Programming Language :: Python :: 3.8\" , ], # $ setup.py publish support. cmdclass = { \"upload\" : UploadCommand }, )","title":"My Setup File"},{"location":"snippets/python/setup/#my-setup-file","text":"#!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pipenv install twine --dev import io import os import sys from shutil import rmtree from setuptools import Command , find_packages , setup # Package meta-data. NAME = \"rbig\" DESCRIPTION = \"Gaussianization Flows in Python.\" URL = \"https://github.com/jejjohnson/rbig\" EMAIL = \"jemanjohnson34@gmail.com\" AUTHOR = \"J. Emmanuel Johnson\" REQUIRES_PYTHON = \">=3.8.0\" VERSION = \"1.1.0\" # Keywords KEYWORDS = [ \"machine learning python scikit-learn gaussianization\" ] # What packages are required for this module to be executed? REQUIRED = [ \"numpy\" , \"scipy\" , \"scikit-learn\" ,] # What packages are optional? EXTRAS = { \"dev\" : [ \"flake8\" , \"pylint\" , # checkers \"black\" , \"isort\" , # formatters \"mypy\" , # Type checking ], \"notebooks\" : [ \"ipykernel\" # Notebooks ], \"examples\" : [ \"matplotlib\" , \"seaborn\" # almost always have plots ], \"tests\" : [ \"pytest\" ], # test library } # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os . path . abspath ( os . path . dirname ( __file__ )) # Import the README and use it as the long-description. # Note: this will only work if 'README.md' is present in your MANIFEST.in file! try : with io . open ( os . path . join ( here , \"README.md\" ), encoding = \"utf-8\" ) as f : long_description = \" \\n \" + f . read () except FileNotFoundError : long_description = DESCRIPTION # Load the package's __version__.py module as a dictionary. about = {} if not VERSION : project_slug = NAME . lower () . replace ( \"-\" , \"_\" ) . replace ( \" \" , \"_\" ) with open ( os . path . join ( here , project_slug , \"__version__.py\" )) as f : exec ( f . read (), about ) else : about [ \"__version__\" ] = VERSION class UploadCommand ( Command ): \"\"\"Support setup.py upload.\"\"\" description = \"Build and publish the package.\" user_options = [] @staticmethod def status ( s ): \"\"\"Prints things in bold.\"\"\" print ( \" \\033 [1m {0} \\033 [0m\" . format ( s )) def initialize_options ( self ): pass def finalize_options ( self ): pass def run ( self ): try : self . status ( \"Removing previous builds\u2026\" ) rmtree ( os . path . join ( here , \"dist\" )) except OSError : pass self . status ( \"Building Source and Wheel (universal) distribution\u2026\" ) os . system ( \" {0} setup.py sdist bdist_wheel --universal\" . format ( sys . executable )) self . status ( \"Uploading the package to PyPI via Twine\u2026\" ) os . system ( \"twine upload dist/*\" ) self . status ( \"Pushing git tags\u2026\" ) os . system ( \"git tag v {0} \" . format ( about [ \"__version__\" ])) os . system ( \"git push --tags\" ) sys . exit () # Where the magic happens: setup ( name = NAME , version = about [ \"__version__\" ], description = DESCRIPTION , long_description = long_description , long_description_content_type = \"text/markdown\" , author = AUTHOR , author_email = EMAIL , python_requires = REQUIRES_PYTHON , url = URL , packages = find_packages ( exclude = [ \"tests\" , \"*.tests\" , \"*.tests.*\" , \"tests.*\" ]), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, setup_requires = [ \"setuptools-yaml\" ], metadata_yaml = \"environment.yml\" , install_requires = REQUIRED , extras_require = EXTRAS , include_package_data = True , license = \"MIT\" , keywords = KEYWORDS , classifiers = [ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers \"Development Status :: 3 - Alpha\" , \"License :: OSI Approved :: MIT License\" , \"Intended Audience :: Science/Research\" , \"Programming Language :: Python\" , \"Programming Language :: Python :: 3.8\" , ], # $ setup.py publish support. cmdclass = { \"upload\" : UploadCommand }, )","title":"My Setup File"},{"location":"snippets/pytorch/device/","text":"Device Agnostic \u00b6 import argparse import torch parser = argparse . ArgumentParser ( description = 'PyTorch Example' ) parser . add_argument ( '--disable-cuda' , action = 'store_true' , help = 'Disable CUDA' ) args = parser . parse_args () args . device = None if not args . disable_cuda and torch . cuda . is_available (): args . device = torch . device ( 'cuda' ) else : args . device = torch . device ( 'cpu' )","title":"Device Agnostic"},{"location":"snippets/pytorch/device/#device-agnostic","text":"import argparse import torch parser = argparse . ArgumentParser ( description = 'PyTorch Example' ) parser . add_argument ( '--disable-cuda' , action = 'store_true' , help = 'Disable CUDA' ) args = parser . parse_args () args . device = None if not args . disable_cuda and torch . cuda . is_available (): args . device = torch . device ( 'cuda' ) else : args . device = torch . device ( 'cpu' )","title":"Device Agnostic"},{"location":"snippets/pytorch/histograms/","text":"Histograms in PyTorch \u00b6 It's not very well known but there is a histogram function in PyTorch. Function # histogram parameters bins = 4 bounds = ( 4 , 0 ) # calculate the histogram hist = torch . histc ( torch . tensor ([ 1. , 2. , 1. ]), bins = bins , min = bounds [ 0 ], max = bounds [ 1 ]) # normalize histogram to sum to 1 hist = hist . div ( hist . sum ()) Calculating Bin Edges Unfortunately, we have to do this manually as the pytorch function doesn't spit out the entire function. # calculate the bin edges bin_edges = torch . linspace ( bounds [ 0 ], bounds [ 1 ], steps = bins )","title":"Histograms in PyTorch"},{"location":"snippets/pytorch/histograms/#histograms-in-pytorch","text":"It's not very well known but there is a histogram function in PyTorch. Function # histogram parameters bins = 4 bounds = ( 4 , 0 ) # calculate the histogram hist = torch . histc ( torch . tensor ([ 1. , 2. , 1. ]), bins = bins , min = bounds [ 0 ], max = bounds [ 1 ]) # normalize histogram to sum to 1 hist = hist . div ( hist . sum ()) Calculating Bin Edges Unfortunately, we have to do this manually as the pytorch function doesn't spit out the entire function. # calculate the bin edges bin_edges = torch . linspace ( bounds [ 0 ], bounds [ 1 ], steps = bins )","title":"Histograms in PyTorch"},{"location":"snippets/pytorch/interp/","text":"Interpolating in PyTorch \u00b6 Officially, there is not interp function in PyTorch. However, we do have the searchsorted function. This function performs a bisection Bisection Search \u00b6 Numpy Implementation \u00b6 np . searchsorted ([ 1 , 2 , 3 , 4 , 5 ]) Example # interpolation method new_y = np . interp ( new_x , old_x , old_y ) # bisection method new_y_ss = old_y [ np . searchsorted ( old_x , new_x , side = 'right' )] PyTorch Implementation \u00b6 def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 Source : Pyro Library | Neural Spline Flows","title":"Interpolating in PyTorch"},{"location":"snippets/pytorch/interp/#interpolating-in-pytorch","text":"Officially, there is not interp function in PyTorch. However, we do have the searchsorted function. This function performs a bisection","title":"Interpolating in PyTorch"},{"location":"snippets/pytorch/interp/#bisection-search","text":"","title":"Bisection Search"},{"location":"snippets/pytorch/interp/#numpy-implementation","text":"np . searchsorted ([ 1 , 2 , 3 , 4 , 5 ]) Example # interpolation method new_y = np . interp ( new_x , old_x , old_y ) # bisection method new_y_ss = old_y [ np . searchsorted ( old_x , new_x , side = 'right' )]","title":"Numpy Implementation"},{"location":"snippets/pytorch/interp/#pytorch-implementation","text":"def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 Source : Pyro Library | Neural Spline Flows","title":"PyTorch Implementation"},{"location":"snippets/pytorch/keops/","text":"KeOps - Gaussian Kernel \u00b6 import torch from pykeops.torch import Kernel , kernel_product # Geneerate the data as pytorch tensors x = torch . randn ( 1000 , 3 , requires_grad = True ) y = torch . randn ( 2000 , 3 , requires_grad = True ) b = torch . randn ( 2000 , 2 , requires_grad = True ) # ARD length_scale length_scale = torch . tensor ([ . 5 ], requires_grad = True ) params = { \"id\" : Kernel ( \"gaussian(x,y)\" ), \"gamma\" : 1. / length_scale ** 2 , } # Differentiable wrt x, y, b, length_scale a = kernel_product ( params , x , y , b )","title":"KeOps - Gaussian Kernel"},{"location":"snippets/pytorch/keops/#keops-gaussian-kernel","text":"import torch from pykeops.torch import Kernel , kernel_product # Geneerate the data as pytorch tensors x = torch . randn ( 1000 , 3 , requires_grad = True ) y = torch . randn ( 2000 , 3 , requires_grad = True ) b = torch . randn ( 2000 , 2 , requires_grad = True ) # ARD length_scale length_scale = torch . tensor ([ . 5 ], requires_grad = True ) params = { \"id\" : Kernel ( \"gaussian(x,y)\" ), \"gamma\" : 1. / length_scale ** 2 , } # Differentiable wrt x, y, b, length_scale a = kernel_product ( params , x , y , b )","title":"KeOps - Gaussian Kernel"},{"location":"snippets/pytorch/loops_tqdm/","text":"Loops with TQDM \u00b6 A simple way to use a nice progress bar instead of polluting your screen with print statements. import tqdm def train ( xtrain , ytrain , model , criterion , optimizer , n_epochs = 1_000 ): with tqdm . trange ( n_epochs ) as bar : for epoch in bar : # loop over the dataset multiple times # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = model ( xtrain ) loss = criterion ( outputs , ytrain ) loss . backward () optimizer . step () # print statistics postfix = dict ( Loss = f \" { loss . item () : .3f } \" ) bar . set_postfix ( postfix ) Source : DeepBayes.ru 2019 Notebook","title":"Loops with TQDM"},{"location":"snippets/pytorch/loops_tqdm/#loops-with-tqdm","text":"A simple way to use a nice progress bar instead of polluting your screen with print statements. import tqdm def train ( xtrain , ytrain , model , criterion , optimizer , n_epochs = 1_000 ): with tqdm . trange ( n_epochs ) as bar : for epoch in bar : # loop over the dataset multiple times # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = model ( xtrain ) loss = criterion ( outputs , ytrain ) loss . backward () optimizer . step () # print statistics postfix = dict ( Loss = f \" { loss . item () : .3f } \" ) bar . set_postfix ( postfix ) Source : DeepBayes.ru 2019 Notebook","title":"Loops with TQDM"},{"location":"snippets/pytorch/multi_kernel/","text":"def kernel_product ( x , y , mode = \"gaussian\" , s = 1. ): x_i = x . unsqueeze ( 1 ) y_j = y . unsqueeze ( 0 ) xmy = (( x_i - y_j ) ** 2 ) . sum ( 2 ) if mode == \"gaussian\" : K = torch . exp ( - xmy / s ** 2 ) ) elif mode == \"laplace\" : K = torch . exp ( - torch . sqrt ( xmy + ( s ** 2 ))) elif mode == \"energy\" : K = torch . pow ( xmy + ( s ** 2 ), -. 25 ) return torch . t ( K )","title":"Multi kernel"},{"location":"snippets/pytorch/numpy/","text":"PyTorch Tensors 2 Numpy Adaptors \u00b6 import torch class TensorNumpyAdapter : \"\"\" Class for adapter interface between numpy array type and Tensor objects in PyTorch. \"\"\" def to_tensor ( self , x ): return torch . from_numpy ( x ) . float () def to_numpy ( self , x ): return x . numpy () Source : PyGlow Example","title":"PyTorch Tensors 2 Numpy Adaptors"},{"location":"snippets/pytorch/numpy/#pytorch-tensors-2-numpy-adaptors","text":"import torch class TensorNumpyAdapter : \"\"\" Class for adapter interface between numpy array type and Tensor objects in PyTorch. \"\"\" def to_tensor ( self , x ): return torch . from_numpy ( x ) . float () def to_numpy ( self , x ): return x . numpy () Source : PyGlow Example","title":"PyTorch Tensors 2 Numpy Adaptors"},{"location":"snippets/pytorch/pytorch_lightning/","text":"Basic Basic Model \u00b6 import torch.nn as nn import pytorch_lightning as pl class Learner ( pl . LightningModule ): def __init__ ( self , model : nn . Module , settings : dict = {}): super () . __init__ () defaults . update ( settings ) self . settings = defaults self . model = model self . c = 0 def forward ( self , x ): return self . model ( x ) def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self . model ( x ) loss = nn . CrossEntropyLoss ()( y_hat , y ) logs = { 'train_loss' : loss } return { 'loss' : loss , 'log' : logs } def configure_optimizers ( self ): return torch . optim . Adam ( self . model . parameters (), lr = 0.005 ) def train_dataloader ( self ): return trainloader","title":"Pytorch lightning"},{"location":"snippets/pytorch/pytorch_lightning/#basic-basic-model","text":"import torch.nn as nn import pytorch_lightning as pl class Learner ( pl . LightningModule ): def __init__ ( self , model : nn . Module , settings : dict = {}): super () . __init__ () defaults . update ( settings ) self . settings = defaults self . model = model self . c = 0 def forward ( self , x ): return self . model ( x ) def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self . model ( x ) loss = nn . CrossEntropyLoss ()( y_hat , y ) logs = { 'train_loss' : loss } return { 'loss' : loss , 'log' : logs } def configure_optimizers ( self ): return torch . optim . Adam ( self . model . parameters (), lr = 0.005 ) def train_dataloader ( self ): return trainloader","title":"Basic Basic Model"},{"location":"snippets/pytorch/rbf_kernel/","text":"This snippet showcases using PyTorch and calculating a kernel function. Below I have a sample script to do an RBF function along with the gradients in PyTorch. from typing import Union import numpy as np import torch # GPU + autodiff library from torch.autograd import grad class RBF : def __init__ ( self , length_scale : float = 1.0 , signal_variance : float = 1.0 , device : Union [ Bool , str ] = None ) -> None : # initialize parameters self . length_scale = torch . tensor ( length_scale , dtype = torch . float32 , device = self . device , requires_grad = True ) self . signal_variance = torch . tensor ( signal_variance , dtype = torch . float32 , device = self . device , requires_grad = True ) if device is None : self . device = torch . device ( \"cpu\" ) else : self . device = device def __call__ ( self , X : np . ndarray , Y : Union [ Bool , np . ndarray ] = None ) -> np . ndarray : # convert inputs to pytorch tensors X = torch . tensor ( X , dtype = torch . float32 , device = self . device ) if Y is None : Y = X else : Y = torch . tensor ( Y , dtype = torch . float32 , device = self . device ) # Divide by length scale X = torch . div ( X , self . length_scale ) Y = torch . div ( Y , self . length_scale ) # Re-indexing X_i = X [:, None , :] # shape (N, D) -> (N, 1, D) Y_j = Y [ None , :, :] # shape (N, D) -> (1, N, D) # Actual Computations sqd = torch . sum ( ( X_i - Y_j ) ** 2 , 2 ) # |X_i - Y_j|^2 K_qq = torch . exp ( - 0.5 * sqd ) # Gaussian Kernel K_qq = torch . mul ( self . signal_variance , K_qq ) # Signal Variance return K_qq . detach () . to_numpy () def gradient_X ( self , X ): return None def gradient_X2 ( self , X ): return None def gradient_XX ( self , X : np . ndarray , Y : Union [ Bool , np . ndarray ] = None ) -> np . ndarray : # Convert to tensor that requires Grad X = torch . tensor ( length_scale , dtype = torch . float32 , device = self . device , requires_grad = True ) if Y is None : Y = X else : Y = torch . tensor ( Y , dtype = torch . float32 , device = self . device , requires_grad = True ) # compute the gradient kernel w.r.t. to the two inputs J = grad ( self . __call__ ( X , Y )) return J def gradient_XX2 ( self , X , Y = None ): return None Below we can see how one would actually do that in practice. # With PyTorch, using the GPU is simple use_gpy = torch . cuda . is_available () dtype = torch . cuda . FloatTensor if use_gpu else torch . FloatTensor N = 5000 # cloud of 5,000 points D = 3 # 3D q = np . random . rand ( N , D ) p = np . random . rand ( N , D ) s = 1. # Store arbitrary arrays on the CPU or GPU: q = torch . from_numpy ( q ) . type ( dtype ) p = torch . from_numpy ( p ) . type ( dtype ) s = torch . Tensor ([ 1. ]) . type ( dtype ) # Tell PyTorch to track the variabls \"q\" and \"p\" q . requires_grad = True p . requires_grad = True # Rescale with length_scale q = torch . div ( q , s ) # Re-indexing q_i = q [:, None , :] # shape (N, D) -> (N, 1, D) q_j = q [ None , :, :] # shape (N, D) -> (1, N, D) # Actual Computations sqd = torch . sum ( ( q_i - q_j ) ** 2 , 2 ) # |q_i - q_j|^2 K_qq = torch . exp ( - sqd / s ** 2 ) # Gaussian Kernel v = K_qq @ p # matrix mult. (N,N) @ (N,D) = (N,D) # Automatic Differentiation [ dq , dp ] = grad ( H , [ q , p ] ) # Hamiltonian H(q,p): .5*<p,v> H = . 5 * torch . dot ( p . view ( - 1 ), v . view ( - 1 ) ) Source : Presentation for Autograd and mathematics.","title":"Rbf kernel"},{"location":"snippets/scipy/cholesky/","text":"The Cholesky Decomposition \u00b6 I use this quite often whenever I'm dealing with Gaussian processes and kernel methods. Instead of doing the solver, we can simply use the cho_factor and cho_solve that's built into the scipy library. Direct Solver \u00b6 # use the direct solver weights = scipy . linalg . solve ( K + alpha * identity , y ) Cholesky Factor \u00b6 # cholesky factor L , lower = scipy . linalg . cho_factor ( K + alpha * identity ) # cholesky solver weights = scipy . linalg . cho_solve (( L , lower ), y )","title":"The Cholesky Decomposition"},{"location":"snippets/scipy/cholesky/#the-cholesky-decomposition","text":"I use this quite often whenever I'm dealing with Gaussian processes and kernel methods. Instead of doing the solver, we can simply use the cho_factor and cho_solve that's built into the scipy library.","title":"The Cholesky Decomposition"},{"location":"snippets/scipy/cholesky/#direct-solver","text":"# use the direct solver weights = scipy . linalg . solve ( K + alpha * identity , y )","title":"Direct Solver"},{"location":"snippets/scipy/cholesky/#cholesky-factor","text":"# cholesky factor L , lower = scipy . linalg . cho_factor ( K + alpha * identity ) # cholesky solver weights = scipy . linalg . cho_solve (( L , lower ), y )","title":"Cholesky Factor"},{"location":"snippets/testing/pytest/","text":"Annotating Tests \u00b6 import pytest @pytest . mark . slow def test_that_runs_slowly (): ... @pytest . mark . data def test_that_goes_on_data (): ... @pytest . mark . slow @pytest . mark . data def test_that_goes_on_data_slowly (): ... > py.test -m \"slow\" > py.test -m \"data\" > py.test -m \"not data\" Source : Eric Ma - Blog","title":"PyTest Tricks"},{"location":"snippets/testing/pytest/#annotating-tests","text":"import pytest @pytest . mark . slow def test_that_runs_slowly (): ... @pytest . mark . data def test_that_goes_on_data (): ... @pytest . mark . slow @pytest . mark . data def test_that_goes_on_data_slowly (): ... > py.test -m \"slow\" > py.test -m \"data\" > py.test -m \"not data\" Source : Eric Ma - Blog","title":"Annotating Tests"},{"location":"snippets/testing/tips/","text":"","title":"Tips"},{"location":"snippets/visualization/creating_gifs/","text":"gifs \u00b6 Animation in Python A Nice tutorial about how we can animate figures.","title":"gifs"},{"location":"snippets/visualization/creating_gifs/#gifs","text":"Animation in Python A Nice tutorial about how we can animate figures.","title":"gifs"},{"location":"snippets/visualization/matplotlib/colorbars/","text":"Colorbars \u00b6 Basics \u00b6 AstroML Plot Colorbar Only \u00b6 StackOverFlow Plot n Colorbars, 1 Plot \u00b6 How to Plot Only One Colorbar for Multiple Plot Using Matplotlib - jdhao's Blog Matplotlib Docs StackOverflow Colorbar Position \u00b6 Matplotlib Docs Scaling \u00b6 cbar = plt . colorbar ( pts , fraction = 0.046 ,) Normalizing \u00b6 import matplotlib.colors as colors boundaries = ( 0.0 , 1.0 ) # plot data pts = ax . pcolor ( X , Y , Z , norm = colors . Normalize ( vmin = boundaries [ 0 ], vmax = boundaries [ 1 ]) cmap = 'grays' ) # plot colorbar cbar = plt . colorbar ( pts , ax = ax , extend = 'both' ) Normalizations Log Scale Symmetric Log Scale Resources Matplotlib Tutorial - ColorMap Norms Jake Vanderplas - Customizing Colorbars","title":"Colorbars"},{"location":"snippets/visualization/matplotlib/colorbars/#colorbars","text":"","title":"Colorbars"},{"location":"snippets/visualization/matplotlib/colorbars/#basics","text":"AstroML","title":"Basics"},{"location":"snippets/visualization/matplotlib/colorbars/#plot-colorbar-only","text":"StackOverFlow","title":"Plot Colorbar Only"},{"location":"snippets/visualization/matplotlib/colorbars/#plot-n-colorbars-1-plot","text":"How to Plot Only One Colorbar for Multiple Plot Using Matplotlib - jdhao's Blog Matplotlib Docs StackOverflow","title":"Plot n Colorbars, 1 Plot"},{"location":"snippets/visualization/matplotlib/colorbars/#colorbar-position","text":"Matplotlib Docs","title":"Colorbar Position"},{"location":"snippets/visualization/matplotlib/colorbars/#scaling","text":"cbar = plt . colorbar ( pts , fraction = 0.046 ,)","title":"Scaling"},{"location":"snippets/visualization/matplotlib/colorbars/#normalizing","text":"import matplotlib.colors as colors boundaries = ( 0.0 , 1.0 ) # plot data pts = ax . pcolor ( X , Y , Z , norm = colors . Normalize ( vmin = boundaries [ 0 ], vmax = boundaries [ 1 ]) cmap = 'grays' ) # plot colorbar cbar = plt . colorbar ( pts , ax = ax , extend = 'both' ) Normalizations Log Scale Symmetric Log Scale Resources Matplotlib Tutorial - ColorMap Norms Jake Vanderplas - Customizing Colorbars","title":"Normalizing"},{"location":"snippets/visualization/matplotlib/legends/","text":"Legend \u00b6 Plot Legend Only \u00b6 StackOverflow Blog Legend Outside of Plot \u00b6 How To Put Legend outside of Axes Properly in Matplotlib? - jdhao's blog","title":"Legend"},{"location":"snippets/visualization/matplotlib/legends/#legend","text":"","title":"Legend"},{"location":"snippets/visualization/matplotlib/legends/#plot-legend-only","text":"StackOverflow Blog","title":"Plot Legend Only"},{"location":"snippets/visualization/matplotlib/legends/#legend-outside-of-plot","text":"How To Put Legend outside of Axes Properly in Matplotlib? - jdhao's blog","title":"Legend Outside of Plot"},{"location":"snippets/visualization/matplotlib/logscale/","text":"Log Scale Plots \u00b6 Proper Ticks \u00b6 Matplotlib Plotting Notes -- Series 1 - jdhaos Blog","title":"Log Scale Plots"},{"location":"snippets/visualization/matplotlib/logscale/#log-scale-plots","text":"","title":"Log Scale Plots"},{"location":"snippets/visualization/matplotlib/logscale/#proper-ticks","text":"Matplotlib Plotting Notes -- Series 1 - jdhaos Blog","title":"Proper Ticks"},{"location":"snippets/visualization/matplotlib/nice_defaults/","text":"Defaults \u00b6 My Go to \u00b6 I go with this setup and it typically gives me decent style of plots for my first pass: import matplotlib import seaborn as sns sns . reset_defaults () sns . set_context ( context = 'talk' , font_scale = 0.7 , rc = { 'font.family' : 'sans-serif' }) % matplotlib inline Note : change the context accordingly (e.g. poster , talk , paper )","title":"Defaults"},{"location":"snippets/visualization/matplotlib/nice_defaults/#defaults","text":"","title":"Defaults"},{"location":"snippets/visualization/matplotlib/nice_defaults/#my-go-to","text":"I go with this setup and it typically gives me decent style of plots for my first pass: import matplotlib import seaborn as sns sns . reset_defaults () sns . set_context ( context = 'talk' , font_scale = 0.7 , rc = { 'font.family' : 'sans-serif' }) % matplotlib inline Note : change the context accordingly (e.g. poster , talk , paper )","title":"My Go to"},{"location":"snippets/visualization/matplotlib/styling/","text":"Styling \u00b6 Styling PLots for Publication with matplotlib A nice tutorial where they show you have to style your plots by using functions to operate on the axes themsevles.","title":"Styling"},{"location":"snippets/visualization/matplotlib/styling/#styling","text":"Styling PLots for Publication with matplotlib A nice tutorial where they show you have to style your plots by using functions to operate on the axes themsevles.","title":"Styling"},{"location":"snippets/visualization/xarray/tifs/","text":"Tif Files \u00b6 Example from XArray","title":"Tif Files"},{"location":"snippets/visualization/xarray/tifs/#tif-files","text":"Example from XArray","title":"Tif Files"},{"location":"talks/","text":"My Talks \u00b6 Fast Friday Talks (FFTs) \u00b6 (2020-01-31) - Kernel Alignment and the Kernel Parameter Analysis Conferences \u00b6 Phi-Week (2019) - Sensitivity Analysis for Gaussian Processes Phi-Week (2019) - Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors AGU (2019) - Climate Model Intercomparison with Multivariate Information Theoretic Measures Groups \u00b6 KERMES (2018) - Kernel Methods for Earth Observation KERMES (2020) - Input Uncertainty Propagation in Gaussian Process Regression Models","title":"My Talks"},{"location":"talks/#my-talks","text":"","title":"My Talks"},{"location":"talks/#fast-friday-talks-ffts","text":"(2020-01-31) - Kernel Alignment and the Kernel Parameter Analysis","title":"Fast Friday Talks (FFTs)"},{"location":"talks/#conferences","text":"Phi-Week (2019) - Sensitivity Analysis for Gaussian Processes Phi-Week (2019) - Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors AGU (2019) - Climate Model Intercomparison with Multivariate Information Theoretic Measures","title":"Conferences"},{"location":"talks/#groups","text":"KERMES (2018) - Kernel Methods for Earth Observation KERMES (2020) - Input Uncertainty Propagation in Gaussian Process Regression Models","title":"Groups"},{"location":"talks/2018_kermes_egp/","text":"KERMES Meetup 2018 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Kernel Methods for Earth Observation: Gaussian Processes and Derivatives \u00b6 Date: 2018 Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/IPL-UV/gp_error_propagation Slides \u00b6","title":"KERMES Meetup 2018"},{"location":"talks/2018_kermes_egp/#kermes-meetup-2018","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"KERMES Meetup 2018"},{"location":"talks/2018_kermes_egp/#kernel-methods-for-earth-observation-gaussian-processes-and-derivatives","text":"Date: 2018","title":"Kernel Methods for Earth Observation: Gaussian Processes and Derivatives"},{"location":"talks/2018_kermes_egp/#resources","text":"Code You can find the code for my experiments on this repository: github.com/IPL-UV/gp_error_propagation","title":"Resources"},{"location":"talks/2018_kermes_egp/#slides","text":"","title":"Slides"},{"location":"talks/2019_agu_rbigclima/","text":"AGU 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Climate Model Intercomparison with Multivariate Information Theoretic Measures \u00b6 Date: 2019 - 12 - 05 Day: Thursday Abstract \u00b6 Earth system models, or climate models, are fundamental tools to understand climate change. The Coupled Model Intercomparison Project (CMIP) provides outputs from many coupled atmosphere-ocean general circulation models for a number of different climate forcing scenarios. CMIP is essential to assess model performance and representativity during the historical period and quantifications of the causes of the spread in future projections. In some aspects, model accuracy has improved significantly over the different CMIP phases, but biases and uncertainties in their projections still remain, and notable differences between models exist. In this work we adopt a novel information theory (IT) perspective for evaluating and comparing climate models. Information content is model independent, and allows us to compare climate model simulations and study their differences in information units. IT measures, such as entropy, total correlation, divergences and mutual information, allow us to potentially encapsulate some interactions and phenomena that each climate model may exhibit. We introduce the rotation-based iterative Gaussianization (RBIG) method to address the inherent problem of high-dimensionality in probability density function estimation and in turn IT measures estimation. The RBIG method is a generative model that is robust to noise and dimensionality and is computationally efficient. We will show intercomparison scenarios between CMIP5 model simulations at a monthly resolution and for key models and variables. We will show assessment of the relative information content, and divergences among models, across time and space. These results provide a better unbiased evaluation of models in addition to being a valid comparative measure of shared information. Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs Slides \u00b6","title":"AGU 2019"},{"location":"talks/2019_agu_rbigclima/#agu-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"AGU 2019"},{"location":"talks/2019_agu_rbigclima/#climate-model-intercomparison-with-multivariate-information-theoretic-measures","text":"Date: 2019 - 12 - 05 Day: Thursday","title":"Climate Model Intercomparison with Multivariate Information Theoretic Measures"},{"location":"talks/2019_agu_rbigclima/#abstract","text":"Earth system models, or climate models, are fundamental tools to understand climate change. The Coupled Model Intercomparison Project (CMIP) provides outputs from many coupled atmosphere-ocean general circulation models for a number of different climate forcing scenarios. CMIP is essential to assess model performance and representativity during the historical period and quantifications of the causes of the spread in future projections. In some aspects, model accuracy has improved significantly over the different CMIP phases, but biases and uncertainties in their projections still remain, and notable differences between models exist. In this work we adopt a novel information theory (IT) perspective for evaluating and comparing climate models. Information content is model independent, and allows us to compare climate model simulations and study their differences in information units. IT measures, such as entropy, total correlation, divergences and mutual information, allow us to potentially encapsulate some interactions and phenomena that each climate model may exhibit. We introduce the rotation-based iterative Gaussianization (RBIG) method to address the inherent problem of high-dimensionality in probability density function estimation and in turn IT measures estimation. The RBIG method is a generative model that is robust to noise and dimensionality and is computationally efficient. We will show intercomparison scenarios between CMIP5 model simulations at a monthly resolution and for key models and variables. We will show assessment of the relative information content, and divergences among models, across time and space. These results provide a better unbiased evaluation of models in addition to being a valid comparative measure of shared information.","title":"Abstract"},{"location":"talks/2019_agu_rbigclima/#resources","text":"Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs","title":"Resources"},{"location":"talks/2019_agu_rbigclima/#slides","text":"","title":"Slides"},{"location":"talks/2019_phiweek_gpsens/","text":"Phi Week 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Sensitivity Analysis for Gaussian Processes: Gradient-Based Methods for Emulation \u00b6 Date: 2019 Resources \u00b6 Code You can find the code for my experiments on my labs repository: github.com/IPL-UV/gp_sens Slides \u00b6","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_gpsens/#phi-week-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_gpsens/#sensitivity-analysis-for-gaussian-processes-gradient-based-methods-for-emulation","text":"Date: 2019","title":"Sensitivity Analysis for Gaussian Processes: Gradient-Based Methods for Emulation"},{"location":"talks/2019_phiweek_gpsens/#resources","text":"Code You can find the code for my experiments on my labs repository: github.com/IPL-UV/gp_sens","title":"Resources"},{"location":"talks/2019_phiweek_gpsens/#slides","text":"","title":"Slides"},{"location":"talks/2019_phiweek_rbigad/","text":"Phi Week 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors \u00b6 Date: 2019 Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs Slides \u00b6","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_rbigad/#phi-week-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_rbigad/#unsupervised-machine-learning-exploring-spatial-temporal-relationships-and-drought-factors","text":"Date: 2019","title":"Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors"},{"location":"talks/2019_phiweek_rbigad/#resources","text":"Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs","title":"Resources"},{"location":"talks/2019_phiweek_rbigad/#slides","text":"","title":"Slides"},{"location":"talks/2020_fft_01_31_hsic_align/","text":"Fast Friday Talk \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com An empirical investigation of Kernel Alignment parameters \u00b6 Date: 2020 - 01 - 31 Day: Friday Time: 1200 Abstract \u00b6 When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring. Slides \u00b6","title":"Fast Friday Talk"},{"location":"talks/2020_fft_01_31_hsic_align/#fast-friday-talk","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Fast Friday Talk"},{"location":"talks/2020_fft_01_31_hsic_align/#an-empirical-investigation-of-kernel-alignment-parameters","text":"Date: 2020 - 01 - 31 Day: Friday Time: 1200","title":"An empirical investigation of Kernel Alignment parameters"},{"location":"talks/2020_fft_01_31_hsic_align/#abstract","text":"When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring.","title":"Abstract"},{"location":"talks/2020_fft_01_31_hsic_align/#slides","text":"","title":"Slides"},{"location":"talks/2020_kermes_egp/","text":"KERMES Meetup 2020 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020 Input Uncertainty Propagation in Gaussian Process Regression Models \u00b6 Resources \u00b6 Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website . Slides \u00b6","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes_egp/#kermes-meetup-2020","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes_egp/#input-uncertainty-propagation-in-gaussian-process-regression-models","text":"","title":"Input Uncertainty Propagation in Gaussian Process Regression Models"},{"location":"talks/2020_kermes_egp/#resources","text":"Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website .","title":"Resources"},{"location":"talks/2020_kermes_egp/#slides","text":"","title":"Slides"},{"location":"talks/fft_2020_01_31_hsic_align/","text":"Fast Friday Talk \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com An empirical investigation of Kernel Alignment parameters \u00b6 Date: 2020 - 01 - 31 Day: Friday Time: 1200 Abstract \u00b6 When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring. Slides \u00b6","title":"Fast Friday Talk"},{"location":"talks/fft_2020_01_31_hsic_align/#fast-friday-talk","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Fast Friday Talk"},{"location":"talks/fft_2020_01_31_hsic_align/#an-empirical-investigation-of-kernel-alignment-parameters","text":"Date: 2020 - 01 - 31 Day: Friday Time: 1200","title":"An empirical investigation of Kernel Alignment parameters"},{"location":"talks/fft_2020_01_31_hsic_align/#abstract","text":"When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring.","title":"Abstract"},{"location":"talks/fft_2020_01_31_hsic_align/#slides","text":"","title":"Slides"},{"location":"thesis/","text":"My Thesis \u00b6 Introduction \u00b6 1.1 Earth Science \u00b6 Outline of problems in Earth science. I'll focus specifically on multivariate, high dimensional Earth science data. Would like to use the DataCubes as inspiration. 1.2 ML Approach \u00b6 Outline my approach to the whole thing. I'm focused on interactions between the data variables described about. Even more specific, I'm focused on the actual ML models themselves and how they can be used. The goal would be conditional density estimation. But point estimates with non-linear functions is a good approximation. The approaches will consist of similarity measures directly , e.g. \\rho \\rho V-Coefficient, Kernel measures and variation of information (information theory). We have a linear, a non-linear and an information theory based. Alternatively, we could use non-linear models but restricted to ones that take into account uncertainty with Gaussian approximations. 1) complete change of variables methods, 2) approximate Gaussian models with input uncertainty, 3) approximate Gaussian models with sensitivity analysis (backwards uncertainty). Keep in mind the ultimate goal: comparing 2 or more variables. 1.3 Outline \u00b6 Chapter 2 - Model Approximations \u00b6 This chapter basically covers my exploits using models that take into account uncertainty either directly (forward) or allow us to use sensitivity analysis (backwards). 1. Modeling w. Uncertainty \u00b6 What is uncertainty? The language of uncertainty (Bayesian stuff)? How do we model it? Use regression and walk through the cases. GPs in a nutshell as my choice. Explain a bit more in depth about what a GP can and cannot do. 2. Sensitivity \u00b6 What is sensitivity? How it's related to interpretability? How it approximates modeled uncertainty. A way to do uncertainty estimation (permutation plots, Sobel, Morris, SHAP). The derivative of kernel methods and how it gives us a way to approximate the sensitivity of the model itself. 3. Applications \u00b6 SAKAME stuff basically. In an uncertain setting (GP) as well as extensions to other kernel methods (KRR, SVM, HSIC, KDE). Mention the Phi-Week application for emulation. Closing thoughts. EGP1.0 stuff as well. Where we should how this can be applied to real data using the Taylor expansion. Chapter 3 - Data Representation \u00b6 We go for a more direct approach. Instead of just going for conditional density estimation (CDE), we just try to look at different ways to represent the data as a way of estimating CDE. I investigate different approaches to doing it including a linear, a non-linear and a PDF estimator. 1. Similarity \u00b6 What is it? How do we define it? How do we visualize it? How it approximates the CDE? 2. Linear and NonLinear \u00b6 We start with the idea of the linear method \\rho \\rho V coefficient and show how it extends to the Kernel methods via a distance metric and a non-linear kernel function. We pay special attention to how one can choose the parameters in order to get the best representation for the unsupervised setting of multi-dimensional data. 3. PDF Estimation \u00b6 A different approach to direct modeling: estimating the density directly...using a model. Show the different methods already done including Knn, exponential family, normalizing flows. And the method we choose - Gaussianization. Also talk about the metrics you can use in the form of information - shannon information, entropy, mutual information, variation of information. 4. Applications \u00b6 RBIG4EO Show the applications of using RBIG on spatial-temporal data Chapter 4 - Information \u00b6 Density Estimation \u00b6 Classical Gaussianization RBIG Information Theory \u00b6 Information RBIG Chapter 5 - Applications \u00b6 1. Sensitivity \u00b6 2. Uncertainty \u00b6 3. Similarity \u00b6","title":"My Thesis"},{"location":"thesis/#my-thesis","text":"","title":"My Thesis"},{"location":"thesis/#introduction","text":"","title":"Introduction"},{"location":"thesis/#11-earth-science","text":"Outline of problems in Earth science. I'll focus specifically on multivariate, high dimensional Earth science data. Would like to use the DataCubes as inspiration.","title":"1.1 Earth Science"},{"location":"thesis/#12-ml-approach","text":"Outline my approach to the whole thing. I'm focused on interactions between the data variables described about. Even more specific, I'm focused on the actual ML models themselves and how they can be used. The goal would be conditional density estimation. But point estimates with non-linear functions is a good approximation. The approaches will consist of similarity measures directly , e.g. \\rho \\rho V-Coefficient, Kernel measures and variation of information (information theory). We have a linear, a non-linear and an information theory based. Alternatively, we could use non-linear models but restricted to ones that take into account uncertainty with Gaussian approximations. 1) complete change of variables methods, 2) approximate Gaussian models with input uncertainty, 3) approximate Gaussian models with sensitivity analysis (backwards uncertainty). Keep in mind the ultimate goal: comparing 2 or more variables.","title":"1.2 ML Approach"},{"location":"thesis/#13-outline","text":"","title":"1.3 Outline"},{"location":"thesis/#chapter-2-model-approximations","text":"This chapter basically covers my exploits using models that take into account uncertainty either directly (forward) or allow us to use sensitivity analysis (backwards).","title":"Chapter 2 - Model Approximations"},{"location":"thesis/#1-modeling-w-uncertainty","text":"What is uncertainty? The language of uncertainty (Bayesian stuff)? How do we model it? Use regression and walk through the cases. GPs in a nutshell as my choice. Explain a bit more in depth about what a GP can and cannot do.","title":"1. Modeling w. Uncertainty"},{"location":"thesis/#2-sensitivity","text":"What is sensitivity? How it's related to interpretability? How it approximates modeled uncertainty. A way to do uncertainty estimation (permutation plots, Sobel, Morris, SHAP). The derivative of kernel methods and how it gives us a way to approximate the sensitivity of the model itself.","title":"2. Sensitivity"},{"location":"thesis/#3-applications","text":"SAKAME stuff basically. In an uncertain setting (GP) as well as extensions to other kernel methods (KRR, SVM, HSIC, KDE). Mention the Phi-Week application for emulation. Closing thoughts. EGP1.0 stuff as well. Where we should how this can be applied to real data using the Taylor expansion.","title":"3. Applications"},{"location":"thesis/#chapter-3-data-representation","text":"We go for a more direct approach. Instead of just going for conditional density estimation (CDE), we just try to look at different ways to represent the data as a way of estimating CDE. I investigate different approaches to doing it including a linear, a non-linear and a PDF estimator.","title":"Chapter 3 - Data Representation"},{"location":"thesis/#1-similarity","text":"What is it? How do we define it? How do we visualize it? How it approximates the CDE?","title":"1. Similarity"},{"location":"thesis/#2-linear-and-nonlinear","text":"We start with the idea of the linear method \\rho \\rho V coefficient and show how it extends to the Kernel methods via a distance metric and a non-linear kernel function. We pay special attention to how one can choose the parameters in order to get the best representation for the unsupervised setting of multi-dimensional data.","title":"2. Linear and NonLinear"},{"location":"thesis/#3-pdf-estimation","text":"A different approach to direct modeling: estimating the density directly...using a model. Show the different methods already done including Knn, exponential family, normalizing flows. And the method we choose - Gaussianization. Also talk about the metrics you can use in the form of information - shannon information, entropy, mutual information, variation of information.","title":"3. PDF Estimation"},{"location":"thesis/#4-applications","text":"RBIG4EO Show the applications of using RBIG on spatial-temporal data","title":"4. Applications"},{"location":"thesis/#chapter-4-information","text":"","title":"Chapter 4 - Information"},{"location":"thesis/#density-estimation","text":"Classical Gaussianization RBIG","title":"Density Estimation"},{"location":"thesis/#information-theory","text":"Information RBIG","title":"Information Theory"},{"location":"thesis/#chapter-5-applications","text":"","title":"Chapter 5 - Applications"},{"location":"thesis/#1-sensitivity","text":"","title":"1. Sensitivity"},{"location":"thesis/#2-uncertainty","text":"","title":"2. Uncertainty"},{"location":"thesis/#3-similarity","text":"","title":"3. Similarity"},{"location":"thesis/contribution/","text":"Related Publications \u00b6 Philosophy \u00b6 Overall, I don't want to waste anything. I find it a shame that we have so many resources separated. For example, the documentation for code (especially dense packages) often have no math associated with this. Another example is the thesis of a graduate student often has no code snippets or lab examples of some of the stuff that they have implemented. Why aren't these fused together? I'm not saying that we need to have super long documents with code snippets, but it costs you nothing to add your relevant thesis snippets to your code documentation. So, I will try to be doing all things at once: Thesis Blog Posts Software Journal Articles Journal Articles \u00b6 SAKAME RBIG 4 RS Letters \u00b6 EGP HSIC Alignment EGP 2.0 Workshops \u00b6 ICML 2019 - RBIG4NF CI - RBIG Colaborations \u00b6 IT 4 DNN ML 4 OCN Drought GSA w. GPR","title":"Related Publications"},{"location":"thesis/contribution/#related-publications","text":"","title":"Related Publications"},{"location":"thesis/contribution/#philosophy","text":"Overall, I don't want to waste anything. I find it a shame that we have so many resources separated. For example, the documentation for code (especially dense packages) often have no math associated with this. Another example is the thesis of a graduate student often has no code snippets or lab examples of some of the stuff that they have implemented. Why aren't these fused together? I'm not saying that we need to have super long documents with code snippets, but it costs you nothing to add your relevant thesis snippets to your code documentation. So, I will try to be doing all things at once: Thesis Blog Posts Software Journal Articles","title":"Philosophy"},{"location":"thesis/contribution/#journal-articles","text":"SAKAME RBIG 4 RS","title":"Journal Articles"},{"location":"thesis/contribution/#letters","text":"EGP HSIC Alignment EGP 2.0","title":"Letters"},{"location":"thesis/contribution/#workshops","text":"ICML 2019 - RBIG4NF CI - RBIG","title":"Workshops"},{"location":"thesis/contribution/#colaborations","text":"IT 4 DNN ML 4 OCN Drought GSA w. GPR","title":"Colaborations"},{"location":"thesis/logistics/","text":"Logistics \u00b6 Inspiration Thesis \u00b6 Neural Density Estimation and Likelihood-Free Inference - Papamakarios, 2019 Broadening the Scope of Gaussian Processes for Large-Scale Learning - Cutajar, 2019 Uncertainty in Deep Learning - Gal (2016) Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Model-Based Understanding of facial expressions - Sauer (2013) From Dependence to Causation - Lopez-Paz (2016) Understanding Random Forests: From Theory to Practice - Louppe (2014) Inspiring Talks \u00b6 Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS 2018/2019 - Part I | Part 2 | Part 3 | Slides Story Principle \u00b6 There is typically a 3-step rule for telling a story especially when you want people to learn something from it. We're stupid, we need repitition. The repeat rule of 3 is what I'll use to structure the thesis. Tell them what you're going to tell them Key Ideas Tell them Data Representation Information Tell them what you told them Discussion Conclusion Writing Principles (SEED) \u00b6 I will try to follow this principle for writing. brief and succinct. I want my statements to really explore and develop one single idea before moving on to the next idea. So in order to keep things in control, I will follow the SEED principle. Statement Explanation Evidence Development Resources * Writing Science: How to write papers that get cited and proposals that get funded - Joshua Schimel (2011) Philosophy \u00b6 Explanations If you can't explain it simply, then you don't understand it enough - Einstein Code Snippets Talk is cheap. Show me the code - Linus Torvald Lab Notebooks OK. But how does it work in practice? AutoDiff all the things! Sleeper Theorems I'll leave it up to the user as an exercise | It's easy to show that | as seen in [1] Parting Words \u00b6 I like to leave parting words. I see this in a blog by Matthew Rocklin all of the time What I did I do What I did I not do What I could have done better What will I do in the future? Another way to look at it is to have strengths and limitations in my discussions and conclusions. In my highschool, I typically had to do 5 of each; sometimes 10 of each if we behaved badly in class. It was tough but it made us think critically and reflect upon our work. Details \u00b6 Often times, we have many details that I think is important to know but not necessarily important to tell the story. Theorems, derivations, side notes, are very important but sometimes I think we can omit them from the main body of the text. I will create a few tabs that will hide some of these details I deem irrelevant for the story. Details These are simply details that I feel are side notes. Code Code examples that maybe tell you how to algorithmically do something. They might also feature snippets of some practical modifcations that may occur in the field. Proof I am not a fan of proofs being in the main body of the text (unless the text is about proofs). In ML, often this is not necessary except for a theoretical paper. In applied settings, we only need main equations and the rest of the details can go in the appendix. Resources Extra links where a better explanation can be given. Reproducibility \u00b6 Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Resources **Source**: Association for Computing Machinery (2016)","title":"Logistics"},{"location":"thesis/logistics/#logistics","text":"","title":"Logistics"},{"location":"thesis/logistics/#inspiration-thesis","text":"Neural Density Estimation and Likelihood-Free Inference - Papamakarios, 2019 Broadening the Scope of Gaussian Processes for Large-Scale Learning - Cutajar, 2019 Uncertainty in Deep Learning - Gal (2016) Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Model-Based Understanding of facial expressions - Sauer (2013) From Dependence to Causation - Lopez-Paz (2016) Understanding Random Forests: From Theory to Practice - Louppe (2014)","title":"Inspiration Thesis"},{"location":"thesis/logistics/#inspiring-talks","text":"Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS 2018/2019 - Part I | Part 2 | Part 3 | Slides","title":"Inspiring Talks"},{"location":"thesis/logistics/#story-principle","text":"There is typically a 3-step rule for telling a story especially when you want people to learn something from it. We're stupid, we need repitition. The repeat rule of 3 is what I'll use to structure the thesis. Tell them what you're going to tell them Key Ideas Tell them Data Representation Information Tell them what you told them Discussion Conclusion","title":"Story Principle"},{"location":"thesis/logistics/#writing-principles-seed","text":"I will try to follow this principle for writing. brief and succinct. I want my statements to really explore and develop one single idea before moving on to the next idea. So in order to keep things in control, I will follow the SEED principle. Statement Explanation Evidence Development Resources * Writing Science: How to write papers that get cited and proposals that get funded - Joshua Schimel (2011)","title":"Writing Principles (SEED)"},{"location":"thesis/logistics/#philosophy","text":"Explanations If you can't explain it simply, then you don't understand it enough - Einstein Code Snippets Talk is cheap. Show me the code - Linus Torvald Lab Notebooks OK. But how does it work in practice? AutoDiff all the things! Sleeper Theorems I'll leave it up to the user as an exercise | It's easy to show that | as seen in [1]","title":"Philosophy"},{"location":"thesis/logistics/#parting-words","text":"I like to leave parting words. I see this in a blog by Matthew Rocklin all of the time What I did I do What I did I not do What I could have done better What will I do in the future? Another way to look at it is to have strengths and limitations in my discussions and conclusions. In my highschool, I typically had to do 5 of each; sometimes 10 of each if we behaved badly in class. It was tough but it made us think critically and reflect upon our work.","title":"Parting Words"},{"location":"thesis/logistics/#details","text":"Often times, we have many details that I think is important to know but not necessarily important to tell the story. Theorems, derivations, side notes, are very important but sometimes I think we can omit them from the main body of the text. I will create a few tabs that will hide some of these details I deem irrelevant for the story. Details These are simply details that I feel are side notes. Code Code examples that maybe tell you how to algorithmically do something. They might also feature snippets of some practical modifcations that may occur in the field. Proof I am not a fan of proofs being in the main body of the text (unless the text is about proofs). In ML, often this is not necessary except for a theoretical paper. In applied settings, we only need main equations and the rest of the details can go in the appendix. Resources Extra links where a better explanation can be given.","title":"Details"},{"location":"thesis/logistics/#reproducibility","text":"Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Resources **Source**: Association for Computing Machinery (2016)","title":"Reproducibility"},{"location":"thesis/outline/","text":"Thesis Outline \u00b6 --- \u00b6 Chapter I - Introduction \u00b6 1.1 Earth Science in the Wild \u00b6 1.1.1 Problems \u00b6 Earth Observation Climate Extreme Events - Drought Ocean 1.1.2 Data \u00b6 Remote Sensing Physical Models Generalized Models Emulation 1.1.3 Drawbacks \u00b6 What/Which? - best model? How? - how does the model work? Why? - get knowledge 1.2 - Fundamental ML Problems \u00b6 1.2.1 - Representations \u00b6 Discriminative Generative Information 1.2.2 - Interpretation \u00b6 Affect (Sensitivity) Noise Uncertainty 1.2.3 - Understanding \u00b6 Correlation Dependence Causation 1.3 - This Thesis \u00b6 1.3.1 - Investigation \u00b6 Research Question: Can we push forward the notion of uncertainty in EO applications? Discriminative Approach - GPs Sensitivity Uncertainty Generative Approach - RBIG IT measures Intermmediate Steps: Investigations that needed to happen Uncertain GPs HSIC Parameter Estimation Applications: Earth Science Data Cubes Ocean Data Drought Indices Climate Models 1.3.2 - OutReach \u00b6 Reproducibility Blog Posts PyCon (?) 1.3.3 - My Contributions \u00b6 Code I try to include links to model zoos whenever possible with tutorials of how to do certain things from scratch. Toy Examples I will do many toy examples to highlight important concepts. Blogs This highlights things that I think the ML community should know Sleeper Theorems There are many things that you should know to read the thesis. Just like many papers. And sometimes it's not possible to put them in the appendix. So I will put it in boxes. Supplementary Material I am a strong advocate of telling the story without the need to go through unnecessary mathematics. So I dump all necessary derivations in the appendix. Including Notation and supplementary material 1.3.4 - Organization \u00b6 Part I - Data Representation Approach Part II - Discriminative Modeling Approach Part III - Generative Modeling Approach --- \u00b6 Chapter II - Data Representation \u00b6 2.1 Kernel Methods \u00b6 Theory Regression (Classification), (KRR, GPR, SVM) Dimensionality Reduction (KPCA, KECA, DRR) Density (KDE, Distribution Regression) Information Theory (Renyi Stuff) Similarity Measures - Covariance v.s. Correlation (HSIC, KA) Sleeper Theorems Mercer's Theorem HSIC \\equiv \\equiv MMD 2.2 Density Estimation \u00b6 Parametric Gaussian Mixture of Gaussians Classical Binning (Histogram) - Kernel - Smooth kNN - Adaptive Neural Density Estimation Normalizing Flows Density Destructor Gaussianization Conditional Density Estimation 2.3 Neural Networks \u00b6 Discriminative Neural Networks Probabilistic Neural Networks Fully Bayesian Neural Networks 2.4 Other \u00b6 Generalized Linear Models Ensemble Methods 2.4 All Connected \u00b6 Kernel Entropy Components Conditional 2.3 Modeling Approaches \u00b6 2.3.1 Discriminative Models \u00b6 2.3.2 Generative Models \u00b6 2.2.3 Information Theory \u00b6 Signal ... Approximate Measures (I, H, MI, TC) Change of Variables \u00b6 --- \u00b6 Chapter III - Discriminative Model \u00b6 1.1 Regression \u00b6 1.1.1 Kernel Ridge Regression \u00b6 1.1.2 Gaussian Process Regression \u00b6 1.3 Sensitivity \u00b6 1.3.1 Concept \u00b6 1.3.2 Derivative \u00b6 1.2 Uncertainty \u00b6 1.4 Applications \u00b6 ESDC Sampling Principal Curves ESDC - HSIC Sensitivity --- \u00b6 Chapter IV - Generative Model \u00b6 4.1 Outline \u00b6 4.2 Probability \u00b6 Concepts Bayesian Formulation Change of Variables Sleeper Theorems Variational Inference Jensen's Inequality Change of Variables 4.3 Generative Models \u00b6 Density Destructors Normalizing Flows Sleeper Theorems MSE vs LL vs KLD 4.4 Information Theory \u00b6 4.4.1 IT Measures \u00b6 Signal ... Approximate Measures (I, H, MI, TC) 4.4.2 Estimators \u00b6 Gaussian KNN/KDP RBIG HSIC/KA 4.5 - Applications \u00b6 4.5.1 Spatial-Temporal Analysis \u00b6 EGU18 ( C ) CI19 ( W ) RBIG 4 RS ( J ) 4.5.1 Droughts \u00b6 CI19 ( W ) RBIG 4 RS ( J ) 4.5.3 Climate Models \u00b6 AGU ( C ) RBIG 4 RS ( J ) 4.6 - Lab Notebooks \u00b6 Change of Variables Proof of Concept RBIG - Step-by-Step Kernel Derivatives (Regression) \u00b6 Regression: KRR, GPR, Classification: SVM Feature Selection: O/KECA Dependence: HSIC, rHSIC Kernel Model Zoo Application Regression: Sampling Regression: Sensitivity Analysis Input Uncertainty Appendix Kernel Methods Theorem Sensitivity Input Uncertainty \u00b6 Extended Literature Review Connection with Kalman Filters GP Model Zoo (GPy, TF, GPyTorch) Context Uncertainty in the Literature Application IASI Ocean bbP Alvaro Data Appendix Taylor Series Moment Matching Variational Inference IT Measures \u00b6 Application Earth Science Data Cubes (spatial-temporal) Drought Indices Climate Models Dependence \u00b6 Covariance vs Correlation HSIC and Kernel Alignment Mutual Information Appendices \u00b6 Mathematical Preliminaries Linear Algebra Probability Theory \u00b6","title":"Thesis Outline"},{"location":"thesis/outline/#thesis-outline","text":"","title":"Thesis Outline"},{"location":"thesis/outline/#-","text":"","title":"---"},{"location":"thesis/outline/#chapter-i-introduction","text":"","title":"Chapter I - Introduction"},{"location":"thesis/outline/#11-earth-science-in-the-wild","text":"","title":"1.1 Earth Science in the Wild"},{"location":"thesis/outline/#111-problems","text":"Earth Observation Climate Extreme Events - Drought Ocean","title":"1.1.1 Problems"},{"location":"thesis/outline/#112-data","text":"Remote Sensing Physical Models Generalized Models Emulation","title":"1.1.2 Data"},{"location":"thesis/outline/#113-drawbacks","text":"What/Which? - best model? How? - how does the model work? Why? - get knowledge","title":"1.1.3 Drawbacks"},{"location":"thesis/outline/#12-fundamental-ml-problems","text":"","title":"1.2 - Fundamental ML Problems"},{"location":"thesis/outline/#121-representations","text":"Discriminative Generative Information","title":"1.2.1 - Representations"},{"location":"thesis/outline/#122-interpretation","text":"Affect (Sensitivity) Noise Uncertainty","title":"1.2.2 - Interpretation"},{"location":"thesis/outline/#123-understanding","text":"Correlation Dependence Causation","title":"1.2.3 - Understanding"},{"location":"thesis/outline/#13-this-thesis","text":"","title":"1.3 - This Thesis"},{"location":"thesis/outline/#131-investigation","text":"Research Question: Can we push forward the notion of uncertainty in EO applications? Discriminative Approach - GPs Sensitivity Uncertainty Generative Approach - RBIG IT measures Intermmediate Steps: Investigations that needed to happen Uncertain GPs HSIC Parameter Estimation Applications: Earth Science Data Cubes Ocean Data Drought Indices Climate Models","title":"1.3.1 - Investigation"},{"location":"thesis/outline/#132-outreach","text":"Reproducibility Blog Posts PyCon (?)","title":"1.3.2 - OutReach"},{"location":"thesis/outline/#133-my-contributions","text":"Code I try to include links to model zoos whenever possible with tutorials of how to do certain things from scratch. Toy Examples I will do many toy examples to highlight important concepts. Blogs This highlights things that I think the ML community should know Sleeper Theorems There are many things that you should know to read the thesis. Just like many papers. And sometimes it's not possible to put them in the appendix. So I will put it in boxes. Supplementary Material I am a strong advocate of telling the story without the need to go through unnecessary mathematics. So I dump all necessary derivations in the appendix. Including Notation and supplementary material","title":"1.3.3 - My Contributions"},{"location":"thesis/outline/#134-organization","text":"Part I - Data Representation Approach Part II - Discriminative Modeling Approach Part III - Generative Modeling Approach","title":"1.3.4 - Organization"},{"location":"thesis/outline/#-_1","text":"","title":"---"},{"location":"thesis/outline/#chapter-ii-data-representation","text":"","title":"Chapter II - Data Representation"},{"location":"thesis/outline/#21-kernel-methods","text":"Theory Regression (Classification), (KRR, GPR, SVM) Dimensionality Reduction (KPCA, KECA, DRR) Density (KDE, Distribution Regression) Information Theory (Renyi Stuff) Similarity Measures - Covariance v.s. Correlation (HSIC, KA) Sleeper Theorems Mercer's Theorem HSIC \\equiv \\equiv MMD","title":"2.1 Kernel Methods"},{"location":"thesis/outline/#22-density-estimation","text":"Parametric Gaussian Mixture of Gaussians Classical Binning (Histogram) - Kernel - Smooth kNN - Adaptive Neural Density Estimation Normalizing Flows Density Destructor Gaussianization Conditional Density Estimation","title":"2.2 Density Estimation"},{"location":"thesis/outline/#23-neural-networks","text":"Discriminative Neural Networks Probabilistic Neural Networks Fully Bayesian Neural Networks","title":"2.3 Neural Networks"},{"location":"thesis/outline/#24-other","text":"Generalized Linear Models Ensemble Methods","title":"2.4 Other"},{"location":"thesis/outline/#24-all-connected","text":"Kernel Entropy Components Conditional","title":"2.4 All Connected"},{"location":"thesis/outline/#23-modeling-approaches","text":"","title":"2.3 Modeling Approaches"},{"location":"thesis/outline/#231-discriminative-models","text":"","title":"2.3.1 Discriminative Models"},{"location":"thesis/outline/#232-generative-models","text":"","title":"2.3.2 Generative Models"},{"location":"thesis/outline/#223-information-theory","text":"Signal ... Approximate Measures (I, H, MI, TC)","title":"2.2.3 Information Theory"},{"location":"thesis/outline/#change-of-variables","text":"","title":"Change of Variables"},{"location":"thesis/outline/#-_2","text":"","title":"---"},{"location":"thesis/outline/#chapter-iii-discriminative-model","text":"","title":"Chapter III - Discriminative Model"},{"location":"thesis/outline/#11-regression","text":"","title":"1.1 Regression"},{"location":"thesis/outline/#111-kernel-ridge-regression","text":"","title":"1.1.1 Kernel Ridge Regression"},{"location":"thesis/outline/#112-gaussian-process-regression","text":"","title":"1.1.2 Gaussian Process Regression"},{"location":"thesis/outline/#13-sensitivity","text":"","title":"1.3 Sensitivity"},{"location":"thesis/outline/#131-concept","text":"","title":"1.3.1 Concept"},{"location":"thesis/outline/#132-derivative","text":"","title":"1.3.2 Derivative"},{"location":"thesis/outline/#12-uncertainty","text":"","title":"1.2 Uncertainty"},{"location":"thesis/outline/#14-applications","text":"ESDC Sampling Principal Curves ESDC - HSIC Sensitivity","title":"1.4 Applications"},{"location":"thesis/outline/#-_3","text":"","title":"---"},{"location":"thesis/outline/#chapter-iv-generative-model","text":"","title":"Chapter IV - Generative Model"},{"location":"thesis/outline/#41-outline","text":"","title":"4.1 Outline"},{"location":"thesis/outline/#42-probability","text":"Concepts Bayesian Formulation Change of Variables Sleeper Theorems Variational Inference Jensen's Inequality Change of Variables","title":"4.2 Probability"},{"location":"thesis/outline/#43-generative-models","text":"Density Destructors Normalizing Flows Sleeper Theorems MSE vs LL vs KLD","title":"4.3 Generative Models"},{"location":"thesis/outline/#44-information-theory","text":"","title":"4.4 Information Theory"},{"location":"thesis/outline/#441-it-measures","text":"Signal ... Approximate Measures (I, H, MI, TC)","title":"4.4.1 IT Measures"},{"location":"thesis/outline/#442-estimators","text":"Gaussian KNN/KDP RBIG HSIC/KA","title":"4.4.2 Estimators"},{"location":"thesis/outline/#45-applications","text":"","title":"4.5 - Applications"},{"location":"thesis/outline/#451-spatial-temporal-analysis","text":"EGU18 ( C ) CI19 ( W ) RBIG 4 RS ( J )","title":"4.5.1 Spatial-Temporal Analysis"},{"location":"thesis/outline/#451-droughts","text":"CI19 ( W ) RBIG 4 RS ( J )","title":"4.5.1 Droughts"},{"location":"thesis/outline/#453-climate-models","text":"AGU ( C ) RBIG 4 RS ( J )","title":"4.5.3 Climate Models"},{"location":"thesis/outline/#46-lab-notebooks","text":"Change of Variables Proof of Concept RBIG - Step-by-Step","title":"4.6 - Lab Notebooks"},{"location":"thesis/outline/#kernel-derivatives-regression","text":"Regression: KRR, GPR, Classification: SVM Feature Selection: O/KECA Dependence: HSIC, rHSIC Kernel Model Zoo Application Regression: Sampling Regression: Sensitivity Analysis Input Uncertainty Appendix Kernel Methods Theorem Sensitivity","title":"Kernel Derivatives (Regression)"},{"location":"thesis/outline/#input-uncertainty","text":"Extended Literature Review Connection with Kalman Filters GP Model Zoo (GPy, TF, GPyTorch) Context Uncertainty in the Literature Application IASI Ocean bbP Alvaro Data Appendix Taylor Series Moment Matching Variational Inference","title":"Input Uncertainty"},{"location":"thesis/outline/#it-measures","text":"Application Earth Science Data Cubes (spatial-temporal) Drought Indices Climate Models","title":"IT Measures"},{"location":"thesis/outline/#dependence","text":"Covariance vs Correlation HSIC and Kernel Alignment Mutual Information","title":"Dependence"},{"location":"thesis/outline/#appendices","text":"Mathematical Preliminaries Linear Algebra Probability Theory","title":"Appendices"},{"location":"thesis/outline/#_1","text":"","title":""},{"location":"thesis/quotes/","text":"","title":"Quotes"},{"location":"thesis/reproducibility/","text":"Reproducibility \u00b6 Concepts \u00b6 Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Source : Association for Computing Machinery (2016) Blog Posts \u00b6 Personal Code \u00b6 Kernel Model Zoo \u00b6 Regression Kernel Ridge Regression Derivatives RBF Kernel ARD Kernel Linear Polynomial ArcTangent Kernels Random Fourier Features randomized Nystrom Approximation Dependence Estimation HSIC, Kernel Alignment randomized HSIC Gaussian Process Model Zoo \u00b6 Exact GP Sparse GP Uncertain Inputs Linearized Extended Unscented Moment-Matching GP Variational Inference Sensitivity Analysis RBIG 2.0 \u00b6 pyRBIG RBIG Flows ITE Tools ESDC Tools \u00b6 Xarray Tutorial Dask Tutorial Package Contribution \u00b6 EmuKit","title":"Reproducibility"},{"location":"thesis/reproducibility/#reproducibility","text":"","title":"Reproducibility"},{"location":"thesis/reproducibility/#concepts","text":"Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Source : Association for Computing Machinery (2016)","title":"Concepts"},{"location":"thesis/reproducibility/#blog-posts","text":"","title":"Blog Posts"},{"location":"thesis/reproducibility/#personal-code","text":"","title":"Personal Code"},{"location":"thesis/reproducibility/#kernel-model-zoo","text":"Regression Kernel Ridge Regression Derivatives RBF Kernel ARD Kernel Linear Polynomial ArcTangent Kernels Random Fourier Features randomized Nystrom Approximation Dependence Estimation HSIC, Kernel Alignment randomized HSIC","title":"Kernel Model Zoo"},{"location":"thesis/reproducibility/#gaussian-process-model-zoo","text":"Exact GP Sparse GP Uncertain Inputs Linearized Extended Unscented Moment-Matching GP Variational Inference Sensitivity Analysis","title":"Gaussian Process Model Zoo"},{"location":"thesis/reproducibility/#rbig-20","text":"pyRBIG RBIG Flows ITE Tools","title":"RBIG 2.0"},{"location":"thesis/reproducibility/#esdc-tools","text":"Xarray Tutorial Dask Tutorial","title":"ESDC Tools"},{"location":"thesis/reproducibility/#package-contribution","text":"EmuKit","title":"Package Contribution"},{"location":"thesis/scratch/","text":"My Thesis: A Overview \u00b6 These are some scratch notes for organizing my thesis. IDEAS to Add \u00b6 Two Views on Regression with PyMC3 and sklearn - prezi Main Idea \u00b6 Spatial-Temporal, High Dimensional, Complex Data - What do we do with it? Data Inherent Features - Point Estimates, Spatial Meaning, Temporal Meaning Understaing - Similarities \u2192 Correlations \u2192 Dependencies \u2192 Causation ML Emulator Attributes - Sensitivity, Uncertainty, Scale, Error Propagation Part I - Sensitivity Analysis \u00b6 Opening up black-box models (i.e. kernel methods) We look at sensitivity methods in the context of kernel methods \"Open the black box\" Non-Bayesian Context (KRR, SVM, KDE, HSIC) Earth Data \"Incomplete\" Bayesian Context Show examples in the context of a Bayesian Model (GPs + Emulation) Publication SAKAME Lab Notebooks \u00b6 GPs + GSA + Emulation - \\phi \\phi -Week Tutorials \u00b6 Regression: KRR, OKRR, RFF Classification: SVM, RFF+SGD Density Estimation: KDE, OKECA Dependence Estimation: HSIC, rHSIC --- \u00b6 ML Problems \u00b6 Representations? Sensitivity Uncertainty Estimates Noise Characterization Background \u00b6 Representations (Kernels, Random Features, Deep Networks) Uncertainty Noise Analysis Key Concepts \u00b6 Representations - Kernels, Random Features, NNs|PNN|BNNs Similarity Measures - Naive, HSIC, IT Uncertainty - Epistemic, Aleatoric, Out-of-Distribution Methods - Discriminative (Model), Density Destructors (Density Estimation) Model-Based \u00b6 Representations Analysis - Derivative, Sensitivity Uncertainty Characterization - Output-Variance (eGP, eSGP), Input-Training (eVGP, BGPLVM) Applications Emulation + Sensitivity Multi-Output + Sensitivity Information-Based \u00b6 IT Measures Classic Methods - single-dimension/multivariate, mean, std dev, pt est. stats generative modeling - VAE, GAN, NF, DDD GAUSSIANIZATION Neural Networks, Deep GPs Noise Characterization Require Densities - Gaussian, Mixture, Histogram, KDE, Neural, RBIG Applications climate model + noise + MI sampling Applications \u00b6 Climate Models Spatial Representations Noise Characterization Information Theory Estimates IASI Error Propagation Uncertainty Estimates Multi-Output Spatial-Temporal ARGP - BBP Data Multi-Output Spatial-Temporal Sensitivity Uncertainty Drought Variables Temporal Emulation Sensitivity Uncertainty","title":"My Thesis: A Overview"},{"location":"thesis/scratch/#my-thesis-a-overview","text":"These are some scratch notes for organizing my thesis.","title":"My Thesis: A Overview"},{"location":"thesis/scratch/#ideas-to-add","text":"Two Views on Regression with PyMC3 and sklearn - prezi","title":"IDEAS to Add"},{"location":"thesis/scratch/#main-idea","text":"Spatial-Temporal, High Dimensional, Complex Data - What do we do with it? Data Inherent Features - Point Estimates, Spatial Meaning, Temporal Meaning Understaing - Similarities \u2192 Correlations \u2192 Dependencies \u2192 Causation ML Emulator Attributes - Sensitivity, Uncertainty, Scale, Error Propagation","title":"Main Idea"},{"location":"thesis/scratch/#part-i-sensitivity-analysis","text":"Opening up black-box models (i.e. kernel methods) We look at sensitivity methods in the context of kernel methods \"Open the black box\" Non-Bayesian Context (KRR, SVM, KDE, HSIC) Earth Data \"Incomplete\" Bayesian Context Show examples in the context of a Bayesian Model (GPs + Emulation) Publication SAKAME","title":"Part I - Sensitivity Analysis"},{"location":"thesis/scratch/#lab-notebooks","text":"GPs + GSA + Emulation - \\phi \\phi -Week","title":"Lab Notebooks"},{"location":"thesis/scratch/#tutorials","text":"Regression: KRR, OKRR, RFF Classification: SVM, RFF+SGD Density Estimation: KDE, OKECA Dependence Estimation: HSIC, rHSIC","title":"Tutorials"},{"location":"thesis/scratch/#-","text":"","title":"---"},{"location":"thesis/scratch/#ml-problems","text":"Representations? Sensitivity Uncertainty Estimates Noise Characterization","title":"ML Problems"},{"location":"thesis/scratch/#background","text":"Representations (Kernels, Random Features, Deep Networks) Uncertainty Noise Analysis","title":"Background"},{"location":"thesis/scratch/#key-concepts","text":"Representations - Kernels, Random Features, NNs|PNN|BNNs Similarity Measures - Naive, HSIC, IT Uncertainty - Epistemic, Aleatoric, Out-of-Distribution Methods - Discriminative (Model), Density Destructors (Density Estimation)","title":"Key Concepts"},{"location":"thesis/scratch/#model-based","text":"Representations Analysis - Derivative, Sensitivity Uncertainty Characterization - Output-Variance (eGP, eSGP), Input-Training (eVGP, BGPLVM) Applications Emulation + Sensitivity Multi-Output + Sensitivity","title":"Model-Based"},{"location":"thesis/scratch/#information-based","text":"IT Measures Classic Methods - single-dimension/multivariate, mean, std dev, pt est. stats generative modeling - VAE, GAN, NF, DDD GAUSSIANIZATION Neural Networks, Deep GPs Noise Characterization Require Densities - Gaussian, Mixture, Histogram, KDE, Neural, RBIG Applications climate model + noise + MI sampling","title":"Information-Based"},{"location":"thesis/scratch/#applications","text":"Climate Models Spatial Representations Noise Characterization Information Theory Estimates IASI Error Propagation Uncertainty Estimates Multi-Output Spatial-Temporal ARGP - BBP Data Multi-Output Spatial-Temporal Sensitivity Uncertainty Drought Variables Temporal Emulation Sensitivity Uncertainty","title":"Applications"},{"location":"thesis/toc/","text":"Data Representations \u00b6 Features Neural Networks Probabilistic Neural Networks","title":"Toc"},{"location":"thesis/toc/#data-representations","text":"Features Neural Networks Probabilistic Neural Networks","title":"Data Representations"},{"location":"thesis/chapters/1_introduction/1.1_earth_sci/","text":"Earth Science Setting \u00b6 Data Representation \u00b6 \\mathcal{X}=\\mathbf{x}(u,v,t,z) \\mathcal{X}=\\mathbf{x}(u,v,t,z) Longitude Latitude Time Variable Application Settings \u00b6 Climate Models Drought Factors","title":"Earth Science Setting"},{"location":"thesis/chapters/1_introduction/1.1_earth_sci/#earth-science-setting","text":"","title":"Earth Science Setting"},{"location":"thesis/chapters/1_introduction/1.1_earth_sci/#data-representation","text":"\\mathcal{X}=\\mathbf{x}(u,v,t,z) \\mathcal{X}=\\mathbf{x}(u,v,t,z) Longitude Latitude Time Variable","title":"Data Representation"},{"location":"thesis/chapters/1_introduction/1.1_earth_sci/#application-settings","text":"Climate Models Drought Factors","title":"Application Settings"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/","text":"Machine Learning Setting \u00b6 Ultimately, the goal for most scientists is to understand everything there is to know about one component in nature. This can be something Earth-related like temperature or drought occurences, something societal like market trends or divorce rates, or something psycological like happiness or happiness. In all of these scenarios, we have something that we want to understand. In a machine learning setting, this is almost equivalent to predicting . From a modeling perspective, once we are able to correctly predict something given a time, a place or other factors, we have effectively learned everything there is to know about that factor. Let's call this component \\mathcal{Y} \\mathcal{Y} and we can choose any example we want. In order to effectively predict \\mathcal{Y} \\mathcal{Y} , I need to be given some other factors that are related to \\mathcal{Y} \\mathcal{Y} . We can call these other factors \\mathcal{X} \\mathcal{X} . The nature of machine learning is to use observations of the factors \\mathcal{X} \\mathcal{X} to allow us to give the equivalent outcome of \\mathcal{Y} \\mathcal{Y} . We will outline two approaches how one would address a problem like this in an ML setting. Machine Learning Model \u00b6 The most complete way to address problems is to consider the following: p(y|\\mathbf{x}; \\theta) p(y|\\mathbf{x}; \\theta) In this setting, we are trying to predict y y given some inputs \\mathbf{x} \\mathbf{x} . This is known as conditional density estimation. In its pure form, this is a very hard problem because in order for this to be possible, we need to know the probability density function of y y and the probability density function of \\mathbf{x} \\mathbf{x} . This implies that we have effectively modeled \\mathcal{Y} \\mathcal{Y} and \\mathcal{X} \\mathcal{X} and have a complete understanding of both components and subsequently understand the component of \\mathcal{Y} \\mathcal{Y} given some instance of \\mathcal{X} \\mathcal{X} . So concretely, we need to know p(\\mathbf{x},y) p(\\mathbf{x},y) as well as p(\\mathbf{x}) p(\\mathbf{x}) . But again, the problem of density estimation is very challenging especially in multivariate and multidimensional settings. As somewhat of a proxy for this setting, we can relax the problem to allow for point estimates or approximations by considering an alternative approach where we try to find a function that allows us to use \\mathbf{x} \\mathbf{x} to predict y y . So instead we have y = f(\\mathbf{x}) y = f(\\mathbf{x}) where essentially, we are trying to find a mapping from \\mathbb{E}[\\mathcal{X}] \\mathbb{E}[\\mathcal{X}] to \\mathbb{E}[\\mathcal{Y}] \\mathbb{E}[\\mathcal{Y}] . In other words, we have a latent function f(\\cdot) f(\\cdot) that uses \\mathbf{x} \\sim \\mathcal{X} \\mathbf{x} \\sim \\mathcal{X} to learn something about y \\sim \\mathcal{Y} y \\sim \\mathcal{Y} . This is the simplest scenario we can construct and we have effectively bypassed the density estimation and are only considering the expected output of \\mathcal{y} \\mathcal{y} given \\mathcal{X} \\mathcal{X} . We won't know exactly what would happen given every single case \\mathcal{X} \\mathcal{X} but we can at least get some approximation. The literature is vast and there are many methods ranging from tree-based algorithms like Decision trees, to deep-learning based algorithms like neural networks. There are many pros and cons to each and every algorithm but they are all trying to do the same thing: estimate y y given \\mathbf{x} \\mathbf{x} . At a very high-level, we have outlined two approaches to addressing the same problem: conditional density estimation - the holy grail of all methods; latent function model: a crude approximation. The literature has shown us is that we can do very well at predicting with latent functions. And it has also been shown that there are many additional assumptions and constraints that we can provide in order to achieve even better approximations to conditional density estimation. While direct density estimation is difficult, there are many modifications we can make to our function approximation even closer to density estimation. The next sections will explore some additions that we can make to allow use to achieve better estimations and they are all related to the notion of uncertainty quantification. Uncertainty Characterization \u00b6 Accounting for or quantifying the uncertainty in our model will allow us to get higher robustness and subsequently improve our prediction accuracy . Robustness is defined as how well a model can predict y y accurately once the variables \\mathbf{x} \\mathbf{x} have been altered or some assumptions have been changed. If I perturbate any input \\mathbf{x} \\mathbf{x} by some small amount \\delta \\delta , I should still be able to yield correct predictions. So a robust model will yield accurate predictions from any \\mathbf{x}\\in \\mathcal{X} \\mathbf{x}\\in \\mathcal{X} . Conversely, if a model isn't robust, there are certain values of \\mathcal{X} \\mathcal{X} that will consistently yield predictions that are inaccurate. Ideally, we want to be able to accurately predict the entire space of \\mathcal{Y} \\mathcal{Y} given any \\mathcal{X} \\mathcal{X} but this can be difficult and sometimes even impossible. The next best thing would be to yield very accurate predictions given some subset of \\mathbf{x} \\subseteq \\mathcal{X} \\mathbf{x} \\subseteq \\mathcal{X} . Knowing the subsets and regions of the input space \\mathcal{X} \\mathcal{X} and also the specific regions in our ML pipeline where we fail to produce good predictions is what uncertainty is. More Data \u00b6 A trivial example is the case where we have only one input \\mathbf{x} \\mathbf{x} and try to create a model that is able to predict y y based on that single observation. If we add another input that is different into our model, we are very likely to incorrectly predict the outcome. So an obvious importance to robustness is to get more data (if you can). This is the single most important way to increase the robustness of your model. The more instances and variations of your inputs your model has seen, the more robust your model will become. The great thing is that this will increase the robustness of any machine learning model and is largely responsible for the success of Deep neural networks. Sensitivity Analysis \u00b6 Often times, modeling uncertainty directly is difficult. An easier problem to solve The study of how the uncertainty in the output of a model can be apportioned to the different sources of uncertainty in the model input. Identify which inputs are most influential for the predictions \"Backward Process\" Examples: Morris, Sobel, Regression A closely related field is the notion of model interpretability . This idea stems from the idea that we aren't satisfied which functions that simply map inputs \\mathbf{x} \\mathbf{x} to outputs y y without giving the user any interpretation for why the model made that decision. Uncertainty Analysis \u00b6 A way to scrutinize uncertainties in the model parameters, the input data, the assumptions and model structures. How uncertainties propagate through the model Effect of the uncertainties on the predictions Identify the best model or policy given the uncertainty \"Foward Analysis\" If we were to directly try to estimate the conditional probability p(y|\\mathbf{x};\\theta) p(y|\\mathbf{x};\\theta) , then we would have y = f(\\mathbf{x})+\\epsilon, \\mathbf{x}\\sim \\mathcal{N}() y = f(\\mathbf{x})+\\epsilon, \\mathbf{x}\\sim \\mathcal{N}() where \\mathbf{x} \\mathbf{x} comes from some probability distribution \\mathcal{P} \\mathcal{P} and there is some noise Language Of Uncertainty \u00b6 Model Output Inputs y=f(x) y=f(x) None None None y=f(x) + \\epsilon y=f(x) + \\epsilon f\\sim \\mathcal{S} f\\sim \\mathcal{S} y \\sim \\mathcal{Q} y \\sim \\mathcal{Q} x \\sim \\mathcal{P} x \\sim \\mathcal{P} y=f(x) + \\epsilon y=f(x) + \\epsilon Name Model Aleatoric \\mathbf{x,y}\\sim \\mathcal{P,Q} \\mathbf{x,y}\\sim \\mathcal{P,Q} Epistemic f\\sim \\mathcal{S} f\\sim \\mathcal{S} Distribution Shift \\mathcal{x}\\sim\\mathcal{X} \\mathcal{x}\\sim\\mathcal{X} Data Representation \u00b6 Modeling is essentially trying to transform \\mathbf{x} \\mathbf{x} s.t. we get a linear relationship between y y and \\mathbf{x} \\mathbf{x} . Mathematics allows us to transform our problem into different spaces where our problem can be solved more easily. Example A very simple example is identifying wave patterns along the coast of the ocean. We know that we can describe a wave using a series of signals which can be obtained using a Fourier decomposition. So by transforming our inputs into the Fourier domain, it is easier to identify which signals are the most representative of our region. And consequently, this allows us to make accurate predictions; but within the Fourier domain. So the representation of our data as sines and cosines made our problem much easier to solve. Data representation is very important and can transform our ML problem into a much easier one, once we have the correct representation. Linear Methods \u00b6 The simplest representation would be a linear transformation using a weight vector \\mathbf{w} \\in \\mathbb{R}^{D} \\mathbf{w} \\in \\mathbb{R}^{D} and a bias term b \\in \\mathbb{R} b \\in \\mathbb{R} . y = \\mathbf{wx} + b y = \\mathbf{wx} + b It is a simple transformation with a linear. It is invertible, it is interpretable and we can easily do sensitivity analysis. In addition, because it is so simple, we could even do more complex predictions for example quantile regression which allows one to predict the upper bound and lower bound of a confidence interval. We can even deal with uncertain inputs by means of error-in-variables regression such as Total LSR , BCES or ODR . In addition, even if we don't use LR directly for modeling, we often use it as a measure of how well our other nonlinear model has performed. Almost all regression statistics are some form of regression, e.g. Pearson is simple linear regression where as Spearman's \\rho \\rho and Kendall's \\tau \\tau are forms of ranked regression. So even if your data is very non-linear, you will end up using simple LR to ensure that your potentially complex, non-linear model has accurate predictions. TODO Example with a linear model fitting a linear relationship and a complex relationship. Joint Plot Neural Networks \u00b6 To get more expressive functions, we can compose a series of transformation such that each compositDeep composite transforms for feature representations y = f_1\\circ\\;f_2\\;\\circ\\;...\\;\\circ\\;f_L( \\mathbf{x}) y = f_1\\circ\\;f_2\\;\\circ\\;...\\;\\circ\\;f_L( \\mathbf{x}) When we concatenate these functions, we are facilitating more complex representations of our dataset. TODO Example with a NN model fitting a linear relationship and a complex relationship. Joint Plot Kernel methods \u00b6 High dimensional transformation TODO Example with a kernel model fitting a linear relationship and a complex relationship. Joint Plot PDF Estimation \u00b6 Probability density estimates TODO Example with a PDF estimator fitting a linear relationship and a complex relationship. Joint Plot Because we are in probability space, we have access to other attributes of our data * Q: How much information about X does y contain? TODO \u00b6 EXAMPLES ML Model Conditional Est Example (Data, Graphical Model) - Example | Example Direct Latent Function Example (Data, Graphical Model) Uncertainty Analysis More Data - look at the residuals Sensitivity Analysis - Sobel, GP Uncertainty - GP (model, input, output) Citations \u00b6 arxiv - Conditional Density Estimation with Neural Networks: BestPractices and Benchmarks - Rothfuss et al (2019)","title":"Machine Learning Setting"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#machine-learning-setting","text":"Ultimately, the goal for most scientists is to understand everything there is to know about one component in nature. This can be something Earth-related like temperature or drought occurences, something societal like market trends or divorce rates, or something psycological like happiness or happiness. In all of these scenarios, we have something that we want to understand. In a machine learning setting, this is almost equivalent to predicting . From a modeling perspective, once we are able to correctly predict something given a time, a place or other factors, we have effectively learned everything there is to know about that factor. Let's call this component \\mathcal{Y} \\mathcal{Y} and we can choose any example we want. In order to effectively predict \\mathcal{Y} \\mathcal{Y} , I need to be given some other factors that are related to \\mathcal{Y} \\mathcal{Y} . We can call these other factors \\mathcal{X} \\mathcal{X} . The nature of machine learning is to use observations of the factors \\mathcal{X} \\mathcal{X} to allow us to give the equivalent outcome of \\mathcal{Y} \\mathcal{Y} . We will outline two approaches how one would address a problem like this in an ML setting.","title":"Machine Learning Setting"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#machine-learning-model","text":"The most complete way to address problems is to consider the following: p(y|\\mathbf{x}; \\theta) p(y|\\mathbf{x}; \\theta) In this setting, we are trying to predict y y given some inputs \\mathbf{x} \\mathbf{x} . This is known as conditional density estimation. In its pure form, this is a very hard problem because in order for this to be possible, we need to know the probability density function of y y and the probability density function of \\mathbf{x} \\mathbf{x} . This implies that we have effectively modeled \\mathcal{Y} \\mathcal{Y} and \\mathcal{X} \\mathcal{X} and have a complete understanding of both components and subsequently understand the component of \\mathcal{Y} \\mathcal{Y} given some instance of \\mathcal{X} \\mathcal{X} . So concretely, we need to know p(\\mathbf{x},y) p(\\mathbf{x},y) as well as p(\\mathbf{x}) p(\\mathbf{x}) . But again, the problem of density estimation is very challenging especially in multivariate and multidimensional settings. As somewhat of a proxy for this setting, we can relax the problem to allow for point estimates or approximations by considering an alternative approach where we try to find a function that allows us to use \\mathbf{x} \\mathbf{x} to predict y y . So instead we have y = f(\\mathbf{x}) y = f(\\mathbf{x}) where essentially, we are trying to find a mapping from \\mathbb{E}[\\mathcal{X}] \\mathbb{E}[\\mathcal{X}] to \\mathbb{E}[\\mathcal{Y}] \\mathbb{E}[\\mathcal{Y}] . In other words, we have a latent function f(\\cdot) f(\\cdot) that uses \\mathbf{x} \\sim \\mathcal{X} \\mathbf{x} \\sim \\mathcal{X} to learn something about y \\sim \\mathcal{Y} y \\sim \\mathcal{Y} . This is the simplest scenario we can construct and we have effectively bypassed the density estimation and are only considering the expected output of \\mathcal{y} \\mathcal{y} given \\mathcal{X} \\mathcal{X} . We won't know exactly what would happen given every single case \\mathcal{X} \\mathcal{X} but we can at least get some approximation. The literature is vast and there are many methods ranging from tree-based algorithms like Decision trees, to deep-learning based algorithms like neural networks. There are many pros and cons to each and every algorithm but they are all trying to do the same thing: estimate y y given \\mathbf{x} \\mathbf{x} . At a very high-level, we have outlined two approaches to addressing the same problem: conditional density estimation - the holy grail of all methods; latent function model: a crude approximation. The literature has shown us is that we can do very well at predicting with latent functions. And it has also been shown that there are many additional assumptions and constraints that we can provide in order to achieve even better approximations to conditional density estimation. While direct density estimation is difficult, there are many modifications we can make to our function approximation even closer to density estimation. The next sections will explore some additions that we can make to allow use to achieve better estimations and they are all related to the notion of uncertainty quantification.","title":"Machine Learning Model"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#uncertainty-characterization","text":"Accounting for or quantifying the uncertainty in our model will allow us to get higher robustness and subsequently improve our prediction accuracy . Robustness is defined as how well a model can predict y y accurately once the variables \\mathbf{x} \\mathbf{x} have been altered or some assumptions have been changed. If I perturbate any input \\mathbf{x} \\mathbf{x} by some small amount \\delta \\delta , I should still be able to yield correct predictions. So a robust model will yield accurate predictions from any \\mathbf{x}\\in \\mathcal{X} \\mathbf{x}\\in \\mathcal{X} . Conversely, if a model isn't robust, there are certain values of \\mathcal{X} \\mathcal{X} that will consistently yield predictions that are inaccurate. Ideally, we want to be able to accurately predict the entire space of \\mathcal{Y} \\mathcal{Y} given any \\mathcal{X} \\mathcal{X} but this can be difficult and sometimes even impossible. The next best thing would be to yield very accurate predictions given some subset of \\mathbf{x} \\subseteq \\mathcal{X} \\mathbf{x} \\subseteq \\mathcal{X} . Knowing the subsets and regions of the input space \\mathcal{X} \\mathcal{X} and also the specific regions in our ML pipeline where we fail to produce good predictions is what uncertainty is.","title":"Uncertainty Characterization"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#more-data","text":"A trivial example is the case where we have only one input \\mathbf{x} \\mathbf{x} and try to create a model that is able to predict y y based on that single observation. If we add another input that is different into our model, we are very likely to incorrectly predict the outcome. So an obvious importance to robustness is to get more data (if you can). This is the single most important way to increase the robustness of your model. The more instances and variations of your inputs your model has seen, the more robust your model will become. The great thing is that this will increase the robustness of any machine learning model and is largely responsible for the success of Deep neural networks.","title":"More Data"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#sensitivity-analysis","text":"Often times, modeling uncertainty directly is difficult. An easier problem to solve The study of how the uncertainty in the output of a model can be apportioned to the different sources of uncertainty in the model input. Identify which inputs are most influential for the predictions \"Backward Process\" Examples: Morris, Sobel, Regression A closely related field is the notion of model interpretability . This idea stems from the idea that we aren't satisfied which functions that simply map inputs \\mathbf{x} \\mathbf{x} to outputs y y without giving the user any interpretation for why the model made that decision.","title":"Sensitivity Analysis"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#uncertainty-analysis","text":"A way to scrutinize uncertainties in the model parameters, the input data, the assumptions and model structures. How uncertainties propagate through the model Effect of the uncertainties on the predictions Identify the best model or policy given the uncertainty \"Foward Analysis\" If we were to directly try to estimate the conditional probability p(y|\\mathbf{x};\\theta) p(y|\\mathbf{x};\\theta) , then we would have y = f(\\mathbf{x})+\\epsilon, \\mathbf{x}\\sim \\mathcal{N}() y = f(\\mathbf{x})+\\epsilon, \\mathbf{x}\\sim \\mathcal{N}() where \\mathbf{x} \\mathbf{x} comes from some probability distribution \\mathcal{P} \\mathcal{P} and there is some noise","title":"Uncertainty Analysis"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#language-of-uncertainty","text":"Model Output Inputs y=f(x) y=f(x) None None None y=f(x) + \\epsilon y=f(x) + \\epsilon f\\sim \\mathcal{S} f\\sim \\mathcal{S} y \\sim \\mathcal{Q} y \\sim \\mathcal{Q} x \\sim \\mathcal{P} x \\sim \\mathcal{P} y=f(x) + \\epsilon y=f(x) + \\epsilon Name Model Aleatoric \\mathbf{x,y}\\sim \\mathcal{P,Q} \\mathbf{x,y}\\sim \\mathcal{P,Q} Epistemic f\\sim \\mathcal{S} f\\sim \\mathcal{S} Distribution Shift \\mathcal{x}\\sim\\mathcal{X} \\mathcal{x}\\sim\\mathcal{X}","title":"Language Of Uncertainty"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#data-representation","text":"Modeling is essentially trying to transform \\mathbf{x} \\mathbf{x} s.t. we get a linear relationship between y y and \\mathbf{x} \\mathbf{x} . Mathematics allows us to transform our problem into different spaces where our problem can be solved more easily. Example A very simple example is identifying wave patterns along the coast of the ocean. We know that we can describe a wave using a series of signals which can be obtained using a Fourier decomposition. So by transforming our inputs into the Fourier domain, it is easier to identify which signals are the most representative of our region. And consequently, this allows us to make accurate predictions; but within the Fourier domain. So the representation of our data as sines and cosines made our problem much easier to solve. Data representation is very important and can transform our ML problem into a much easier one, once we have the correct representation.","title":"Data Representation"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#linear-methods","text":"The simplest representation would be a linear transformation using a weight vector \\mathbf{w} \\in \\mathbb{R}^{D} \\mathbf{w} \\in \\mathbb{R}^{D} and a bias term b \\in \\mathbb{R} b \\in \\mathbb{R} . y = \\mathbf{wx} + b y = \\mathbf{wx} + b It is a simple transformation with a linear. It is invertible, it is interpretable and we can easily do sensitivity analysis. In addition, because it is so simple, we could even do more complex predictions for example quantile regression which allows one to predict the upper bound and lower bound of a confidence interval. We can even deal with uncertain inputs by means of error-in-variables regression such as Total LSR , BCES or ODR . In addition, even if we don't use LR directly for modeling, we often use it as a measure of how well our other nonlinear model has performed. Almost all regression statistics are some form of regression, e.g. Pearson is simple linear regression where as Spearman's \\rho \\rho and Kendall's \\tau \\tau are forms of ranked regression. So even if your data is very non-linear, you will end up using simple LR to ensure that your potentially complex, non-linear model has accurate predictions. TODO Example with a linear model fitting a linear relationship and a complex relationship. Joint Plot","title":"Linear Methods"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#neural-networks","text":"To get more expressive functions, we can compose a series of transformation such that each compositDeep composite transforms for feature representations y = f_1\\circ\\;f_2\\;\\circ\\;...\\;\\circ\\;f_L( \\mathbf{x}) y = f_1\\circ\\;f_2\\;\\circ\\;...\\;\\circ\\;f_L( \\mathbf{x}) When we concatenate these functions, we are facilitating more complex representations of our dataset. TODO Example with a NN model fitting a linear relationship and a complex relationship. Joint Plot","title":"Neural Networks"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#kernel-methods","text":"High dimensional transformation TODO Example with a kernel model fitting a linear relationship and a complex relationship. Joint Plot","title":"Kernel methods"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#pdf-estimation","text":"Probability density estimates TODO Example with a PDF estimator fitting a linear relationship and a complex relationship. Joint Plot Because we are in probability space, we have access to other attributes of our data * Q: How much information about X does y contain?","title":"PDF Estimation"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#todo","text":"EXAMPLES ML Model Conditional Est Example (Data, Graphical Model) - Example | Example Direct Latent Function Example (Data, Graphical Model) Uncertainty Analysis More Data - look at the residuals Sensitivity Analysis - Sobel, GP Uncertainty - GP (model, input, output)","title":"TODO"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/#citations","text":"arxiv - Conditional Density Estimation with Neural Networks: BestPractices and Benchmarks - Rothfuss et al (2019)","title":"Citations"},{"location":"thesis/chapters/1_introduction/1.3_thesis/","text":"Motivation \u00b6 So I have been looking into ways that we can use information theory measures (ITMs) to do summary statistics for large data structures. The data structures can be thought of as cubes where we care about the latitude, longitude and the time for different variables. Concretely, we are looking at a 4D dataset in the form of $$ \\mathbf{X} = X(\\phi,\\lambda, t, z) $$ where \\phi \\phi is the latitude, \\lambda \\lambda is the longitude, t t is the time, z z is the variable. So we have a dataset where we have spatial, temporal and variable considerations. Part I - Sensitivity Analysis \u00b6 We look at sensitivity methods in the context of kernel methods Latent Function Only - y=f(x) y=f(x) \"Open the black box\" Non-Bayesian Context (KRR, SVM, KDE, HSIC) Application - Earth Data Part II - Input Uncertainty Quantification \u00b6 We add another level of complexity Uncertain Inputs x\\sim \\mathbb{P} x\\sim \\mathbb{P} Bayesian Model Context - y=f(x)+\\epsilon y=f(x)+\\epsilon Input Uncertainty Crude Approximation - ( Paper : eGP) Fully Bayesian Approach Scaled - ( Paper : eGP2.0) Application - IASI Data Part III - Similarity Analysis \u00b6 Last Level of complexity Measure similarity directly \\text{sim}(\\mathcal{X,Y}) \\text{sim}(\\mathcal{X,Y}) Single Variable: RBIG 4 EO Application - Spatial, Temporal Data Different Variables: HSIC Experiment - Empirical Studies","title":"1.3 thesis"},{"location":"thesis/chapters/1_introduction/1.3_thesis/#motivation","text":"So I have been looking into ways that we can use information theory measures (ITMs) to do summary statistics for large data structures. The data structures can be thought of as cubes where we care about the latitude, longitude and the time for different variables. Concretely, we are looking at a 4D dataset in the form of $$ \\mathbf{X} = X(\\phi,\\lambda, t, z) $$ where \\phi \\phi is the latitude, \\lambda \\lambda is the longitude, t t is the time, z z is the variable. So we have a dataset where we have spatial, temporal and variable considerations.","title":"Motivation"},{"location":"thesis/chapters/1_introduction/1.3_thesis/#part-i-sensitivity-analysis","text":"We look at sensitivity methods in the context of kernel methods Latent Function Only - y=f(x) y=f(x) \"Open the black box\" Non-Bayesian Context (KRR, SVM, KDE, HSIC) Application - Earth Data","title":"Part I - Sensitivity Analysis"},{"location":"thesis/chapters/1_introduction/1.3_thesis/#part-ii-input-uncertainty-quantification","text":"We add another level of complexity Uncertain Inputs x\\sim \\mathbb{P} x\\sim \\mathbb{P} Bayesian Model Context - y=f(x)+\\epsilon y=f(x)+\\epsilon Input Uncertainty Crude Approximation - ( Paper : eGP) Fully Bayesian Approach Scaled - ( Paper : eGP2.0) Application - IASI Data","title":"Part II - Input Uncertainty Quantification"},{"location":"thesis/chapters/1_introduction/1.3_thesis/#part-iii-similarity-analysis","text":"Last Level of complexity Measure similarity directly \\text{sim}(\\mathcal{X,Y}) \\text{sim}(\\mathcal{X,Y}) Single Variable: RBIG 4 EO Application - Spatial, Temporal Data Different Variables: HSIC Experiment - Empirical Studies","title":"Part III - Similarity Analysis"},{"location":"thesis/chapters/2_models/sensitivity/","text":"Inspiration \u00b6 Sensitivity Analysis: An Introduction Intro to SA - Saltelli SA an Intro - Tarantola GSA - Julia package Global Sensitivity Analysis PyTorch Captum Library Algorithms Global Sensitivity Analysis: The Primer","title":"Sensitivity"},{"location":"thesis/chapters/2_models/sensitivity/#inspiration","text":"Sensitivity Analysis: An Introduction Intro to SA - Saltelli SA an Intro - Tarantola GSA - Julia package Global Sensitivity Analysis PyTorch Captum Library Algorithms Global Sensitivity Analysis: The Primer","title":"Inspiration"},{"location":"thesis/chapters/2_models/similarity/","text":"Similarity \u00b6 What is it? \u00b6 When making comparisons between objects, the simplest question we can ask ourselves is how 'similar' one object is to another. It's a simple question but it's very difficult to answer. Simalirity in everyday life is somewhat easy to grasp intuitively but it's not easy to convey specific instructions to a computer. A 1 . For example, the saying, \"it's like comparing apples to oranges\" is usually said when you try to tell someone that something is not comparable. But actually, we can compare apples and oranges. We can compare the shape, color, composition and even how delicious we personally think they are. So let's consider the datacube structure that was mentioned above. How would we compare two variables z_1 z_1 and z_2 z_2 . Trends \u00b6 In this representation, we are essentially doing one type or processing and then A parallel coordinate visualization is practical byt only certain pairwise comparisons are possible. If we look at just the temporal component, then we could just plot the time series at different points along the globe. Trends often do not expose similarity in an intuitive way. Constraints \u00b6 Orthogonal Transformations \u00b6 Linear Transformations \u00b6 Isotropic Scaling \u00b6 Multivariate \u00b6 Curse of Dimensionality \u00b6 Classic Methods \u00b6 Summary Statistics \u00b6 Boxplots allow us to sumarize these statistics Taylor Diagram \u00b6 A Taylor Diagram is a concise statistical summary of how well patterns match each other in terms of their correlation, their root-mean-square difference and the ratio of their variances. With this diagram, we can simultaneously plot each of the summary statistics, e.g. standard deviation, root mean squared error (RMSE) and the R correlation coefficient. The original reference can be found here 2 . $$ \\text{RMS}^2 = \\sigma_x 2+\\sigma_y 2-2\\sigma_x\\sigma_yR_{xy} \\ c^2 = a^2 + b^2 - 2ab \\cos \\theta $$ This is a well understood diagram in the Earth science and climate community. It is also easy to compute. Correlation \u00b6 A scatterplot matrix can be impractical for many outputs. Example with Anscombes Quartet why IT measures might be useful for correlation plots. HSIC \u00b6 Mutual Information \u00b6 Information Theory \u00b6 Mutual Information is the counterpart to using information theory methods. It requires an estimation step which may introduce additional uncertainties Extends nicely to different types of data (e.g. discrete, categorical, multivariate, multidimentional) Exposes non-linearities which may be difficult to see via (linear) correlations Kernel Approximations: Although there are some differences for different estimators, relative distances are consistent A Primer \u00b6 Entropy - measure of information uncertainty of X X Joint Entropy - uncertinaty of X,Y X,Y Conditional Entropy - uncertainty of X X given that I know Y Y Mutual Information - how much knowning X X reduces the uncertainty of Y Y I(X,Y)= I(X,Y)= Normalized Mutual Information \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} Variation of Information \u00b6 A measure of distance in information theory space. VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) where: VI(X,Y)=0 VI(X,Y)=0 Iff X X and Y Y are the same H(X,Y)=H(X)=H(Y)=I(X,Y) H(X,Y)=H(X)=H(Y)=I(X,Y) VI(X,Y) < H(X,Y) VI(X,Y) < H(X,Y) If X X and Y Y are different but dependent H(X,Y)<H(X) + H(Y) H(X,Y)<H(X) + H(Y) VI(X,Y)=H(X,Y) VI(X,Y)=H(X,Y) if X X and Y Y are independent H(X,Y)=H(X) + H(Y) H(X,Y)=H(X) + H(Y) I(X,Y)=0 I(X,Y)=0 Questions \u00b6 Are there correlations across seasons or latitudes Are there large descrepancies in the different outputs? Classes of Methods \u00b6 Resources \u00b6 Websites \u00b6 Taylor Diagrams Papers \u00b6","title":"Similarity"},{"location":"thesis/chapters/2_models/similarity/#similarity","text":"","title":"Similarity"},{"location":"thesis/chapters/2_models/similarity/#what-is-it","text":"When making comparisons between objects, the simplest question we can ask ourselves is how 'similar' one object is to another. It's a simple question but it's very difficult to answer. Simalirity in everyday life is somewhat easy to grasp intuitively but it's not easy to convey specific instructions to a computer. A 1 . For example, the saying, \"it's like comparing apples to oranges\" is usually said when you try to tell someone that something is not comparable. But actually, we can compare apples and oranges. We can compare the shape, color, composition and even how delicious we personally think they are. So let's consider the datacube structure that was mentioned above. How would we compare two variables z_1 z_1 and z_2 z_2 .","title":"What is it?"},{"location":"thesis/chapters/2_models/similarity/#trends","text":"In this representation, we are essentially doing one type or processing and then A parallel coordinate visualization is practical byt only certain pairwise comparisons are possible. If we look at just the temporal component, then we could just plot the time series at different points along the globe. Trends often do not expose similarity in an intuitive way.","title":"Trends"},{"location":"thesis/chapters/2_models/similarity/#constraints","text":"","title":"Constraints"},{"location":"thesis/chapters/2_models/similarity/#orthogonal-transformations","text":"","title":"Orthogonal Transformations"},{"location":"thesis/chapters/2_models/similarity/#linear-transformations","text":"","title":"Linear Transformations"},{"location":"thesis/chapters/2_models/similarity/#isotropic-scaling","text":"","title":"Isotropic Scaling"},{"location":"thesis/chapters/2_models/similarity/#multivariate","text":"","title":"Multivariate"},{"location":"thesis/chapters/2_models/similarity/#curse-of-dimensionality","text":"","title":"Curse of Dimensionality"},{"location":"thesis/chapters/2_models/similarity/#classic-methods","text":"","title":"Classic Methods"},{"location":"thesis/chapters/2_models/similarity/#summary-statistics","text":"Boxplots allow us to sumarize these statistics","title":"Summary Statistics"},{"location":"thesis/chapters/2_models/similarity/#taylor-diagram","text":"A Taylor Diagram is a concise statistical summary of how well patterns match each other in terms of their correlation, their root-mean-square difference and the ratio of their variances. With this diagram, we can simultaneously plot each of the summary statistics, e.g. standard deviation, root mean squared error (RMSE) and the R correlation coefficient. The original reference can be found here 2 . $$ \\text{RMS}^2 = \\sigma_x 2+\\sigma_y 2-2\\sigma_x\\sigma_yR_{xy} \\ c^2 = a^2 + b^2 - 2ab \\cos \\theta $$ This is a well understood diagram in the Earth science and climate community. It is also easy to compute.","title":"Taylor Diagram"},{"location":"thesis/chapters/2_models/similarity/#correlation","text":"A scatterplot matrix can be impractical for many outputs. Example with Anscombes Quartet why IT measures might be useful for correlation plots.","title":"Correlation"},{"location":"thesis/chapters/2_models/similarity/#hsic","text":"","title":"HSIC"},{"location":"thesis/chapters/2_models/similarity/#mutual-information","text":"","title":"Mutual Information"},{"location":"thesis/chapters/2_models/similarity/#information-theory","text":"Mutual Information is the counterpart to using information theory methods. It requires an estimation step which may introduce additional uncertainties Extends nicely to different types of data (e.g. discrete, categorical, multivariate, multidimentional) Exposes non-linearities which may be difficult to see via (linear) correlations Kernel Approximations: Although there are some differences for different estimators, relative distances are consistent","title":"Information Theory"},{"location":"thesis/chapters/2_models/similarity/#a-primer","text":"Entropy - measure of information uncertainty of X X Joint Entropy - uncertinaty of X,Y X,Y Conditional Entropy - uncertainty of X X given that I know Y Y Mutual Information - how much knowning X X reduces the uncertainty of Y Y I(X,Y)= I(X,Y)= Normalized Mutual Information \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}}","title":"A Primer"},{"location":"thesis/chapters/2_models/similarity/#variation-of-information","text":"A measure of distance in information theory space. VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) where: VI(X,Y)=0 VI(X,Y)=0 Iff X X and Y Y are the same H(X,Y)=H(X)=H(Y)=I(X,Y) H(X,Y)=H(X)=H(Y)=I(X,Y) VI(X,Y) < H(X,Y) VI(X,Y) < H(X,Y) If X X and Y Y are different but dependent H(X,Y)<H(X) + H(Y) H(X,Y)<H(X) + H(Y) VI(X,Y)=H(X,Y) VI(X,Y)=H(X,Y) if X X and Y Y are independent H(X,Y)=H(X) + H(Y) H(X,Y)=H(X) + H(Y) I(X,Y)=0 I(X,Y)=0","title":"Variation of Information"},{"location":"thesis/chapters/2_models/similarity/#questions","text":"Are there correlations across seasons or latitudes Are there large descrepancies in the different outputs?","title":"Questions"},{"location":"thesis/chapters/2_models/similarity/#classes-of-methods","text":"","title":"Classes of Methods"},{"location":"thesis/chapters/2_models/similarity/#resources","text":"","title":"Resources"},{"location":"thesis/chapters/2_models/similarity/#websites","text":"Taylor Diagrams","title":"Websites"},{"location":"thesis/chapters/2_models/similarity/#papers","text":"","title":"Papers"},{"location":"thesis/chapters/2_models/uncertainty/","text":"Uncertainty \u00b6 Overview of Overview \u00b6 Models w. Uncertainty Epistemic - use Bayes approaches Aleatoric - use Bayes Output - Homeoscedastic, Heteroscedastic Input? - augment Out-of-Dist: outside the scope Measure it Directly Expected Uncertainty PDF Estimation What is Uncertainty? \u00b6 Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Out-of-Distribution, Distribution Shift when the distribution we learn from is different from the testing data. Expected Uncertainty Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input. Aleatoric Uncertainty, \\sigma^2 \\sigma^2 \u00b6 This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day... Intuition: 1D Regression \u00b6 Sources Intution Examples - Colab Notebook Real Function \u00b6 Intuition: Confidence Intervals \u00b6 Figure : Intuition of for the Taylor expansion for a model: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\epsilon_y \\epsilon_y - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 \u00b6 The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find Uncertainty in the Error Generalization \u00b6 First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model. Uncertainty Over Functions \u00b6 In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 . Distribution Shift \u00b6 In ML, we typically assume that our data is stationary; meaning that it will always come from the same distribution. This is not always the case as sometimes we just only observed a portion of the data, e.g. space and time. Need Example : Earth science, in time and space? This is also essential in the causality realm where such a bad assumption could lead to incorrect causal graphs.","title":"Uncertainty"},{"location":"thesis/chapters/2_models/uncertainty/#uncertainty","text":"","title":"Uncertainty"},{"location":"thesis/chapters/2_models/uncertainty/#overview-of-overview","text":"Models w. Uncertainty Epistemic - use Bayes approaches Aleatoric - use Bayes Output - Homeoscedastic, Heteroscedastic Input? - augment Out-of-Dist: outside the scope Measure it Directly Expected Uncertainty PDF Estimation","title":"Overview of Overview"},{"location":"thesis/chapters/2_models/uncertainty/#what-is-uncertainty","text":"Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Out-of-Distribution, Distribution Shift when the distribution we learn from is different from the testing data. Expected Uncertainty Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input.","title":"What is Uncertainty?"},{"location":"thesis/chapters/2_models/uncertainty/#aleatoric-uncertainty-sigma2sigma2","text":"This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day...","title":"Aleatoric Uncertainty, \\sigma^2\\sigma^2"},{"location":"thesis/chapters/2_models/uncertainty/#intuition-1d-regression","text":"Sources Intution Examples - Colab Notebook","title":"Intuition: 1D Regression"},{"location":"thesis/chapters/2_models/uncertainty/#real-function","text":"","title":"Real Function"},{"location":"thesis/chapters/2_models/uncertainty/#intuition-confidence-intervals","text":"Figure : Intuition of for the Taylor expansion for a model: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\epsilon_y \\epsilon_y - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x .","title":"Intuition: Confidence Intervals"},{"location":"thesis/chapters/2_models/uncertainty/#epistemic-uncertainty-nu_2nu_2","text":"The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find","title":"Epistemic Uncertainty, \\nu_{**}^2\\nu_{**}^2"},{"location":"thesis/chapters/2_models/uncertainty/#uncertainty-in-the-error-generalization","text":"First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model.","title":"Uncertainty in the Error Generalization"},{"location":"thesis/chapters/2_models/uncertainty/#uncertainty-over-functions","text":"In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 .","title":"Uncertainty Over Functions"},{"location":"thesis/chapters/2_models/uncertainty/#distribution-shift","text":"In ML, we typically assume that our data is stationary; meaning that it will always come from the same distribution. This is not always the case as sometimes we just only observed a portion of the data, e.g. space and time. Need Example : Earth science, in time and space? This is also essential in the causality realm where such a bad assumption could lead to incorrect causal graphs.","title":"Distribution Shift"},{"location":"thesis/chapters/3_similarity/hybrid/","text":"Hybrid Approaches \u00b6 NN + GP - Prob Neural Net NN + GP - DGP NN + Random - RFE","title":"Hybrid Approaches"},{"location":"thesis/chapters/3_similarity/hybrid/#hybrid-approaches","text":"NN + GP - Prob Neural Net NN + GP - DGP NN + Random - RFE","title":"Hybrid Approaches"},{"location":"thesis/chapters/3_similarity/kernels/","text":"Kernel Methods \u00b6 Kernel Methods What is a Kernel function? Interpreting Kernel Functions (Parameters) Derivatives + Sensivity Literature Gaussian Processes Deep Gaussian Processes Family Kernel Randomized Composition Regression KRR RFF SSGPR Bayesian GPR SSGRP SSGPR 1 Kernel Methods \u00b6 1.1 What is a Kernel Function? \u00b6 Similarity Measure + NonLinear Function higher order features Examples * Regression (KRR, GPR) * Classification (SVM) * Feature Selection (KPOLS) * Dimensionality Reduction () * Output Normalized Methods (Laplacian Eigenmaps) * Density Estimation (KDE) * Dependence Estimation (HSIC) * Distance (MMD) * Scaling (Nystrom, rNystrom, RKS) * [Blog](http://evelinag.com/Ariadne/covarianceFunctions.html) * GPML Book - [Chapter 4 - Covariance Functions](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) * Gustau - [Kernel Methods in Machine Learning](http://isp.uv.es/courses.html) **Intuition** * [From Dependence to Causation](https://arxiv.org/pdf/1607.03300.pdf) > Nice introduction with a nice plot. Also discusses the nystrom method randomized kernels. I like like to 'different representations'. Can use example where we can do dot products between features to use a nice linear method. * [Kernel Methods for Pattern Analysis]() 1.2 Interpreting Kernel Parameters \u00b6 Example RBF Kernel \u00b6 This is by far the most popular kernel you will come across when you are working with kernel methods. It's the most generalizable and it's fairly easy to train and interpret. K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} where: \\nu^2 > 0 \\nu^2 > 0 is the signal variance \\lambda > 0 \\lambda > 0 is the length scale \\sigma_{\\text{noise}}^2 \\sigma_{\\text{noise}}^2 is the noise variance Distance Metric - d(x,y)_{E} d(x,y)_{E} This is the distance metric used to describe the kernel function. For the RBF kernel, this distance metric is the Euclidean distance ( ||x-y||_2^2 ||x-y||_2^2 ). But we can switch this measure for another, e.g. the L_1 L_1 -norm produces the Laplacian kernel. In most 'stationary' kernels, this measure is the foundation for this family of kernels. It acts as a smoother. Signal Variance - \\nu^2>0 \\nu^2>0 This is simply a scaling factor. It determines the variation of function values from their mean. Small values of \\nu^2 \\nu^2 characterize functions that stay close to their mean value, larger values allow for more variation. If the signal variance is too large, the modelled function will be more susceptible to outliers. Note : Almost every kernel has this parameter in front as it is just a scale factor. Length Scale - \\lambda > 0 \\lambda > 0 This parameter describes how smooth a function is. Small \\lambda \\lambda means the function values change quickly, large \\lambda \\lambda characterizes functions that change slowly. This parameter determines how far we can reliably extrapolate from the training data. Note: Put \\lambda \\lambda is constant, automatic relevance determine, diagonal, etc. Noise Variance - \\sigma_{\\text{noise}}^2>0 \\sigma_{\\text{noise}}^2>0 This parameter describes how much noise we expect to be present in the data. Other Kernels \u00b6 Issues : * Limited Expressiveness * Neural Networks - Infinitely Wide Other Kernels * Spectral Mixture Kernel * ArcCosine - Neural Networks * Neural Tangent Kernels * Deep Kernel learning * Convolutional Kernels **Kernel Resources** * Learning with Spectral Kernels - [Prezi](https://www.hiit.fi/wp-content/uploads/2018/04/Spectral-Kernels-S12.pdf) * Learning Scalable Deep Kernels with Recurrent Structure - Al Shedivat (2017) * Deep Kernel Learning - Wilson (2015) * Deep convolutional Gaussian processes - Blomqvist (2018) * Stochastic Variational Deep Kernel Learning - Wilson (2016) * Non-Stationary Spectral Kernels - Remes (2017) **Resources** * Kernel Cookbook * A Visual Comparison of Gaussian Regression Kernels - [towardsdatascience](https://towardsdatascience.com/a-visual-comparison-of-gaussian-process-regression-kernels-8d47f2c9f63c) * [Kernel Functions](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/) * [PyKernels](https://github.com/gmum/pykernels) * [Sklearn kernels](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/gaussian_process/kernels.py#L1146) **Key Figures** * [Markus Heinonen](https://users.aalto.fi/~heinom10/) **Cutting Edge** * [Bayesian Approaches to Distribution Regression](http://www.gatsby.ucl.ac.uk/~dougals/slides/bdr-nips/#/) * HSIC * [HSIC, A Measure of Independence?](http://www.cmap.polytechnique.fr/~zoltan.szabo/talks/invited_talk/Zoltan_Szabo_invited_talk_EPFL_LIONS_28_02_2018_slides.pdf) * [Measuring Dependence and Conditional Dependence with Kernels](http://people.tuebingen.mpg.de/causal-learning/slides/CaMaL_Fukumizu.pdf) 3 Derivatives & Sensitivity \u00b6 **Resources** * Computing gradients via GPR - [stack](https://stats.stackexchange.com/questions/373446/computing-gradients-via-gaussian-process-regression) > Nice overfew of formula in a very easy way. * [Derivatives of GP](https://mathoverflow.net/questions/289424/derivatives-of-gaussian-processes) > Simple way to take a derivative of a kernel * [Differentiating GPs](http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf) * [GPyTorch Implementation](https://gpytorch.readthedocs.io/en/latest/examples/10_GP_Regression_Derivative_Information/index.html) **Literature** * [Direct Density Derivative and its Applications to KLD](http://proceedings.mlr.press/v38/sasaki15.pdf) * [Kernel Derivative Approximation with RFF](https://arxiv.org/abs/1810.05207) * An SVD and Derivative Kernel Approach to Learning from Geometric Data - Wong * Scaling Gaussian Process Regression with Derivatives - Eriksson 2018 * MultiDimensional SVM to Include the Samples of the Derivatives in the Reconstruction Function - Perez-Cruz () * Linearized Gaussian Processes for Fast Data-driven Model Predictive Control - Nghiem et. al. (2019) Bayesian Treatment: Gaussian Processes \u00b6 Composition: Deep Gaussian Processes \u00b6 Connections \u00b6","title":"Kernel Methods"},{"location":"thesis/chapters/3_similarity/kernels/#kernel-methods","text":"Kernel Methods What is a Kernel function? Interpreting Kernel Functions (Parameters) Derivatives + Sensivity Literature Gaussian Processes Deep Gaussian Processes Family Kernel Randomized Composition Regression KRR RFF SSGPR Bayesian GPR SSGRP SSGPR","title":"Kernel Methods"},{"location":"thesis/chapters/3_similarity/kernels/#1-kernel-methods","text":"","title":"1 Kernel Methods"},{"location":"thesis/chapters/3_similarity/kernels/#11-what-is-a-kernel-function","text":"Similarity Measure + NonLinear Function higher order features Examples * Regression (KRR, GPR) * Classification (SVM) * Feature Selection (KPOLS) * Dimensionality Reduction () * Output Normalized Methods (Laplacian Eigenmaps) * Density Estimation (KDE) * Dependence Estimation (HSIC) * Distance (MMD) * Scaling (Nystrom, rNystrom, RKS) * [Blog](http://evelinag.com/Ariadne/covarianceFunctions.html) * GPML Book - [Chapter 4 - Covariance Functions](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) * Gustau - [Kernel Methods in Machine Learning](http://isp.uv.es/courses.html) **Intuition** * [From Dependence to Causation](https://arxiv.org/pdf/1607.03300.pdf) > Nice introduction with a nice plot. Also discusses the nystrom method randomized kernels. I like like to 'different representations'. Can use example where we can do dot products between features to use a nice linear method. * [Kernel Methods for Pattern Analysis]()","title":"1.1 What is a Kernel Function?"},{"location":"thesis/chapters/3_similarity/kernels/#12-interpreting-kernel-parameters","text":"","title":"1.2 Interpreting Kernel Parameters"},{"location":"thesis/chapters/3_similarity/kernels/#example-rbf-kernel","text":"This is by far the most popular kernel you will come across when you are working with kernel methods. It's the most generalizable and it's fairly easy to train and interpret. K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} where: \\nu^2 > 0 \\nu^2 > 0 is the signal variance \\lambda > 0 \\lambda > 0 is the length scale \\sigma_{\\text{noise}}^2 \\sigma_{\\text{noise}}^2 is the noise variance Distance Metric - d(x,y)_{E} d(x,y)_{E} This is the distance metric used to describe the kernel function. For the RBF kernel, this distance metric is the Euclidean distance ( ||x-y||_2^2 ||x-y||_2^2 ). But we can switch this measure for another, e.g. the L_1 L_1 -norm produces the Laplacian kernel. In most 'stationary' kernels, this measure is the foundation for this family of kernels. It acts as a smoother. Signal Variance - \\nu^2>0 \\nu^2>0 This is simply a scaling factor. It determines the variation of function values from their mean. Small values of \\nu^2 \\nu^2 characterize functions that stay close to their mean value, larger values allow for more variation. If the signal variance is too large, the modelled function will be more susceptible to outliers. Note : Almost every kernel has this parameter in front as it is just a scale factor. Length Scale - \\lambda > 0 \\lambda > 0 This parameter describes how smooth a function is. Small \\lambda \\lambda means the function values change quickly, large \\lambda \\lambda characterizes functions that change slowly. This parameter determines how far we can reliably extrapolate from the training data. Note: Put \\lambda \\lambda is constant, automatic relevance determine, diagonal, etc. Noise Variance - \\sigma_{\\text{noise}}^2>0 \\sigma_{\\text{noise}}^2>0 This parameter describes how much noise we expect to be present in the data.","title":"Example RBF Kernel"},{"location":"thesis/chapters/3_similarity/kernels/#other-kernels","text":"Issues : * Limited Expressiveness * Neural Networks - Infinitely Wide Other Kernels * Spectral Mixture Kernel * ArcCosine - Neural Networks * Neural Tangent Kernels * Deep Kernel learning * Convolutional Kernels **Kernel Resources** * Learning with Spectral Kernels - [Prezi](https://www.hiit.fi/wp-content/uploads/2018/04/Spectral-Kernels-S12.pdf) * Learning Scalable Deep Kernels with Recurrent Structure - Al Shedivat (2017) * Deep Kernel Learning - Wilson (2015) * Deep convolutional Gaussian processes - Blomqvist (2018) * Stochastic Variational Deep Kernel Learning - Wilson (2016) * Non-Stationary Spectral Kernels - Remes (2017) **Resources** * Kernel Cookbook * A Visual Comparison of Gaussian Regression Kernels - [towardsdatascience](https://towardsdatascience.com/a-visual-comparison-of-gaussian-process-regression-kernels-8d47f2c9f63c) * [Kernel Functions](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/) * [PyKernels](https://github.com/gmum/pykernels) * [Sklearn kernels](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/gaussian_process/kernels.py#L1146) **Key Figures** * [Markus Heinonen](https://users.aalto.fi/~heinom10/) **Cutting Edge** * [Bayesian Approaches to Distribution Regression](http://www.gatsby.ucl.ac.uk/~dougals/slides/bdr-nips/#/) * HSIC * [HSIC, A Measure of Independence?](http://www.cmap.polytechnique.fr/~zoltan.szabo/talks/invited_talk/Zoltan_Szabo_invited_talk_EPFL_LIONS_28_02_2018_slides.pdf) * [Measuring Dependence and Conditional Dependence with Kernels](http://people.tuebingen.mpg.de/causal-learning/slides/CaMaL_Fukumizu.pdf)","title":"Other Kernels"},{"location":"thesis/chapters/3_similarity/kernels/#3-derivatives-sensitivity","text":"**Resources** * Computing gradients via GPR - [stack](https://stats.stackexchange.com/questions/373446/computing-gradients-via-gaussian-process-regression) > Nice overfew of formula in a very easy way. * [Derivatives of GP](https://mathoverflow.net/questions/289424/derivatives-of-gaussian-processes) > Simple way to take a derivative of a kernel * [Differentiating GPs](http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf) * [GPyTorch Implementation](https://gpytorch.readthedocs.io/en/latest/examples/10_GP_Regression_Derivative_Information/index.html) **Literature** * [Direct Density Derivative and its Applications to KLD](http://proceedings.mlr.press/v38/sasaki15.pdf) * [Kernel Derivative Approximation with RFF](https://arxiv.org/abs/1810.05207) * An SVD and Derivative Kernel Approach to Learning from Geometric Data - Wong * Scaling Gaussian Process Regression with Derivatives - Eriksson 2018 * MultiDimensional SVM to Include the Samples of the Derivatives in the Reconstruction Function - Perez-Cruz () * Linearized Gaussian Processes for Fast Data-driven Model Predictive Control - Nghiem et. al. (2019)","title":"3 Derivatives &amp; Sensitivity"},{"location":"thesis/chapters/3_similarity/kernels/#bayesian-treatment-gaussian-processes","text":"","title":"Bayesian Treatment: Gaussian Processes"},{"location":"thesis/chapters/3_similarity/kernels/#composition-deep-gaussian-processes","text":"","title":"Composition: Deep Gaussian Processes"},{"location":"thesis/chapters/3_similarity/kernels/#connections","text":"","title":"Connections"},{"location":"thesis/chapters/3_similarity/neural_nets/","text":"Neural Networks \u00b6 So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well Resources * Jee - Prob CV * Masterclass in regression - Aboleth - docs Weight Space \u00b6 This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence Bayesian Treatment \u00b6 Probabilistic Neural Network \u00b6 This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions. Learning: Maximum Likelihood \u00b6 Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} Final Layer \u00b6 This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric) Bayesian Neural Network \u00b6 So there are a few Benchmark datasets we can look at to determine Current top: * MC Dropout * Mean-Field Variational Inference * Deep Ensembles * Ensemble MC Dropout Benchmark Repos: * [OATML](https://github.com/OATML/bdl-benchmarks) * [Hugh Salimbeni](https://github.com/hughsalimbeni/bayesian_benchmarks) **Resources** * Neural Network Diagrams - [stack](https://softwarerecs.stackexchange.com/questions/47841/drawing-neural-networks#targetText=Drawing%20neural%20networks&targetText=Similar%20to%20the%20figures%20in,multilayer%20perceptron%20(neural%20network).) * MLSS 2019, Moscow - Yarin Gal - [Prezi I](http://bdl101.ml/MLSS_2019_BDL_1.pdf) | [Prezi II](http://bdl101.ml/MLSS_2019_BDL_2.pdf) * Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - [Blog](https://medium.com/lean-in-women-in-tech-india/fast-and-scalable-estimation-of-uncertainty-using-bayesian-deep-learning-e312571042bb) * Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - [Blog](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) * How Bayesian Methods Embody Occam's razor - [blog](https://medium.com/neuralspace/how-bayesian-methods-embody-occams-razor-43f3d0253137) * DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - [blog](https://medium.com/@ahmdtaha/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15) * Uncertainty Estimation in Supervised Learning - [Video](https://www.youtube.com/watch?v=P4WUl7TDdLo&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW&index=29&t=0s) | [Slides](https://github.com/bayesgroup/deepbayes-2019/tree/master/lectures/day6) **Blogs** * [Regression with Probabilistic Layers in TensorFlow Probability](https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf) * [Variational Inference for Bayesian Neural Networks](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/) (2019) | TensorFlow * Brenden Hasz * [Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability](https://brendanhasz.github.io/2018/12/03/tfp-regression.html) * [Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference](https://brendanhasz.github.io/2019/06/12/tfp-gmm.html) * [Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0](https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html) * Yarin Gal * [What My Deep Model Doesn't Know...](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) * High Level Series of Posts * [Probabilistic Deep Learning: Bayes by Backprop](https://medium.com/neuralspace/probabilistic-deep-learning-bayes-by-backprop-c4a3de0d9743) * [When machine learning meets complexity: why Bayesian deep learning is unavoidable](https://medium.com/neuralspace/when-machine-learning-meets-complexity-why-bayesian-deep-learning-is-unavoidable-55c97aa2a9cc) * [Bayesian Convolutional Neural Networks with Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) * [Reflections on Bayesian Inference in Probabilistic Deep Learning](https://medium.com/@laumannfelix/reflections-on-bayesian-inference-in-probabilistic-deep-learning-416376e42dc0) **Software** * [TensorFlow Probability]() * [Edward2]() * [PyTorch]() * [Pyro]() **Papers** * DropOut as Bayesian Approximation - [Paper](https://arxiv.org/pdf/1506.02142.pdf) | [Code]() | [Tutorial](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/) * Uncertainty Decomposition in BNNs with Latent Variables - [arxiv](https://arxiv.org/abs/1706.08495) * Practical Deep Learning with Bayesian Principles - [arxiv](https://arxiv.org/abs/1906.02506) * Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - [Paper]() * Probabilistic Numerics and Uncertainty in Computations - [Paper](https://arxiv.org/pdf/1506.01326.pdf) * Bayesian Inference of Log Determinants - [Paper](https://arxiv.org/pdf/1704.01445.pdf) **Code** * [A Regression Master Class with Aboleth](https://aboleth.readthedocs.io/en/stable/tutorials/some_regressors.html) * BNN Implementations - [Github](https://github.com/JavierAntoran/Bayesian-Neural-Networks) * A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - [Github](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)","title":"Neural Networks"},{"location":"thesis/chapters/3_similarity/neural_nets/#neural-networks","text":"So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well Resources * Jee - Prob CV * Masterclass in regression - Aboleth - docs","title":"Neural Networks"},{"location":"thesis/chapters/3_similarity/neural_nets/#weight-space","text":"This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence","title":"Weight Space"},{"location":"thesis/chapters/3_similarity/neural_nets/#bayesian-treatment","text":"","title":"Bayesian Treatment"},{"location":"thesis/chapters/3_similarity/neural_nets/#probabilistic-neural-network","text":"This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions.","title":"Probabilistic Neural Network"},{"location":"thesis/chapters/3_similarity/neural_nets/#learning-maximum-likelihood","text":"Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned}","title":"Learning: Maximum Likelihood"},{"location":"thesis/chapters/3_similarity/neural_nets/#final-layer","text":"This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric)","title":"Final Layer"},{"location":"thesis/chapters/3_similarity/neural_nets/#bayesian-neural-network","text":"So there are a few Benchmark datasets we can look at to determine Current top: * MC Dropout * Mean-Field Variational Inference * Deep Ensembles * Ensemble MC Dropout Benchmark Repos: * [OATML](https://github.com/OATML/bdl-benchmarks) * [Hugh Salimbeni](https://github.com/hughsalimbeni/bayesian_benchmarks) **Resources** * Neural Network Diagrams - [stack](https://softwarerecs.stackexchange.com/questions/47841/drawing-neural-networks#targetText=Drawing%20neural%20networks&targetText=Similar%20to%20the%20figures%20in,multilayer%20perceptron%20(neural%20network).) * MLSS 2019, Moscow - Yarin Gal - [Prezi I](http://bdl101.ml/MLSS_2019_BDL_1.pdf) | [Prezi II](http://bdl101.ml/MLSS_2019_BDL_2.pdf) * Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - [Blog](https://medium.com/lean-in-women-in-tech-india/fast-and-scalable-estimation-of-uncertainty-using-bayesian-deep-learning-e312571042bb) * Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - [Blog](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) * How Bayesian Methods Embody Occam's razor - [blog](https://medium.com/neuralspace/how-bayesian-methods-embody-occams-razor-43f3d0253137) * DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - [blog](https://medium.com/@ahmdtaha/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15) * Uncertainty Estimation in Supervised Learning - [Video](https://www.youtube.com/watch?v=P4WUl7TDdLo&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW&index=29&t=0s) | [Slides](https://github.com/bayesgroup/deepbayes-2019/tree/master/lectures/day6) **Blogs** * [Regression with Probabilistic Layers in TensorFlow Probability](https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf) * [Variational Inference for Bayesian Neural Networks](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/) (2019) | TensorFlow * Brenden Hasz * [Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability](https://brendanhasz.github.io/2018/12/03/tfp-regression.html) * [Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference](https://brendanhasz.github.io/2019/06/12/tfp-gmm.html) * [Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0](https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html) * Yarin Gal * [What My Deep Model Doesn't Know...](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) * High Level Series of Posts * [Probabilistic Deep Learning: Bayes by Backprop](https://medium.com/neuralspace/probabilistic-deep-learning-bayes-by-backprop-c4a3de0d9743) * [When machine learning meets complexity: why Bayesian deep learning is unavoidable](https://medium.com/neuralspace/when-machine-learning-meets-complexity-why-bayesian-deep-learning-is-unavoidable-55c97aa2a9cc) * [Bayesian Convolutional Neural Networks with Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) * [Reflections on Bayesian Inference in Probabilistic Deep Learning](https://medium.com/@laumannfelix/reflections-on-bayesian-inference-in-probabilistic-deep-learning-416376e42dc0) **Software** * [TensorFlow Probability]() * [Edward2]() * [PyTorch]() * [Pyro]() **Papers** * DropOut as Bayesian Approximation - [Paper](https://arxiv.org/pdf/1506.02142.pdf) | [Code]() | [Tutorial](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/) * Uncertainty Decomposition in BNNs with Latent Variables - [arxiv](https://arxiv.org/abs/1706.08495) * Practical Deep Learning with Bayesian Principles - [arxiv](https://arxiv.org/abs/1906.02506) * Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - [Paper]() * Probabilistic Numerics and Uncertainty in Computations - [Paper](https://arxiv.org/pdf/1506.01326.pdf) * Bayesian Inference of Log Determinants - [Paper](https://arxiv.org/pdf/1704.01445.pdf) **Code** * [A Regression Master Class with Aboleth](https://aboleth.readthedocs.io/en/stable/tutorials/some_regressors.html) * BNN Implementations - [Github](https://github.com/JavierAntoran/Bayesian-Neural-Networks) * A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - [Github](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)","title":"Bayesian Neural Network"},{"location":"thesis/chapters/3_similarity/random_kernels/","text":"Randomized \u00b6 Nystrom Approximation \u00b6 Random Fourier Features \u00b6 Bayesian Treatment \u00b6 Sparse Spectrum Gaussian Processes \u00b6 Composition \u00b6 Random Feature Expansions \u00b6","title":"Randomized"},{"location":"thesis/chapters/3_similarity/random_kernels/#randomized","text":"","title":"Randomized"},{"location":"thesis/chapters/3_similarity/random_kernels/#nystrom-approximation","text":"","title":"Nystrom Approximation"},{"location":"thesis/chapters/3_similarity/random_kernels/#random-fourier-features","text":"","title":"Random Fourier Features"},{"location":"thesis/chapters/3_similarity/random_kernels/#bayesian-treatment","text":"","title":"Bayesian Treatment"},{"location":"thesis/chapters/3_similarity/random_kernels/#sparse-spectrum-gaussian-processes","text":"","title":"Sparse Spectrum Gaussian Processes"},{"location":"thesis/chapters/3_similarity/random_kernels/#composition","text":"","title":"Composition"},{"location":"thesis/chapters/3_similarity/random_kernels/#random-feature-expansions","text":"","title":"Random Feature Expansions"},{"location":"thesis/chapters/4_information/density/0_overview/","text":"Overview \u00b6 blog * VAE Non-Invertible (VAEs) Invertible: Inverse Sampling Theorem (Uniform and PDFs) Change of Variables Core Topics Parametric Gaussianization Deep Density Destructors Information Theory Measures Generalized Divisive Normalization Supplementary Inverse Sampling Theorem Change of Variables Formula Entropy NegEntropy Classical * Parametric * Histogram * KDE * kNN Neural Density Deep Density Destructor Normalizing Flows Parametric Gaussianization Original: Projection Pursuit, Gaussianization Modern: Rotation-Based Iterative Gaussianization (RBIG) Approx: Generalized Divisive Normalization (GDN)","title":"Overview"},{"location":"thesis/chapters/4_information/density/0_overview/#overview","text":"blog * VAE Non-Invertible (VAEs) Invertible: Inverse Sampling Theorem (Uniform and PDFs) Change of Variables Core Topics Parametric Gaussianization Deep Density Destructors Information Theory Measures Generalized Divisive Normalization Supplementary Inverse Sampling Theorem Change of Variables Formula Entropy NegEntropy Classical * Parametric * Histogram * KDE * kNN Neural Density Deep Density Destructor Normalizing Flows Parametric Gaussianization Original: Projection Pursuit, Gaussianization Modern: Rotation-Based Iterative Gaussianization (RBIG) Approx: Generalized Divisive Normalization (GDN)","title":"Overview"},{"location":"thesis/chapters/4_information/density/classical/","text":"Density Estimation \u00b6 Classical (Binning, Kernel, Adaptive | KNN) Change of Variables (NF, Density Destructor, Gaussianization) Conditional","title":"Density Estimation"},{"location":"thesis/chapters/4_information/density/classical/#density-estimation","text":"Classical (Binning, Kernel, Adaptive | KNN) Change of Variables (NF, Density Destructor, Gaussianization) Conditional","title":"Density Estimation"},{"location":"thesis/chapters/4_information/density/gaussianization/","text":"Parametric Gaussianization \u00b6 Key Papers: * Gaussianization - Chen and Gopinath (2001) * Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2010) Why Gaussianization? \u00b6 Gaussianization : Transform multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . History \u00b6 Cost Function \u00b6 So, we have essentially described a model that transforms the data from the original data distribution \\mathcal{D} \\mathcal{D} to the normal distribution \\mathcal{N} \\mathcal{N} so now the question is: how well did we approximate the base distribution \\mathcal{N} \\mathcal{N} . We can use something called negentropy which is how far the transformed distribution is from the normal distribution. More concretely, it is the KLD between the transformed distribution, P_y P_y and the standard normal distribution, \\mathcal{N}\\sim(0, \\mathbf{I}) \\mathcal{N}\\sim(0, \\mathbf{I}) . We can write down the standard definition of entropy like so D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx However, it might make a bit more sense intuitively to rewrite this equation in terms of expectations. \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] This basically says want the expected value between the probabilities of our approximate base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) and the real base distribution \\mathcal{N}(z) \\mathcal{N}(z) . We have the equation of \\mathcal{P}_x(x) \\mathcal{P}_x(x) in terms of the probability of the base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) , so we can plug that into our negentropy \\mathcal{J}(\\mathcal{P}_z) \\mathcal{J}(\\mathcal{P}_z) formulation \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] We can unravel the log probabilities to something much simpler: \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, it's difficult to compute the expectations in terms of the base distribution z z . Instead let's make it factor of our data. We can do this by unravelling the \\mathbb{E}_z \\mathbb{E}_z \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Again, we utilize the fact that we've done a change of variables which means we can rewrite the expectation in terms of the Data distribution: \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] which means we can simplify this to be the expectation w.r.t. to the data distribution: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, to be more concrete about where our variables are coming from, we can substitute the z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) into our negentropy formulation: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] So now when it comes to minimizing the loss function, we just need to take the derivative w.r.t. to the parameters \\theta \\theta . All of our terms in this equation are dependent on the parameter \\theta \\theta . \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] The derivative of an expectation of something is the same as the expectation of a derivative ( \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] ) using the dominated convergence theorem ( stackoverflow ). So we can just take the derivative w.r.t. \\theta \\theta inside of the expectation $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x)) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} (x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ Let's take it term by term. First of all, we can see that the \\log \\mathcal{P}_x(x) \\log \\mathcal{P}_x(x) has no parameters dependent upon \\theta \\theta so we can immediately cancel that term. $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\cancel{\\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x))} - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} (x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ The second term ??? third term Practically speaking, this is a bit difficult to calculate. Instead we can do a procedure that measures how much more Gaussian the approximate base distribution has become as a result of the transformation \\mathcal{G}_{\\theta}(x) \\mathcal{G}_{\\theta}(x) . -- In the Wild \u00b6 Gaussianization for Fast and Accurate Inference from Cosmological Data Nice formula for how to calculate the likelihood.","title":"Parametric Gaussianization"},{"location":"thesis/chapters/4_information/density/gaussianization/#parametric-gaussianization","text":"Key Papers: * Gaussianization - Chen and Gopinath (2001) * Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2010)","title":"Parametric Gaussianization"},{"location":"thesis/chapters/4_information/density/gaussianization/#why-gaussianization","text":"Gaussianization : Transform multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"thesis/chapters/4_information/density/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"thesis/chapters/4_information/density/gaussianization/#history","text":"","title":"History"},{"location":"thesis/chapters/4_information/density/gaussianization/#cost-function","text":"So, we have essentially described a model that transforms the data from the original data distribution \\mathcal{D} \\mathcal{D} to the normal distribution \\mathcal{N} \\mathcal{N} so now the question is: how well did we approximate the base distribution \\mathcal{N} \\mathcal{N} . We can use something called negentropy which is how far the transformed distribution is from the normal distribution. More concretely, it is the KLD between the transformed distribution, P_y P_y and the standard normal distribution, \\mathcal{N}\\sim(0, \\mathbf{I}) \\mathcal{N}\\sim(0, \\mathbf{I}) . We can write down the standard definition of entropy like so D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx However, it might make a bit more sense intuitively to rewrite this equation in terms of expectations. \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] This basically says want the expected value between the probabilities of our approximate base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) and the real base distribution \\mathcal{N}(z) \\mathcal{N}(z) . We have the equation of \\mathcal{P}_x(x) \\mathcal{P}_x(x) in terms of the probability of the base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) , so we can plug that into our negentropy \\mathcal{J}(\\mathcal{P}_z) \\mathcal{J}(\\mathcal{P}_z) formulation \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] We can unravel the log probabilities to something much simpler: \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, it's difficult to compute the expectations in terms of the base distribution z z . Instead let's make it factor of our data. We can do this by unravelling the \\mathbb{E}_z \\mathbb{E}_z \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Again, we utilize the fact that we've done a change of variables which means we can rewrite the expectation in terms of the Data distribution: \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] which means we can simplify this to be the expectation w.r.t. to the data distribution: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, to be more concrete about where our variables are coming from, we can substitute the z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) into our negentropy formulation: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] So now when it comes to minimizing the loss function, we just need to take the derivative w.r.t. to the parameters \\theta \\theta . All of our terms in this equation are dependent on the parameter \\theta \\theta . \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] The derivative of an expectation of something is the same as the expectation of a derivative ( \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] ) using the dominated convergence theorem ( stackoverflow ). So we can just take the derivative w.r.t. \\theta \\theta inside of the expectation $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x)) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} (x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ Let's take it term by term. First of all, we can see that the \\log \\mathcal{P}_x(x) \\log \\mathcal{P}_x(x) has no parameters dependent upon \\theta \\theta so we can immediately cancel that term. $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\cancel{\\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x))} - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} (x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ The second term ??? third term Practically speaking, this is a bit difficult to calculate. Instead we can do a procedure that measures how much more Gaussian the approximate base distribution has become as a result of the transformation \\mathcal{G}_{\\theta}(x) \\mathcal{G}_{\\theta}(x) . --","title":"Cost Function"},{"location":"thesis/chapters/4_information/density/gaussianization/#in-the-wild","text":"Gaussianization for Fast and Accurate Inference from Cosmological Data Nice formula for how to calculate the likelihood.","title":"In the Wild"},{"location":"thesis/chapters/4_information/density/nfs/","text":"Normalizing Flows \u00b6 Density Destructor \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| Survey of Literature \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Normalizing Flows"},{"location":"thesis/chapters/4_information/density/nfs/#normalizing-flows","text":"","title":"Normalizing Flows"},{"location":"thesis/chapters/4_information/density/nfs/#density-destructor","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Density Destructor"},{"location":"thesis/chapters/4_information/density/nfs/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"thesis/chapters/4_information/density/nfs/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"thesis/chapters/4_information/density/parametric/","text":"","title":"Parametric"},{"location":"thesis/chapters/4_information/density/rbig/","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 Motivation \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Algorithm \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"thesis/chapters/4_information/density/rbig/#rotation-based-iterative-gaussianization-rbig","text":"","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"thesis/chapters/4_information/density/rbig/#motivation","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Motivation"},{"location":"thesis/chapters/4_information/density/rbig/#algorithm","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Algorithm"},{"location":"thesis/chapters/4_information/density/rbig/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"thesis/chapters/4_information/density/rbig/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"thesis/chapters/4_information/density/rbig/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"thesis/chapters/4_information/density/rbig/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"thesis/chapters/4_information/information/1_information/","text":"Information Theory \u00b6 In this report, I will be outlining what exactly Information theory is and what it means in the machine learning context. Many times when people ask me what I do, I say that I look at Information theory (IT) measures in the context of a generative network. But sometimes I have a difficult time convincing myself that these measures are actually useful. I think this is partially because I don't fully understand the magnitude of IT measures and what they can do. So this post is designed to help me (and others) really dig deep into the space of information measures and I hope this will help someone else who is also interested in understanding IT without any formal classes. This post will not be formula heavy and will instead focus on concepts. See the end of the post for some additional references where you can explore each of these ideas even further. Definition: What is Information? \u00b6 Firstly, let's start with the definition of information . This is a difficult definition and I've seen many different explanations which are outlined below. The amount of surprise, i.e. I'm surprised when something that doesn't usually happen, happens. - Jesus Malo, 2019 This is my first definition which really resonates with me till this day. I think the concept of surprise is quite easy to grasp because it is intuitive. The first example I have is when I google the answer to my question. Things that are informative Information is what remains after every iota of natural redundancy has been squeezed out of a message, and after every aimless syllable of noise has been removed. It is the unfettered essence that passes from computer to computer, from satellite to Earth, from eye to brain, and (over many generations of natural selection) from the natural world to the collective gene pool of every species. - James V Stone, 2015 This is a very good general definition of information from a broad perspective. It is very involved so it stresses how information can be thought of from many different perspectives including remote sensing, neuroscience and biology. It also tries to hint at the distinction between useful information and noise: signal (useful) and noise (useless). This definition comes from the first chapter of the book written by James Stone (2015). I highly recommend you read it as it gives the best introduction to information theory that I have ever seen. Information is the resolution of uncertainty. - Shannon C, 1948 This is the classic definition of uncertainty given by the creator himself. ... In information theory, one variable provides information about another variable when knowledge of the first, on average, reduces uncertainty in the second. - Cover & Thomas, 2006 This is technically the definition of mutual information (MI) which is the extension of entropy to 2 or more variables. However, I think this definition actually captures almost all aspects of the information theory measures Concrete Definitions \u00b6 So we have expressed how these methods work in words but I am Deterministic Perspective \u00b6 The number of data points that one needs to represent a signal without an error. - Gabor, 1946 \\text{Information} \\propto \\Delta x \\cdot \\Delta f \\text{Information} \\propto \\Delta x \\cdot \\Delta f Units: Wavelets (Log ones) Probabilistic Perspective \u00b6 The reduction in uncertainty that was caused by the knowledge of a signal. - Shannon, 1948 \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx Units: Bits In this definition, the uncertainty is related to the volume of the PDF (more volume means more uncertainty). Information theory makes statements about the shapes of probability distributions. Interpretable Quantities: Bits \u00b6 Key Idea : One bit is the amount of information required to choose between two equally probable alternatives Key Idea : If you have n n bits of information then you can choose from m=2^n m=2^n equally probable alternatives. Equivalently, if you have to choose between m m equally probable alternatives, then you need n=\\log_2 m n=\\log_2 m bits of information. Key Idea : A bit is the amount of information required to choose between two equally probable alternatives (e.g. left/right), whereas a binary digit is the value of a binary variable, which can adapt one of two possible values (i.e. 0/1). Motivation: Why use IT measures? \u00b6 I often wonder why should one use information theory measures in the first place. What makes IT a useful tool in the different science fields. I found this introduction from [1] to have the best characterization of why one should use IT measures as they outline a few reasons why one can use IT measures. Information theory is model independent This statement is very powerful but they basically mean that we do not need to make any assumptions about the structure or the interactions between the data. We input the data and we get out a quantitative measure that tells us the relationship within our data. That allows us to be flexible as we should be able to capture different types of phenomena without the need to make assumptions and assumed models that could result in limiting results. No need to make a pre-defined model. It sounds nice as a first step analysis. Warning: Often, we do need to make assumptions about our data, e.g. independence between observations, constant output, and also the bin size that is needed when capturing the probability distribution. These parameters can definitely have an impact on the final numbers that we obtain. Information theory is data independent This statement means that we can apply IT measures on any type of data. We can also do certain transformations of said data and then apply the IT measures on the resulting transformation. This allows us to monitor how the information changes through transformations which can be useful in applications ranging from biological such as the brain to machine learning such as deep neural networks. Information theory can detect linear and nonlinear interactions This speaks for itself. As we get more and more data and get better at measuring with better sensors, we can no longer describe the word with linear approximations. The relationships between variables are often non-linear and we need models that are able to capture these nonlinear interactions. IT measures can capture both and this can be useful in practice. Information theory is naturally multivariate In principle, IT measures are well-calibrated enough to handle large multivariate (and multi-dimensional) systems. The easiest number of variables to understand include 2-3 variables but the techniques can be applied to a higher number of variables. Warning: In practice, you may find that many techniques are not equipped to handle high-dimensional and high-multivariate simply because the method of PDF estimation becomes super difficult with big data. This is essential to calculate any kind of IT measure so this can end up being the bottleneck in many algorithms. Information theory produces results in general units of bits (or nats) The units from IT measures are universal. No matter what the inputs are of the units, we will get the same amount of information in the output. This allows for easy comparisons between different variables under different transformations. Warning: The units are universal and absolute but it does not mean they convey meaning or understanding. I think it's important to look at them from a relative perspective, e.g. IT measure 1 has 2 bits which is higher than IT measure 2 which has only 1 bit. Overall, we can see that IT measures can handle multi-dimension and multivariate data, are model-free and have universal units of comparison. They are dependent on accurate PDF estimates and this. They have a strong argument for preliminary analysis of variables and variable interactions as well as complex systems. [1]: Timme & Lapish, A Tutorial for Information Theory in Neuroscience , eNeuro , 2018 What can IT measures tell us? \u00b6 The authors from [1] also pose a good question for us to answer about why one should use IT measures. I agree with the authors that it is important to know what types of questions we can answer given some model or measurement. In the context of IT measures, they highlight a few things we can gain: IT measures can quantify uncertainty of one or more variables. It is possible to quantify how much a variable expected to vary as well as the expected noise that we use in the system. 2. IT measures can be used to restrict the space of possible models. I have personal experience with this as we used this to determine how many spatial-spectral-temporal features were necessary for inputs to a model. We also looked at the information content of different variables and could determine how well this did. We also can highlight things we cannot do: We cannot produce models that describe how a system functions. A good example is Mutual information. It can tell you how many bits of information is shared between data but not anything about how the system of variables are related; just that the variables are related to an extent. 2. The output units are universal so they cannot be used to produce outputs in terms of the original input variables. In other words, I cannot use the outputs of the IT measures as outputs to a model as they make little sense in the real world. Again, the authors highlight (and I would like to highlight as well): IT measures can be a good way to help build your model, e.g. it can limit the amount of variables you would like to use based on expected uncertainty or mutual information content. It is a quantified measure in absolute relative units which can help the user make decisions in what variables to include or what transformations to use. Information (Revisited) \u00b6 Now we will start to do a deep dive into IT measures and define them in a more concrete sense. I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} The intuitive definition is important because it really showcases how the heuristic works in the end. I'm actually not entirely sure if there is a mathematical way to formalate this without some sort of axiom that we mentioned before about surprise and uncertainty . We use logs because... Example Pt I: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve Entropy \u00b6 This is an upper bound on the amount of information you can convey without any loss ( source ). More entropy means more randomness or uncertainty H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx We use logs so that wee get sums of entropies. It implies independence but the log also forces sums. H(Y|X) = H(X,Y)-H(X) H(Y|X) = H(X,Y)-H(X) H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy Examples \u00b6 Example Pt II: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve Under Transformations \u00b6 In my line of work, we work with generative models that utilize the change of variable formulation in order to estimate some distribution with H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] Under rotation: Entropy is invariant Under scale: Entropy is ...??? Computational Cost...? Relative Entropy (Kullback Leibler Divergence) \u00b6 This is the measure of the distance between two distributions. I like the term relative entropy because it offers a different perspective in relation to information theory measures. D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 If you've studied machine learning then you are fully aware that it is not a distance as this measure is not symmetric i.e. D_{KL}(p||q) \\neq D_{KL}(q||p) D_{KL}(p||q) \\neq D_{KL}(q||p) . Under Transformations \u00b6 The KLD is invariance under invertible affine transformations, e.g. b = \\mu + Ga, \\nabla F = G b = \\mu + Ga, \\nabla F = G Mutual Information \u00b6 This is the reduction of uncertainty of one random variable due to the knowledge of another (like the definition above). It is the amount of information one r.v. contains about another r.v.. Total Correlation \u00b6 This isn't really talked about outside of the ML community but I think this is a useful measure to have; especially when dealing with multi-dimensional and multi-variate datesets. PDF Estimation \u00b6 For almost all of these measures to work, we need to have a really good PDF estimation of our dataset, \\mathcal{X} \\mathcal{X} . This is a hard problem and should not be taken lightly. There is an entire field of methods that can be used, e.g. autoregressive models, generative networks, and Gaussianization. One of the simplest techniques (and often fairly effective) is just histogram transformation. I work specifically with Gaussianization methods and we have found that a simple histogram transformation works really well. It also led to some properties which allow one to estimate some IT measures in unison with PDF estimation. Another way of estimating PDFs would be to look at kernel methods (Parezen Windows). A collaborator works with this methodology and has found success in utitlize kernel methods and have also been able to provide good IT measures through these techniques. References \u00b6 Supplementary Material \u00b6 GPs and IT \u00b6 References \u00b6 Gaussian Processes and Information Theory \u00b6 Information Theory \u00b6 Information Theory Tutorial: The Manifold Things Information Measures - YouTube On Measures of Entropy and Information - Understanding Interdependency Through Complex Information Sharing - Rosas et. al. (2016) The Information Bottleneck of Deep Learning - Youtube Maximum Entropy Distributions - blog Articles A New Outlook on Shannon's Information Measures - Yeung (1991) - pdf A Brief Introduction to Shannon's Information Theory - Chen (2018) - arxiv Information Theory for Intelligent People - DeDeo (2018) - pdf Blogs Visual Information Theory - Colah (2015) - blog Better Intuition for Information Theory - Kirsch (2019) - blog A Brief History of Information Theory - Vasiloudis (2019) - blog Information Theory of Deep Learning - Sharma (2019) - blog Books Information theory Software \u00b6 Python Packages \u00b6 Discrete Information Theory ( dit ) - Github | Docs Python Information Theory Measures (****) - Github | Docs Parallelized Mutual Information Measures - blog | Github Implementations \u00b6 Mutual Information Calculation with Numpy - stackoverflow Unexplored Stuff \u00b6 These are my extra notes from resources I have found.","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#information-theory","text":"In this report, I will be outlining what exactly Information theory is and what it means in the machine learning context. Many times when people ask me what I do, I say that I look at Information theory (IT) measures in the context of a generative network. But sometimes I have a difficult time convincing myself that these measures are actually useful. I think this is partially because I don't fully understand the magnitude of IT measures and what they can do. So this post is designed to help me (and others) really dig deep into the space of information measures and I hope this will help someone else who is also interested in understanding IT without any formal classes. This post will not be formula heavy and will instead focus on concepts. See the end of the post for some additional references where you can explore each of these ideas even further.","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#definition-what-is-information","text":"Firstly, let's start with the definition of information . This is a difficult definition and I've seen many different explanations which are outlined below. The amount of surprise, i.e. I'm surprised when something that doesn't usually happen, happens. - Jesus Malo, 2019 This is my first definition which really resonates with me till this day. I think the concept of surprise is quite easy to grasp because it is intuitive. The first example I have is when I google the answer to my question. Things that are informative Information is what remains after every iota of natural redundancy has been squeezed out of a message, and after every aimless syllable of noise has been removed. It is the unfettered essence that passes from computer to computer, from satellite to Earth, from eye to brain, and (over many generations of natural selection) from the natural world to the collective gene pool of every species. - James V Stone, 2015 This is a very good general definition of information from a broad perspective. It is very involved so it stresses how information can be thought of from many different perspectives including remote sensing, neuroscience and biology. It also tries to hint at the distinction between useful information and noise: signal (useful) and noise (useless). This definition comes from the first chapter of the book written by James Stone (2015). I highly recommend you read it as it gives the best introduction to information theory that I have ever seen. Information is the resolution of uncertainty. - Shannon C, 1948 This is the classic definition of uncertainty given by the creator himself. ... In information theory, one variable provides information about another variable when knowledge of the first, on average, reduces uncertainty in the second. - Cover & Thomas, 2006 This is technically the definition of mutual information (MI) which is the extension of entropy to 2 or more variables. However, I think this definition actually captures almost all aspects of the information theory measures","title":"Definition: What is Information?"},{"location":"thesis/chapters/4_information/information/1_information/#concrete-definitions","text":"So we have expressed how these methods work in words but I am","title":"Concrete Definitions"},{"location":"thesis/chapters/4_information/information/1_information/#deterministic-perspective","text":"The number of data points that one needs to represent a signal without an error. - Gabor, 1946 \\text{Information} \\propto \\Delta x \\cdot \\Delta f \\text{Information} \\propto \\Delta x \\cdot \\Delta f Units: Wavelets (Log ones)","title":"Deterministic Perspective"},{"location":"thesis/chapters/4_information/information/1_information/#probabilistic-perspective","text":"The reduction in uncertainty that was caused by the knowledge of a signal. - Shannon, 1948 \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx Units: Bits In this definition, the uncertainty is related to the volume of the PDF (more volume means more uncertainty). Information theory makes statements about the shapes of probability distributions.","title":"Probabilistic Perspective"},{"location":"thesis/chapters/4_information/information/1_information/#interpretable-quantities-bits","text":"Key Idea : One bit is the amount of information required to choose between two equally probable alternatives Key Idea : If you have n n bits of information then you can choose from m=2^n m=2^n equally probable alternatives. Equivalently, if you have to choose between m m equally probable alternatives, then you need n=\\log_2 m n=\\log_2 m bits of information. Key Idea : A bit is the amount of information required to choose between two equally probable alternatives (e.g. left/right), whereas a binary digit is the value of a binary variable, which can adapt one of two possible values (i.e. 0/1).","title":"Interpretable Quantities: Bits"},{"location":"thesis/chapters/4_information/information/1_information/#motivation-why-use-it-measures","text":"I often wonder why should one use information theory measures in the first place. What makes IT a useful tool in the different science fields. I found this introduction from [1] to have the best characterization of why one should use IT measures as they outline a few reasons why one can use IT measures. Information theory is model independent This statement is very powerful but they basically mean that we do not need to make any assumptions about the structure or the interactions between the data. We input the data and we get out a quantitative measure that tells us the relationship within our data. That allows us to be flexible as we should be able to capture different types of phenomena without the need to make assumptions and assumed models that could result in limiting results. No need to make a pre-defined model. It sounds nice as a first step analysis. Warning: Often, we do need to make assumptions about our data, e.g. independence between observations, constant output, and also the bin size that is needed when capturing the probability distribution. These parameters can definitely have an impact on the final numbers that we obtain. Information theory is data independent This statement means that we can apply IT measures on any type of data. We can also do certain transformations of said data and then apply the IT measures on the resulting transformation. This allows us to monitor how the information changes through transformations which can be useful in applications ranging from biological such as the brain to machine learning such as deep neural networks. Information theory can detect linear and nonlinear interactions This speaks for itself. As we get more and more data and get better at measuring with better sensors, we can no longer describe the word with linear approximations. The relationships between variables are often non-linear and we need models that are able to capture these nonlinear interactions. IT measures can capture both and this can be useful in practice. Information theory is naturally multivariate In principle, IT measures are well-calibrated enough to handle large multivariate (and multi-dimensional) systems. The easiest number of variables to understand include 2-3 variables but the techniques can be applied to a higher number of variables. Warning: In practice, you may find that many techniques are not equipped to handle high-dimensional and high-multivariate simply because the method of PDF estimation becomes super difficult with big data. This is essential to calculate any kind of IT measure so this can end up being the bottleneck in many algorithms. Information theory produces results in general units of bits (or nats) The units from IT measures are universal. No matter what the inputs are of the units, we will get the same amount of information in the output. This allows for easy comparisons between different variables under different transformations. Warning: The units are universal and absolute but it does not mean they convey meaning or understanding. I think it's important to look at them from a relative perspective, e.g. IT measure 1 has 2 bits which is higher than IT measure 2 which has only 1 bit. Overall, we can see that IT measures can handle multi-dimension and multivariate data, are model-free and have universal units of comparison. They are dependent on accurate PDF estimates and this. They have a strong argument for preliminary analysis of variables and variable interactions as well as complex systems. [1]: Timme & Lapish, A Tutorial for Information Theory in Neuroscience , eNeuro , 2018","title":"Motivation: Why use IT measures?"},{"location":"thesis/chapters/4_information/information/1_information/#what-can-it-measures-tell-us","text":"The authors from [1] also pose a good question for us to answer about why one should use IT measures. I agree with the authors that it is important to know what types of questions we can answer given some model or measurement. In the context of IT measures, they highlight a few things we can gain: IT measures can quantify uncertainty of one or more variables. It is possible to quantify how much a variable expected to vary as well as the expected noise that we use in the system. 2. IT measures can be used to restrict the space of possible models. I have personal experience with this as we used this to determine how many spatial-spectral-temporal features were necessary for inputs to a model. We also looked at the information content of different variables and could determine how well this did. We also can highlight things we cannot do: We cannot produce models that describe how a system functions. A good example is Mutual information. It can tell you how many bits of information is shared between data but not anything about how the system of variables are related; just that the variables are related to an extent. 2. The output units are universal so they cannot be used to produce outputs in terms of the original input variables. In other words, I cannot use the outputs of the IT measures as outputs to a model as they make little sense in the real world. Again, the authors highlight (and I would like to highlight as well): IT measures can be a good way to help build your model, e.g. it can limit the amount of variables you would like to use based on expected uncertainty or mutual information content. It is a quantified measure in absolute relative units which can help the user make decisions in what variables to include or what transformations to use.","title":"What can IT measures tell us?"},{"location":"thesis/chapters/4_information/information/1_information/#information-revisited","text":"Now we will start to do a deep dive into IT measures and define them in a more concrete sense. I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} The intuitive definition is important because it really showcases how the heuristic works in the end. I'm actually not entirely sure if there is a mathematical way to formalate this without some sort of axiom that we mentioned before about surprise and uncertainty . We use logs because... Example Pt I: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve","title":"Information (Revisited)"},{"location":"thesis/chapters/4_information/information/1_information/#entropy","text":"This is an upper bound on the amount of information you can convey without any loss ( source ). More entropy means more randomness or uncertainty H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx We use logs so that wee get sums of entropies. It implies independence but the log also forces sums. H(Y|X) = H(X,Y)-H(X) H(Y|X) = H(X,Y)-H(X) H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy","title":"Entropy"},{"location":"thesis/chapters/4_information/information/1_information/#examples","text":"Example Pt II: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve","title":"Examples"},{"location":"thesis/chapters/4_information/information/1_information/#under-transformations","text":"In my line of work, we work with generative models that utilize the change of variable formulation in order to estimate some distribution with H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] Under rotation: Entropy is invariant Under scale: Entropy is ...??? Computational Cost...?","title":"Under Transformations"},{"location":"thesis/chapters/4_information/information/1_information/#relative-entropy-kullback-leibler-divergence","text":"This is the measure of the distance between two distributions. I like the term relative entropy because it offers a different perspective in relation to information theory measures. D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 If you've studied machine learning then you are fully aware that it is not a distance as this measure is not symmetric i.e. D_{KL}(p||q) \\neq D_{KL}(q||p) D_{KL}(p||q) \\neq D_{KL}(q||p) .","title":"Relative Entropy (Kullback Leibler Divergence)"},{"location":"thesis/chapters/4_information/information/1_information/#under-transformations_1","text":"The KLD is invariance under invertible affine transformations, e.g. b = \\mu + Ga, \\nabla F = G b = \\mu + Ga, \\nabla F = G","title":"Under Transformations"},{"location":"thesis/chapters/4_information/information/1_information/#mutual-information","text":"This is the reduction of uncertainty of one random variable due to the knowledge of another (like the definition above). It is the amount of information one r.v. contains about another r.v..","title":"Mutual Information"},{"location":"thesis/chapters/4_information/information/1_information/#total-correlation","text":"This isn't really talked about outside of the ML community but I think this is a useful measure to have; especially when dealing with multi-dimensional and multi-variate datesets.","title":"Total Correlation"},{"location":"thesis/chapters/4_information/information/1_information/#pdf-estimation","text":"For almost all of these measures to work, we need to have a really good PDF estimation of our dataset, \\mathcal{X} \\mathcal{X} . This is a hard problem and should not be taken lightly. There is an entire field of methods that can be used, e.g. autoregressive models, generative networks, and Gaussianization. One of the simplest techniques (and often fairly effective) is just histogram transformation. I work specifically with Gaussianization methods and we have found that a simple histogram transformation works really well. It also led to some properties which allow one to estimate some IT measures in unison with PDF estimation. Another way of estimating PDFs would be to look at kernel methods (Parezen Windows). A collaborator works with this methodology and has found success in utitlize kernel methods and have also been able to provide good IT measures through these techniques.","title":"PDF Estimation"},{"location":"thesis/chapters/4_information/information/1_information/#references","text":"","title":"References"},{"location":"thesis/chapters/4_information/information/1_information/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"thesis/chapters/4_information/information/1_information/#gps-and-it","text":"","title":"GPs and IT"},{"location":"thesis/chapters/4_information/information/1_information/#references_1","text":"","title":"References"},{"location":"thesis/chapters/4_information/information/1_information/#gaussian-processes-and-information-theory","text":"","title":"Gaussian Processes and Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#information-theory_1","text":"Information Theory Tutorial: The Manifold Things Information Measures - YouTube On Measures of Entropy and Information - Understanding Interdependency Through Complex Information Sharing - Rosas et. al. (2016) The Information Bottleneck of Deep Learning - Youtube Maximum Entropy Distributions - blog Articles A New Outlook on Shannon's Information Measures - Yeung (1991) - pdf A Brief Introduction to Shannon's Information Theory - Chen (2018) - arxiv Information Theory for Intelligent People - DeDeo (2018) - pdf Blogs Visual Information Theory - Colah (2015) - blog Better Intuition for Information Theory - Kirsch (2019) - blog A Brief History of Information Theory - Vasiloudis (2019) - blog Information Theory of Deep Learning - Sharma (2019) - blog Books Information theory","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#software","text":"","title":"Software"},{"location":"thesis/chapters/4_information/information/1_information/#python-packages","text":"Discrete Information Theory ( dit ) - Github | Docs Python Information Theory Measures (****) - Github | Docs Parallelized Mutual Information Measures - blog | Github","title":"Python Packages"},{"location":"thesis/chapters/4_information/information/1_information/#implementations","text":"Mutual Information Calculation with Numpy - stackoverflow","title":"Implementations"},{"location":"thesis/chapters/4_information/information/1_information/#unexplored-stuff","text":"These are my extra notes from resources I have found.","title":"Unexplored Stuff"},{"location":"thesis/chapters/4_information/information/2_rbig/","text":"TODO \u00b6","title":"TODO"},{"location":"thesis/chapters/4_information/information/2_rbig/#todo","text":"","title":"TODO"},{"location":"thesis/chapters/5_applications/sensitivity/","text":"Sensitivity Applications \u00b6 Emulators (GPGSA) \u00b6 SAKAME \u00b6 MultiOuput Ocean \u00b6 MOGPs","title":"Sensitivity Applications"},{"location":"thesis/chapters/5_applications/sensitivity/#sensitivity-applications","text":"","title":"Sensitivity Applications"},{"location":"thesis/chapters/5_applications/sensitivity/#emulators-gpgsa","text":"","title":"Emulators (GPGSA)"},{"location":"thesis/chapters/5_applications/sensitivity/#sakame","text":"","title":"SAKAME"},{"location":"thesis/chapters/5_applications/sensitivity/#multiouput-ocean","text":"MOGPs","title":"MultiOuput Ocean"},{"location":"thesis/chapters/5_applications/similarity/","text":"Similarity Applications \u00b6 Spatial-Temporal (RBIG4EO) \u00b6 Drought Factors (RBIG4EO) \u00b6 Climate Models (RBIG4EO) \u00b6","title":"Similarity Applications"},{"location":"thesis/chapters/5_applications/similarity/#similarity-applications","text":"","title":"Similarity Applications"},{"location":"thesis/chapters/5_applications/similarity/#spatial-temporal-rbig4eo","text":"","title":"Spatial-Temporal (RBIG4EO)"},{"location":"thesis/chapters/5_applications/similarity/#drought-factors-rbig4eo","text":"","title":"Drought Factors (RBIG4EO)"},{"location":"thesis/chapters/5_applications/similarity/#climate-models-rbig4eo","text":"","title":"Climate Models (RBIG4EO)"},{"location":"thesis/chapters/5_applications/uncertainty/","text":"Uncertainty Applications \u00b6 Input Uncertainty (EGP, EGP2.0) \u00b6 EGP 1.0 \u00b6 EGP 2.0 \u00b6 Modeling Uncertainty (BNN4...) \u00b6","title":"Uncertainty Applications"},{"location":"thesis/chapters/5_applications/uncertainty/#uncertainty-applications","text":"","title":"Uncertainty Applications"},{"location":"thesis/chapters/5_applications/uncertainty/#input-uncertainty-egp-egp20","text":"","title":"Input Uncertainty (EGP, EGP2.0)"},{"location":"thesis/chapters/5_applications/uncertainty/#egp-10","text":"","title":"EGP 1.0"},{"location":"thesis/chapters/5_applications/uncertainty/#egp-20","text":"","title":"EGP 2.0"},{"location":"thesis/chapters/5_applications/uncertainty/#modeling-uncertainty-bnn4","text":"","title":"Modeling Uncertainty (BNN4...)"},{"location":"tutorials/","text":"Tutorials \u00b6 NNs in PyTorch \u00b6 Abstract Some basic tutorials about how one can code Neural networks from scratch. It will feature from scratch as well as how to refactor your code. The ultimate goal is to build Bayesian NN and a Deep GP from scratch. TBD Part I - From Scratch Part II - Refactored eGP2.0 I - Variational GP Jax n EGPs \u00b6 Abstract How to use Jax to do derivatives of kernel methods as well as how to use it for GPs with Uncertain Inputs. vmap jit GP From Scratch eGP I - Taylor Expansion eGP II - MCMC Old School Manifold Learning \u00b6 Abstract Some notes about how we can do manifold learning Laplacian Eigenmaps Semi-Supervised Manifold Alignment","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/#nns-in-pytorch","text":"Abstract Some basic tutorials about how one can code Neural networks from scratch. It will feature from scratch as well as how to refactor your code. The ultimate goal is to build Bayesian NN and a Deep GP from scratch. TBD Part I - From Scratch Part II - Refactored eGP2.0 I - Variational GP","title":"NNs in PyTorch"},{"location":"tutorials/#jax-n-egps","text":"Abstract How to use Jax to do derivatives of kernel methods as well as how to use it for GPs with Uncertain Inputs. vmap jit GP From Scratch eGP I - Taylor Expansion eGP II - MCMC","title":"Jax n EGPs"},{"location":"tutorials/#old-school-manifold-learning","text":"Abstract Some notes about how we can do manifold learning Laplacian Eigenmaps Semi-Supervised Manifold Alignment","title":"Old School Manifold Learning"},{"location":"tutorials/earth_sci/","text":"Earth Science Tools \u00b6 These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing. Key Helpful Functions \u00b6 Density Cubes We can extract 'density cubes' which transform point estimates to estimates with spatial and temporal features. It is like a sliding window except in lat-lon-time space. Very useful when attempted to get do density estimation. A demo notebook can be found here . Regrid Function Utilizing the xESMF package, we can regrid datasets based on a reference dataset. Very useful when we don't have datasets with the same spatial dimensions. ShapeFile Masks This has various functions to allows one to use shapefiles to mask datasets. I have only a few sample functions that parse shape files of interest like countries or US states. But it is flexible enough to use other interesting attributes like population. We only need the raster functions. A demo notebook can be found here . Useful Resources \u00b6 Manipulating ShapeFiles \u00b6 RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase. Visualization \u00b6 xarray > The xarray docs have a few examples for how one can plot. geopandas > This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy > A package that handles all of the projections needed for better visualizations of the globe. Works well with geopandas and xarray. folium > This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot > A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews > This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . Maps in Scientific Python > A great tutorial by Rabernat","title":"Earth Science Tools"},{"location":"tutorials/earth_sci/#earth-science-tools","text":"These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing.","title":"Earth Science Tools"},{"location":"tutorials/earth_sci/#key-helpful-functions","text":"Density Cubes We can extract 'density cubes' which transform point estimates to estimates with spatial and temporal features. It is like a sliding window except in lat-lon-time space. Very useful when attempted to get do density estimation. A demo notebook can be found here . Regrid Function Utilizing the xESMF package, we can regrid datasets based on a reference dataset. Very useful when we don't have datasets with the same spatial dimensions. ShapeFile Masks This has various functions to allows one to use shapefiles to mask datasets. I have only a few sample functions that parse shape files of interest like countries or US states. But it is flexible enough to use other interesting attributes like population. We only need the raster functions. A demo notebook can be found here .","title":"Key Helpful Functions"},{"location":"tutorials/earth_sci/#useful-resources","text":"","title":"Useful Resources"},{"location":"tutorials/earth_sci/#manipulating-shapefiles","text":"RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase.","title":"Manipulating ShapeFiles"},{"location":"tutorials/earth_sci/#visualization","text":"xarray > The xarray docs have a few examples for how one can plot. geopandas > This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy > A package that handles all of the projections needed for better visualizations of the globe. Works well with geopandas and xarray. folium > This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot > A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews > This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . Maps in Scientific Python > A great tutorial by Rabernat","title":"Visualization"},{"location":"tutorials/earth_sci/_ideas/","text":"Ideas \u00b6 Dask Tutorial - gufunc \u00b6 In this tutorial, I would like to see how we can use dask to do fast calculations on the Earth Science data cubes. This will be based on the following tutorials: Calculating Pearson and Spearman - XArray Docs | Dask Docs Generalized ufuncs - Dask Docs Apply ufunc with Xarray - Docs Algorithms Pearson Correlation Coefficient Spearman Correlation Coefficient \\rho \\rho -V Coefficient Centered Kernel Alignment Smoothing Scipy | Scipy - From scratch | StackOverFlow Shape Files and Rasters \u00b6 Corteva GeoCube Corteva RioXarray - Clip | Clip Box | Demo Xarray to CSV Loading Data \u00b6 CMIP6 \u00b6 I've used this data for my experiments, so I would like to make a quick tutorial about how we can load this, query the parts you need, and do operations. CMIPData Docs Demo with Anomalies - Normal | Parallel CMIP6 Preprocessing Pangeo Examples Pangeo Datastore ERA5 \u00b6 This is some reanalysis data that I've used in my experiments. I would like to do the same as the above part. Pangeo Datastore Pangeo Examples ML Flow Tutorials Earth Science Data Cubes \u00b6 - Done!","title":"Ideas"},{"location":"tutorials/earth_sci/_ideas/#ideas","text":"","title":"Ideas"},{"location":"tutorials/earth_sci/_ideas/#dask-tutorial-gufunc","text":"In this tutorial, I would like to see how we can use dask to do fast calculations on the Earth Science data cubes. This will be based on the following tutorials: Calculating Pearson and Spearman - XArray Docs | Dask Docs Generalized ufuncs - Dask Docs Apply ufunc with Xarray - Docs Algorithms Pearson Correlation Coefficient Spearman Correlation Coefficient \\rho \\rho -V Coefficient Centered Kernel Alignment Smoothing Scipy | Scipy - From scratch | StackOverFlow","title":"Dask Tutorial - gufunc"},{"location":"tutorials/earth_sci/_ideas/#shape-files-and-rasters","text":"Corteva GeoCube Corteva RioXarray - Clip | Clip Box | Demo Xarray to CSV","title":"Shape Files and Rasters"},{"location":"tutorials/earth_sci/_ideas/#loading-data","text":"","title":"Loading Data"},{"location":"tutorials/earth_sci/_ideas/#cmip6","text":"I've used this data for my experiments, so I would like to make a quick tutorial about how we can load this, query the parts you need, and do operations. CMIPData Docs Demo with Anomalies - Normal | Parallel CMIP6 Preprocessing Pangeo Examples Pangeo Datastore","title":"CMIP6"},{"location":"tutorials/earth_sci/_ideas/#era5","text":"This is some reanalysis data that I've used in my experiments. I would like to do the same as the above part. Pangeo Datastore Pangeo Examples ML Flow Tutorials","title":"ERA5"},{"location":"tutorials/earth_sci/_ideas/#earth-science-data-cubes","text":"- Done!","title":"Earth Science Data Cubes"},{"location":"tutorials/earth_sci/esdc_streaming/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Earth Science Data Cubes \u00b6 Author: J. Emmanuel Johnson Source: Notebook I | Notebook II The kind folks behind the Earth System Data Lab (Data Cube System for Copernicus, DCS4SOP ) have been developing a package called xcube which will allow processing ESDC a lot easier. I tried using it a while back but I didn't have any success so I kind of gave up on it and decided to use some of my own functions. But they recently announced on their forum that the package was ready. So I decided to test it out and see how it works. My Background \u00b6 I've been working with the datacubes for quite some time and I've learned a lot thanks to their efforts. I was even a part of their Early Adopters program which allowed me to use their homegrown JupyterLab system (sign up for access here ). I learned a lot about xarray data structures in general, and it really motivated me to try and incorporate different spatial-temporal data representation considerations in my research and thesis. What was super nice was that they had all of the data readily available for use and all I had to do was tinker with my algorithms on their server. But I still wanted some stuff to be on my home machine, my lab machine and possibly on google colab. Nothing heavy but just some light protoyping. So this is nice for those of us who don't necessarily want to use their system but would still like to play with the data. They went the extra mile and even made this package called xcube . Now, for my purposes, I like it because it handles a lot of masking and clipping. But that's just the tip of the iceberg... #@title Install Appropriate packages # requirements !pip install xarray zarr shapely affine rasterio geopandas # Cartopy !apt-get install libproj-dev proj-data proj-bin !apt-get install libgeos-dev !pip install --upgrade -e \"git+https://github.com/SciTools/Cartopy.git#egg=cartopy\" # xcube package from source !pip install --upgrade \"git+https://github.com/dcs4cop/xcube.git#egg=xcube\" --pre Requirement already satisfied: xarray in /usr/local/lib/python3.6/dist-packages (0.15.1) Collecting zarr Downloading https://files.pythonhosted.org/packages/a3/87/383d77399148ef0772da3472b513ecf143252e7c365c51b0f06714800366/zarr-2.4.0.tar.gz (3.3MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3MB 4.9MB/s Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (1.7.0) Collecting affine Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl Collecting rasterio Downloading https://files.pythonhosted.org/packages/c7/81/13321f88f582a00705c5f348724728e8999136e19d6e7c56f7e6ac9bb7f9/rasterio-1.1.3-cp36-cp36m-manylinux1_x86_64.whl (18.1MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18.1MB 221kB/s Collecting geopandas Downloading https://files.pythonhosted.org/packages/83/c5/3cf9cdc39a6f2552922f79915f36b45a95b71fd343cfc51170a5b6ddb6e8/geopandas-0.7.0-py2.py3-none-any.whl (928kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 931kB 45.9MB/s Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.6/dist-packages (from xarray) (1.0.3) Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from xarray) (1.18.2) Requirement already satisfied: setuptools>=41.2 in /usr/local/lib/python3.6/dist-packages (from xarray) (46.1.3) Collecting asciitree Downloading https://files.pythonhosted.org/packages/2d/6a/885bc91484e1aa8f618f6f0228d76d0e67000b0fdd6090673b777e311913/asciitree-0.3.3.tar.gz Collecting fasteners Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl Collecting numcodecs>=0.6.4 Downloading https://files.pythonhosted.org/packages/53/2a/1dc435cbd1d082827190a3e46168fd04f74e266e91313969d5a1aab601bf/numcodecs-0.6.4.tar.gz (3.8MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8MB 35.6MB/s Collecting snuggs>=1.4.1 Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl Collecting click-plugins Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0) Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.1) Collecting cligj>=0.5 Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl Collecting pyproj>=2.2.0 Downloading https://files.pythonhosted.org/packages/ce/37/705ee471f71130d4ceee41bbcb06f3b52175cb89273cbb5755ed5e6374e0/pyproj-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (10.4MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4MB 35.9MB/s Collecting fiona Downloading https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.7MB 330kB/s Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->xarray) (2018.9) Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->xarray) (2.8.1) Collecting monotonic>=0.1 Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners->zarr) (1.12.0) Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7) Collecting munch Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl Building wheels for collected packages: zarr, asciitree, numcodecs Building wheel for zarr (setup.py) ... done Created wheel for zarr: filename=zarr-2.4.0-cp36-none-any.whl size=127066 sha256=15a01a752735178395c62259774a61067bb995b595672010b4e6ce9c1b6b5289 Stored in directory: /root/.cache/pip/wheels/e1/5b/25/24c685604b91139aba00a5b6299b53e7a0661f737f27782559 Building wheel for asciitree (setup.py) ... done Created wheel for asciitree: filename=asciitree-0.3.3-cp36-none-any.whl size=5037 sha256=7edc59bb90e23a65af89b80c00531734a45399622f80e90d6308bfc410a2ef7b Stored in directory: /root/.cache/pip/wheels/1d/d9/58/9808b306744df0208fccc640d3d9952a5bc7468502d42897d5 Building wheel for numcodecs (setup.py) ... done Created wheel for numcodecs: filename=numcodecs-0.6.4-cp36-cp36m-linux_x86_64.whl size=3887834 sha256=29d82c507f5118052062d8eabc2f5d73ac157160933bd3d802a9ce320824fdcc Stored in directory: /root/.cache/pip/wheels/ca/07/ed/fea2e120cbb91d90b577c5ac56b4b082024f56fcd88e9afa55 Successfully built zarr asciitree numcodecs Installing collected packages: asciitree, monotonic, fasteners, numcodecs, zarr, affine, snuggs, click-plugins, cligj, rasterio, pyproj, munch, fiona, geopandas Successfully installed affine-2.3.0 asciitree-0.3.3 click-plugins-1.1.1 cligj-0.5.0 fasteners-0.15 fiona-1.8.13.post1 geopandas-0.7.0 monotonic-1.5 munch-2.5.0 numcodecs-0.6.4 pyproj-2.6.0 rasterio-1.1.3 snuggs-1.4.7 zarr-2.4.0 Reading package lists... Done Building dependency tree Reading state information... Done proj-data is already the newest version (4.9.3-2). proj-data set to manually installed. The following NEW packages will be installed: libproj-dev proj-bin 0 upgraded, 2 newly installed, 0 to remove and 25 not upgraded. Need to get 232 kB of archives. After this operation, 1,220 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libproj-dev amd64 4.9.3-2 [199 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 proj-bin amd64 4.9.3-2 [32.3 kB] Fetched 232 kB in 1s (221 kB/s) Selecting previously unselected package libproj-dev:amd64. (Reading database ... 144568 files and directories currently installed.) Preparing to unpack .../libproj-dev_4.9.3-2_amd64.deb ... Unpacking libproj-dev:amd64 (4.9.3-2) ... Selecting previously unselected package proj-bin. Preparing to unpack .../proj-bin_4.9.3-2_amd64.deb ... Unpacking proj-bin (4.9.3-2) ... Setting up libproj-dev:amd64 (4.9.3-2) ... Setting up proj-bin (4.9.3-2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: libgdal-doc The following NEW packages will be installed: libgeos-dev 0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded. Need to get 73.1 kB of archives. After this operation, 486 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgeos-dev amd64 3.6.2-1build2 [73.1 kB] Fetched 73.1 kB in 1s (103 kB/s) Selecting previously unselected package libgeos-dev. (Reading database ... 144601 files and directories currently installed.) Preparing to unpack .../libgeos-dev_3.6.2-1build2_amd64.deb ... Unpacking libgeos-dev (3.6.2-1build2) ... Setting up libgeos-dev (3.6.2-1build2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Obtaining cartopy from git+https://github.com/SciTools/Cartopy.git#egg=cartopy Cloning https://github.com/SciTools/Cartopy.git to ./src/cartopy Running command git clone -q https://github.com/SciTools/Cartopy.git /content/src/cartopy Requirement already satisfied, skipping upgrade: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.18.2) Requirement already satisfied, skipping upgrade: shapely>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.7.0) Collecting pyshp>=1.1.4 Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 225kB 4.6MB/s Requirement already satisfied, skipping upgrade: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.12.0) Requirement already satisfied, skipping upgrade: setuptools>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cartopy) (46.1.3) Building wheels for collected packages: pyshp Building wheel for pyshp (setup.py) ... done Created wheel for pyshp: filename=pyshp-2.1.0-cp36-none-any.whl size=32609 sha256=efc7f234f85d0ce31808aba7dd43198b43e72e98160f95ca5baf460b0995375d Stored in directory: /root/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a Successfully built pyshp Installing collected packages: pyshp, cartopy Running setup.py develop for cartopy Successfully installed cartopy pyshp-2.1.0 Collecting xcube Cloning https://github.com/dcs4cop/xcube.git to /tmp/pip-install-a7xvjrrb/xcube Running command git clone -q https://github.com/dcs4cop/xcube.git /tmp/pip-install-a7xvjrrb/xcube Building wheels for collected packages: xcube Building wheel for xcube (setup.py) ... done Created wheel for xcube: filename=xcube-0.5.0.dev0-cp36-none-any.whl size=254711 sha256=af0624d45739778a65f51561a4c17150748e96a81cfd600c60f90c7ca6653336 Stored in directory: /tmp/pip-ephem-wheel-cache-k0atts5z/wheels/72/9a/d0/f71cb2f8bc29e8ca6db15d80daaf4330b814aba9b2aaadd6b1 Successfully built xcube Installing collected packages: xcube Successfully installed xcube-0.5.0.dev0 #@title Core Packages import shapely import geopandas as gpd from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor # core xcube packages from xcube.core.dsio import open_cube from xcube.core.geom import ( clip_dataset_by_geometry, mask_dataset_by_geometry, clip_dataset_by_geometry, rasterize_features ) # plotting packages import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs # load cube from bit bucket cube_from_s3_bucket = open_cube(\"https://obs.eu-de.otc.t-systems.com/obs-esdc-v2.0.0/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\") cube_from_s3_bucket 1. Visualizing the Datacube \u00b6 There have been some fantastic upgrades to the xarray package. For example, visualizing the datacube is great and has an html format which drop-down boxes in a recent [release]((http://xarray.pydata.org/en/stable/whats-new.html#new-features). #@title Visualize a Variable cube_from_s3_bucket[['land_surface_temperature', 'root']] Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.DataArray 'land_surface_temperature' time : 1702 lat : 720 lon : 1440 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Coordinates: (3) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') Attributes: (11) ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 units : K url : http://data.globtemperature.info/ 2. Visualizing Maps \u00b6 We can use the built-in function with xarray. There are more advanced ways but that's for another time. fig, ax = plt.subplots(figsize=(10,7)) gpp_data = cube_from_s3_bucket.gross_primary_productivity.sel(time=slice('June-2010', 'June-2010')) gpp_data.mean(dim='time', skipna=True).plot.pcolormesh( ax=ax, cmap='viridis', robust=True, ) ax.set_title('Gross Primary Productivity') # ax.coastlines() plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) 2. Making Subsets \u00b6 Often times we never really want to work with the entire globe: we want to take a subset. I have been working with the western part of EuroAsia and a bit with the north of Africa. One way we can do it is naively by simply slicing the dataframe with the locations we want. In this example, I'll use the following coordinates: Latitude: 35N-71.5N Longitude: 18E - 60W #@title Slicing # subset using xarray slicing europe = gpp_data.sel(lat=slice(71.5, 35.5), lon=slice(-18.0, 60.0)) fig, ax = plt.subplots(figsize=(10,7)) europe.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) We can also use the bounding box provided by the xcube package which is much cleaner and more flexible. x1 = -18.0 # lon1, degree x2 = 60.0 # lon2, degree y1 = 35.5 # lat1, degree y2 = 71.5 # lat2, degree coords = x1, y1, x2, y2 #@title xcube # convert into shapely bounding box bbox = shapely.geometry.box(*coords) # subset the cube with the bounding box europe = xcube.core.geom.clip_dataset_by_geometry(gpp_data, bbox) fig, ax = plt.subplots(figsize=(10,7)) europe.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) 3. Shape Files and Masks \u00b6 That was my custom area. But what about if someone gives you a specific shape file that they want you to subset. Or perhaps you would like to subset an entire region, e.g. North America or Spain. Dealing with shape files is quite easy but having to convert them into rasters isn't exactly trivial. We'll use geopandas to deal with the shape file. # get shape file location in package shp_file_loc = gpd.datasets.get_path('naturalearth_lowres') # read shape file shp_gdf = gpd.read_file(shp_file_loc) shp_gdf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop_est continent name iso_a3 gdp_md_est geometry 0 920938 Oceania Fiji FJI 8374.0 MULTIPOLYGON (((180.00000 -16.06713, 180.00000... 1 53950935 Africa Tanzania TZA 150600.0 POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... 2 603253 Africa W. Sahara ESH 906.5 POLYGON ((-8.66559 27.65643, -8.66512 27.58948... 3 35623680 North America Canada CAN 1674000.0 MULTIPOLYGON (((-122.84000 49.00000, -122.9742... 4 326625791 North America United States of America USA 18560000.0 MULTIPOLYGON (((-122.84000 49.00000, -120.0000... So we have a shape file with columns as queries. The queries are basically all of the columns except for the geometry as that holds the shape file. The post important features are the continent and the geometry. We'll need these. For this, Let's extract North America. query = 'North America' column = 'continent' # subset shape files within NA na_shp_clf = shp_gdf[shp_gdf[column] == query] # let's collapse all shape files into one big shapefile na_shp_clf = na_shp_clf.dissolve(by='continent').head() na_shp_clf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry pop_est name iso_a3 gdp_md_est continent North America MULTIPOLYGON (((-61.68000 10.76000, -61.10500 ... 35623680 Canada CAN 1674000.0 na_shp_clf.plot() <matplotlib.axes._subplots.AxesSubplot at 0x7f07c17606a0> Now we can use the convenience function mask_dataset_by_geometry so that we can set all values not in NA to NANs. masked_gpp = mask_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), na_shp_clf.geometry.values[0] ) masked_gpp Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 307 lon : 640 time : 4 Coordinates: (3) lat (lat) float32 83.875 83.625 ... 7.625 7.375 array([83.875, 83.625, 83.375, ..., 7.875, 7.625, 7.375], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -171.875 -171.625 ... -12.125 array([-171.875, -171.625, -171.375, ..., -12.625, -12.375, -12.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 307, 640), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 3.14 MB 785.92 kB Shape (4, 307, 640) (1, 307, 640) Count 1716 Tasks 4 Chunks Type float32 numpy.ndarray 640 307 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:43:14.640733 geospatial_lat_max : 84.0 geospatial_lat_min : 7.25 geospatial_lon_max : -12.0 geospatial_lon_min : -172.0 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) We can do the same thing with Spain. query = 'Spain' column = 'name' spain_shp_df = shp_gdf[shp_gdf[column] == query] # let's collapse all shape files into one big shapefile spain_shp_df = spain_shp_df.dissolve(by='continent').head() # plot spain_shp_df.plot(); spain_shp_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry pop_est name iso_a3 gdp_md_est continent Europe POLYGON ((-7.45373 37.09779, -7.53711 37.42890... 48958159 Spain ESP 1690000.0 Let's see the difference between mask and clip. #@title Masking masked_gpp = mask_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), spain_shp_df.geometry.values[0]) fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show(); masked_gpp /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 32 lon : 51 time : 4 Coordinates: (3) lat (lat) float32 43.875 43.625 ... 36.375 36.125 array([43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -9.375 -9.125 ... 2.875 3.125 array([-9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875, -7.625, -7.375, -7.125, -6.875, -6.625, -6.375, -6.125, -5.875, -5.625, -5.375, -5.125, -4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875, -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625, -0.375, -0.125, 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 32, 51), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 26.11 kB 6.53 kB Shape (4, 32, 51) (1, 32, 51) Count 1716 Tasks 4 Chunks Type float32 numpy.ndarray 51 32 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:48:52.627587 geospatial_lat_max : 44.0 geospatial_lat_min : 36.0 geospatial_lon_max : 3.25 geospatial_lon_min : -9.5 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 #@title Clipping masked_gpp = clip_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), spain_shp_df.geometry.values[0]) fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show(); masked_gpp /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 32 lon : 51 time : 4 Coordinates: (3) lat (lat) float32 43.875 43.625 ... 36.375 36.125 array([43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -9.375 -9.125 ... 2.875 3.125 array([-9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875, -7.625, -7.375, -7.125, -6.875, -6.625, -6.375, -6.125, -5.875, -5.625, -5.375, -5.125, -4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875, -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625, -0.375, -0.125, 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 32, 51), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 26.11 kB 6.53 kB Shape (4, 32, 51) (1, 32, 51) Count 1711 Tasks 4 Chunks Type float32 numpy.ndarray 51 32 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:48:36.984828 geospatial_lat_max : 44.0 geospatial_lat_min : 36.0 geospatial_lon_max : 3.25 geospatial_lon_min : -9.5 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 We see that clip formulates a bounding box around the data given the geometry whereas mask will be more precise with the shape file geometry. So if you want more precision in your regions, you'll probably want to mask the data whereas if it doesn't matter to you very much (not to me as you saw in my crude bounding box above), then clip is enough. What Now? \u00b6 Where there are many more things to explore. I just touched the surface because for my needs, this is already perfect as I'm more focused on other things. But I would like to point out some other nuggets that they have: More Information They have a link which shows all of the cubes they currently have. They have a few with different spatial resolutions and some configurations are more optimized for spatial computations than temporal computations. The latest public complete list of all variables can be found here . And more information about the organization DCS4COP can be found here . Viewer They have their own viewer that you can run locally or view the cubes via the server","title":"Esdc streaming"},{"location":"tutorials/earth_sci/esdc_streaming/#earth-science-data-cubes","text":"Author: J. Emmanuel Johnson Source: Notebook I | Notebook II The kind folks behind the Earth System Data Lab (Data Cube System for Copernicus, DCS4SOP ) have been developing a package called xcube which will allow processing ESDC a lot easier. I tried using it a while back but I didn't have any success so I kind of gave up on it and decided to use some of my own functions. But they recently announced on their forum that the package was ready. So I decided to test it out and see how it works.","title":"Earth Science Data Cubes"},{"location":"tutorials/earth_sci/esdc_streaming/#my-background","text":"I've been working with the datacubes for quite some time and I've learned a lot thanks to their efforts. I was even a part of their Early Adopters program which allowed me to use their homegrown JupyterLab system (sign up for access here ). I learned a lot about xarray data structures in general, and it really motivated me to try and incorporate different spatial-temporal data representation considerations in my research and thesis. What was super nice was that they had all of the data readily available for use and all I had to do was tinker with my algorithms on their server. But I still wanted some stuff to be on my home machine, my lab machine and possibly on google colab. Nothing heavy but just some light protoyping. So this is nice for those of us who don't necessarily want to use their system but would still like to play with the data. They went the extra mile and even made this package called xcube . Now, for my purposes, I like it because it handles a lot of masking and clipping. But that's just the tip of the iceberg... #@title Install Appropriate packages # requirements !pip install xarray zarr shapely affine rasterio geopandas # Cartopy !apt-get install libproj-dev proj-data proj-bin !apt-get install libgeos-dev !pip install --upgrade -e \"git+https://github.com/SciTools/Cartopy.git#egg=cartopy\" # xcube package from source !pip install --upgrade \"git+https://github.com/dcs4cop/xcube.git#egg=xcube\" --pre Requirement already satisfied: xarray in /usr/local/lib/python3.6/dist-packages (0.15.1) Collecting zarr Downloading https://files.pythonhosted.org/packages/a3/87/383d77399148ef0772da3472b513ecf143252e7c365c51b0f06714800366/zarr-2.4.0.tar.gz (3.3MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3MB 4.9MB/s Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (1.7.0) Collecting affine Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl Collecting rasterio Downloading https://files.pythonhosted.org/packages/c7/81/13321f88f582a00705c5f348724728e8999136e19d6e7c56f7e6ac9bb7f9/rasterio-1.1.3-cp36-cp36m-manylinux1_x86_64.whl (18.1MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18.1MB 221kB/s Collecting geopandas Downloading https://files.pythonhosted.org/packages/83/c5/3cf9cdc39a6f2552922f79915f36b45a95b71fd343cfc51170a5b6ddb6e8/geopandas-0.7.0-py2.py3-none-any.whl (928kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 931kB 45.9MB/s Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.6/dist-packages (from xarray) (1.0.3) Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from xarray) (1.18.2) Requirement already satisfied: setuptools>=41.2 in /usr/local/lib/python3.6/dist-packages (from xarray) (46.1.3) Collecting asciitree Downloading https://files.pythonhosted.org/packages/2d/6a/885bc91484e1aa8f618f6f0228d76d0e67000b0fdd6090673b777e311913/asciitree-0.3.3.tar.gz Collecting fasteners Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl Collecting numcodecs>=0.6.4 Downloading https://files.pythonhosted.org/packages/53/2a/1dc435cbd1d082827190a3e46168fd04f74e266e91313969d5a1aab601bf/numcodecs-0.6.4.tar.gz (3.8MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8MB 35.6MB/s Collecting snuggs>=1.4.1 Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl Collecting click-plugins Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0) Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.1) Collecting cligj>=0.5 Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl Collecting pyproj>=2.2.0 Downloading https://files.pythonhosted.org/packages/ce/37/705ee471f71130d4ceee41bbcb06f3b52175cb89273cbb5755ed5e6374e0/pyproj-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (10.4MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4MB 35.9MB/s Collecting fiona Downloading https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.7MB 330kB/s Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->xarray) (2018.9) Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->xarray) (2.8.1) Collecting monotonic>=0.1 Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners->zarr) (1.12.0) Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7) Collecting munch Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl Building wheels for collected packages: zarr, asciitree, numcodecs Building wheel for zarr (setup.py) ... done Created wheel for zarr: filename=zarr-2.4.0-cp36-none-any.whl size=127066 sha256=15a01a752735178395c62259774a61067bb995b595672010b4e6ce9c1b6b5289 Stored in directory: /root/.cache/pip/wheels/e1/5b/25/24c685604b91139aba00a5b6299b53e7a0661f737f27782559 Building wheel for asciitree (setup.py) ... done Created wheel for asciitree: filename=asciitree-0.3.3-cp36-none-any.whl size=5037 sha256=7edc59bb90e23a65af89b80c00531734a45399622f80e90d6308bfc410a2ef7b Stored in directory: /root/.cache/pip/wheels/1d/d9/58/9808b306744df0208fccc640d3d9952a5bc7468502d42897d5 Building wheel for numcodecs (setup.py) ... done Created wheel for numcodecs: filename=numcodecs-0.6.4-cp36-cp36m-linux_x86_64.whl size=3887834 sha256=29d82c507f5118052062d8eabc2f5d73ac157160933bd3d802a9ce320824fdcc Stored in directory: /root/.cache/pip/wheels/ca/07/ed/fea2e120cbb91d90b577c5ac56b4b082024f56fcd88e9afa55 Successfully built zarr asciitree numcodecs Installing collected packages: asciitree, monotonic, fasteners, numcodecs, zarr, affine, snuggs, click-plugins, cligj, rasterio, pyproj, munch, fiona, geopandas Successfully installed affine-2.3.0 asciitree-0.3.3 click-plugins-1.1.1 cligj-0.5.0 fasteners-0.15 fiona-1.8.13.post1 geopandas-0.7.0 monotonic-1.5 munch-2.5.0 numcodecs-0.6.4 pyproj-2.6.0 rasterio-1.1.3 snuggs-1.4.7 zarr-2.4.0 Reading package lists... Done Building dependency tree Reading state information... Done proj-data is already the newest version (4.9.3-2). proj-data set to manually installed. The following NEW packages will be installed: libproj-dev proj-bin 0 upgraded, 2 newly installed, 0 to remove and 25 not upgraded. Need to get 232 kB of archives. After this operation, 1,220 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libproj-dev amd64 4.9.3-2 [199 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 proj-bin amd64 4.9.3-2 [32.3 kB] Fetched 232 kB in 1s (221 kB/s) Selecting previously unselected package libproj-dev:amd64. (Reading database ... 144568 files and directories currently installed.) Preparing to unpack .../libproj-dev_4.9.3-2_amd64.deb ... Unpacking libproj-dev:amd64 (4.9.3-2) ... Selecting previously unselected package proj-bin. Preparing to unpack .../proj-bin_4.9.3-2_amd64.deb ... Unpacking proj-bin (4.9.3-2) ... Setting up libproj-dev:amd64 (4.9.3-2) ... Setting up proj-bin (4.9.3-2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: libgdal-doc The following NEW packages will be installed: libgeos-dev 0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded. Need to get 73.1 kB of archives. After this operation, 486 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgeos-dev amd64 3.6.2-1build2 [73.1 kB] Fetched 73.1 kB in 1s (103 kB/s) Selecting previously unselected package libgeos-dev. (Reading database ... 144601 files and directories currently installed.) Preparing to unpack .../libgeos-dev_3.6.2-1build2_amd64.deb ... Unpacking libgeos-dev (3.6.2-1build2) ... Setting up libgeos-dev (3.6.2-1build2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Obtaining cartopy from git+https://github.com/SciTools/Cartopy.git#egg=cartopy Cloning https://github.com/SciTools/Cartopy.git to ./src/cartopy Running command git clone -q https://github.com/SciTools/Cartopy.git /content/src/cartopy Requirement already satisfied, skipping upgrade: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.18.2) Requirement already satisfied, skipping upgrade: shapely>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.7.0) Collecting pyshp>=1.1.4 Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 225kB 4.6MB/s Requirement already satisfied, skipping upgrade: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.12.0) Requirement already satisfied, skipping upgrade: setuptools>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cartopy) (46.1.3) Building wheels for collected packages: pyshp Building wheel for pyshp (setup.py) ... done Created wheel for pyshp: filename=pyshp-2.1.0-cp36-none-any.whl size=32609 sha256=efc7f234f85d0ce31808aba7dd43198b43e72e98160f95ca5baf460b0995375d Stored in directory: /root/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a Successfully built pyshp Installing collected packages: pyshp, cartopy Running setup.py develop for cartopy Successfully installed cartopy pyshp-2.1.0 Collecting xcube Cloning https://github.com/dcs4cop/xcube.git to /tmp/pip-install-a7xvjrrb/xcube Running command git clone -q https://github.com/dcs4cop/xcube.git /tmp/pip-install-a7xvjrrb/xcube Building wheels for collected packages: xcube Building wheel for xcube (setup.py) ... done Created wheel for xcube: filename=xcube-0.5.0.dev0-cp36-none-any.whl size=254711 sha256=af0624d45739778a65f51561a4c17150748e96a81cfd600c60f90c7ca6653336 Stored in directory: /tmp/pip-ephem-wheel-cache-k0atts5z/wheels/72/9a/d0/f71cb2f8bc29e8ca6db15d80daaf4330b814aba9b2aaadd6b1 Successfully built xcube Installing collected packages: xcube Successfully installed xcube-0.5.0.dev0 #@title Core Packages import shapely import geopandas as gpd from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor # core xcube packages from xcube.core.dsio import open_cube from xcube.core.geom import ( clip_dataset_by_geometry, mask_dataset_by_geometry, clip_dataset_by_geometry, rasterize_features ) # plotting packages import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs # load cube from bit bucket cube_from_s3_bucket = open_cube(\"https://obs.eu-de.otc.t-systems.com/obs-esdc-v2.0.0/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\") cube_from_s3_bucket","title":"My Background"},{"location":"tutorials/earth_sci/esdc_streaming/#1-visualizing-the-datacube","text":"There have been some fantastic upgrades to the xarray package. For example, visualizing the datacube is great and has an html format which drop-down boxes in a recent [release]((http://xarray.pydata.org/en/stable/whats-new.html#new-features). #@title Visualize a Variable cube_from_s3_bucket[['land_surface_temperature', 'root']] Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.DataArray 'land_surface_temperature' time : 1702 lat : 720 lon : 1440 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Coordinates: (3) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') Attributes: (11) ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 units : K url : http://data.globtemperature.info/","title":"1. Visualizing the Datacube"},{"location":"tutorials/earth_sci/esdc_streaming/#2-visualizing-maps","text":"We can use the built-in function with xarray. There are more advanced ways but that's for another time. fig, ax = plt.subplots(figsize=(10,7)) gpp_data = cube_from_s3_bucket.gross_primary_productivity.sel(time=slice('June-2010', 'June-2010')) gpp_data.mean(dim='time', skipna=True).plot.pcolormesh( ax=ax, cmap='viridis', robust=True, ) ax.set_title('Gross Primary Productivity') # ax.coastlines() plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out)","title":"2. Visualizing Maps"},{"location":"tutorials/earth_sci/esdc_streaming/#2-making-subsets","text":"Often times we never really want to work with the entire globe: we want to take a subset. I have been working with the western part of EuroAsia and a bit with the north of Africa. One way we can do it is naively by simply slicing the dataframe with the locations we want. In this example, I'll use the following coordinates: Latitude: 35N-71.5N Longitude: 18E - 60W #@title Slicing # subset using xarray slicing europe = gpp_data.sel(lat=slice(71.5, 35.5), lon=slice(-18.0, 60.0)) fig, ax = plt.subplots(figsize=(10,7)) europe.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) We can also use the bounding box provided by the xcube package which is much cleaner and more flexible. x1 = -18.0 # lon1, degree x2 = 60.0 # lon2, degree y1 = 35.5 # lat1, degree y2 = 71.5 # lat2, degree coords = x1, y1, x2, y2 #@title xcube # convert into shapely bounding box bbox = shapely.geometry.box(*coords) # subset the cube with the bounding box europe = xcube.core.geom.clip_dataset_by_geometry(gpp_data, bbox) fig, ax = plt.subplots(figsize=(10,7)) europe.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/core.py:348: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison o = func(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out)","title":"2.  Making Subsets"},{"location":"tutorials/earth_sci/esdc_streaming/#3-shape-files-and-masks","text":"That was my custom area. But what about if someone gives you a specific shape file that they want you to subset. Or perhaps you would like to subset an entire region, e.g. North America or Spain. Dealing with shape files is quite easy but having to convert them into rasters isn't exactly trivial. We'll use geopandas to deal with the shape file. # get shape file location in package shp_file_loc = gpd.datasets.get_path('naturalearth_lowres') # read shape file shp_gdf = gpd.read_file(shp_file_loc) shp_gdf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop_est continent name iso_a3 gdp_md_est geometry 0 920938 Oceania Fiji FJI 8374.0 MULTIPOLYGON (((180.00000 -16.06713, 180.00000... 1 53950935 Africa Tanzania TZA 150600.0 POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... 2 603253 Africa W. Sahara ESH 906.5 POLYGON ((-8.66559 27.65643, -8.66512 27.58948... 3 35623680 North America Canada CAN 1674000.0 MULTIPOLYGON (((-122.84000 49.00000, -122.9742... 4 326625791 North America United States of America USA 18560000.0 MULTIPOLYGON (((-122.84000 49.00000, -120.0000... So we have a shape file with columns as queries. The queries are basically all of the columns except for the geometry as that holds the shape file. The post important features are the continent and the geometry. We'll need these. For this, Let's extract North America. query = 'North America' column = 'continent' # subset shape files within NA na_shp_clf = shp_gdf[shp_gdf[column] == query] # let's collapse all shape files into one big shapefile na_shp_clf = na_shp_clf.dissolve(by='continent').head() na_shp_clf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry pop_est name iso_a3 gdp_md_est continent North America MULTIPOLYGON (((-61.68000 10.76000, -61.10500 ... 35623680 Canada CAN 1674000.0 na_shp_clf.plot() <matplotlib.axes._subplots.AxesSubplot at 0x7f07c17606a0> Now we can use the convenience function mask_dataset_by_geometry so that we can set all values not in NA to NANs. masked_gpp = mask_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), na_shp_clf.geometry.values[0] ) masked_gpp Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 307 lon : 640 time : 4 Coordinates: (3) lat (lat) float32 83.875 83.625 ... 7.625 7.375 array([83.875, 83.625, 83.375, ..., 7.875, 7.625, 7.375], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -171.875 -171.625 ... -12.125 array([-171.875, -171.625, -171.375, ..., -12.625, -12.375, -12.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 307, 640), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 3.14 MB 785.92 kB Shape (4, 307, 640) (1, 307, 640) Count 1716 Tasks 4 Chunks Type float32 numpy.ndarray 640 307 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:43:14.640733 geospatial_lat_max : 84.0 geospatial_lat_min : 7.25 geospatial_lon_max : -12.0 geospatial_lon_min : -172.0 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show() /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) We can do the same thing with Spain. query = 'Spain' column = 'name' spain_shp_df = shp_gdf[shp_gdf[column] == query] # let's collapse all shape files into one big shapefile spain_shp_df = spain_shp_df.dissolve(by='continent').head() # plot spain_shp_df.plot(); spain_shp_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry pop_est name iso_a3 gdp_md_est continent Europe POLYGON ((-7.45373 37.09779, -7.53711 37.42890... 48958159 Spain ESP 1690000.0 Let's see the difference between mask and clip. #@title Masking masked_gpp = mask_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), spain_shp_df.geometry.values[0]) fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show(); masked_gpp /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 32 lon : 51 time : 4 Coordinates: (3) lat (lat) float32 43.875 43.625 ... 36.375 36.125 array([43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -9.375 -9.125 ... 2.875 3.125 array([-9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875, -7.625, -7.375, -7.125, -6.875, -6.625, -6.375, -6.125, -5.875, -5.625, -5.375, -5.125, -4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875, -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625, -0.375, -0.125, 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 32, 51), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 26.11 kB 6.53 kB Shape (4, 32, 51) (1, 32, 51) Count 1716 Tasks 4 Chunks Type float32 numpy.ndarray 51 32 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:48:52.627587 geospatial_lat_max : 44.0 geospatial_lat_min : 36.0 geospatial_lon_max : 3.25 geospatial_lon_min : -9.5 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 #@title Clipping masked_gpp = clip_dataset_by_geometry( cube_from_s3_bucket[['gross_primary_productivity']].sel(time=slice('June-2010', 'June-2010')), spain_shp_df.geometry.values[0]) fig, ax = plt.subplots(figsize=(10,7)) masked_gpp.gross_primary_productivity.mean(dim='time').plot.imshow(ax=ax, cmap='viridis', robust=False) ax.set_title('Gross Primary Productivity') plt.show(); masked_gpp /usr/local/lib/python3.6/dist-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 32 lon : 51 time : 4 Coordinates: (3) lat (lat) float32 43.875 43.625 ... 36.375 36.125 array([43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125], dtype=float32) time (time) datetime64[ns] 2010-06-06 ... 2010-06-30 bounds : time_bnds long_name : time standard_name : time array(['2010-06-06T00:00:00.000000000', '2010-06-14T00:00:00.000000000', '2010-06-22T00:00:00.000000000', '2010-06-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -9.375 -9.125 ... 2.875 3.125 array([-9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875, -7.625, -7.375, -7.125, -6.875, -6.625, -6.375, -6.125, -5.875, -5.625, -5.375, -5.125, -4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875, -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625, -0.375, -0.125, 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125], dtype=float32) Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 32, 51), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 26.11 kB 6.53 kB Shape (4, 32, 51) (1, 32, 51) Count 1711 Tasks 4 Chunks Type float32 numpy.ndarray 51 32 4 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-04-15T19:48:36.984828 geospatial_lat_max : 44.0 geospatial_lat_min : 36.0 geospatial_lon_max : 3.25 geospatial_lon_min : -9.5 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 We see that clip formulates a bounding box around the data given the geometry whereas mask will be more precise with the shape file geometry. So if you want more precision in your regions, you'll probably want to mask the data whereas if it doesn't matter to you very much (not to me as you saw in my crude bounding box above), then clip is enough.","title":"3. Shape Files and Masks"},{"location":"tutorials/earth_sci/esdc_streaming/#what-now","text":"Where there are many more things to explore. I just touched the surface because for my needs, this is already perfect as I'm more focused on other things. But I would like to point out some other nuggets that they have: More Information They have a link which shows all of the cubes they currently have. They have a few with different spatial resolutions and some configurations are more optimized for spatial computations than temporal computations. The latest public complete list of all variables can be found here . And more information about the organization DCS4COP can be found here . Viewer They have their own viewer that you can run locally or view the cubes via the server","title":"What Now?"},{"location":"tutorials/gaussian_processes/_ideas/","text":"From Scratch \u00b6","title":" ideas"},{"location":"tutorials/gaussian_processes/_ideas/#from-scratch","text":"","title":"From Scratch"},{"location":"tutorials/jax/","text":"Jax \u00b6 A new library that I think will take on the machine learning world and be very competitive with deep learning architectures. It's fast, runs on GPUs and you can use the native numpy way of coding. Below are some helpful resources as well as some of my own hand-crafted tutorials. Learning Jax \u00b6 If you want more information about Jax in general: Jax - Scipy Lecture 2020 - Jake VanderPlas - Video A really good introduction to Jax by one of the contributors. Really aimed at the general public but detailed enough so that you can get a really good idea about how to use it. You Don't Know Jax - Colin Raffel - Blog A very simple introduction in blog format. Goes through the basics and is very clear. Getting started with Hax (MLPs, CNNs, & RNNs) - Robert Tjarko Lange - Video A nice tutorial but dives a bit deeper into some of the deep learning models you'll typically find. Implements everything from scratch and highlights the jax features in the process. Then proceeds to use the built-in stax library and refactors some parts. Probably the most complete advanced tutorial you'll find right now. From PyTorch to Jax: towards neural net frameworks that purify stateful code - Sabrina J. Mielke - blog This tutorial goes from scratch and then really gets into the nitty gritty aspects that allow you to really customize your code to ensure that you follow the functional paradigm. It even shows you how to get 'classes' while still respecting the jax restrictions. From a philosophy standpoint, I think it's just awesome. And from a practical standpoint, it basically goes from a simple Jax function to something you'll see in the Haiku library (a fully formed jax library for neural networks). Just awesome. Accelerated ML Research via Composable Function Transformations in Python - neurips 2019 A tutorial about Jax but mainly from a programming research standpoint. Taylor-Mode AD for Higher-Order Derivatives in Jax - neurips 2019 Very similar to the above tutorial. My Tutorials \u00b6 I plan to use Jax and get accustomed to functional programming. I quite like it and it's a different way of doing things. Jax itself is super interesting and I really like the bells and whistles that it has to offer. The access to vmap , jit , and grad . vmap Automatic handling of batch dimensions (samples) so that you can write your code in vector format. Jit Compilation Jit compilation. Making your code fast with some restrictions. Classes Just a small example of the change in philosophy. Most deep learning libraries have you write things in terms of classes. But jax is purely functional (unless you use a dedicated library like haiku ) so you need to change your coding style. Cool Libraries I follow \u00b6 This is just a rolling list of really cool libraries that I follow. Sometimes I actively use it and sometimes I just like to browse code and get good ideas. Flax This should probably be in your repertoire of libraries. A really good and simple library for neural networks from Google itself. Strictly functional and super popular. Haiku Another very popular deep learning library built on top of Jax. This one gives the illusion of PyTorch/TensorFlow because the modules look very class oriented. But it still follows Jax protocol. Very neat how they managed to do that. To see it from scratch, see the above tutorial. Elegy A new library based on Jax and Haiku which has a similar style to keras. Still very new but it has potential. I find it interesting because the natural progression from Jax+Haiku is something similar to keras. I'm glad someone took up that mantle. Numpyro | Paper A probabilistic framework which focuses on mcmc sampling schemes (e.g. HMC/NUTS). It also has variational inference procedures. Kalman Jax This library is used for Markov GPs for time series. But they have a lot of little GP nuggets. Especially approximate inference algorithms, e.g. extended EP, statistically linearized EP, extended EP, etc. NuX Normalizing Flows using jax jax-flows Normalizing Flows using Jax. Jax Cosmo Applied to astrophysics but they have some nice routines that are not found in the main jax library (e.g. quad and interp)","title":"Jax"},{"location":"tutorials/jax/#jax","text":"A new library that I think will take on the machine learning world and be very competitive with deep learning architectures. It's fast, runs on GPUs and you can use the native numpy way of coding. Below are some helpful resources as well as some of my own hand-crafted tutorials.","title":"Jax"},{"location":"tutorials/jax/#learning-jax","text":"If you want more information about Jax in general: Jax - Scipy Lecture 2020 - Jake VanderPlas - Video A really good introduction to Jax by one of the contributors. Really aimed at the general public but detailed enough so that you can get a really good idea about how to use it. You Don't Know Jax - Colin Raffel - Blog A very simple introduction in blog format. Goes through the basics and is very clear. Getting started with Hax (MLPs, CNNs, & RNNs) - Robert Tjarko Lange - Video A nice tutorial but dives a bit deeper into some of the deep learning models you'll typically find. Implements everything from scratch and highlights the jax features in the process. Then proceeds to use the built-in stax library and refactors some parts. Probably the most complete advanced tutorial you'll find right now. From PyTorch to Jax: towards neural net frameworks that purify stateful code - Sabrina J. Mielke - blog This tutorial goes from scratch and then really gets into the nitty gritty aspects that allow you to really customize your code to ensure that you follow the functional paradigm. It even shows you how to get 'classes' while still respecting the jax restrictions. From a philosophy standpoint, I think it's just awesome. And from a practical standpoint, it basically goes from a simple Jax function to something you'll see in the Haiku library (a fully formed jax library for neural networks). Just awesome. Accelerated ML Research via Composable Function Transformations in Python - neurips 2019 A tutorial about Jax but mainly from a programming research standpoint. Taylor-Mode AD for Higher-Order Derivatives in Jax - neurips 2019 Very similar to the above tutorial.","title":"Learning Jax"},{"location":"tutorials/jax/#my-tutorials","text":"I plan to use Jax and get accustomed to functional programming. I quite like it and it's a different way of doing things. Jax itself is super interesting and I really like the bells and whistles that it has to offer. The access to vmap , jit , and grad . vmap Automatic handling of batch dimensions (samples) so that you can write your code in vector format. Jit Compilation Jit compilation. Making your code fast with some restrictions. Classes Just a small example of the change in philosophy. Most deep learning libraries have you write things in terms of classes. But jax is purely functional (unless you use a dedicated library like haiku ) so you need to change your coding style.","title":"My Tutorials"},{"location":"tutorials/jax/#cool-libraries-i-follow","text":"This is just a rolling list of really cool libraries that I follow. Sometimes I actively use it and sometimes I just like to browse code and get good ideas. Flax This should probably be in your repertoire of libraries. A really good and simple library for neural networks from Google itself. Strictly functional and super popular. Haiku Another very popular deep learning library built on top of Jax. This one gives the illusion of PyTorch/TensorFlow because the modules look very class oriented. But it still follows Jax protocol. Very neat how they managed to do that. To see it from scratch, see the above tutorial. Elegy A new library based on Jax and Haiku which has a similar style to keras. Still very new but it has potential. I find it interesting because the natural progression from Jax+Haiku is something similar to keras. I'm glad someone took up that mantle. Numpyro | Paper A probabilistic framework which focuses on mcmc sampling schemes (e.g. HMC/NUTS). It also has variational inference procedures. Kalman Jax This library is used for Markov GPs for time series. But they have a lot of little GP nuggets. Especially approximate inference algorithms, e.g. extended EP, statistically linearized EP, extended EP, etc. NuX Normalizing Flows using jax jax-flows Normalizing Flows using Jax. Jax Cosmo Applied to astrophysics but they have some nice routines that are not found in the main jax library (e.g. quad and interp)","title":"Cool Libraries I follow"},{"location":"tutorials/jax/bisection/","text":"Bisection search \u00b6 This is a very interesting thing that you can do instead of interpolation. This comes from the fact that PyTorch doesn't have a native interpolation algorithm. Although there are saints on the internet who have implemented their own native algorithm, the library itself doesn't have a core one. I find it very interesting that they don't and I'm not sure why. Is there something difficult about automatic differentiation and interpolation? In any case, one thing that I did see was bisection search instead of interpolation. I've also seen it in the rv_histogram function in the scipy library. This function allows you to construct an empirical distribution based on histograms. You construct the histograms using the np.histogram function and then you go through and normalize the PDF estimates found in the bins followed by creating the CDF by using the cumsum function. Now with these, you should be able to calculate the PDF, the CDF, the quantile function and virtually any other distribution function that you may need, even on new data. So, how do you calculate quantities for new data? Let's break two functions down step by step: the PDF and the CDF/Quantile. Estimating PDFs \u00b6 Recall, a histogram works by putting bins in intervals along the domain of your data. Then you calculate how many times you get values within those bins. The probability is the number of times you have data within that bin divided by the width of the bin. To make a density, you need to normalize the entire distribution so that it sums to 1. So in order to estimate the PDF for new data, you need to find where are the bins closest to your data. Interpolation \u00b6 One thing you can do is to interpolate. You find the support that is closest to your query points X X and then output the corresponding values for the estimated histogram values. Old school algebra, this looks like: \\frac{x}{?} = \\frac{\\text{support}}{\\text{hist pdf}} \\frac{x}{?} = \\frac{\\text{support}}{\\text{hist pdf}} So in code, this translates to: x_pdf_est = np . interp ( X_new , X_hist_bins , X_hist_pdf ) The numpy implementation has been shown to be quite fast. Sometimes I've used the scipy formula but apparently the numpy implementation it's maginitudes faster than the scipy implementation. However, I believe the scipy implementation has some extra goodies like extrapolation if you're outside the bounds of your distribution. Bisection Search \u00b6 Alternatively, we could just do a bisection search. Now, it may not be as precise especially if we don't have enough bins, but it will be faster than interpolation. This works but 1) find the closest support values and then 2) use the corresponding values in the histogram PDF. # find the closest bins X_bins_est = np . search_sorted ( X_new , X_hist_bins ) # select the pdf of the bins X_pdf_est = X_hist_pdf [ X_bins_est ] Estimating CDF / Quantiles \u00b6 This is CDFs \u00b6 \\frac{X}{?} = \\frac{Quantiles}{References} \\frac{X}{?} = \\frac{Quantiles}{References} X_uniform = np . interp ( X , X_cdf , X_ref ) With the bisection search X_uniform = X_ref [ np . search_sorted ( X_cdf , X )] Quantiles \u00b6 \\frac{X}{?} = \\frac{References}{Quantiles} \\frac{X}{?} = \\frac{References}{Quantiles} X_approx = np . interp ( X_uniform , X_references , X_quantiles ) With the bisection search X_approx = X_quantiles [ np . search_sorted ( X_references , X_uniform )] Application \u00b6 In my application, I frequently work with normalizing flows, in particular Gaussianization. Essentially, you transform a univariate distribution to a Guassian distribution by computing the empirical CDF of the distribution followed by the Inverse of the Gaussian CDF. Now you can calculate the probability distribution using samples from the Gaussian distribution Other Implementations \u00b6 PyTorch \u00b6 def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 This function is a little difficult to understand. Note : this is what happens who you have no comments, no type hints and no documentation about the sizes or what the dimensions are... So now, let's update the documentation so that it's clearer what's going on. def search_sorted ( bin_locations : torch . Tensor , inputs : torch . Tensor , eps : float = 1e-6 ) -> torch . Tensor : \"\"\"Differentiable bisection search. Parameters ---------- bin_locations : torch.Tensor, (n_samples, n_features) inputs : torch.Tensor, (n_samples, n_features) eps : float, default=1e-6 regularization to be added Returns ------- X_bins : torch.Tensor, (n_samples, n_features) corresponding bin locations of the inputs Example ------- \"\"\" return None Source : Pyro Library | Neural Spline Flows","title":"Bisection search"},{"location":"tutorials/jax/bisection/#bisection-search","text":"This is a very interesting thing that you can do instead of interpolation. This comes from the fact that PyTorch doesn't have a native interpolation algorithm. Although there are saints on the internet who have implemented their own native algorithm, the library itself doesn't have a core one. I find it very interesting that they don't and I'm not sure why. Is there something difficult about automatic differentiation and interpolation? In any case, one thing that I did see was bisection search instead of interpolation. I've also seen it in the rv_histogram function in the scipy library. This function allows you to construct an empirical distribution based on histograms. You construct the histograms using the np.histogram function and then you go through and normalize the PDF estimates found in the bins followed by creating the CDF by using the cumsum function. Now with these, you should be able to calculate the PDF, the CDF, the quantile function and virtually any other distribution function that you may need, even on new data. So, how do you calculate quantities for new data? Let's break two functions down step by step: the PDF and the CDF/Quantile.","title":"Bisection search"},{"location":"tutorials/jax/bisection/#estimating-pdfs","text":"Recall, a histogram works by putting bins in intervals along the domain of your data. Then you calculate how many times you get values within those bins. The probability is the number of times you have data within that bin divided by the width of the bin. To make a density, you need to normalize the entire distribution so that it sums to 1. So in order to estimate the PDF for new data, you need to find where are the bins closest to your data.","title":"Estimating PDFs"},{"location":"tutorials/jax/bisection/#interpolation","text":"One thing you can do is to interpolate. You find the support that is closest to your query points X X and then output the corresponding values for the estimated histogram values. Old school algebra, this looks like: \\frac{x}{?} = \\frac{\\text{support}}{\\text{hist pdf}} \\frac{x}{?} = \\frac{\\text{support}}{\\text{hist pdf}} So in code, this translates to: x_pdf_est = np . interp ( X_new , X_hist_bins , X_hist_pdf ) The numpy implementation has been shown to be quite fast. Sometimes I've used the scipy formula but apparently the numpy implementation it's maginitudes faster than the scipy implementation. However, I believe the scipy implementation has some extra goodies like extrapolation if you're outside the bounds of your distribution.","title":"Interpolation"},{"location":"tutorials/jax/bisection/#bisection-search_1","text":"Alternatively, we could just do a bisection search. Now, it may not be as precise especially if we don't have enough bins, but it will be faster than interpolation. This works but 1) find the closest support values and then 2) use the corresponding values in the histogram PDF. # find the closest bins X_bins_est = np . search_sorted ( X_new , X_hist_bins ) # select the pdf of the bins X_pdf_est = X_hist_pdf [ X_bins_est ]","title":"Bisection Search"},{"location":"tutorials/jax/bisection/#estimating-cdf-quantiles","text":"This is","title":"Estimating CDF / Quantiles"},{"location":"tutorials/jax/bisection/#cdfs","text":"\\frac{X}{?} = \\frac{Quantiles}{References} \\frac{X}{?} = \\frac{Quantiles}{References} X_uniform = np . interp ( X , X_cdf , X_ref ) With the bisection search X_uniform = X_ref [ np . search_sorted ( X_cdf , X )]","title":"CDFs"},{"location":"tutorials/jax/bisection/#quantiles","text":"\\frac{X}{?} = \\frac{References}{Quantiles} \\frac{X}{?} = \\frac{References}{Quantiles} X_approx = np . interp ( X_uniform , X_references , X_quantiles ) With the bisection search X_approx = X_quantiles [ np . search_sorted ( X_references , X_uniform )]","title":"Quantiles"},{"location":"tutorials/jax/bisection/#application","text":"In my application, I frequently work with normalizing flows, in particular Gaussianization. Essentially, you transform a univariate distribution to a Guassian distribution by computing the empirical CDF of the distribution followed by the Inverse of the Gaussian CDF. Now you can calculate the probability distribution using samples from the Gaussian distribution","title":"Application"},{"location":"tutorials/jax/bisection/#other-implementations","text":"","title":"Other Implementations"},{"location":"tutorials/jax/bisection/#pytorch","text":"def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 This function is a little difficult to understand. Note : this is what happens who you have no comments, no type hints and no documentation about the sizes or what the dimensions are... So now, let's update the documentation so that it's clearer what's going on. def search_sorted ( bin_locations : torch . Tensor , inputs : torch . Tensor , eps : float = 1e-6 ) -> torch . Tensor : \"\"\"Differentiable bisection search. Parameters ---------- bin_locations : torch.Tensor, (n_samples, n_features) inputs : torch.Tensor, (n_samples, n_features) eps : float, default=1e-6 regularization to be added Returns ------- X_bins : torch.Tensor, (n_samples, n_features) corresponding bin locations of the inputs Example ------- \"\"\" return None Source : Pyro Library | Neural Spline Flows","title":"PyTorch"},{"location":"tutorials/jax/classes/","text":"Classes \u00b6 There was an interesting example where someone had asked about how one can deal with classes. I have been a supporter of classes for a while now but now I've started to rethinking my convictions. OOP code is quite difficult to maintain by yourself and if you don't do the abstract correctly, you run into problems later on. So...I've started to branch out a bit. Let's get into it. Let's say we have two parameters that we want to update. So naturally, I would build a class: class Idea : def __init__ ( self , w , b ): self . w = w self . b = b Now we want to do something. Let's say it's a neural network and we want to perform a transformation. So let's implement a predict function. class NN : def __init__ ( self , w , b ): self . w = w self . b = b def predict ( self , input ): return np . dot ( self . w , x ) + self . b Easy enough, now we would like to train this. So we need a step function which handles the gradients and another function for the training loop. class NN : def __init__ ( self , w , b ): self . w = w self . b = b def predict ( self , input ): return np . dot ( self . w , x ) + self . b def step ( self , input ): Functional Version \u00b6 Params = namedtuple ( \"Params\" , [ \"w\" , \"b\" ]) def predict ( params , inputs ): return np . dot ( params . w , x ) + params . b def step ( params , inputs , lr = 0.1 ): # get gradients grads = dloss ( params , inputs ) # update parameters w_new = params . w - lr * grad . w b_new = params . b - lr * grad . b # return new parameter tuple return Params ( w_new , b_new ) for i in range ( 1_000 ): params = step ( params , inputs ) Resources \u00b6 Github Issue An example straight from the developers of how to create classes using jax. They're not full traditional classes but they do it in such a way that it works.","title":"Classes"},{"location":"tutorials/jax/classes/#classes","text":"There was an interesting example where someone had asked about how one can deal with classes. I have been a supporter of classes for a while now but now I've started to rethinking my convictions. OOP code is quite difficult to maintain by yourself and if you don't do the abstract correctly, you run into problems later on. So...I've started to branch out a bit. Let's get into it. Let's say we have two parameters that we want to update. So naturally, I would build a class: class Idea : def __init__ ( self , w , b ): self . w = w self . b = b Now we want to do something. Let's say it's a neural network and we want to perform a transformation. So let's implement a predict function. class NN : def __init__ ( self , w , b ): self . w = w self . b = b def predict ( self , input ): return np . dot ( self . w , x ) + self . b Easy enough, now we would like to train this. So we need a step function which handles the gradients and another function for the training loop. class NN : def __init__ ( self , w , b ): self . w = w self . b = b def predict ( self , input ): return np . dot ( self . w , x ) + self . b def step ( self , input ):","title":"Classes"},{"location":"tutorials/jax/classes/#functional-version","text":"Params = namedtuple ( \"Params\" , [ \"w\" , \"b\" ]) def predict ( params , inputs ): return np . dot ( params . w , x ) + params . b def step ( params , inputs , lr = 0.1 ): # get gradients grads = dloss ( params , inputs ) # update parameters w_new = params . w - lr * grad . w b_new = params . b - lr * grad . b # return new parameter tuple return Params ( w_new , b_new ) for i in range ( 1_000 ): params = step ( params , inputs )","title":"Functional Version"},{"location":"tutorials/jax/classes/#resources","text":"Github Issue An example straight from the developers of how to create classes using jax. They're not full traditional classes but they do it in such a way that it works.","title":"Resources"},{"location":"tutorials/jax/ideas/","text":"Jax Tutorial Ideas \u00b6 Tutorials \u00b6 vmap jit grad , jacobian , hessian optimizers containers - Dict , Tuple , NamedTuple Use Cases \u00b6 Gaussian Processes Kernel Matrices and Derivatives Kernel Density Estimation - Blog | Optimized Kernel Ridge Regression (OKRR) - Notebook Optimized Kernel Entropy Components Analysis (OKECA) Centered Kernel Alignment (CKA) Gaussianization Flows Projects \u00b6 Uncertain Inputs for GPs Gaussianization Flows - Case Study Uncertain Inputs for Gaussian Processes Linearized GP (Taylor Expansion, 1 st Order, 2 nd Order) Moment Matching (RBF) Scratch - Loops, numba Jax MCMC Posterior Approximation Scratch Numpyro - NUTS/HMC Variational GP Numpyro - SVI Data \u00b6 1D Examples 2D Examples IASI Example Ocean Data Future Stuff \u00b6 Sparse Models Deep Models DKL Models Resources \u00b6 GPS VI 4 GPs w. Jax | BBVI GP JaxGP GPs and DGPs with Jax n Flax OKECA PCA Projections","title":"Jax Tutorial Ideas"},{"location":"tutorials/jax/ideas/#jax-tutorial-ideas","text":"","title":"Jax Tutorial Ideas"},{"location":"tutorials/jax/ideas/#tutorials","text":"vmap jit grad , jacobian , hessian optimizers containers - Dict , Tuple , NamedTuple","title":"Tutorials"},{"location":"tutorials/jax/ideas/#use-cases","text":"Gaussian Processes Kernel Matrices and Derivatives Kernel Density Estimation - Blog | Optimized Kernel Ridge Regression (OKRR) - Notebook Optimized Kernel Entropy Components Analysis (OKECA) Centered Kernel Alignment (CKA) Gaussianization Flows","title":"Use Cases"},{"location":"tutorials/jax/ideas/#projects","text":"Uncertain Inputs for GPs Gaussianization Flows - Case Study Uncertain Inputs for Gaussian Processes Linearized GP (Taylor Expansion, 1 st Order, 2 nd Order) Moment Matching (RBF) Scratch - Loops, numba Jax MCMC Posterior Approximation Scratch Numpyro - NUTS/HMC Variational GP Numpyro - SVI","title":"Projects"},{"location":"tutorials/jax/ideas/#data","text":"1D Examples 2D Examples IASI Example Ocean Data","title":"Data"},{"location":"tutorials/jax/ideas/#future-stuff","text":"Sparse Models Deep Models DKL Models","title":"Future Stuff"},{"location":"tutorials/jax/ideas/#resources","text":"GPS VI 4 GPs w. Jax | BBVI GP JaxGP GPs and DGPs with Jax n Flax OKECA PCA Projections","title":"Resources"},{"location":"tutorials/jax/init_funcs/","text":"","title":"Init funcs"},{"location":"tutorials/jax/jit/","text":"Jit \u00b6 Fixing some Arguments \u00b6 So jax.jit doesn't play nice sometimes when you try to compile functions with arguments that are functions themselves. A lot of times you'll get an error. One trick I learned to get around this is to \"partially fit\" the function. Let's do an example: def predict ( func , params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output If we try to jit compile this...well it won't less us and will give us an error message. predict_f_jitted = jax . jit ( predict ) Instead we need to make some arguments static so that once the function is compiled, it doesn't change those particular arguments. In our case, it would be the func. So to do this, we can do the following: predict_f_jited = jax . jit ( predict , static_argnums = ( 0 ,)) An now, we won't get an error message! Now, those functional experts will probably say that this isn't a good way to do function programming and we should be using context. For example: def func ( params , input_vector ): # do stuff ... return output def predict ( params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output In this case, we should be able to apply the jit function because we have saved the previous function via the context. I personally don't like this but mainly because I'm not used to it. I like my functions to be relatively independent and I'm not very good at managing context. I imagine it's super useful when you want to make a script that has all of the functions that you need all within the .py file. That way you can handle the context as you progress through the script. For if you want independent files (which I like for sanity purposes), I don't see how this is possible. But again, it's a personal preference. Plus, I'm not an expert by any means. Using the decorator \u00b6 We can also use the functools.partial decorator to wrap our function so that once it's called, it will automatically compile the function. from functools import partial @partial ( jax . jit , static_argnums = ( 0 )) def predict ( func , params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output I use this quite often if I have functions that I know I won't change certain parameters the moment I start using them. It also goes well with my style of having a lot of individual functions in separate files.","title":"Jit"},{"location":"tutorials/jax/jit/#jit","text":"","title":"Jit"},{"location":"tutorials/jax/jit/#fixing-some-arguments","text":"So jax.jit doesn't play nice sometimes when you try to compile functions with arguments that are functions themselves. A lot of times you'll get an error. One trick I learned to get around this is to \"partially fit\" the function. Let's do an example: def predict ( func , params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output If we try to jit compile this...well it won't less us and will give us an error message. predict_f_jitted = jax . jit ( predict ) Instead we need to make some arguments static so that once the function is compiled, it doesn't change those particular arguments. In our case, it would be the func. So to do this, we can do the following: predict_f_jited = jax . jit ( predict , static_argnums = ( 0 ,)) An now, we won't get an error message! Now, those functional experts will probably say that this isn't a good way to do function programming and we should be using context. For example: def func ( params , input_vector ): # do stuff ... return output def predict ( params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output In this case, we should be able to apply the jit function because we have saved the previous function via the context. I personally don't like this but mainly because I'm not used to it. I like my functions to be relatively independent and I'm not very good at managing context. I imagine it's super useful when you want to make a script that has all of the functions that you need all within the .py file. That way you can handle the context as you progress through the script. For if you want independent files (which I like for sanity purposes), I don't see how this is possible. But again, it's a personal preference. Plus, I'm not an expert by any means.","title":"Fixing some Arguments"},{"location":"tutorials/jax/jit/#using-the-decorator","text":"We can also use the functools.partial decorator to wrap our function so that once it's called, it will automatically compile the function. from functools import partial @partial ( jax . jit , static_argnums = ( 0 )) def predict ( func , params , input_vector ): # intermediate function x_trans = func ( params , input_vector ) # real function output = params [ 'w' ] * input_vector + x_trans return output I use this quite often if I have functions that I know I won't change certain parameters the moment I start using them. It also goes well with my style of having a lot of individual functions in separate files.","title":"Using the decorator"},{"location":"tutorials/jax/optimize/","text":"Optimizing Using Jax \u00b6 From Scratch \u00b6 Step Function \u00b6 # STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss And now we need to actually go through and initialize the parameters. # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) And lastly let's do the actual loop. # initialize progress bar postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # store loss values losses . append ( value . mean ()) # store parameters for display postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" # update progress bar bar . set_postfix ( postfix ) Resources \u00b6 Using Scipy Optimize \u00b6 Scipy Optimize | Minimize | StackOverFlow Scipy Lectures Real Python - Scientific Python: Using Scipy for Optimization","title":"Optimizing Using Jax"},{"location":"tutorials/jax/optimize/#optimizing-using-jax","text":"","title":"Optimizing Using Jax"},{"location":"tutorials/jax/optimize/#from-scratch","text":"","title":"From Scratch"},{"location":"tutorials/jax/optimize/#step-function","text":"# STEP FUNCTION @jax . jit def step ( params , X , y , opt_state ): # calculate loss loss = mll_loss ( params , X , y ) # calculate gradient of loss grads = dloss ( params , X , y ) # update optimizer state opt_state = opt_update ( 0 , grads , opt_state ) # update params params = get_params ( opt_state ) return params , opt_state , loss And now we need to actually go through and initialize the parameters. # TRAINING PARARMETERS n_epochs = 500 if not args . smoke_test else 2 learning_rate = 0.01 losses = list () # initialize optimizer opt_init , opt_update , get_params = optimizers . rmsprop ( step_size = learning_rate ) # initialize parameters opt_state = opt_init ( params ) # get initial parameters params = get_params ( opt_state ) And lastly let's do the actual loop. # initialize progress bar postfix = {} with tqdm . trange ( n_epochs ) as bar : for i in bar : # 1 step - optimize function params , opt_state , value = step ( params , X , y , opt_state ) # store loss values losses . append ( value . mean ()) # store parameters for display postfix = {} for ikey in params . keys (): postfix [ ikey ] = f \" { params [ ikey ] : .2f } \" postfix [ \"Loss\" ] = f \" { onp . array ( losses [ - 1 ]) : .2f } \" # update progress bar bar . set_postfix ( postfix )","title":"Step Function"},{"location":"tutorials/jax/optimize/#resources","text":"","title":"Resources"},{"location":"tutorials/jax/optimize/#using-scipy-optimize","text":"Scipy Optimize | Minimize | StackOverFlow Scipy Lectures Real Python - Scientific Python: Using Scipy for Optimization","title":"Using Scipy Optimize"},{"location":"tutorials/jax/vmap/","text":"VMAP \u00b6 This is a really nifty function that allows you to automatically handle batch dimensions without any cost! It allows you to write your code in a vectorized format and then just use this transformation to handle the batching. On the website, they have an example if you have a linear function written in vector format, and you want to make predictions for multiple samples. So an example function is: def predict ( params , input_vector ): w , b = params output_vec = np . dot ( w , input_vector ) + b output_vec = np . tanh ( output_vec ) return output_vec The thing to notice is that the input vector is on the right-hand side of the matrix multiplication. Normally, when we want to handle samples, we do np.dot(inputs, W) because we are using matrix multiplication to handle the samples. So mathematically, a vector would be: y = \\mathbf{w}\\mathbf{x} + b y = \\mathbf{w}\\mathbf{x} + b where \\mathbf{x,w} \\in \\mathbb{R}^{D} \\mathbf{x,w} \\in \\mathbb{R}^{D} are vectors and y,b \\in \\mathbb{R} y,b \\in \\mathbb{R} are scalars. In almost every machine learning tutorial ever , they teach you to do change that for the following: output_vec = np . dot ( w , input_vector ) + b because it is basically implied that we've collected all of our data points into a matrix \\mathbf{X} \\in \\mathbb{R}^{N \\times D} \\mathbf{X} \\in \\mathbb{R}^{N \\times D} where every row is a sample and every column is a feature. It is equivalent to the notation: y = \\mathbf{Xw} + b y = \\mathbf{Xw} + b So we don't have to worry about this step. Well, pivoting off of the vector notation, if we wanted to actually account for samples, then we would need to run our predict function multiple times which (in python) is inefficient and it would take ages. They provide an example of how one could do it using pure functional python. from functools import partial predictions = np . stack ( list ( map ( partial ( predict , params ), input_batch ))) This is a pure functional programming as it features a composition of functions. I'm not well versed in functional programming and I had to stare at this for a long time before I understood. So I decided to break this down bit by bit. 1. \"Partial\" fit function the predict function \u00b6 predict_f = partial ( predict , params ) We fix the params variable in the predict function by \"partially fitting\" the function. We are essentially fixing the params variable in the predict function and then reusing this function later. So our function went from predict(param, X) to predict_(X) using the partial function. Note : Why not use the lambda function? This can have problems if for some reason you alter the parameters argument. The lambda still leaves the variable open. Example: if I say params=1 and then I use partial fit, that param will always be 1 for the predict_ function. If I use the following: predict_l = lambda x : predict ( params , x ) now I can change the params to be 2 and this will be reflected in the function. It depends on the application but I would avoid lambda functions when it comes to using vmap as you can easily get recursion issues. Make a decision, fix the function, then batch process it. 2. \"Map\" batches through new function \u00b6 preds = list(map(predict_f, input_batch)) Now we decide to map all of the batch inputs through the predict_f. It's like doing a list comprehension: preds = [predict_f(ix) for ix in input_batch] but it's most succinct. Now our output is a list of all of the predictions. 3. \"stack\" predictions to create an array of outputs \u00b6 preds = np . stack ( preds ) Now we can create the output array but stacking all of the entries on top of each other. We started off with an array of batches, and we looped through all of the inputs. The output from the map function is a list of all batches. And to get an array, we need to stack or concatenate the entries. Note : For a list of 2D arrays, we need to use the np.vstack or np.hstack depending on if we want to stack arrays via the samples or via the features. \"vmap\" batch inputs jax style \u00b6 # create a batch function predict_batch_f = jax . vmap ( partial ( predict , params )) # profit, free inputs with batches predictions = jax . predict_batch_f ( input_batch ) Jax has this convenience function vmap which essentially vectorizes your operations. This is very similar to the numpy vectorize function. However, the numpy implementation isn't for performance whereas the jax vmap function \"just works\" at apparently no extra cost. We've essentially collapsed the stack and map operations into one. So now we can use our predict function above which was created in vector format, to handle batches, essentially for free! More Explicit Version \u00b6 # create a batch function predict_batch_f = jax . vmap ( predict , in_axes = ( None , 0 )) # profit, free inputs with batches predictions = predict_batch_f ( params , input_batch ) Personally, I find this one more readable than the standard Jax implementation. Notice how we don't have to use the partial function because we have specified which arguments are to be vectorized and which no. I think this is better for 3 reasons: It's more readable. I understand exactly which arguments have batch dimensions and which do not. The other makes assumptions that only the first input ( after partially fitting your function) is the one with batch dimensions. If you're function doesn't have too many arguments, then this is a bit cleaner in my opinion. If not, it may be best to fix a number of arguments in your function first to reduce the clutter. You cannot partially fit arguments in whichever order you please. For example, I cannot partially fit the input_batch and leave the params argument free. I don't know why this is the case. There are many tricks to get around this like using a closure but I think it's just easier to use the in_axes arguments and just be explicit. Why do you needs this? \u00b6 Gradients \u00b6 Well, even if you are going to write your code in a way that supports batch sizes, you're probably using this so that you can calculate gradients using the jax.grad function. You can only take a gradients of functions that output a scalar value. So this is the only way you can vectorize the computation without doing explicit loops. Kernel Matrices \u00b6 vv = lambda x , y : np . vdot ( x , y ) mv = jax . vmap ( vv , ( 0 , None ), 0 ) mm = vmap ( mv , ( None , 1 ), 1 )","title":"VMAP"},{"location":"tutorials/jax/vmap/#vmap","text":"This is a really nifty function that allows you to automatically handle batch dimensions without any cost! It allows you to write your code in a vectorized format and then just use this transformation to handle the batching. On the website, they have an example if you have a linear function written in vector format, and you want to make predictions for multiple samples. So an example function is: def predict ( params , input_vector ): w , b = params output_vec = np . dot ( w , input_vector ) + b output_vec = np . tanh ( output_vec ) return output_vec The thing to notice is that the input vector is on the right-hand side of the matrix multiplication. Normally, when we want to handle samples, we do np.dot(inputs, W) because we are using matrix multiplication to handle the samples. So mathematically, a vector would be: y = \\mathbf{w}\\mathbf{x} + b y = \\mathbf{w}\\mathbf{x} + b where \\mathbf{x,w} \\in \\mathbb{R}^{D} \\mathbf{x,w} \\in \\mathbb{R}^{D} are vectors and y,b \\in \\mathbb{R} y,b \\in \\mathbb{R} are scalars. In almost every machine learning tutorial ever , they teach you to do change that for the following: output_vec = np . dot ( w , input_vector ) + b because it is basically implied that we've collected all of our data points into a matrix \\mathbf{X} \\in \\mathbb{R}^{N \\times D} \\mathbf{X} \\in \\mathbb{R}^{N \\times D} where every row is a sample and every column is a feature. It is equivalent to the notation: y = \\mathbf{Xw} + b y = \\mathbf{Xw} + b So we don't have to worry about this step. Well, pivoting off of the vector notation, if we wanted to actually account for samples, then we would need to run our predict function multiple times which (in python) is inefficient and it would take ages. They provide an example of how one could do it using pure functional python. from functools import partial predictions = np . stack ( list ( map ( partial ( predict , params ), input_batch ))) This is a pure functional programming as it features a composition of functions. I'm not well versed in functional programming and I had to stare at this for a long time before I understood. So I decided to break this down bit by bit.","title":"VMAP"},{"location":"tutorials/jax/vmap/#1-partial-fit-function-the-predict-function","text":"predict_f = partial ( predict , params ) We fix the params variable in the predict function by \"partially fitting\" the function. We are essentially fixing the params variable in the predict function and then reusing this function later. So our function went from predict(param, X) to predict_(X) using the partial function. Note : Why not use the lambda function? This can have problems if for some reason you alter the parameters argument. The lambda still leaves the variable open. Example: if I say params=1 and then I use partial fit, that param will always be 1 for the predict_ function. If I use the following: predict_l = lambda x : predict ( params , x ) now I can change the params to be 2 and this will be reflected in the function. It depends on the application but I would avoid lambda functions when it comes to using vmap as you can easily get recursion issues. Make a decision, fix the function, then batch process it.","title":"1. \"Partial\" fit function the predict function"},{"location":"tutorials/jax/vmap/#2-map-batches-through-new-function","text":"preds = list(map(predict_f, input_batch)) Now we decide to map all of the batch inputs through the predict_f. It's like doing a list comprehension: preds = [predict_f(ix) for ix in input_batch] but it's most succinct. Now our output is a list of all of the predictions.","title":"2. \"Map\" batches through new function"},{"location":"tutorials/jax/vmap/#3-stack-predictions-to-create-an-array-of-outputs","text":"preds = np . stack ( preds ) Now we can create the output array but stacking all of the entries on top of each other. We started off with an array of batches, and we looped through all of the inputs. The output from the map function is a list of all batches. And to get an array, we need to stack or concatenate the entries. Note : For a list of 2D arrays, we need to use the np.vstack or np.hstack depending on if we want to stack arrays via the samples or via the features.","title":"3. \"stack\" predictions to create an array of outputs"},{"location":"tutorials/jax/vmap/#vmap-batch-inputs-jax-style","text":"# create a batch function predict_batch_f = jax . vmap ( partial ( predict , params )) # profit, free inputs with batches predictions = jax . predict_batch_f ( input_batch ) Jax has this convenience function vmap which essentially vectorizes your operations. This is very similar to the numpy vectorize function. However, the numpy implementation isn't for performance whereas the jax vmap function \"just works\" at apparently no extra cost. We've essentially collapsed the stack and map operations into one. So now we can use our predict function above which was created in vector format, to handle batches, essentially for free!","title":"\"vmap\" batch inputs jax style"},{"location":"tutorials/jax/vmap/#more-explicit-version","text":"# create a batch function predict_batch_f = jax . vmap ( predict , in_axes = ( None , 0 )) # profit, free inputs with batches predictions = predict_batch_f ( params , input_batch ) Personally, I find this one more readable than the standard Jax implementation. Notice how we don't have to use the partial function because we have specified which arguments are to be vectorized and which no. I think this is better for 3 reasons: It's more readable. I understand exactly which arguments have batch dimensions and which do not. The other makes assumptions that only the first input ( after partially fitting your function) is the one with batch dimensions. If you're function doesn't have too many arguments, then this is a bit cleaner in my opinion. If not, it may be best to fix a number of arguments in your function first to reduce the clutter. You cannot partially fit arguments in whichever order you please. For example, I cannot partially fit the input_batch and leave the params argument free. I don't know why this is the case. There are many tricks to get around this like using a closure but I think it's just easier to use the in_axes arguments and just be explicit.","title":"More Explicit Version"},{"location":"tutorials/jax/vmap/#why-do-you-needs-this","text":"","title":"Why do you needs this?"},{"location":"tutorials/jax/vmap/#gradients","text":"Well, even if you are going to write your code in a way that supports batch sizes, you're probably using this so that you can calculate gradients using the jax.grad function. You can only take a gradients of functions that output a scalar value. So this is the only way you can vectorize the computation without doing explicit loops.","title":"Gradients"},{"location":"tutorials/jax/vmap/#kernel-matrices","text":"vv = lambda x , y : np . vdot ( x , y ) mv = jax . vmap ( vv , ( 0 , None ), 0 ) mm = vmap ( mv , ( None , 1 ), 1 )","title":"Kernel Matrices"},{"location":"tutorials/jax/lab_tutorials/bandwidth/","text":"Bandwidth Approximation \u00b6 Median \u00b6 median = np . argsort ( np . linalg . norm ( np . abs ( dists ), ord = 2 , axis =- 1 ))[ int ( dists . shape [ 0 ] / 2 )] bandwidth = factor * np . abs ( dists )[ median ] ** 2 + 1e-5","title":"Bandwidth Approximation"},{"location":"tutorials/jax/lab_tutorials/bandwidth/#bandwidth-approximation","text":"","title":"Bandwidth Approximation"},{"location":"tutorials/jax/lab_tutorials/bandwidth/#median","text":"median = np . argsort ( np . linalg . norm ( np . abs ( dists ), ord = 2 , axis =- 1 ))[ int ( dists . shape [ 0 ] / 2 )] bandwidth = factor * np . abs ( dists )[ median ] ** 2 + 1e-5","title":"Median"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/","text":"Kernel Derivatives \u00b6 Linear Kernel \u00b6 RBF Kernel \u00b6 k(x,y) = \\exp(-\\gamma ||x-y||_2^2) k(x,y) = \\exp(-\\gamma ||x-y||_2^2) 1 st Derivative \u00b6 We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} For the RBF Kernel, it's this: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) 2. Cross-Covariance Term - 2 nd Derivative \u00b6 Recall the 1 st derivative is: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) So now we repeat. First we decompose the function using the product rule: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial x^j} k(x,y) + k(x,y) \\frac{\\partial }{\\partial x^j} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial x^j} k(x,y) + k(x,y) \\frac{\\partial }{\\partial x^j} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} The first term is basically the 1 st Derivative squared and the 2 nd term is a constant. So after applying the derivative and simplifying, we get: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= 4 \\gamma^2 (x^j - y^j)^2 k(x,y) -2 \\gamma k(x,y)\\\\ &= \\left[ 4\\gamma^2(x^j - y^j)^2 - 2\\gamma\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ &= 2 \\gamma \\left[ 2\\gamma(x^j - y^j)^2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= 4 \\gamma^2 (x^j - y^j)^2 k(x,y) -2 \\gamma k(x,y)\\\\ &= \\left[ 4\\gamma^2(x^j - y^j)^2 - 2\\gamma\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ &= 2 \\gamma \\left[ 2\\gamma(x^j - y^j)^2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ \\end{aligned} 3. Cross-Covariance Term - 2 nd Derivative (Partial Derivatives) \u00b6 Recall the 1 st derivative is: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) So now we repeat. First we decompose the function using the product rule. But this time, we need to do the product rule first w.r.t. x^j x^j and then w.r.t. y^k y^k . \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial y^k} k(x,y) + k(x,y) \\frac{\\partial }{\\partial y^k} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial y^k} k(x,y) + k(x,y) \\frac{\\partial }{\\partial y^k} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} So now let's start expanding and collapsing terms: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= 4 \\gamma^2 (x^j - y^j)(x^k - y^k) k(x,y) \\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= 4 \\gamma^2 (x^j - y^j)(x^k - y^k) k(x,y) \\\\ \\end{aligned} The second term should go to zero and the first term is the same except it has different dimensions (w.r.t. y y instead of x x ). \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y^k)(x^j - y^j) k(\\mathbf{x}, \\mathbf{y}) \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y^k)(x^j - y^j) k(\\mathbf{x}, \\mathbf{y})","title":"Kernel Derivatives"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#kernel-derivatives","text":"","title":"Kernel Derivatives"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#linear-kernel","text":"","title":"Linear Kernel"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#rbf-kernel","text":"k(x,y) = \\exp(-\\gamma ||x-y||_2^2) k(x,y) = \\exp(-\\gamma ||x-y||_2^2)","title":"RBF Kernel"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#1st-derivative","text":"We can calculate the cross-covariance term K_{fg}(\\mathbf{x,x}) K_{fg}(\\mathbf{x,x}) . We apply the following operation K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'}) $$ If we multiply the terms across, we get: $$ K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'} For the RBF Kernel, it's this: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y)","title":"1st Derivative"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#2-cross-covariance-term-2nd-derivative","text":"Recall the 1 st derivative is: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) So now we repeat. First we decompose the function using the product rule: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial x^j} k(x,y) + k(x,y) \\frac{\\partial }{\\partial x^j} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial x^j} k(x,y) + k(x,y) \\frac{\\partial }{\\partial x^j} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} The first term is basically the 1 st Derivative squared and the 2 nd term is a constant. So after applying the derivative and simplifying, we get: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= 4 \\gamma^2 (x^j - y^j)^2 k(x,y) -2 \\gamma k(x,y)\\\\ &= \\left[ 4\\gamma^2(x^j - y^j)^2 - 2\\gamma\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ &= 2 \\gamma \\left[ 2\\gamma(x^j - y^j)^2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &= 4 \\gamma^2 (x^j - y^j)^2 k(x,y) -2 \\gamma k(x,y)\\\\ &= \\left[ 4\\gamma^2(x^j - y^j)^2 - 2\\gamma\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ &= 2 \\gamma \\left[ 2\\gamma(x^j - y^j)^2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\ \\end{aligned}","title":"2. Cross-Covariance Term - 2nd Derivative"},{"location":"tutorials/jax/lab_tutorials/kernel_derivatives/#3-cross-covariance-term-2nd-derivative-partial-derivatives","text":"Recall the 1 st derivative is: \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) \\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y) So now we repeat. First we decompose the function using the product rule. But this time, we need to do the product rule first w.r.t. x^j x^j and then w.r.t. y^k y^k . \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial y^k} k(x,y) + k(x,y) \\frac{\\partial }{\\partial y^k} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= -2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial y^k} k(x,y) + k(x,y) \\frac{\\partial }{\\partial y^k} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\ \\end{aligned} So now let's start expanding and collapsing terms: \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= 4 \\gamma^2 (x^j - y^j)(x^k - y^k) k(x,y) \\\\ \\end{aligned} \\begin{aligned} \\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &= 4 \\gamma^2 (x^j - y^j)(x^k - y^k) k(x,y) \\\\ \\end{aligned} The second term should go to zero and the first term is the same except it has different dimensions (w.r.t. y y instead of x x ). \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y^k)(x^j - y^j) k(\\mathbf{x}, \\mathbf{y}) \\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} = 4 \\gamma^2 (x^k - y^k)(x^j - y^j) k(\\mathbf{x}, \\mathbf{y})","title":"3. Cross-Covariance Term - 2nd Derivative (Partial Derivatives)"},{"location":"tutorials/jax/lab_tutorials/kernels/","text":"Kernel Matrices \u00b6 Linear Kernel \u00b6 def linear_kernel ( x : np . ndarray , y : np . ndarray ) -> float : return np . inner ( x , y ) Polynomial Kernel \u00b6 def polynomial_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : return np . power ( params [ 'alpha' ] * np . inner ( x , y )), params [ 'degree' ] Sigmoid Kernel \u00b6 def sigmoid_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : return np . tanh ( params [ 'alpha' ], np . inner ( x , y ) + c ) RBF Kernel \u00b6 k(x,y) = \\exp(-\\gamma ||x-y||_2^2) k(x,y) = \\exp(-\\gamma ||x-y||_2^2) def rbf_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : \"\"\"The RBF Kernel\"\"\" # calculate the kernel return np . exp ( - params [ 'gamma' ] * ( sqeuclidean_distances ( x , y )) ) ARD Kernel \u00b6 def ard_kernel ( params , x , y ): \"\"\"The RBF Kernel\"\"\" # scale the data x = np . divide ( x , params [ 'length_scale' ]) y = np . divide ( y , params [ 'length_scale' ]) # calculate the kernel return np . exp ( - ( sqeuclidean_distances ( x , y )) ) Gram Matrix \u00b6 Method I - single call def gram ( kernel_func , params , X , Y = None ): if Y is None : return vmap ( lambda x : vmap ( lambda y : kernel_func ( params , x , y ))( X ))( X ) else : return vmap ( lambda x : vmap ( lambda y : kernel_func ( params , x , y ))( Y ))( X ) Method II - multiple calls # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y )","title":"Kernel Matrices"},{"location":"tutorials/jax/lab_tutorials/kernels/#kernel-matrices","text":"","title":"Kernel Matrices"},{"location":"tutorials/jax/lab_tutorials/kernels/#linear-kernel","text":"def linear_kernel ( x : np . ndarray , y : np . ndarray ) -> float : return np . inner ( x , y )","title":"Linear Kernel"},{"location":"tutorials/jax/lab_tutorials/kernels/#polynomial-kernel","text":"def polynomial_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : return np . power ( params [ 'alpha' ] * np . inner ( x , y )), params [ 'degree' ]","title":"Polynomial Kernel"},{"location":"tutorials/jax/lab_tutorials/kernels/#sigmoid-kernel","text":"def sigmoid_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : return np . tanh ( params [ 'alpha' ], np . inner ( x , y ) + c )","title":"Sigmoid Kernel"},{"location":"tutorials/jax/lab_tutorials/kernels/#rbf-kernel","text":"k(x,y) = \\exp(-\\gamma ||x-y||_2^2) k(x,y) = \\exp(-\\gamma ||x-y||_2^2) def rbf_kernel ( params , x : np . ndarray , y : np . ndarray ) -> float : \"\"\"The RBF Kernel\"\"\" # calculate the kernel return np . exp ( - params [ 'gamma' ] * ( sqeuclidean_distances ( x , y )) )","title":"RBF Kernel"},{"location":"tutorials/jax/lab_tutorials/kernels/#ard-kernel","text":"def ard_kernel ( params , x , y ): \"\"\"The RBF Kernel\"\"\" # scale the data x = np . divide ( x , params [ 'length_scale' ]) y = np . divide ( y , params [ 'length_scale' ]) # calculate the kernel return np . exp ( - ( sqeuclidean_distances ( x , y )) )","title":"ARD Kernel"},{"location":"tutorials/jax/lab_tutorials/kernels/#gram-matrix","text":"Method I - single call def gram ( kernel_func , params , X , Y = None ): if Y is None : return vmap ( lambda x : vmap ( lambda y : kernel_func ( params , x , y ))( X ))( X ) else : return vmap ( lambda x : vmap ( lambda y : kernel_func ( params , x , y ))( Y ))( X ) Method II - multiple calls # Covariance Matrix def covariance_matrix ( kernel_func , x , y ): mapx1 = jax . vmap ( lambda x , y : kernel_func ( x , y ), in_axes = ( 0 , None ), out_axes = 0 ) mapx2 = jax . vmap ( lambda x , y : mapx1 ( x , y ), in_axes = ( None , 0 ), out_axes = 1 ) return mapx2 ( x , y )","title":"Gram Matrix"},{"location":"tutorials/jax/lab_tutorials/montecarlo/","text":"MonteCarlo \u00b6 Numpyro \u00b6 Probabilistic programming with numpy by Jax for autograd and JIT compilation to GPU/TPU/CPU. Predict \u00b6 def predict ( model , rng_key , samples , X ): model = handlers . substitute ( handlers . seed ( model , rng_key ), samples ) model_trace = handlers . trace ( model ) . get_trace ( X = X , Y = None ) Y = model_trace [ \"Y\" ][ \"value\" ] return Y Sample \u00b6 def sample ( model , n_samples : int , n_warmup : int , n_chains : int , seed : int , chain_method : str = \"parallel\" , summary : bool = True , ** kwargs : Dict = {}, ): # generate random key rng_key = random . PRNGKey ( seed ) # generate model from NUTS kernel = NUTS ( model ) # Note: sampling mcmc = MCMC ( kernel , n_warmup , n_samples , n_chains , chain_method = chain_method ) mcmc . run ( rng_key , ** kwargs ) if summary : mcmc . print_summary () return mcmc MCX \u00b6 A library to compile probabilitistc programs for performant Inference on CPU & GPU from jax import numpy as np import mcx import mcx.distributions as dist x_data = np . array ([ 2.3 , 8.2 , 1.8 ]) y_data = np . array ([ 1.7 , 7. , 3.1 ]) @mcx . model def linear_regression ( x , lmbda = 1. ): scale @ dist . Exponential ( lmbda ) coefs @ dist . Normal ( np . zeros ( np . shape ( x )[ - 1 ])) y = np . dot ( x , coefs ) predictions @ dist . Normal ( y , scale ) return predictions rng_key = jax . random . PRNGKey ( 0 ) # Sample the model forward, conditioning on the value of `x` mcx . sample_forward ( rng_key , linear_regression , x = x_data , num_samples = 10_000 ) # Sample from the posterior distribution using HMC kernel = mcx . HMC ( step_size = 0.01 , num_integration_steps = 100 , inverse_mass_matrix = np . array ([ 1. , 1. ]), ) observations = { 'x' : x_data , 'predictions' : y_data , 'lmbda' : 3. } sampler = mcx . sample ( rng_key , linear_regression , kernel , ** observations ) trace = sampler . run ()","title":"MonteCarlo"},{"location":"tutorials/jax/lab_tutorials/montecarlo/#montecarlo","text":"","title":"MonteCarlo"},{"location":"tutorials/jax/lab_tutorials/montecarlo/#numpyro","text":"Probabilistic programming with numpy by Jax for autograd and JIT compilation to GPU/TPU/CPU.","title":"Numpyro"},{"location":"tutorials/jax/lab_tutorials/montecarlo/#predict","text":"def predict ( model , rng_key , samples , X ): model = handlers . substitute ( handlers . seed ( model , rng_key ), samples ) model_trace = handlers . trace ( model ) . get_trace ( X = X , Y = None ) Y = model_trace [ \"Y\" ][ \"value\" ] return Y","title":"Predict"},{"location":"tutorials/jax/lab_tutorials/montecarlo/#sample","text":"def sample ( model , n_samples : int , n_warmup : int , n_chains : int , seed : int , chain_method : str = \"parallel\" , summary : bool = True , ** kwargs : Dict = {}, ): # generate random key rng_key = random . PRNGKey ( seed ) # generate model from NUTS kernel = NUTS ( model ) # Note: sampling mcmc = MCMC ( kernel , n_warmup , n_samples , n_chains , chain_method = chain_method ) mcmc . run ( rng_key , ** kwargs ) if summary : mcmc . print_summary () return mcmc","title":"Sample"},{"location":"tutorials/jax/lab_tutorials/montecarlo/#mcx","text":"A library to compile probabilitistc programs for performant Inference on CPU & GPU from jax import numpy as np import mcx import mcx.distributions as dist x_data = np . array ([ 2.3 , 8.2 , 1.8 ]) y_data = np . array ([ 1.7 , 7. , 3.1 ]) @mcx . model def linear_regression ( x , lmbda = 1. ): scale @ dist . Exponential ( lmbda ) coefs @ dist . Normal ( np . zeros ( np . shape ( x )[ - 1 ])) y = np . dot ( x , coefs ) predictions @ dist . Normal ( y , scale ) return predictions rng_key = jax . random . PRNGKey ( 0 ) # Sample the model forward, conditioning on the value of `x` mcx . sample_forward ( rng_key , linear_regression , x = x_data , num_samples = 10_000 ) # Sample from the posterior distribution using HMC kernel = mcx . HMC ( step_size = 0.01 , num_integration_steps = 100 , inverse_mass_matrix = np . array ([ 1. , 1. ]), ) observations = { 'x' : x_data , 'predictions' : y_data , 'lmbda' : 3. } sampler = mcx . sample ( rng_key , linear_regression , kernel , ** observations ) trace = sampler . run ()","title":"MCX"},{"location":"tutorials/jax/lab_tutorials/pairwise/","text":"Pairwise \u00b6 d(x,y) = \\sum_{i=1}^N (x_i - y_i)^2 d(x,y) = \\sum_{i=1}^N (x_i - y_i)^2 Vector Representation \u00b6 def sqeuclidean_distances ( x : np . ndarray , y : np . ndarray ) -> float : return np . sum ( ( x - y ) ** 2 ) Numpy Implementation \u00b6 d ( x , y ) = np . sqrt ( np . dot ( x , x ) - 2.0 * np . dot ( x , y ) + np . dot ( y , y )) Density Ratio Example Pairwise euclidean distances with einsum, dot project and vmap. Einsum \u00b6 XX = np . einsum ( \"ik,ik->i\" , x , x ) YY = np . einsum ( \"ik,ik->i\" , y , y ) XY = np . einsum ( \"ik,jk->ij\" , x , y ) if not square : dists = np . sqrt ( XX [:, np . newaxis ] + YY [ np . newaxis , :] - 2 * XY ) else : dists = XX [:, np . newaxis ] + YY [ np . newaxis , :] - 2 * XY Dot Products \u00b6 XX = np . dot ( x , x ) YY = np . dot ( y , y ) XY = np . dot ( x , y ) if not square : dists = np . sqrt ( XX + YY - 2 * XY ) else : dists = XX + YY - 2 * XY Pairwise Distances \u00b6 dists = jit ( vmap ( vmap ( partial ( dist , ** arg ), in_axes = ( None , 0 )), in_axes = ( 0 , None )))","title":"Pairwise"},{"location":"tutorials/jax/lab_tutorials/pairwise/#pairwise","text":"d(x,y) = \\sum_{i=1}^N (x_i - y_i)^2 d(x,y) = \\sum_{i=1}^N (x_i - y_i)^2","title":"Pairwise"},{"location":"tutorials/jax/lab_tutorials/pairwise/#vector-representation","text":"def sqeuclidean_distances ( x : np . ndarray , y : np . ndarray ) -> float : return np . sum ( ( x - y ) ** 2 )","title":"Vector Representation"},{"location":"tutorials/jax/lab_tutorials/pairwise/#numpy-implementation","text":"d ( x , y ) = np . sqrt ( np . dot ( x , x ) - 2.0 * np . dot ( x , y ) + np . dot ( y , y )) Density Ratio Example Pairwise euclidean distances with einsum, dot project and vmap.","title":"Numpy Implementation"},{"location":"tutorials/jax/lab_tutorials/pairwise/#einsum","text":"XX = np . einsum ( \"ik,ik->i\" , x , x ) YY = np . einsum ( \"ik,ik->i\" , y , y ) XY = np . einsum ( \"ik,jk->ij\" , x , y ) if not square : dists = np . sqrt ( XX [:, np . newaxis ] + YY [ np . newaxis , :] - 2 * XY ) else : dists = XX [:, np . newaxis ] + YY [ np . newaxis , :] - 2 * XY","title":"Einsum"},{"location":"tutorials/jax/lab_tutorials/pairwise/#dot-products","text":"XX = np . dot ( x , x ) YY = np . dot ( y , y ) XY = np . dot ( x , y ) if not square : dists = np . sqrt ( XX + YY - 2 * XY ) else : dists = XX + YY - 2 * XY","title":"Dot Products"},{"location":"tutorials/jax/lab_tutorials/pairwise/#pairwise-distances","text":"dists = jit ( vmap ( vmap ( partial ( dist , ** arg ), in_axes = ( None , 0 )), in_axes = ( 0 , None )))","title":"Pairwise Distances"},{"location":"tutorials/jax/lab_tutorials/resources/","text":"Resources \u00b6 Pairwise Distances \u00b6 Libraries \u00b6 Gaussian Processes \u00b6 LADAX GPs and DeepGPs. Using Layers of distributions. class SVGP ( nn . Module ): def apply ( self , x ): kernel_fn = kernel_provider ( x , ** kernel_fn_kwargs ) inducing_var = inducing_variable_provider ( x , kernel_fn , ** inducing_var_kwargs ) vgp = SVGPLayer ( x , mean_fn , kernel_fn , inducing_var ) return vgp MonteCarlo \u00b6 Numpyro MCX Kernel Methods \u00b6 Kernel Density Estimation - jax_cosmo Gaussian Kernel Polynomial \u00b6 Interpolation - jax_cosmo Normalizing Flows \u00b6 Normalizing Flows in JAX - ChrisWaites Statistical Learning \u00b6 Rethinking Statistical Learning - Fehlepsi","title":"Resources"},{"location":"tutorials/jax/lab_tutorials/resources/#resources","text":"","title":"Resources"},{"location":"tutorials/jax/lab_tutorials/resources/#pairwise-distances","text":"","title":"Pairwise Distances"},{"location":"tutorials/jax/lab_tutorials/resources/#libraries","text":"","title":"Libraries"},{"location":"tutorials/jax/lab_tutorials/resources/#gaussian-processes","text":"LADAX GPs and DeepGPs. Using Layers of distributions. class SVGP ( nn . Module ): def apply ( self , x ): kernel_fn = kernel_provider ( x , ** kernel_fn_kwargs ) inducing_var = inducing_variable_provider ( x , kernel_fn , ** inducing_var_kwargs ) vgp = SVGPLayer ( x , mean_fn , kernel_fn , inducing_var ) return vgp","title":"Gaussian Processes"},{"location":"tutorials/jax/lab_tutorials/resources/#montecarlo","text":"Numpyro MCX","title":"MonteCarlo"},{"location":"tutorials/jax/lab_tutorials/resources/#kernel-methods","text":"Kernel Density Estimation - jax_cosmo Gaussian Kernel","title":"Kernel Methods"},{"location":"tutorials/jax/lab_tutorials/resources/#polynomial","text":"Interpolation - jax_cosmo","title":"Polynomial"},{"location":"tutorials/jax/lab_tutorials/resources/#normalizing-flows","text":"Normalizing Flows in JAX - ChrisWaites","title":"Normalizing Flows"},{"location":"tutorials/jax/lab_tutorials/resources/#statistical-learning","text":"Rethinking Statistical Learning - Fehlepsi","title":"Statistical Learning"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/","text":"Using Jupyter Notebooks for VSCode Remote Computing \u00b6 In this tutorial, I will quickly be going over how one can open up a Jupyter Notebook in VSCode from one that has been activated on a slurm server through an interactive node. 1. Connect to the server via VSCode \u00b6 2. Connect to an interactive node \u00b6 Try to use something explicit like the following command: srun --nodes = 1 --ntasks-per-node = 1 --cpus-per-task = 28 --time 100 :00:00 --exclude = nodo17 --job-name bash-jupyter --pty bash -i 3. Start a Jupyter Notebook \u00b6 conda activate jupyterlab jupyter notebook --ip localhost --port 3001 --no-browser 4. Open Jupyter Notebook in VSCode \u00b6 At this point, something should pop up asking you if you would like to enter a token or your password for your notebook.","title":"Using Jupyter Notebooks for VSCode Remote Computing"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#using-jupyter-notebooks-for-vscode-remote-computing","text":"In this tutorial, I will quickly be going over how one can open up a Jupyter Notebook in VSCode from one that has been activated on a slurm server through an interactive node.","title":"Using Jupyter Notebooks for VSCode Remote Computing"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#1-connect-to-the-server-via-vscode","text":"","title":"1. Connect to the server via VSCode"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#2-connect-to-an-interactive-node","text":"Try to use something explicit like the following command: srun --nodes = 1 --ntasks-per-node = 1 --cpus-per-task = 28 --time 100 :00:00 --exclude = nodo17 --job-name bash-jupyter --pty bash -i","title":"2. Connect to an interactive node"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#3-start-a-jupyter-notebook","text":"conda activate jupyterlab jupyter notebook --ip localhost --port 3001 --no-browser","title":"3. Start a Jupyter Notebook"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#4-open-jupyter-notebook-in-vscode","text":"At this point, something should pop up asking you if you would like to enter a token or your password for your notebook.","title":"4. Open Jupyter Notebook in VSCode"},{"location":"tutorials/manifold_learning/3_ssma/","text":"Semisupervised Manifold Alignment \u00b6 Main References \u00b6 Semisupervised Manifold Alignment of Multimodal Remote Sensing Images - Tuia et al. Classification of Urban Multi-Angular Image Sequences by Aligning their Manifolds - Trolliet et al. Multisensor Alignment of Image Manifolds - Tuia et al. Domain Adaption using Manifold Alignment - Trolliet Outline \u00b6 In this chapter, I introduce the semisupervsied manifold alignment (SSMA) method that was presented in [ linktopaper ]. I give a brief overview of the algorithm in section 1.1 followed by some notation declarations in 1.2. I proceed to talk about the cost function in section 1.3 followed by explicit construction of some cost function components in sections 1.4, 1.5 and 1.6. I give some insight into the projection functions in section 1.7 followed by some advantages of this method with other alignment methods in section 1.8. I conclude this chapter with some insight into the computational complexity of the SSMA method in section 1.9. 1.1 Overview \u00b6 The main idea of the semisupervised manifold alignment (SSMA) method is to align individual manifolds by projecting them into a joint latent space \\mathcal{F} \\mathcal{F} . With respect to remote sensing, the authors of [ linktopaper ] have provided evidence to support the notion that the local geometry of each image dataset is preserved and the regions with similar classes are brought together whilst the regions with dissimilar classes are pushed apart in the SSMA embedding. By using graph Laplacians for each of the similarity and dissimilarity terms, a Rayleigh Quotient function is minimized to extract projection functions. These projection functions are used to project the individual manifolds into a joint latent space. Explicit Algorithm ( in words ) \u00b6 1.2 Notation \u00b6 Let's have a series of M M images with each data matrix X^{m} X^{m} , where m=1,2,\\ldots, M m=1,2,\\ldots, M . Let each matrix X^{m} X^{m} be split into labeled samples \\{x_{i}^{m} \\}^{u_{m}}_{i=1} \\{x_{i}^{m} \\}^{u_{m}}_{i=1} and unlabeled samples \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} . Typically, there are many more labeled samples than unlabeled samples so let's assume l_{m} << u_{m} l_{m} << u_{m} . Let our data matrix be of d_{m}\\text{x }n_{m} d_{m}\\text{x }n_{m} dimensions which says that d d dimensional data by n_{m} n_{m} labeled and unlabeled samples. Succinctly, we have X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} , where n_{m}=l_{m}+u_{m} n_{m}=l_{m}+u_{m} . We can explicitly write each data matrix into a block diagonal matrix X X where X=diag(X_{1}, \\ldots, X_{M}) X=diag(X_{1}, \\ldots, X_{M}) and X \\in \\mathbb{R}^{d\\text{x}N} X \\in \\mathbb{R}^{d\\text{x}N} . X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} N N is the total number of labeled samples for all of the images and d d is the overall dimension of all of the images, i.e. N=\\sum_{m}^{M}n_{m} N=\\sum_{m}^{M}n_{m} and d=\\sum_{m}^{M}d_{m} d=\\sum_{m}^{M}d_{m} . Note: These images do not necessarily need to be exact replicas of the the same spectral density, the same spatial dimension or even the same sensor. Practically, this means that we do not need to have the same m m for each matrix X^{m} X^{m} . 1.3 Semisupervised Loss Function \u00b6 In [ linktopaper ], they construct a semisupervised loss function with the idea of aligning M M images to a common representation. Ultimately, they created M M projection functions \\mathcal{F} \\mathcal{F} that mapped image X^{m} X^{m} from X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} to f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} where m=1,2, \\ldots, M m=1,2, \\ldots, M . They ensure that the projection function f f brings samples from the same class closer together and pushes samples from different classes apart whilst still preserving the geometry of each individual manifold. Abstractly, the aim is to maximize the distance between the dissimilarities between the data sets and minimize the similarities between the datasets. This can be expressed via the following equation: \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} As you can see, if the denominator gets very large then this cost function will get very small. The Rayleigh Quotient is used because the problem has terms that need to be minimized and terms that need to be maximized. F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F F is a d d x d d projection matrix and the row blocks of F F correspond to the domain specific projection functions f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} that project the data matrix X^{m} X^{m} into the joint latent space. The matrix A A corresponds to the term that needs to be minimized and the matrix B B corresponds to the term that needs to be maximized. The authors break the matrix A A into two components, a geometric component G G and a class similarity component, S S . They want to preserve the manifold of each data matrix X^{m} X^{m} and bring same classes closer together in the joint latent subspace manifold. They combine the geometric component, G G and the similarity component, S S to create the discriminant component A A , i.e. A=S+\\mu G A=S+\\mu G where \\mu \\mu is a free parameter to monitor the tradeoff between the local manifold and the class similarity. The authors choose to use three graph Laplacian matrices to construct the terms in the Rayleigh quotient, G G , S S , and B B . 1.4 Geometric Spectral Similarity Term \u00b6 The G G term preserves the manifold of each individual data set X^{m} X^{m} throughout the transformation as no inter-domain relationships are considered. Affinity matrices are constructed for each X^{m} X^{m} and then put into a block diagonal matrix W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} . There are many ways to construct affinity matrices including k k nearest neighbour graphs ( k k -NN), \\epsilon \\epsilon -neighbourhood graphs ( \\epsilon \\epsilon -N) and Gaussian graphs. The graphs are assumed to be undirected so the affinity matrix should be symmetric, i.e. if x_{i} x_{i} is connected to x_{j} x_{j} then x_{j} x_{j} is connected to x_{i} x_{i} . An example of the final matrix W_g W_g is like so: W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} Entries in the matrices W_g^m W_g^m are W_{g}^{m}(i,j)=1 W_{g}^{m}(i,j)=1 if x_{i} x_{i} and x_{j} x_{j} are connected and 0 otherwise. Using the k k -NN weighted graph, this term ensures that the samples in the original m m domain remains in the same proximity in the new latent projected space. A graph laplacian, L_{g}^{m} L_{g}^{m} is constructed from each of the affinity matrices and put into a block diagonal matrix L_g L_g , where L_g^m=D_{g}^{m}-W_{g}^{m} L_g^m=D_{g}^{m}-W_{g}^{m} and D_{g}^{m} D_{g}^{m} is the degree matrix defined as D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) . The laplacian matrix L L is positive semi-definite and symmetric. An example of the final matrix D_g D_g and L_g L_g are below: D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} which just simplifies to L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} The term that needs to be minimized is now: G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) ( Put a note about the graph Laplacian. Maybe an appendix section? ) 1.5 Class Label Similarity Term \u00b6 The S S term ensures that the inter-domain classes are brought together in the joint latent space by pulling labeled samples together. The author use a matrix of class similarities to act as an affinity matrix. Entries in the matrices W_s^{m,m'}(i,j)=1 W_s^{m,m'}(i,j)=1 if the samples between class m m and m' m' have the same label and 0 otherwise where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_s L_s is constructed in the same way as the geometric term is. The term that needs to be minimized is now: S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) 1.6 Class Label Dissimilarity Term \u00b6 The B B term ensures that the inter-domain classes are pushed apart in the joint latent space by pushing different labeled samples apart. The author use a matrix of class dissimilarities to act as an affinity matrix. Entries in the matrices W_d^{m,m'}(i,j)=1 W_d^{m,m'}(i,j)=1 if the samples between class m m and m' m' have different labels and 0 otherwise, where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix is only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_d L_d is constructed in the same way as the geometric term is. The term that needs to be minimized is now: B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} which again simplifies to $$ B = tr(F^T X L_d X^T F) $$ 1.7 Projection Functions \u00b6 We can aggregate all of the graph Laplacians ( L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} ) and then use the Rayleigh Quoitient formulation to get our cost function that needs to be minimized: F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} (I want to talk more about the constraints...) We can find a solution to this minimization problem by looking for the smallest eigenvalues \\gamma_i \\gamma_i of the generalized eigenvalue problem: X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma The optimal solution to the generalized eigenvalue problem contains the projection functions necessary to project each X^m X^m into the joint latent space. If we look at F_{opt} F_{opt} , we can see the matrix will be built as follows: F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} To put it succinctly, we have a projection of X^m X^m from domain m m to the joint latent space F F of dimension, d d : \\mathcal{P}_{f}(X^m)=f^{mT}X^m \\mathcal{P}_{f}(X^m)=f^{mT}X^m Furthermore, any data matrix X^m X^m can be projected into the latent space of data matrix X^{m'} X^{m'} . Let X^p=X^mf^m X^p=X^mf^m be the projection of data matrix X^m X^m into the joint latent space. This can be translated into the latent space of X^{m'} X^{m'} via the translation X^{m1} =X^mf^m(f^{m'})^\\dagger X^{m1} =X^mf^m(f^{m'})^\\dagger where (f^{m'})^\\dagger (f^{m'})^\\dagger is the pseudoinverse of the eigenvectors of domain m' m' . 1.8 Properties and Comparison to other methods \u00b6 The authors outline some significant properties that set asside the SSMA method from other methods that have been used for domain adaption. Linearity \u00b6 This method constructs and defines explicit projection functions that can project data matrices X^m X^m into the joint latent space. That being said, this means that the Multisensor \u00b6 This method only considers the geometry of each individual X^m X^m exclusively and so there is no restriction on the number of hyperspectral image bands nor on the spectral band properties or amount. Multidomain \u00b6 This method can align domains to a joint latent space so there is no limitation on the number of domains. There is no requirement for there to be a leading domain where all other domains are similar to; although it would be wise to do so. PDF-based \u00b6 This method aligns the underlying manifold for each domain and so there is no need for there to be an co-registration of source domains nor does there need to be images with the same spatial or spectral resolution. Invertibility \u00b6 This method creates explicit projection functions that map from one target domain to a joint latent space. Likewise, it is also possible to project data from one target domain to another target domain. 1.9 Computational Complexity \u00b6 The SSMA method is a typical graph-based computational problem. The bulk of the computational effort comes with constructing the graph Laplacians, specifically the geometric term. A k-NN k-NN method is used to create the affinity matrix which can be on order ( order? ). The similarity and dissimilary terms are constructed by convolving vectors with the class labels which results in very sparse matrices. Storing matrices can be memory intense but there exists methods to combat this limitation. (Need to go into methods which can reduce the storage for large matrices.) There can be a significant cost to compute the eigenvalues of the generalized eigenvalue problem which can be of order ( order? ). We can combat the cost by either reducing the size of the problem or improving the algebraic computational process. Some iterative methods can be used to reduce the computational complexity from \\mathcal{O}(d^3) \\mathcal{O}(d^3) to \\mathcal{O}log(d) \\mathcal{O}log(d) . The cost to compute the eigenvalue decomposition can be avoided using faster methods such as approximate random projection singular value decomposition (ARSVD), Jacobi-Davidson QR (JDQR) factorization method, or multigrid methods.","title":"Semisupervised Manifold Alignment"},{"location":"tutorials/manifold_learning/3_ssma/#semisupervised-manifold-alignment","text":"","title":"Semisupervised Manifold Alignment"},{"location":"tutorials/manifold_learning/3_ssma/#main-references","text":"Semisupervised Manifold Alignment of Multimodal Remote Sensing Images - Tuia et al. Classification of Urban Multi-Angular Image Sequences by Aligning their Manifolds - Trolliet et al. Multisensor Alignment of Image Manifolds - Tuia et al. Domain Adaption using Manifold Alignment - Trolliet","title":"Main References"},{"location":"tutorials/manifold_learning/3_ssma/#outline","text":"In this chapter, I introduce the semisupervsied manifold alignment (SSMA) method that was presented in [ linktopaper ]. I give a brief overview of the algorithm in section 1.1 followed by some notation declarations in 1.2. I proceed to talk about the cost function in section 1.3 followed by explicit construction of some cost function components in sections 1.4, 1.5 and 1.6. I give some insight into the projection functions in section 1.7 followed by some advantages of this method with other alignment methods in section 1.8. I conclude this chapter with some insight into the computational complexity of the SSMA method in section 1.9.","title":"Outline"},{"location":"tutorials/manifold_learning/3_ssma/#11-overview","text":"The main idea of the semisupervised manifold alignment (SSMA) method is to align individual manifolds by projecting them into a joint latent space \\mathcal{F} \\mathcal{F} . With respect to remote sensing, the authors of [ linktopaper ] have provided evidence to support the notion that the local geometry of each image dataset is preserved and the regions with similar classes are brought together whilst the regions with dissimilar classes are pushed apart in the SSMA embedding. By using graph Laplacians for each of the similarity and dissimilarity terms, a Rayleigh Quotient function is minimized to extract projection functions. These projection functions are used to project the individual manifolds into a joint latent space.","title":"1.1 Overview"},{"location":"tutorials/manifold_learning/3_ssma/#explicit-algorithm-in-words","text":"","title":"Explicit Algorithm (in words)"},{"location":"tutorials/manifold_learning/3_ssma/#12-notation","text":"Let's have a series of M M images with each data matrix X^{m} X^{m} , where m=1,2,\\ldots, M m=1,2,\\ldots, M . Let each matrix X^{m} X^{m} be split into labeled samples \\{x_{i}^{m} \\}^{u_{m}}_{i=1} \\{x_{i}^{m} \\}^{u_{m}}_{i=1} and unlabeled samples \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} . Typically, there are many more labeled samples than unlabeled samples so let's assume l_{m} << u_{m} l_{m} << u_{m} . Let our data matrix be of d_{m}\\text{x }n_{m} d_{m}\\text{x }n_{m} dimensions which says that d d dimensional data by n_{m} n_{m} labeled and unlabeled samples. Succinctly, we have X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} , where n_{m}=l_{m}+u_{m} n_{m}=l_{m}+u_{m} . We can explicitly write each data matrix into a block diagonal matrix X X where X=diag(X_{1}, \\ldots, X_{M}) X=diag(X_{1}, \\ldots, X_{M}) and X \\in \\mathbb{R}^{d\\text{x}N} X \\in \\mathbb{R}^{d\\text{x}N} . X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} N N is the total number of labeled samples for all of the images and d d is the overall dimension of all of the images, i.e. N=\\sum_{m}^{M}n_{m} N=\\sum_{m}^{M}n_{m} and d=\\sum_{m}^{M}d_{m} d=\\sum_{m}^{M}d_{m} . Note: These images do not necessarily need to be exact replicas of the the same spectral density, the same spatial dimension or even the same sensor. Practically, this means that we do not need to have the same m m for each matrix X^{m} X^{m} .","title":"1.2 Notation"},{"location":"tutorials/manifold_learning/3_ssma/#13-semisupervised-loss-function","text":"In [ linktopaper ], they construct a semisupervised loss function with the idea of aligning M M images to a common representation. Ultimately, they created M M projection functions \\mathcal{F} \\mathcal{F} that mapped image X^{m} X^{m} from X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} to f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} where m=1,2, \\ldots, M m=1,2, \\ldots, M . They ensure that the projection function f f brings samples from the same class closer together and pushes samples from different classes apart whilst still preserving the geometry of each individual manifold. Abstractly, the aim is to maximize the distance between the dissimilarities between the data sets and minimize the similarities between the datasets. This can be expressed via the following equation: \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} As you can see, if the denominator gets very large then this cost function will get very small. The Rayleigh Quotient is used because the problem has terms that need to be minimized and terms that need to be maximized. F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F F is a d d x d d projection matrix and the row blocks of F F correspond to the domain specific projection functions f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} that project the data matrix X^{m} X^{m} into the joint latent space. The matrix A A corresponds to the term that needs to be minimized and the matrix B B corresponds to the term that needs to be maximized. The authors break the matrix A A into two components, a geometric component G G and a class similarity component, S S . They want to preserve the manifold of each data matrix X^{m} X^{m} and bring same classes closer together in the joint latent subspace manifold. They combine the geometric component, G G and the similarity component, S S to create the discriminant component A A , i.e. A=S+\\mu G A=S+\\mu G where \\mu \\mu is a free parameter to monitor the tradeoff between the local manifold and the class similarity. The authors choose to use three graph Laplacian matrices to construct the terms in the Rayleigh quotient, G G , S S , and B B .","title":"1.3 Semisupervised Loss Function"},{"location":"tutorials/manifold_learning/3_ssma/#14-geometric-spectral-similarity-term","text":"The G G term preserves the manifold of each individual data set X^{m} X^{m} throughout the transformation as no inter-domain relationships are considered. Affinity matrices are constructed for each X^{m} X^{m} and then put into a block diagonal matrix W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} . There are many ways to construct affinity matrices including k k nearest neighbour graphs ( k k -NN), \\epsilon \\epsilon -neighbourhood graphs ( \\epsilon \\epsilon -N) and Gaussian graphs. The graphs are assumed to be undirected so the affinity matrix should be symmetric, i.e. if x_{i} x_{i} is connected to x_{j} x_{j} then x_{j} x_{j} is connected to x_{i} x_{i} . An example of the final matrix W_g W_g is like so: W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} Entries in the matrices W_g^m W_g^m are W_{g}^{m}(i,j)=1 W_{g}^{m}(i,j)=1 if x_{i} x_{i} and x_{j} x_{j} are connected and 0 otherwise. Using the k k -NN weighted graph, this term ensures that the samples in the original m m domain remains in the same proximity in the new latent projected space. A graph laplacian, L_{g}^{m} L_{g}^{m} is constructed from each of the affinity matrices and put into a block diagonal matrix L_g L_g , where L_g^m=D_{g}^{m}-W_{g}^{m} L_g^m=D_{g}^{m}-W_{g}^{m} and D_{g}^{m} D_{g}^{m} is the degree matrix defined as D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) . The laplacian matrix L L is positive semi-definite and symmetric. An example of the final matrix D_g D_g and L_g L_g are below: D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} which just simplifies to L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} The term that needs to be minimized is now: G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) ( Put a note about the graph Laplacian. Maybe an appendix section? )","title":"1.4 Geometric Spectral Similarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#15-class-label-similarity-term","text":"The S S term ensures that the inter-domain classes are brought together in the joint latent space by pulling labeled samples together. The author use a matrix of class similarities to act as an affinity matrix. Entries in the matrices W_s^{m,m'}(i,j)=1 W_s^{m,m'}(i,j)=1 if the samples between class m m and m' m' have the same label and 0 otherwise where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_s L_s is constructed in the same way as the geometric term is. The term that needs to be minimized is now: S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F)","title":"1.5 Class Label Similarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#16-class-label-dissimilarity-term","text":"The B B term ensures that the inter-domain classes are pushed apart in the joint latent space by pushing different labeled samples apart. The author use a matrix of class dissimilarities to act as an affinity matrix. Entries in the matrices W_d^{m,m'}(i,j)=1 W_d^{m,m'}(i,j)=1 if the samples between class m m and m' m' have different labels and 0 otherwise, where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix is only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_d L_d is constructed in the same way as the geometric term is. The term that needs to be minimized is now: B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} which again simplifies to $$ B = tr(F^T X L_d X^T F) $$","title":"1.6 Class Label Dissimilarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#17-projection-functions","text":"We can aggregate all of the graph Laplacians ( L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} ) and then use the Rayleigh Quoitient formulation to get our cost function that needs to be minimized: F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} (I want to talk more about the constraints...) We can find a solution to this minimization problem by looking for the smallest eigenvalues \\gamma_i \\gamma_i of the generalized eigenvalue problem: X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma The optimal solution to the generalized eigenvalue problem contains the projection functions necessary to project each X^m X^m into the joint latent space. If we look at F_{opt} F_{opt} , we can see the matrix will be built as follows: F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} To put it succinctly, we have a projection of X^m X^m from domain m m to the joint latent space F F of dimension, d d : \\mathcal{P}_{f}(X^m)=f^{mT}X^m \\mathcal{P}_{f}(X^m)=f^{mT}X^m Furthermore, any data matrix X^m X^m can be projected into the latent space of data matrix X^{m'} X^{m'} . Let X^p=X^mf^m X^p=X^mf^m be the projection of data matrix X^m X^m into the joint latent space. This can be translated into the latent space of X^{m'} X^{m'} via the translation X^{m1} =X^mf^m(f^{m'})^\\dagger X^{m1} =X^mf^m(f^{m'})^\\dagger where (f^{m'})^\\dagger (f^{m'})^\\dagger is the pseudoinverse of the eigenvectors of domain m' m' .","title":"1.7 Projection Functions"},{"location":"tutorials/manifold_learning/3_ssma/#18-properties-and-comparison-to-other-methods","text":"The authors outline some significant properties that set asside the SSMA method from other methods that have been used for domain adaption.","title":"1.8 Properties and Comparison to other methods"},{"location":"tutorials/manifold_learning/3_ssma/#linearity","text":"This method constructs and defines explicit projection functions that can project data matrices X^m X^m into the joint latent space. That being said, this means that the","title":"Linearity"},{"location":"tutorials/manifold_learning/3_ssma/#multisensor","text":"This method only considers the geometry of each individual X^m X^m exclusively and so there is no restriction on the number of hyperspectral image bands nor on the spectral band properties or amount.","title":"Multisensor"},{"location":"tutorials/manifold_learning/3_ssma/#multidomain","text":"This method can align domains to a joint latent space so there is no limitation on the number of domains. There is no requirement for there to be a leading domain where all other domains are similar to; although it would be wise to do so.","title":"Multidomain"},{"location":"tutorials/manifold_learning/3_ssma/#pdf-based","text":"This method aligns the underlying manifold for each domain and so there is no need for there to be an co-registration of source domains nor does there need to be images with the same spatial or spectral resolution.","title":"PDF-based"},{"location":"tutorials/manifold_learning/3_ssma/#invertibility","text":"This method creates explicit projection functions that map from one target domain to a joint latent space. Likewise, it is also possible to project data from one target domain to another target domain.","title":"Invertibility"},{"location":"tutorials/manifold_learning/3_ssma/#19-computational-complexity","text":"The SSMA method is a typical graph-based computational problem. The bulk of the computational effort comes with constructing the graph Laplacians, specifically the geometric term. A k-NN k-NN method is used to create the affinity matrix which can be on order ( order? ). The similarity and dissimilary terms are constructed by convolving vectors with the class labels which results in very sparse matrices. Storing matrices can be memory intense but there exists methods to combat this limitation. (Need to go into methods which can reduce the storage for large matrices.) There can be a significant cost to compute the eigenvalues of the generalized eigenvalue problem which can be of order ( order? ). We can combat the cost by either reducing the size of the problem or improving the algebraic computational process. Some iterative methods can be used to reduce the computational complexity from \\mathcal{O}(d^3) \\mathcal{O}(d^3) to \\mathcal{O}log(d) \\mathcal{O}log(d) . The cost to compute the eigenvalue decomposition can be avoided using faster methods such as approximate random projection singular value decomposition (ARSVD), Jacobi-Davidson QR (JDQR) factorization method, or multigrid methods.","title":"1.9 Computational Complexity"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/","text":"Laplacian Eigenmaps \u00b6 Given m m points \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N , we assume the m m points belong to an n n dimensional manifold where n n is much smaller or equal to N N . Our objective is to find a lower dimensional representation \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n where n<<N n<<N . Step 1. Construct the Adjacency Matrix \u00b6 We build a graph G G whose nodes i i and j j are connected if x_i x_i is among k k -NN or \\epsilon \\epsilon -ball graph. The distances between the data points are measured in the euclidean distance metric. This adjacency matrix A A represents the connectivity of the original data where k k , the number of neighbours, is the free parameter. Step 2. Use the heat kernel as weights for the Adjacency Matrix \u00b6 We want weighted edges on the graph to represent the proximity of the vertices to their adjacent neighbours. One common kernel to use is the diffusion weight matrix, W W . We can define W W like so: W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. Step 3. Solve the eigenvalue problem \u00b6 Let D D be a diagonal matrix D_{ii}= \\sum_{j}W_ij D_{ii}= \\sum_{j}W_ij . We can denote the lower dimensional representation by an m m x n n matrix y=(y_1, y_2, \\ldots, y_m)^T y=(y_1, y_2, \\ldots, y_m)^T where each row vector y_i \\in \\mathcal{R}^n y_i \\in \\mathcal{R}^n . Now, we want to minimize the following cost function \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} which is equivalent to minimizing the following \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) where L=D-W L=D-W is an m m x m m laplacian operator and I I is the identity matrix. The constraint y^TDy=I y^TDy=I denotes... The solution to this minimization problem is given by finding the first n n eigenvalue solutions to the generalized eigenvalue problem: Lf=\\lambda Df Lf=\\lambda Df <span><span class=\"MathJax_Preview\">Lf=\\lambda Df</span><script type=\"math/tex\">Lf=\\lambda Df Step 4. Normalized Eigenvalue Problem (optional) \u00b6 If the graph is fully connected, then \\mathbf{1}=(1,1, \\ldots, 1)^T \\mathbf{1}=(1,1, \\ldots, 1)^T is the only eigenvector with eigenvalue 0. Instead of the above generalized eigenvalue problem, we can solve for the \\text{min tr}(y^TLy) \\text{min tr}(y^TLy) subject to y^TDy=I y^TDy=I and y^TD^{\\frac{1}{2}}y=0 y^TD^{\\frac{1}{2}}y=0 . We can apply the z=D^{\\frac{1}{2}}y z=D^{\\frac{1}{2}}y transformation to yield the following eigenvalue problem: \\text{min tr}(z^T\\mathcal{L}z) \\text{min tr}(z^T\\mathcal{L}z) subject to the constraints z^Tz=I z^Tz=I and z^T\\mathbf{1}=0 z^T\\mathbf{1}=0 where \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} . \\mathcal{L}f=\\lambda f \\mathcal{L}f=\\lambda f ( Need some help understanding the significance of the normalized laplacian. )","title":"6 laplacian eigenmaps"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#laplacian-eigenmaps","text":"Given m m points \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N , we assume the m m points belong to an n n dimensional manifold where n n is much smaller or equal to N N . Our objective is to find a lower dimensional representation \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n where n<<N n<<N .","title":"Laplacian Eigenmaps"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-1-construct-the-adjacency-matrix","text":"We build a graph G G whose nodes i i and j j are connected if x_i x_i is among k k -NN or \\epsilon \\epsilon -ball graph. The distances between the data points are measured in the euclidean distance metric. This adjacency matrix A A represents the connectivity of the original data where k k , the number of neighbours, is the free parameter.","title":"Step 1. Construct the Adjacency Matrix"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-2-use-the-heat-kernel-as-weights-for-the-adjacency-matrix","text":"We want weighted edges on the graph to represent the proximity of the vertices to their adjacent neighbours. One common kernel to use is the diffusion weight matrix, W W . We can define W W like so: W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right.","title":"Step 2. Use the heat kernel as weights for the Adjacency Matrix"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-3-solve-the-eigenvalue-problem","text":"Let D D be a diagonal matrix D_{ii}= \\sum_{j}W_ij D_{ii}= \\sum_{j}W_ij . We can denote the lower dimensional representation by an m m x n n matrix y=(y_1, y_2, \\ldots, y_m)^T y=(y_1, y_2, \\ldots, y_m)^T where each row vector y_i \\in \\mathcal{R}^n y_i \\in \\mathcal{R}^n . Now, we want to minimize the following cost function \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} which is equivalent to minimizing the following \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) where L=D-W L=D-W is an m m x m m laplacian operator and I I is the identity matrix. The constraint y^TDy=I y^TDy=I denotes... The solution to this minimization problem is given by finding the first n n eigenvalue solutions to the generalized eigenvalue problem: Lf=\\lambda Df Lf=\\lambda Df <span><span class=\"MathJax_Preview\">Lf=\\lambda Df</span><script type=\"math/tex\">Lf=\\lambda Df","title":"Step 3. Solve the eigenvalue problem"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-4-normalized-eigenvalue-problem-optional","text":"If the graph is fully connected, then \\mathbf{1}=(1,1, \\ldots, 1)^T \\mathbf{1}=(1,1, \\ldots, 1)^T is the only eigenvector with eigenvalue 0. Instead of the above generalized eigenvalue problem, we can solve for the \\text{min tr}(y^TLy) \\text{min tr}(y^TLy) subject to y^TDy=I y^TDy=I and y^TD^{\\frac{1}{2}}y=0 y^TD^{\\frac{1}{2}}y=0 . We can apply the z=D^{\\frac{1}{2}}y z=D^{\\frac{1}{2}}y transformation to yield the following eigenvalue problem: \\text{min tr}(z^T\\mathcal{L}z) \\text{min tr}(z^T\\mathcal{L}z) subject to the constraints z^Tz=I z^Tz=I and z^T\\mathbf{1}=0 z^T\\mathbf{1}=0 where \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} . \\mathcal{L}f=\\lambda f \\mathcal{L}f=\\lambda f ( Need some help understanding the significance of the normalized laplacian. )","title":"Step 4. Normalized Eigenvalue Problem (optional)"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/","text":"NN from scratch \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com PyTorch Colab Notebook: Link Sources: * What is torch.nn really? - PyTorch Import Data \u00b6 We will be using the california housing dataset. We will do some standard preprocessing including Normalizing the inputs, removing the mean from the outputs, and splitting the data into train, validation and testing. # Import Data cal_housing = datasets . fetch_california_housing () # extract training and targets X = cal_housing . data y = cal_housing . target # Normalize Input data X = StandardScaler () . fit_transform ( X ) # Remove mean from Output data y = StandardScaler ( with_std = False ) . fit_transform ( X ) # split data into training and test Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , test_size = 0.1 , random_state = 0 ) # split training data into train and validation Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtrain , ytrain , train_size = 0.8 , random_state = 123 ) Convert Inputs to Tensors \u00b6 We need to convert the data from np.ndarray to a Tensor . PyTorch # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( torch . FloatTensor , ( Xtrain , ytrain , Xvalid , yvalid ) ) TensorFlow # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( tf . convert_to_tensor , ( Xtrain , ytrain , Xvalid , yvalid ), ) Details One small thing I've noticed is that TensorFlow handles numpy arrays much better than PyTorch. PyTorch is very strict: you need to convert your data to a PyTorch tensor before you start using PyTorch functions. TensorFlow is a bit more flexible sometimes and can convert your data into tf.tensors. Parameters \u00b6 The first thing we need to do is define the components that we're working with. We are doing a simple linear regression problem so we only need the following components: weight matrix: \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} bias vector: b \\in \\mathcal{R}^{D \\times 1} b \\in \\mathcal{R}^{D \\times 1} PyTorch import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ) # bias vector bias = torch . zeros ( output_dim ) Tracking Gradients \u00b6 PyTorch weights . requires_grad_ () bias . requires_grad_ () Model \u00b6 So again, the model is simple: y = \\mathbf{x}_b \\mathbf{W} + b y = \\mathbf{x}_b \\mathbf{W} + b PyTorch # define the model as a function def model ( x_batch : torch . tensor ): return x_batch @ weights + bias batch_size = 64 # mini-batch from training data xb = x_train [: batch_size ] # predictions preds = model ( xb ) # check if there is grad function preds [ 0 ] You should get the following output: tensor([0.3591], grad_fn=<SelectBackward>) That grad_fn lets you know that we're tracking the gradients. Loss Function \u00b6 We're doing a simple loss: mean squared error. \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 PyTorch # define mse loss def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) # set loss function to mse loss_func = mse_loss # get sample batch yb = y_train [: batch_size ] # get loss loss = loss_func ( preds , yb ) # check if there is grad function print ( loss ) Training \u00b6 PyTorch batch_size = 100 learning_rate = 0.01 epochs = 20 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # Loss back propagation loss . backward () # add running loss losses . append ( loss . item ()) # manually calculate gradients with torch . no_grad (): # update the weights individually weights -= learning_rate * weights . grad bias -= learning_rate * bias . grad # zero the weights, bias parameters weights . grad . zero_ () bias . grad . zero_ () # Update status bar postfix = dict ( Epoch = f \" { iepoch } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Your output will look something like this. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00, 2.50it/s, Epoch=19, Loss=0.545] Appendix \u00b6 Extra Python Packages Used \u00b6 scikit-learn The default machine learning package for python. It has a LOT of good algorithms and preprocessing features that are useful. Apparently the biggest use of the sklearn library is the train_test_split function. I'm definitely guilty of that too. tqdm A nice full featured status bar which can help eliminate some of the redundant outputs. typing A built-in type checker. This allows one to type check your inputs. It helps with code readability and to catch silly errors like having bad inputs.","title":"NN from scratch"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#nn-from-scratch","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com PyTorch Colab Notebook: Link Sources: * What is torch.nn really? - PyTorch","title":"NN from scratch"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#import-data","text":"We will be using the california housing dataset. We will do some standard preprocessing including Normalizing the inputs, removing the mean from the outputs, and splitting the data into train, validation and testing. # Import Data cal_housing = datasets . fetch_california_housing () # extract training and targets X = cal_housing . data y = cal_housing . target # Normalize Input data X = StandardScaler () . fit_transform ( X ) # Remove mean from Output data y = StandardScaler ( with_std = False ) . fit_transform ( X ) # split data into training and test Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , test_size = 0.1 , random_state = 0 ) # split training data into train and validation Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtrain , ytrain , train_size = 0.8 , random_state = 123 )","title":"Import Data"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#convert-inputs-to-tensors","text":"We need to convert the data from np.ndarray to a Tensor . PyTorch # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( torch . FloatTensor , ( Xtrain , ytrain , Xvalid , yvalid ) ) TensorFlow # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( tf . convert_to_tensor , ( Xtrain , ytrain , Xvalid , yvalid ), ) Details One small thing I've noticed is that TensorFlow handles numpy arrays much better than PyTorch. PyTorch is very strict: you need to convert your data to a PyTorch tensor before you start using PyTorch functions. TensorFlow is a bit more flexible sometimes and can convert your data into tf.tensors.","title":"Convert Inputs to Tensors"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#parameters","text":"The first thing we need to do is define the components that we're working with. We are doing a simple linear regression problem so we only need the following components: weight matrix: \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} bias vector: b \\in \\mathcal{R}^{D \\times 1} b \\in \\mathcal{R}^{D \\times 1} PyTorch import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ) # bias vector bias = torch . zeros ( output_dim )","title":"Parameters"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#tracking-gradients","text":"PyTorch weights . requires_grad_ () bias . requires_grad_ ()","title":"Tracking Gradients"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#model","text":"So again, the model is simple: y = \\mathbf{x}_b \\mathbf{W} + b y = \\mathbf{x}_b \\mathbf{W} + b PyTorch # define the model as a function def model ( x_batch : torch . tensor ): return x_batch @ weights + bias batch_size = 64 # mini-batch from training data xb = x_train [: batch_size ] # predictions preds = model ( xb ) # check if there is grad function preds [ 0 ] You should get the following output: tensor([0.3591], grad_fn=<SelectBackward>) That grad_fn lets you know that we're tracking the gradients.","title":"Model"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#loss-function","text":"We're doing a simple loss: mean squared error. \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 PyTorch # define mse loss def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) # set loss function to mse loss_func = mse_loss # get sample batch yb = y_train [: batch_size ] # get loss loss = loss_func ( preds , yb ) # check if there is grad function print ( loss )","title":"Loss Function"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#training","text":"PyTorch batch_size = 100 learning_rate = 0.01 epochs = 20 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # Loss back propagation loss . backward () # add running loss losses . append ( loss . item ()) # manually calculate gradients with torch . no_grad (): # update the weights individually weights -= learning_rate * weights . grad bias -= learning_rate * bias . grad # zero the weights, bias parameters weights . grad . zero_ () bias . grad . zero_ () # Update status bar postfix = dict ( Epoch = f \" { iepoch } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Your output will look something like this. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00, 2.50it/s, Epoch=19, Loss=0.545]","title":"Training"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#appendix","text":"","title":"Appendix"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#extra-python-packages-used","text":"scikit-learn The default machine learning package for python. It has a LOT of good algorithms and preprocessing features that are useful. Apparently the biggest use of the sklearn library is the train_test_split function. I'm definitely guilty of that too. tqdm A nice full featured status bar which can help eliminate some of the redundant outputs. typing A built-in type checker. This allows one to type check your inputs. It helps with code readability and to catch silly errors like having bad inputs.","title":"Extra Python Packages Used"},{"location":"tutorials/pytorch_nns/2_refactor/","text":"Refactoring \u00b6 Model (and Parameters) \u00b6 Old Way From Scratch from torch import nn import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) # define model def model ( x_batch : torch . tensor ): return x_batch @ weights + bias # set linear model lr_model = model Refactored class LinearModel ( nn . Module ): \"\"\"A Linear Model Parameters ---------- input_dim : int, The input dimension for the linear model (# input features) output_dim : int, the output Dimension for the linear model (# outputs) Attributes ---------- weights : torch.Tensor (input_dim x output_dim) the parameter for the linear model weights bias : torch.Tensor (output_dim) the parameter for the linear model bias Methods ------- forward : torch.tensor (input_dim x output_dim) the forward pass through the linear model \"\"\" def __init__ ( self , input_dim : int , output_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) def forward ( self , x_batch : torch . tensor ): return x_batch @ self . weights + self . bias input_dim = x_train . shape [ 1 ] output_dim = y_train . shape [ 1 ] lr_model = LinearModel ( input_dim , output_dim ) So we have effectively encapsulated that entire parameter definition within a single understandable function. We have the parameters defined when we initialize the model and then we have the forward method which allows us to perform the operation. Loss Function \u00b6 We can also look and use the built-in loss functions. The mse is a very common loss function so it should be available within the library. Old Way In PyTorch, we need to look at the nn.functional.mse_loss module or the nn.MSELoss() . The latter has more options as it is a class and not a function but the former will do for now. So we can change the old way: def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) Refactored to a simplified version. import torch.nn.functional as F # set loss function to mse loss_func = F . mse_loss Optimizer \u00b6 Another refactor opportunity is to use a built-in optimizer. I don't want to have to calculate the gradient for each of the weights multiplied by the learning rate. from torch import optim learning_rate = 0.01 # use stochastic gradient descent opt = optim . SGD ( lr_model . parameters (), lr = learning_rate ) Training \u00b6 So after all of that hard work, the training procedure will look a lot cleaner because we have encapsulated a lot of operations using the built-in operations. Now we can focus on other things. Refactored batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Datasets and DataLoaders \u00b6 Now there are some extra things we can do to reduce the amount of code and make this neater. We can use Datasets and DataLoaders . Dataset \u00b6 In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. So instead of from torch.utils.data import TensorDataset train_ds = TensorDataset ( x_train , y_train ) lr_model , opt = get_lr_model () batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) DataLoader \u00b6 from torch.utils.data import TensorDataset , DataLoader batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Validation set \u00b6 So because it's so easy, we can now add that validation set. I would have dreaded doing that before due to the lengthy code. But now, it's a piece of cake. from torch.utils.data import TensorDataset , DataLoader # training set batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size , shuffle = True ) # validation set valid_ds = TensorDataset ( x_valid , y_valid ) valid_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 train_losses , valid_losses = [], [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # put in training mode lr_model . train () # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss train_losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) # put in evaluation model lr_model . eval () with torch . no_grad (): for xbatch , ybatch in valid_dl : loss = loss_func ( lr_model ( xbatch ), ybatch ) valid_losses . append ( loss ) Appendix \u00b6 Python Concepts \u00b6 Python Classes More information can be found here . Comments Do them. Always. It might seem like a sunk cost, but it will save you time in the end.","title":"Refactoring"},{"location":"tutorials/pytorch_nns/2_refactor/#refactoring","text":"","title":"Refactoring"},{"location":"tutorials/pytorch_nns/2_refactor/#model-and-parameters","text":"Old Way From Scratch from torch import nn import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) # define model def model ( x_batch : torch . tensor ): return x_batch @ weights + bias # set linear model lr_model = model Refactored class LinearModel ( nn . Module ): \"\"\"A Linear Model Parameters ---------- input_dim : int, The input dimension for the linear model (# input features) output_dim : int, the output Dimension for the linear model (# outputs) Attributes ---------- weights : torch.Tensor (input_dim x output_dim) the parameter for the linear model weights bias : torch.Tensor (output_dim) the parameter for the linear model bias Methods ------- forward : torch.tensor (input_dim x output_dim) the forward pass through the linear model \"\"\" def __init__ ( self , input_dim : int , output_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) def forward ( self , x_batch : torch . tensor ): return x_batch @ self . weights + self . bias input_dim = x_train . shape [ 1 ] output_dim = y_train . shape [ 1 ] lr_model = LinearModel ( input_dim , output_dim ) So we have effectively encapsulated that entire parameter definition within a single understandable function. We have the parameters defined when we initialize the model and then we have the forward method which allows us to perform the operation.","title":"Model (and Parameters)"},{"location":"tutorials/pytorch_nns/2_refactor/#loss-function","text":"We can also look and use the built-in loss functions. The mse is a very common loss function so it should be available within the library. Old Way In PyTorch, we need to look at the nn.functional.mse_loss module or the nn.MSELoss() . The latter has more options as it is a class and not a function but the former will do for now. So we can change the old way: def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) Refactored to a simplified version. import torch.nn.functional as F # set loss function to mse loss_func = F . mse_loss","title":"Loss Function"},{"location":"tutorials/pytorch_nns/2_refactor/#optimizer","text":"Another refactor opportunity is to use a built-in optimizer. I don't want to have to calculate the gradient for each of the weights multiplied by the learning rate. from torch import optim learning_rate = 0.01 # use stochastic gradient descent opt = optim . SGD ( lr_model . parameters (), lr = learning_rate )","title":"Optimizer"},{"location":"tutorials/pytorch_nns/2_refactor/#training","text":"So after all of that hard work, the training procedure will look a lot cleaner because we have encapsulated a lot of operations using the built-in operations. Now we can focus on other things. Refactored batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"Training"},{"location":"tutorials/pytorch_nns/2_refactor/#datasets-and-dataloaders","text":"Now there are some extra things we can do to reduce the amount of code and make this neater. We can use Datasets and DataLoaders .","title":"Datasets and DataLoaders"},{"location":"tutorials/pytorch_nns/2_refactor/#dataset","text":"In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. So instead of from torch.utils.data import TensorDataset train_ds = TensorDataset ( x_train , y_train ) lr_model , opt = get_lr_model () batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"Dataset"},{"location":"tutorials/pytorch_nns/2_refactor/#dataloader","text":"from torch.utils.data import TensorDataset , DataLoader batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"DataLoader"},{"location":"tutorials/pytorch_nns/2_refactor/#validation-set","text":"So because it's so easy, we can now add that validation set. I would have dreaded doing that before due to the lengthy code. But now, it's a piece of cake. from torch.utils.data import TensorDataset , DataLoader # training set batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size , shuffle = True ) # validation set valid_ds = TensorDataset ( x_valid , y_valid ) valid_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 train_losses , valid_losses = [], [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # put in training mode lr_model . train () # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss train_losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) # put in evaluation model lr_model . eval () with torch . no_grad (): for xbatch , ybatch in valid_dl : loss = loss_func ( lr_model ( xbatch ), ybatch ) valid_losses . append ( loss )","title":"Validation set"},{"location":"tutorials/pytorch_nns/2_refactor/#appendix","text":"","title":"Appendix"},{"location":"tutorials/pytorch_nns/2_refactor/#python-concepts","text":"Python Classes More information can be found here . Comments Do them. Always. It might seem like a sunk cost, but it will save you time in the end.","title":"Python Concepts"},{"location":"tutorials/pytorch_nns/_ideas/","text":"PyTorch Ideas \u00b6 1. Neural Networks from scratch \u00b6 BNN 2. \u00b6","title":"PyTorch Ideas"},{"location":"tutorials/pytorch_nns/_ideas/#pytorch-ideas","text":"","title":"PyTorch Ideas"},{"location":"tutorials/pytorch_nns/_ideas/#1-neural-networks-from-scratch","text":"BNN","title":"1. Neural Networks from scratch"},{"location":"tutorials/pytorch_nns/_ideas/#2","text":"","title":"2."},{"location":"tutorials/pytorch_nns/lightining/","text":"PyTorch Lightning \u00b6 Debugging Tips \u00b6 There are 5 validation runs before the training loop starts (built-in) fast_dev_run - runs 1 batch of training and testing data (like compiling) overfit_pct=0.01 - can my model overfit on 1% of my data? The loss should go to 0... train_percent_check=0.1, val_percent_check=0.01 - same thing but with specific numbers Running Accuracy \u00b6 First in the init portion of your code: self . last_accuracies = [] Then in the validation or train step: # append accuracies self . last_accuracies . append ( ... ) val_acc = ... Get all the accuracies: # mean of n previous steps torch . stack ( self . last_accuracies [ - n_previous_steps :] . mean ()) Custom Callbacks \u00b6 You can use the built-in method: trainer = Trainer ( early_stop_callback = True ) This method will look for the 'val_loss' that can be found within the training. Alternatively, you can use a custom callback where you can set whatever loss you would like: early_stop_callback = EarlyStopping ( monitor = 'tc_loss' , min_delta = 0.0 , patience = 3 , verbose = False , mode = 'min' ) trainer = Trainer ( early_stop_callback = early_stop_callback ) Tensorboard \u00b6","title":"PyTorch Lightning"},{"location":"tutorials/pytorch_nns/lightining/#pytorch-lightning","text":"","title":"PyTorch Lightning"},{"location":"tutorials/pytorch_nns/lightining/#debugging-tips","text":"There are 5 validation runs before the training loop starts (built-in) fast_dev_run - runs 1 batch of training and testing data (like compiling) overfit_pct=0.01 - can my model overfit on 1% of my data? The loss should go to 0... train_percent_check=0.1, val_percent_check=0.01 - same thing but with specific numbers","title":"Debugging Tips"},{"location":"tutorials/pytorch_nns/lightining/#running-accuracy","text":"First in the init portion of your code: self . last_accuracies = [] Then in the validation or train step: # append accuracies self . last_accuracies . append ( ... ) val_acc = ... Get all the accuracies: # mean of n previous steps torch . stack ( self . last_accuracies [ - n_previous_steps :] . mean ())","title":"Running Accuracy"},{"location":"tutorials/pytorch_nns/lightining/#custom-callbacks","text":"You can use the built-in method: trainer = Trainer ( early_stop_callback = True ) This method will look for the 'val_loss' that can be found within the training. Alternatively, you can use a custom callback where you can set whatever loss you would like: early_stop_callback = EarlyStopping ( monitor = 'tc_loss' , min_delta = 0.0 , patience = 3 , verbose = False , mode = 'min' ) trainer = Trainer ( early_stop_callback = early_stop_callback )","title":"Custom Callbacks"},{"location":"tutorials/pytorch_nns/lightining/#tensorboard","text":"","title":"Tensorboard"},{"location":"tutorials/remote_computing/copying_files/","text":"Copying Files \u00b6 From Server to Desktop \u00b6 SCP \u00b6 A lot of times you'll get coworkers who can't access or they don't use (or don't want to learn) how to use the server effectively. So they might ask you to help them transfer some files. One way to do it is to use the scp package. The command I use is below. scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/ This works fairly well. And then I'll use wetransfer to send and recieve the files. Done.","title":"Copying Files"},{"location":"tutorials/remote_computing/copying_files/#copying-files","text":"","title":"Copying Files"},{"location":"tutorials/remote_computing/copying_files/#from-server-to-desktop","text":"","title":"From Server to Desktop"},{"location":"tutorials/remote_computing/copying_files/#scp","text":"A lot of times you'll get coworkers who can't access or they don't use (or don't want to learn) how to use the server effectively. So they might ask you to help them transfer some files. One way to do it is to use the scp package. The command I use is below. scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/ This works fairly well. And then I'll use wetransfer to send and recieve the files. Done.","title":"SCP"},{"location":"tutorials/remote_computing/documentation/","text":"Documentation \u00b6 Having good documentation is amazing but doing documentation sucks... - Me It's something we should do but we really don't do. I actually do write a lot of comments in my code ( sometimes ) but I never really present it anywhere for people to see. It's mainly for me so that I can remember what in the world I was doing. I do type-hints and I also use the numpy -style documentation so I'm forced to be a bit verbose in my explanations. But again, actually porting the code to documentation is the problem. In addition, when I'm coding stuff, there is often a lot of theory and mathematics that go behind the scenes. So I need somewhere to put it where it looks nice. Below, I outlined 3 requirements for what I wanted. Markdown : I know it, I love it, and I don't want to use Restructured Text. I find the syntax not very pleasant. Easy - I don't want to work to hard on this as I have other things I want to work on. Nice Format - I like things to look nice. So anything where the outcome is quite attractive is nice for me. Given these requirements, I have outlined below my process for doing documentation. Python Documentation - pdoc \u00b6 So this is the package I use to actually port my comments to actual markdown files. I like the package pdoc . So far, I have found it to be the easiest way to generate documentation from your python files to markdown files. It just works. It's also easy to use over an ssh connection so really removes some of the computational burden I have with my laptop. I have a few useful commands below and near the bottom of the document, I talk about installation. Commands \u00b6 Development For development, use this command. The --http command is so that I can edit my files remotely. pdoc PKG --output-dir DIR --http HOST:PORT Deploying For deployment, I do the exact same thing except I don't actually use the --http argument because there is no need to view it. pdoc PKG --output-dir DIR Webpages - MkDocs \u00b6 This is what I use for generating the documentation. I find it has the most features for the least amount of work. In particular I use the Material for MkDocs version. Almost everything you need is in the configuration file and the rest is just the extensions. The mathjax took a bit to get used to but other than that, everything works fine. Commands \u00b6 Development mkdocs serve --dirtyreload -a localhost:3001 Deploying mkdocs gh-deploy My .yml Config File # Project information site_name: Research Journal site_description: My Personal Research Journal site_author: J. Emmanuel Johnson site_url: https://jejjohnson.github.io/research_journal # Repository repo_name: jejjohnson/research_journal repo_url: https://github.com/jejjohnson/research_journal # Configuration theme: name: material language: en palette: primary: black accent: gray font: text: source code pro code: source code pro plugins: - search - mknotebooks: execute: false write_markdown: true timeout: 600 # Copyright copyright: Copyright & copy ; 2020 J. Emmanuel Johnson markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.codehilite: guess_lang: false - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:pymdownx.emoji.twemoji emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.highlight: linenums_style: pymdownx-inline - pymdownx.inlinehilite - pymdownx.keys # - pymdownx.magiclink: # repo_url_shorthand: true # user: squidfunk # repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde extra_javascript: - javascripts/extra.js - https://polyfill.io/v3/polyfill.min.js?features = es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js # - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML extra: # disqus: XHR39t5kZv social: # - type: 'envelope' # link: 'http://www.shortwhale.com/ericmjl' - icon: fontawesome/brands/github link: 'https://github.com/jejjohnson' - icon: fontawesome/brands/twitter link: 'https://twitter.com/jejjohnson' - icon: fontawesome/brands/linkedin link: 'https://linkedin.com/in/jejjohnson' - icon: fontawesome/solid/globe link: 'https://jejjohnson.netlify.com' My Setup \u00b6 Below are my specifics on how I setup the documentation on my system. Remote Access I have found that I tend to keep a lot of files for my documentation and it can be very heavy. None of the frameworks I've outlined are super light so it can be very taxing on your little laptop. So I highly recommend that you use a server (or desktop station if you have one). I'm fortunate in that I have a server for my work so I use that and that really reduces the amount of computational power that I have to use! Installation \u00b6 I have everything installed under one package. I've never had any dependency issues so I never saw any reason to have separate environments. conda create --name my_docs python = 3 .8 conda activate my_docs pip install mkdocs-material mknotebooks pymdown-extensions pdoc3 Folder Structure \u00b6 Typically I like to have my documentation segmented into 2 main parts: my python package documentation and my specific notes. You'll notice in the tree structure below the PKG as a subdirectory of the docs . The files generated within there are from the pdoc command. The other subdirectories within the docs folder are things that I control myself using the mkdocs . That's how I structure it. I need something automated for the python documentation as well as the \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 PKG \u2502 \u251c\u2500\u2500 Examples \u2502 \u251c\u2500\u2500 Theory \u2502 \u251c\u2500\u2500 Walk-Throughs ... Preferred Style \u00b6 I personally like the numpy documentation style. It takes up quite a lot of vertical space so it makes your code documents a lot longer than they need to be. But I like it because it forces me to really try to explain what in the world is going on. Documentation is good but a lot of times I go digging into the code itself to understand what's going on. And personally I absolutely hate clutter documentation (e.g. when you have 10 arguments and they're all on top of each other with no space in-between the lines, so basically the ReStructuredText style). The Google-style is a bit more spacious but still, the arguments are often on top of each other which is still a bit harder to read for me. So, given that I've explained my preferences, I use the numpy style. Example: Numpy-Style Documentation Here is an example: def function ( arg1 : int , arg2 : float ) -> None : \"\"\"Summary Line Extended description of the function. Parameters ---------- arg1 : int Description of argument 1 arg2 : int Description of argument 2 Returns ------- score : float Description of return value See Also -------- otherFunc: a related function Examples -------- A good (or quick) illustration of how to use the function >>> arg1 = 2 >>> arg2 = 10. >>> function(arg1, arg2) output \"\"\" You can find a full example from the Sphinx webpage. More examples with the numpydoc package for Sphinx. Tip Because it's a pain to do documentation, I usually do it first before I actually code the function. Obviously I will have to change it later. But it actually forces to me think about what I want the function to do before I start coding. It's a sort of...forces me to think about the preliminary requirements or scope of my function beforehand. If I find that I'm writing too many things, then maybe I should shorten the function to do something smaller and more manageable. Makefile \u00b6 Typing in those commands all the time is cumbersome. So I created a Makefile which shortens the command a bit. Makefile # JUPYTER NOTEBOOKS notebooks_to_docs: ## Move notebooks to docs notebooks directory @printf \"\\033[1;34mCreating notebook directory...\\033[0m\\n\" mkdir -p docs/notebooks @printf \"\\033[1;34mRemoving old notebooks...\\033[0m\\n\" rm -rf docs/notebooks/*.ipynb @printf \"\\033[1;34mCopying Notebooks to directory...\\033[0m\\n\" cp notebooks/*.ipynb docs/notebooks @printf \"\\033[1;34mDone!\\033[0m\\n\" jlab_html: mkdir -p docs/notebooks jupyter nbconvert notebooks/*.ipynb --to html --output-dir docs/notebooks/ ##@ Documentation pdoc: ## Generate python API Documentation with pdoc @printf \"\\033[1;34mCreating python API documentation with pdoc...\\033[0m\\n\" pdoc --html --overwrite ${ PKGROOT } --html-dir docs/ @ touch $@ @printf \"\\033[1;34mpdoc completed!\\033[0m\\n\\n\" pdoc-live: ## Start python API live documentation @printf \"\\033[1;34mStarting live documentation with pdoc...\\033[0m\\n\" pdoc ${ PKGROOT } --http localhost:8801 mkdocs: notebooks_to_docs ## Build site documentation with mkdocs @printf \"\\033[1;34mCreating full documentation with mkdocs...\\033[0m\\n\" mkdocs build --config-file mkdocs.yml --clean --theme material --site-dir site/ @printf \"\\033[1;34mmkdocs completed!\\033[0m\\n\\n\" docs-live: notebooks_to_docs pdoc ## Build mkdocs documentation live @printf \"\\033[1;34mStarting live docs with mkdocs...\\033[0m\\n\" mkdocs serve --dev-addr localhost:8802 --theme material docs-live-d: notebooks_to_docs pdoc ## Build mkdocs documentation live (quicker reload) @printf \"\\033[1;34mStarting live docs with mkdocs...\\033[0m\\n\" mkdocs serve --dev-addr localhost:8802 --dirtyreload --theme material docs: pdoc mkdocs ## Build the docs (pdoc, MkDocs) docsdeploy: docs ## Deploy Documentation @printf \"\\033[1;34mDeploying docs...\\033[0m\\n\" mkdocs gh-deploy @printf \"\\033[1;34mDocs delopyed!\\033[0m\\n\\n\"","title":"Documentation"},{"location":"tutorials/remote_computing/documentation/#documentation","text":"Having good documentation is amazing but doing documentation sucks... - Me It's something we should do but we really don't do. I actually do write a lot of comments in my code ( sometimes ) but I never really present it anywhere for people to see. It's mainly for me so that I can remember what in the world I was doing. I do type-hints and I also use the numpy -style documentation so I'm forced to be a bit verbose in my explanations. But again, actually porting the code to documentation is the problem. In addition, when I'm coding stuff, there is often a lot of theory and mathematics that go behind the scenes. So I need somewhere to put it where it looks nice. Below, I outlined 3 requirements for what I wanted. Markdown : I know it, I love it, and I don't want to use Restructured Text. I find the syntax not very pleasant. Easy - I don't want to work to hard on this as I have other things I want to work on. Nice Format - I like things to look nice. So anything where the outcome is quite attractive is nice for me. Given these requirements, I have outlined below my process for doing documentation.","title":"Documentation"},{"location":"tutorials/remote_computing/documentation/#python-documentation-pdoc","text":"So this is the package I use to actually port my comments to actual markdown files. I like the package pdoc . So far, I have found it to be the easiest way to generate documentation from your python files to markdown files. It just works. It's also easy to use over an ssh connection so really removes some of the computational burden I have with my laptop. I have a few useful commands below and near the bottom of the document, I talk about installation.","title":"Python Documentation - pdoc"},{"location":"tutorials/remote_computing/documentation/#commands","text":"Development For development, use this command. The --http command is so that I can edit my files remotely. pdoc PKG --output-dir DIR --http HOST:PORT Deploying For deployment, I do the exact same thing except I don't actually use the --http argument because there is no need to view it. pdoc PKG --output-dir DIR","title":"Commands"},{"location":"tutorials/remote_computing/documentation/#webpages-mkdocs","text":"This is what I use for generating the documentation. I find it has the most features for the least amount of work. In particular I use the Material for MkDocs version. Almost everything you need is in the configuration file and the rest is just the extensions. The mathjax took a bit to get used to but other than that, everything works fine.","title":"Webpages - MkDocs"},{"location":"tutorials/remote_computing/documentation/#commands_1","text":"Development mkdocs serve --dirtyreload -a localhost:3001 Deploying mkdocs gh-deploy My .yml Config File # Project information site_name: Research Journal site_description: My Personal Research Journal site_author: J. Emmanuel Johnson site_url: https://jejjohnson.github.io/research_journal # Repository repo_name: jejjohnson/research_journal repo_url: https://github.com/jejjohnson/research_journal # Configuration theme: name: material language: en palette: primary: black accent: gray font: text: source code pro code: source code pro plugins: - search - mknotebooks: execute: false write_markdown: true timeout: 600 # Copyright copyright: Copyright & copy ; 2020 J. Emmanuel Johnson markdown_extensions: - markdown.extensions.admonition - markdown.extensions.attr_list - markdown.extensions.codehilite: guess_lang: false - markdown.extensions.def_list - markdown.extensions.footnotes - markdown.extensions.meta - markdown.extensions.toc: permalink: true - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:pymdownx.emoji.twemoji emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.highlight: linenums_style: pymdownx-inline - pymdownx.inlinehilite - pymdownx.keys # - pymdownx.magiclink: # repo_url_shorthand: true # user: squidfunk # repo: mkdocs-material - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde extra_javascript: - javascripts/extra.js - https://polyfill.io/v3/polyfill.min.js?features = es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js # - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML extra: # disqus: XHR39t5kZv social: # - type: 'envelope' # link: 'http://www.shortwhale.com/ericmjl' - icon: fontawesome/brands/github link: 'https://github.com/jejjohnson' - icon: fontawesome/brands/twitter link: 'https://twitter.com/jejjohnson' - icon: fontawesome/brands/linkedin link: 'https://linkedin.com/in/jejjohnson' - icon: fontawesome/solid/globe link: 'https://jejjohnson.netlify.com'","title":"Commands"},{"location":"tutorials/remote_computing/documentation/#my-setup","text":"Below are my specifics on how I setup the documentation on my system. Remote Access I have found that I tend to keep a lot of files for my documentation and it can be very heavy. None of the frameworks I've outlined are super light so it can be very taxing on your little laptop. So I highly recommend that you use a server (or desktop station if you have one). I'm fortunate in that I have a server for my work so I use that and that really reduces the amount of computational power that I have to use!","title":"My Setup"},{"location":"tutorials/remote_computing/documentation/#installation","text":"I have everything installed under one package. I've never had any dependency issues so I never saw any reason to have separate environments. conda create --name my_docs python = 3 .8 conda activate my_docs pip install mkdocs-material mknotebooks pymdown-extensions pdoc3","title":"Installation"},{"location":"tutorials/remote_computing/documentation/#folder-structure","text":"Typically I like to have my documentation segmented into 2 main parts: my python package documentation and my specific notes. You'll notice in the tree structure below the PKG as a subdirectory of the docs . The files generated within there are from the pdoc command. The other subdirectories within the docs folder are things that I control myself using the mkdocs . That's how I structure it. I need something automated for the python documentation as well as the \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 PKG \u2502 \u251c\u2500\u2500 Examples \u2502 \u251c\u2500\u2500 Theory \u2502 \u251c\u2500\u2500 Walk-Throughs ...","title":"Folder Structure"},{"location":"tutorials/remote_computing/documentation/#preferred-style","text":"I personally like the numpy documentation style. It takes up quite a lot of vertical space so it makes your code documents a lot longer than they need to be. But I like it because it forces me to really try to explain what in the world is going on. Documentation is good but a lot of times I go digging into the code itself to understand what's going on. And personally I absolutely hate clutter documentation (e.g. when you have 10 arguments and they're all on top of each other with no space in-between the lines, so basically the ReStructuredText style). The Google-style is a bit more spacious but still, the arguments are often on top of each other which is still a bit harder to read for me. So, given that I've explained my preferences, I use the numpy style. Example: Numpy-Style Documentation Here is an example: def function ( arg1 : int , arg2 : float ) -> None : \"\"\"Summary Line Extended description of the function. Parameters ---------- arg1 : int Description of argument 1 arg2 : int Description of argument 2 Returns ------- score : float Description of return value See Also -------- otherFunc: a related function Examples -------- A good (or quick) illustration of how to use the function >>> arg1 = 2 >>> arg2 = 10. >>> function(arg1, arg2) output \"\"\" You can find a full example from the Sphinx webpage. More examples with the numpydoc package for Sphinx. Tip Because it's a pain to do documentation, I usually do it first before I actually code the function. Obviously I will have to change it later. But it actually forces to me think about what I want the function to do before I start coding. It's a sort of...forces me to think about the preliminary requirements or scope of my function beforehand. If I find that I'm writing too many things, then maybe I should shorten the function to do something smaller and more manageable.","title":"Preferred Style"},{"location":"tutorials/remote_computing/documentation/#makefile","text":"Typing in those commands all the time is cumbersome. So I created a Makefile which shortens the command a bit. Makefile # JUPYTER NOTEBOOKS notebooks_to_docs: ## Move notebooks to docs notebooks directory @printf \"\\033[1;34mCreating notebook directory...\\033[0m\\n\" mkdir -p docs/notebooks @printf \"\\033[1;34mRemoving old notebooks...\\033[0m\\n\" rm -rf docs/notebooks/*.ipynb @printf \"\\033[1;34mCopying Notebooks to directory...\\033[0m\\n\" cp notebooks/*.ipynb docs/notebooks @printf \"\\033[1;34mDone!\\033[0m\\n\" jlab_html: mkdir -p docs/notebooks jupyter nbconvert notebooks/*.ipynb --to html --output-dir docs/notebooks/ ##@ Documentation pdoc: ## Generate python API Documentation with pdoc @printf \"\\033[1;34mCreating python API documentation with pdoc...\\033[0m\\n\" pdoc --html --overwrite ${ PKGROOT } --html-dir docs/ @ touch $@ @printf \"\\033[1;34mpdoc completed!\\033[0m\\n\\n\" pdoc-live: ## Start python API live documentation @printf \"\\033[1;34mStarting live documentation with pdoc...\\033[0m\\n\" pdoc ${ PKGROOT } --http localhost:8801 mkdocs: notebooks_to_docs ## Build site documentation with mkdocs @printf \"\\033[1;34mCreating full documentation with mkdocs...\\033[0m\\n\" mkdocs build --config-file mkdocs.yml --clean --theme material --site-dir site/ @printf \"\\033[1;34mmkdocs completed!\\033[0m\\n\\n\" docs-live: notebooks_to_docs pdoc ## Build mkdocs documentation live @printf \"\\033[1;34mStarting live docs with mkdocs...\\033[0m\\n\" mkdocs serve --dev-addr localhost:8802 --theme material docs-live-d: notebooks_to_docs pdoc ## Build mkdocs documentation live (quicker reload) @printf \"\\033[1;34mStarting live docs with mkdocs...\\033[0m\\n\" mkdocs serve --dev-addr localhost:8802 --dirtyreload --theme material docs: pdoc mkdocs ## Build the docs (pdoc, MkDocs) docsdeploy: docs ## Deploy Documentation @printf \"\\033[1;34mDeploying docs...\\033[0m\\n\" mkdocs gh-deploy @printf \"\\033[1;34mDocs delopyed!\\033[0m\\n\\n\"","title":"Makefile"},{"location":"tutorials/remote_computing/vscode_jlab/","text":"Remote Machines: JupyterLab + VSCode \u00b6 J. Emmanuel Johnson Last Updated: 17-07-2020 So most people like to use a combination of a dedicated IDE as well as JupyterLab. If you don't you probably should... But it's a bit annoying when we need both. Some people try to use the built-in jupyter notebook support from VSCode. But it sucks. It's not good enough and it's quite slow compared to JupyterLab. Another thing people do is they use the Notebook Instances from the GCP webpage. This is convenient but the biggest problem with this is that it's not in your home directory. So you have to play games with the directories which is a pain in the butt. In addition, the permissions are weird so some python code doesn't play nice when you want to do execute commands using python (and sometimes the terminal - need sudo ). So this tutorial provides the following: a simple way to open jupyterlab with your vscode ide you don't have to do the ssh server -L xxxx:localhost:xxxx with the extra port you will be able to access all of your other conda environments using this jupyterlab makes working with jupyterlab in conjunction with vscode a lot easier. 1. Connect VSCode to your VM \u00b6 2. Setup Your JupyterLab Environment \u00b6 Note : You only have to do this once! Make sure conda is already installed. 2.1 Create a .yml file with requirements \u00b6 name : jupyterlab channels : - defaults - conda-forge dependencies : - python=3.8 # GUI - conda-forge::jupyterlab # JupyterLab GUI - conda-forge::nb_conda_kernels # Access to other conda kernels - conda-forge::spyder-kernels # Access via spyder kernels - conda-forge::nodejs # for extensions in jupyterlab - pyviz::holoviews - bokeh - bokeh::jupyter_bokeh # Bokeh - tqdm # For status bars - pip # To install other packages - pip : - ipykernel - ipywidgets - jupyter-server-proxy - dask_labextension - nbserverproxy I've typed it out but you can also check out the file on github . 2.2 JupyterLab and other python kernels \u00b6 So you may be wondering if we need to do this with every conda environment we create. No. We just need to have a general JupyterLab environment that calls other environments. The important thing here is that we have the jupyterlab package installed as well as nb_conda_kernels package. This allows the jupyterlab to be able to use any other python kernel that's in your user space (sometimes common shared ones but it depends). Now, all other conda environments will need to have the ipykernel package installed and it will be visible from your JupyterLab environment. 2.1 Create a Conda Environment \u00b6 # create the environment with your .yml file conda env create --name jupyterlab -f jupyterlab.yml # activate the environment source activate jupyterlab # or perhaps # conda activate jupyterlab 2.3 Install and the Jupyterlab Manager \u00b6 This will enable you to have extensions for your Jupyterlab. There are so many cool ones out there. I'm particularly fond of the variable inspector and the table of contents . JupyterLab has gotten awesome so you can install most new extensions using the JupyterLab GUI. # Install jupyter lab extension maager jupyter labextension install @jupyter-widgets/jupyterlab-manager # Enable jupyter serverextension enable --py jupyterlab-manager 3 Start your Jupyterlab Instance through VSCode terminal \u00b6 jupyter-lab --no-browser --port 2005 This should appear: The most important thing is that this should appear. Notice now that you have two links to use and you can click on them. Now you're good! Your browser should open a JupyterLab notebook on it's own! What Happened? \u00b6 Well VSCode rocks and basically opened an ssh port through vscode itself. So now we can access it through our browser as if we did the ssh stuff ourselves. 3.1 Bonus - Automatic Function \u00b6 We can automate this to have a bit more flexibility on the port number. I added this function to my .profile (you can use .bashrc or .zshrc ) and now I can just call this function whenever I need to open a JupyterLab using the VSCode terminal. # JUPYTER NOTEBOOK STUFF function jpt(){ # Fires-up a Jupyter notebook by supplying a specific port conda activate jupyterlab jupyter-lab --no-browser --port=$1 } 3.2 Bonus - Outside of VSCode \u00b6 One caveat is that you need to have VSCode open for your JupyterLab to run. So if you close it, it closed the JupyterLab session. One thing you could do is open another ssh port using the gcloud command. # sign in and have a port open gcloud compute ssh --project XXX --zone XXXX USER@VM-INSTANCE -- -L 2005:localhost:2005 # start jupyterlab conda activate jupyterlab jupyter-lab --no-browser --port 2005","title":"Remote Machines: JupyterLab + VSCode"},{"location":"tutorials/remote_computing/vscode_jlab/#remote-machines-jupyterlab-vscode","text":"J. Emmanuel Johnson Last Updated: 17-07-2020 So most people like to use a combination of a dedicated IDE as well as JupyterLab. If you don't you probably should... But it's a bit annoying when we need both. Some people try to use the built-in jupyter notebook support from VSCode. But it sucks. It's not good enough and it's quite slow compared to JupyterLab. Another thing people do is they use the Notebook Instances from the GCP webpage. This is convenient but the biggest problem with this is that it's not in your home directory. So you have to play games with the directories which is a pain in the butt. In addition, the permissions are weird so some python code doesn't play nice when you want to do execute commands using python (and sometimes the terminal - need sudo ). So this tutorial provides the following: a simple way to open jupyterlab with your vscode ide you don't have to do the ssh server -L xxxx:localhost:xxxx with the extra port you will be able to access all of your other conda environments using this jupyterlab makes working with jupyterlab in conjunction with vscode a lot easier.","title":"Remote Machines: JupyterLab + VSCode"},{"location":"tutorials/remote_computing/vscode_jlab/#1-connect-vscode-to-your-vm","text":"","title":"1. Connect VSCode to your VM"},{"location":"tutorials/remote_computing/vscode_jlab/#2-setup-your-jupyterlab-environment","text":"Note : You only have to do this once! Make sure conda is already installed.","title":"2. Setup Your JupyterLab Environment"},{"location":"tutorials/remote_computing/vscode_jlab/#21-create-a-yml-file-with-requirements","text":"name : jupyterlab channels : - defaults - conda-forge dependencies : - python=3.8 # GUI - conda-forge::jupyterlab # JupyterLab GUI - conda-forge::nb_conda_kernels # Access to other conda kernels - conda-forge::spyder-kernels # Access via spyder kernels - conda-forge::nodejs # for extensions in jupyterlab - pyviz::holoviews - bokeh - bokeh::jupyter_bokeh # Bokeh - tqdm # For status bars - pip # To install other packages - pip : - ipykernel - ipywidgets - jupyter-server-proxy - dask_labextension - nbserverproxy I've typed it out but you can also check out the file on github .","title":"2.1 Create a .yml file with requirements"},{"location":"tutorials/remote_computing/vscode_jlab/#22-jupyterlab-and-other-python-kernels","text":"So you may be wondering if we need to do this with every conda environment we create. No. We just need to have a general JupyterLab environment that calls other environments. The important thing here is that we have the jupyterlab package installed as well as nb_conda_kernels package. This allows the jupyterlab to be able to use any other python kernel that's in your user space (sometimes common shared ones but it depends). Now, all other conda environments will need to have the ipykernel package installed and it will be visible from your JupyterLab environment.","title":"2.2 JupyterLab and other python kernels"},{"location":"tutorials/remote_computing/vscode_jlab/#21-create-a-conda-environment","text":"# create the environment with your .yml file conda env create --name jupyterlab -f jupyterlab.yml # activate the environment source activate jupyterlab # or perhaps # conda activate jupyterlab","title":"2.1 Create a Conda Environment"},{"location":"tutorials/remote_computing/vscode_jlab/#23-install-and-the-jupyterlab-manager","text":"This will enable you to have extensions for your Jupyterlab. There are so many cool ones out there. I'm particularly fond of the variable inspector and the table of contents . JupyterLab has gotten awesome so you can install most new extensions using the JupyterLab GUI. # Install jupyter lab extension maager jupyter labextension install @jupyter-widgets/jupyterlab-manager # Enable jupyter serverextension enable --py jupyterlab-manager","title":"2.3 Install and the Jupyterlab Manager"},{"location":"tutorials/remote_computing/vscode_jlab/#3-start-your-jupyterlab-instance-through-vscode-terminal","text":"jupyter-lab --no-browser --port 2005 This should appear: The most important thing is that this should appear. Notice now that you have two links to use and you can click on them. Now you're good! Your browser should open a JupyterLab notebook on it's own!","title":"3 Start your Jupyterlab Instance through VSCode terminal"},{"location":"tutorials/remote_computing/vscode_jlab/#what-happened","text":"Well VSCode rocks and basically opened an ssh port through vscode itself. So now we can access it through our browser as if we did the ssh stuff ourselves.","title":"What Happened?"},{"location":"tutorials/remote_computing/vscode_jlab/#31-bonus-automatic-function","text":"We can automate this to have a bit more flexibility on the port number. I added this function to my .profile (you can use .bashrc or .zshrc ) and now I can just call this function whenever I need to open a JupyterLab using the VSCode terminal. # JUPYTER NOTEBOOK STUFF function jpt(){ # Fires-up a Jupyter notebook by supplying a specific port conda activate jupyterlab jupyter-lab --no-browser --port=$1 }","title":"3.1 Bonus - Automatic Function"},{"location":"tutorials/remote_computing/vscode_jlab/#32-bonus-outside-of-vscode","text":"One caveat is that you need to have VSCode open for your JupyterLab to run. So if you close it, it closed the JupyterLab session. One thing you could do is open another ssh port using the gcloud command. # sign in and have a port open gcloud compute ssh --project XXX --zone XXXX USER@VM-INSTANCE -- -L 2005:localhost:2005 # start jupyterlab conda activate jupyterlab jupyter-lab --no-browser --port 2005","title":"3.2 Bonus - Outside of VSCode"},{"location":"tutorials/tf2/slides/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson What is Deep Learning? \u00b6 Deep Learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. - Yann LeCun Deep Learning is a collection of tools to build complex modular differentiable functions. - Danilo Rezende It's more or less a tool... \u00b6 Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Software Perspective \u00b6 Who is your audience? What's your scope? Modular design Influencing minds... User 1 \u00b6 My employer gave me some data of landmass in Africa and wants me to find some huts. He thinks Deep Learning can help. User 2 \u00b6 I think I would like one network for my X X and y y . I also think maybe I should have another network with shared weights and a latent space. Maybe I coud also have two or three input locations. In addition... User 3 \u00b6 I want to implement a Neural Network with convolutional layers and a noise contrastive prior. The weights of the network will be parameterized by Normal distributions. I would also like a training scheme with a mixture of Importance sampling and variational inference with a custom KLD loss. One Deep Learning library to rule them all...! Probably a bad idea... Deep Learning Library Gold Rush \u00b6 Currently more than 10+ mainstream libraries All tech companies want a piece Growth of PyTorch \u00b6 Why? \u00b6 Simple (Pythonic) Great API Performance vs Productivity Tradeoff Easy to Install... Game: Which Library? \u00b6 My Suggestions \u00b6 Productivity: Fastai From Scratch: JAX Research: PyTorch Production/Industry: TensorFlow Basics \u00b6 Tensors Variables Automatic differentiation (AutoGrad) Tensors \u00b6 Constants \u00b6 # create constant x = tf . constant ([[ 5 , 2 ], [ 1 , 3 ]]) print ( x ) tf.Tensor( [[5 2] [1 3]], shape=(2, 2), dtype=int32) \u00b6 Standard \u00b6 # create ones tensor t_ones = tf . ones ( shape = ( 2 , 1 )) # create zeros tensor t_zeros = tf . zeros ( shape = ( 2 , 1 )) \u00b6 Standard Randomized \u00b6 # pretty standard tf . random . normal ( shape = ( 2 , 2 ), mean = 0. , stddev = 1. ) # pretty much the same tf . random . uniform ( shape = ( 2 , 2 ), minval = 0 , maxval = 10 ) Variables \u00b6 # set initial value initial_value = tf . random . normal ( shape = ( 2 , 2 )) # set variable a = tf . Variable ( initial_value ) Options (constraint, trainable, shape) All math operations Updates \u00b6 # new value b = tf . random . uniform ( shape = ( 2 , 2 )) # set value a . assign ( b ) # increment (a + b) a . assign_add ( b ) # dencrement (a - b) a . assign_sub ( new_a ) Gradients \u00b6 Gradient Function \u00b6 # init variable a = tf . Variable ( init_value ) # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # calculate gradient ( dc/da ) dc_da = tf . gradients ( c , a ) # calculate multiple gradients dc_da , dc_db = tf . gradients ( c , [ a , b ]) New : GradientTape Defines the scope literally \"record operations\" # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) Nested Gradients \u00b6 # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as outer_tape : with tf . GradientTape () as inner_tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) # extract gradients ( d2c/da2 ) d2c_da2 = outer_tape . gradient ( dc_da , a ) Gradients in PyTorch \u00b6 Same gradient function torch.autograd.grad There is no Tape Each variable has their own gradient # init variable a = torch . tensor ( init_value , requires_grad = True ) # do operation c = math . sqrt ( a ** 2 + b ** 2 ) # calculate gradients ( dc/da ) c . backward ( a ) # extract gradients dc_da = a . grad TF: Engine Module \u00b6 Layer Network - DAG graph Model Sequential Various Subclasses \u00b6 Layers Metric Loss Callbacks Optimizer Regularizers, Constraints Layer Class \u00b6 The core abstraction Everything is a Layer ...or interacts with a layer Example Layer \u00b6 y = \\mathbf{W}x + b y = \\mathbf{W}x + b # Subclass Layer class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () # Make Parameters def call ( self , inputs ): # Do stuff return inputs 1 - Constructor \u00b6 # Inherit Layer class class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () 2 - Parameters, \\mathbf{W} \\mathbf{W} \u00b6 # initialize weights (random) w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) 2 - Parameter, b b \u00b6 # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) 3 - Call Function, \\mathbf{W}x +b \\mathbf{W}x +b \u00b6 def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b PyTorch (the same...) \u00b6 class Linear ( nn . Module ): def __init__ ( self , units : int , input_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , units ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( units ), requires_grad = True ) def forward ( self , inputs ): return inputs @ self . weights + self . bias Using it \u00b6 # data x_train = ... # initialize linear layer linear_layer = Linear ( units = 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x ) TensorFlow build \u00b6 Know the # of nodes Don't know the input shape More conventional For example... def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) More convenient... # data x_train = ... # initialize linear layer (without input dims) linear_layer = Linear ( units = 4 ) # internally -> calls x.shape y = linear_layer ( x ) We can nest as many Layers as we want. Linear \u00b6 class Linear ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () # call linear layer self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x Linear Block \u00b6 class LinearBlock ( Layer ): def __init__ ( self ): super () . __init__ () self . lin_1 = Linear ( 32 ) self . lin_2 = Linear ( 32 ) self . lin_3 = Linear ( 1 ) def call ( self , inputs ): x = self . lin_1 ( x ) x = self . lin_2 ( x ) x = self . lin_3 ( x ) return x Training TF2.X, PyTorch \u00b6 Losses \u00b6 TensorFlow # example loss function loss_func = torch . nn . MSELoss () PyTorch # example loss function loss_fn = tf . keras . losses . MSELoss () Optimizers \u00b6 TensorFlow # example optimizer optimizer = tf . keras . optimizers . Adam () PyTorch # example optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.01 ) Full Training Loop (PyTorch) \u00b6 # Loop through batches for x , y in dataset : # initialize gradients optimizer . zero_grad () # predictions for minibatch ypred = lr_model ( xbatch ) # loss value for minibatch loss = loss_func ( ypred , ybatch ) # find gradients loss . backward () # apply optimization optimizer . step () Full Training Loop (TF2.X) \u00b6 for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , model . trainable_weights )) TensorFlow Nuggets \u00b6 Training Call \u00b6 Allows training versus inference mode Just need an extra argument training=True in the call method Prob Models, e.g. Batch Norm., Variational Inference Example \u00b6 ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Add Loss \u00b6 \"Add Losses on the fly\" Each layer has it's own regularization Examples: KLD, Activation or Weight Regularization Example - Model \u00b6 class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) # Add loss during the call self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x \u00b6 Example - Training \u00b6 mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function opt = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) Compile Code \u00b6 Use a decorator, @tf.function Optional Easy performance booster Example - Graphs \u00b6 @tf . function def train_step ( dataset ): for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) return loss Model Class \u00b6 Can do everything a Layer can do Built-in functionality a.k.a. Keras territory TF and PyTorch part ways Definitions \u00b6 Layer : A closed sequence of operation e.g. convolutional layer, recurrent layer, resnet block, attention block. Model : The top layer of your algorithm e.g. Deep learning model, deep neural network. Training Functionality \u00b6 .compile() .fit() .evaulate() .predict() .save() .summary() .plot_model() Example - \u00b6 # loss function loss = tf . keras . losses . MSELoss ( from_logits = True ) # accuracy metrics accuracy = tf . keras . metrics . SparseCategoricalAccuracy () # optimizer optimizer = tf . keras . optimizers . Adam () # compile to graph model . compile ( optimizer = optimizer , loss = loss , metrics = [ accuracy ]) # Fit Model model . fit ( dataset , epochs = 3 ) # Test Data loss , acc = model . evaluate ( test_dataset ) Functional Models \u00b6 Creates DAG Model Class with Extras Only in TF Simple Example \u00b6 # input checks x = tf . keras . layers . Flatten ( shape = 28 , 28 ))( inputs ) # Layer 1 x = tf . keras . layers . Dense ( 512 , activation = tf . nn . relu )( inputs ) # Layer 2 x = tf . keras . layers . Dropout ( 0.2 )( x ) # outputs x = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )( x ) # create model class model = tf . keras . Model ( inputs , outputs ) # compile model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ] ) Example - Graph Output \u00b6 We can go crazy... \u00b6 Sequential Models \u00b6 Predifined PyTorch & TF In TF, Model class PyTorch model = nn . Sequential ( torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 10 ), ) TensorFlow model = tf . keras . Sequential ([ layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 10 ) ]) Datasets \u00b6 Convenience Functions Take care of loading, iterations, batches Normally \u00b6 n_batches = ( n_samples - 1 ) // batch_size + 1 for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # get subset from data xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] PyTorch - Datasets \u00b6 # create dataset train_ds = TensorDataset ( x_train , y_train ) # Loop through batches for start_idx , end_idx in range ( batch_idx ): # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # Do stuff... Note: In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. PyTorch - DataLoaders \u00b6 # create dataset train_ds = TensorDataset ( x_train , y_train ) # create dataloader train_dl = DataLoader ( train_ds , batch_size = 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff... TF - Both... \u00b6 # create dataset train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) # create dataloader train_dl = train_ds . batch ( 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff... What We Covered \u00b6 DL Framework Idea Layers and Models Sequential Model What We didn't Cover \u00b6 Callbacks Distributed Training Multiple GPUs All options under the sun Tensorboard (Built-in Jupyter Notebooks!) Summary \u00b6 TensorFlow Training \u00b6","title":"Slides"},{"location":"tutorials/tf2/slides/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"tutorials/tf2/slides/#what-is-deep-learning","text":"Deep Learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. - Yann LeCun Deep Learning is a collection of tools to build complex modular differentiable functions. - Danilo Rezende","title":"What is Deep Learning?"},{"location":"tutorials/tf2/slides/#its-more-or-less-a-tool","text":"Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions","title":"It's more or less a tool..."},{"location":"tutorials/tf2/slides/#software-perspective","text":"Who is your audience? What's your scope? Modular design Influencing minds...","title":"Software Perspective"},{"location":"tutorials/tf2/slides/#user-1","text":"My employer gave me some data of landmass in Africa and wants me to find some huts. He thinks Deep Learning can help.","title":"User 1"},{"location":"tutorials/tf2/slides/#user-2","text":"I think I would like one network for my X X and y y . I also think maybe I should have another network with shared weights and a latent space. Maybe I coud also have two or three input locations. In addition...","title":"User 2"},{"location":"tutorials/tf2/slides/#user-3","text":"I want to implement a Neural Network with convolutional layers and a noise contrastive prior. The weights of the network will be parameterized by Normal distributions. I would also like a training scheme with a mixture of Importance sampling and variational inference with a custom KLD loss. One Deep Learning library to rule them all...! Probably a bad idea...","title":"User 3"},{"location":"tutorials/tf2/slides/#deep-learning-library-gold-rush","text":"Currently more than 10+ mainstream libraries All tech companies want a piece","title":"Deep Learning Library Gold Rush"},{"location":"tutorials/tf2/slides/#growth-of-pytorch","text":"","title":"Growth of PyTorch"},{"location":"tutorials/tf2/slides/#why","text":"Simple (Pythonic) Great API Performance vs Productivity Tradeoff Easy to Install...","title":"Why?"},{"location":"tutorials/tf2/slides/#game-which-library","text":"","title":"Game: Which Library?"},{"location":"tutorials/tf2/slides/#my-suggestions","text":"Productivity: Fastai From Scratch: JAX Research: PyTorch Production/Industry: TensorFlow","title":"My Suggestions"},{"location":"tutorials/tf2/slides/#basics","text":"Tensors Variables Automatic differentiation (AutoGrad)","title":"Basics"},{"location":"tutorials/tf2/slides/#tensors","text":"","title":"Tensors"},{"location":"tutorials/tf2/slides/#constants","text":"# create constant x = tf . constant ([[ 5 , 2 ], [ 1 , 3 ]]) print ( x )","title":"Constants"},{"location":"tutorials/tf2/slides/#tftensor-5-2-1-3-shape2-2-dtypeint32","text":"","title":"tf.Tensor(\n[[5 2]\n [1 3]], shape=(2, 2), dtype=int32)\n"},{"location":"tutorials/tf2/slides/#standard","text":"","title":"Standard"},{"location":"tutorials/tf2/slides/#create-ones-tensor-t_ones-tfonesshape2-1-create-zeros-tensor-t_zeros-tfzerosshape2-1","text":"","title":"# create ones tensor\nt_ones = tf.ones(shape=(2, 1))\n\n# create zeros tensor\nt_zeros = tf.zeros(shape=(2, 1))\n"},{"location":"tutorials/tf2/slides/#standard-randomized","text":"# pretty standard tf . random . normal ( shape = ( 2 , 2 ), mean = 0. , stddev = 1. ) # pretty much the same tf . random . uniform ( shape = ( 2 , 2 ), minval = 0 , maxval = 10 )","title":"Standard Randomized"},{"location":"tutorials/tf2/slides/#variables","text":"# set initial value initial_value = tf . random . normal ( shape = ( 2 , 2 )) # set variable a = tf . Variable ( initial_value ) Options (constraint, trainable, shape) All math operations","title":"Variables"},{"location":"tutorials/tf2/slides/#updates","text":"# new value b = tf . random . uniform ( shape = ( 2 , 2 )) # set value a . assign ( b ) # increment (a + b) a . assign_add ( b ) # dencrement (a - b) a . assign_sub ( new_a )","title":"Updates"},{"location":"tutorials/tf2/slides/#gradients","text":"","title":"Gradients"},{"location":"tutorials/tf2/slides/#gradient-function","text":"# init variable a = tf . Variable ( init_value ) # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # calculate gradient ( dc/da ) dc_da = tf . gradients ( c , a ) # calculate multiple gradients dc_da , dc_db = tf . gradients ( c , [ a , b ]) New : GradientTape Defines the scope literally \"record operations\" # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a )","title":"Gradient Function"},{"location":"tutorials/tf2/slides/#nested-gradients","text":"# init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as outer_tape : with tf . GradientTape () as inner_tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) # extract gradients ( d2c/da2 ) d2c_da2 = outer_tape . gradient ( dc_da , a )","title":"Nested Gradients"},{"location":"tutorials/tf2/slides/#gradients-in-pytorch","text":"Same gradient function torch.autograd.grad There is no Tape Each variable has their own gradient # init variable a = torch . tensor ( init_value , requires_grad = True ) # do operation c = math . sqrt ( a ** 2 + b ** 2 ) # calculate gradients ( dc/da ) c . backward ( a ) # extract gradients dc_da = a . grad","title":"Gradients in PyTorch"},{"location":"tutorials/tf2/slides/#tf-engine-module","text":"Layer Network - DAG graph Model Sequential","title":"TF: Engine Module"},{"location":"tutorials/tf2/slides/#various-subclasses","text":"Layers Metric Loss Callbacks Optimizer Regularizers, Constraints","title":"Various Subclasses"},{"location":"tutorials/tf2/slides/#layer-class","text":"The core abstraction Everything is a Layer ...or interacts with a layer","title":"Layer Class"},{"location":"tutorials/tf2/slides/#example-layer","text":"y = \\mathbf{W}x + b y = \\mathbf{W}x + b # Subclass Layer class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () # Make Parameters def call ( self , inputs ): # Do stuff return inputs","title":"Example Layer"},{"location":"tutorials/tf2/slides/#1-constructor","text":"# Inherit Layer class class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ ()","title":"1 - Constructor"},{"location":"tutorials/tf2/slides/#2-parameters-mathbfwmathbfw","text":"# initialize weights (random) w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True )","title":"2 - Parameters, \\mathbf{W}\\mathbf{W}"},{"location":"tutorials/tf2/slides/#2-parameter-bb","text":"# initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True )","title":"2 - Parameter, bb"},{"location":"tutorials/tf2/slides/#3-call-function-mathbfwx-bmathbfwx-b","text":"def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b","title":"3 -  Call Function, \\mathbf{W}x +b\\mathbf{W}x +b"},{"location":"tutorials/tf2/slides/#pytorch-the-same","text":"class Linear ( nn . Module ): def __init__ ( self , units : int , input_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , units ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( units ), requires_grad = True ) def forward ( self , inputs ): return inputs @ self . weights + self . bias","title":"PyTorch (the same...)"},{"location":"tutorials/tf2/slides/#using-it","text":"# data x_train = ... # initialize linear layer linear_layer = Linear ( units = 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x )","title":"Using it"},{"location":"tutorials/tf2/slides/#tensorflow-build","text":"Know the # of nodes Don't know the input shape More conventional For example... def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) More convenient... # data x_train = ... # initialize linear layer (without input dims) linear_layer = Linear ( units = 4 ) # internally -> calls x.shape y = linear_layer ( x ) We can nest as many Layers as we want.","title":"TensorFlow build"},{"location":"tutorials/tf2/slides/#linear","text":"class Linear ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () # call linear layer self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x","title":"Linear"},{"location":"tutorials/tf2/slides/#linear-block","text":"class LinearBlock ( Layer ): def __init__ ( self ): super () . __init__ () self . lin_1 = Linear ( 32 ) self . lin_2 = Linear ( 32 ) self . lin_3 = Linear ( 1 ) def call ( self , inputs ): x = self . lin_1 ( x ) x = self . lin_2 ( x ) x = self . lin_3 ( x ) return x","title":"Linear Block"},{"location":"tutorials/tf2/slides/#training-tf2x-pytorch","text":"","title":"Training TF2.X, PyTorch"},{"location":"tutorials/tf2/slides/#losses","text":"TensorFlow # example loss function loss_func = torch . nn . MSELoss () PyTorch # example loss function loss_fn = tf . keras . losses . MSELoss ()","title":"Losses"},{"location":"tutorials/tf2/slides/#optimizers","text":"TensorFlow # example optimizer optimizer = tf . keras . optimizers . Adam () PyTorch # example optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.01 )","title":"Optimizers"},{"location":"tutorials/tf2/slides/#full-training-loop-pytorch","text":"# Loop through batches for x , y in dataset : # initialize gradients optimizer . zero_grad () # predictions for minibatch ypred = lr_model ( xbatch ) # loss value for minibatch loss = loss_func ( ypred , ybatch ) # find gradients loss . backward () # apply optimization optimizer . step ()","title":"Full Training Loop (PyTorch)"},{"location":"tutorials/tf2/slides/#full-training-loop-tf2x","text":"for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , model . trainable_weights ))","title":"Full Training Loop (TF2.X)"},{"location":"tutorials/tf2/slides/#tensorflow-nuggets","text":"","title":"TensorFlow Nuggets"},{"location":"tutorials/tf2/slides/#training-call","text":"Allows training versus inference mode Just need an extra argument training=True in the call method Prob Models, e.g. Batch Norm., Variational Inference","title":"Training Call"},{"location":"tutorials/tf2/slides/#example","text":"... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x","title":"Example"},{"location":"tutorials/tf2/slides/#add-loss","text":"\"Add Losses on the fly\" Each layer has it's own regularization Examples: KLD, Activation or Weight Regularization","title":"Add Loss"},{"location":"tutorials/tf2/slides/#example-model","text":"","title":"Example - Model"},{"location":"tutorials/tf2/slides/#class-mlplayer-def-__init__self-units32-reg1e-3-super__init__-selflinear-linearunits-selfreg-reg-def-callself-inputs-x-selflinearinputs-x-tfnnrelux-add-loss-during-the-call-selfadd_losstfreduce_sumoutput-2-selfreg-return-x","text":"","title":"class MLP(Layer):\n    def __init__(self, units=32, reg=1e-3):\n        super().__init__()\n        self.linear = Linear(units)\n        self.reg = reg\n    def call(self, inputs):\n        x = self.linear(inputs)\n        x = tf.nn.relu(x)\n        # Add loss during the call\n        self.add_loss(tf.reduce_sum(output ** 2) * self.reg)\n        return x\n"},{"location":"tutorials/tf2/slides/#example-training","text":"mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function opt = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights ))","title":"Example - Training"},{"location":"tutorials/tf2/slides/#compile-code","text":"Use a decorator, @tf.function Optional Easy performance booster","title":"Compile Code"},{"location":"tutorials/tf2/slides/#example-graphs","text":"@tf . function def train_step ( dataset ): for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) return loss","title":"Example - Graphs"},{"location":"tutorials/tf2/slides/#model-class","text":"Can do everything a Layer can do Built-in functionality a.k.a. Keras territory TF and PyTorch part ways","title":"Model Class"},{"location":"tutorials/tf2/slides/#definitions","text":"Layer : A closed sequence of operation e.g. convolutional layer, recurrent layer, resnet block, attention block. Model : The top layer of your algorithm e.g. Deep learning model, deep neural network.","title":"Definitions"},{"location":"tutorials/tf2/slides/#training-functionality","text":".compile() .fit() .evaulate() .predict() .save() .summary() .plot_model()","title":"Training Functionality"},{"location":"tutorials/tf2/slides/#example-","text":"# loss function loss = tf . keras . losses . MSELoss ( from_logits = True ) # accuracy metrics accuracy = tf . keras . metrics . SparseCategoricalAccuracy () # optimizer optimizer = tf . keras . optimizers . Adam () # compile to graph model . compile ( optimizer = optimizer , loss = loss , metrics = [ accuracy ]) # Fit Model model . fit ( dataset , epochs = 3 ) # Test Data loss , acc = model . evaluate ( test_dataset )","title":"Example -"},{"location":"tutorials/tf2/slides/#functional-models","text":"Creates DAG Model Class with Extras Only in TF","title":"Functional Models"},{"location":"tutorials/tf2/slides/#simple-example","text":"# input checks x = tf . keras . layers . Flatten ( shape = 28 , 28 ))( inputs ) # Layer 1 x = tf . keras . layers . Dense ( 512 , activation = tf . nn . relu )( inputs ) # Layer 2 x = tf . keras . layers . Dropout ( 0.2 )( x ) # outputs x = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )( x ) # create model class model = tf . keras . Model ( inputs , outputs ) # compile model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ] )","title":"Simple Example"},{"location":"tutorials/tf2/slides/#example-graph-output","text":"","title":"Example - Graph Output"},{"location":"tutorials/tf2/slides/#we-can-go-crazy","text":"","title":"We can go crazy..."},{"location":"tutorials/tf2/slides/#sequential-models","text":"Predifined PyTorch & TF In TF, Model class PyTorch model = nn . Sequential ( torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 10 ), ) TensorFlow model = tf . keras . Sequential ([ layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 10 ) ])","title":"Sequential Models"},{"location":"tutorials/tf2/slides/#datasets","text":"Convenience Functions Take care of loading, iterations, batches","title":"Datasets"},{"location":"tutorials/tf2/slides/#normally","text":"n_batches = ( n_samples - 1 ) // batch_size + 1 for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # get subset from data xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ]","title":"Normally"},{"location":"tutorials/tf2/slides/#pytorch-datasets","text":"# create dataset train_ds = TensorDataset ( x_train , y_train ) # Loop through batches for start_idx , end_idx in range ( batch_idx ): # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # Do stuff... Note: In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically.","title":"PyTorch - Datasets"},{"location":"tutorials/tf2/slides/#pytorch-dataloaders","text":"# create dataset train_ds = TensorDataset ( x_train , y_train ) # create dataloader train_dl = DataLoader ( train_ds , batch_size = 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff...","title":"PyTorch - DataLoaders"},{"location":"tutorials/tf2/slides/#tf-both","text":"# create dataset train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) # create dataloader train_dl = train_ds . batch ( 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff...","title":"TF - Both..."},{"location":"tutorials/tf2/slides/#what-we-covered","text":"DL Framework Idea Layers and Models Sequential Model","title":"What We Covered"},{"location":"tutorials/tf2/slides/#what-we-didnt-cover","text":"Callbacks Distributed Training Multiple GPUs All options under the sun Tensorboard (Built-in Jupyter Notebooks!)","title":"What We didn't Cover"},{"location":"tutorials/tf2/slides/#summary","text":"","title":"Summary"},{"location":"tutorials/tf2/slides/#tensorflow-training","text":"","title":"TensorFlow Training"},{"location":"tutorials/tf2/docs/css/theme/","text":"Dependencies \u00b6 Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup Creating a Theme \u00b6 To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Index"},{"location":"tutorials/tf2/docs/css/theme/#dependencies","text":"Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup","title":"Dependencies"},{"location":"tutorials/tf2/docs/css/theme/#creating-a-theme","text":"To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Creating a Theme"},{"location":"tutorials/tf2/docs/plugin/markdown/example/","text":"Markdown Demo \u00b6 External 1.1 \u00b6 Content 1.1 Note: This will only appear in the speaker notes window. External 1.2 \u00b6 Content 1.2 External 2 \u00b6 Content 2.1 External 3.1 \u00b6 Content 3.1 External 3.2 \u00b6 Content 3.2 External 3.3 \u00b6","title":"Markdown Demo"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#markdown-demo","text":"","title":"Markdown Demo"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-11","text":"Content 1.1 Note: This will only appear in the speaker notes window.","title":"External 1.1"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-12","text":"Content 1.2","title":"External 1.2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-2","text":"Content 2.1","title":"External 2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-31","text":"Content 3.1","title":"External 3.1"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-32","text":"Content 3.2","title":"External 3.2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-33","text":"","title":"External 3.3"}]}