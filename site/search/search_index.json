{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com This is my research journal of various topics that I'm interested in for my research. My research is mostly in using Machine learning methods in various applications of remote sensing and climate sciences. The predominant algorithms of interest include kernel methods like Gaussian Processes (GPs) and Invertible Flows like Rotation-Based Iterative Gaussianization (RBIG) for density and dependence estimation. My Talks \u00b6 Some talks that I have done along with code and slides. My Resources \u00b6 My resources for all things python and tech. I like to tinker with different packages so I try to document my findings. My Projects \u00b6 My projects that I am involved in mostly related to academia. My Thesis \u00b6 My Thesis and everything related to it. I decided to compile all of my notes in markdown and of course the final product will be available in LaTeX format. My Tutorials \u00b6 I do like to give back to the community. So I have compiled some tutorials that will hopefully be helpful to other people. My Snippets \u00b6 I write lots of bits of code everywhere and it tends to be disorganized. I'm trying to organize my bits of code everywhere into digestable snippets; kind of like a personal reference.","title":"Overview"},{"location":"#overview","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com This is my research journal of various topics that I'm interested in for my research. My research is mostly in using Machine learning methods in various applications of remote sensing and climate sciences. The predominant algorithms of interest include kernel methods like Gaussian Processes (GPs) and Invertible Flows like Rotation-Based Iterative Gaussianization (RBIG) for density and dependence estimation.","title":"Overview"},{"location":"#my-talks","text":"Some talks that I have done along with code and slides.","title":"My Talks"},{"location":"#my-resources","text":"My resources for all things python and tech. I like to tinker with different packages so I try to document my findings.","title":"My Resources"},{"location":"#my-projects","text":"My projects that I am involved in mostly related to academia.","title":"My Projects"},{"location":"#my-thesis","text":"My Thesis and everything related to it. I decided to compile all of my notes in markdown and of course the final product will be available in LaTeX format.","title":"My Thesis"},{"location":"#my-tutorials","text":"I do like to give back to the community. So I have compiled some tutorials that will hopefully be helpful to other people.","title":"My Tutorials"},{"location":"#my-snippets","text":"I write lots of bits of code everywhere and it tends to be disorganized. I'm trying to organize my bits of code everywhere into digestable snippets; kind of like a personal reference.","title":"My Snippets"},{"location":"logistics/","text":"Logistics \u00b6 Notebooks \u00b6 I really like notebooks as a way to display information. Strategy Do Experiments Upload Notebook (meta data, sections, comments) Refactor Notebook (Clean, sections, labels, only important info) Re-Upload Notebook Organize and MKDocsify-it","title":"Logistics"},{"location":"logistics/#logistics","text":"","title":"Logistics"},{"location":"logistics/#notebooks","text":"I really like notebooks as a way to display information. Strategy Do Experiments Upload Notebook (meta data, sections, comments) Refactor Notebook (Clean, sections, labels, only important info) Re-Upload Notebook Organize and MKDocsify-it","title":"Notebooks"},{"location":"appendix/","text":"My Appendices \u00b6 Concepts \u00b6 Matrix Tricks Sleeper Concepts Bayesian \u00b6 Introduction Regression Variational Inference Monte Carlo Gaussian Distribution Gaussian Processes \u00b6 Introduction Sparse Gaussian Processes Input Uncertainty Taylor Expansion Variational Information Kernels \u00b6 HSIC MMD Information Theory \u00b6 Entropy Formulas Variation of Information Density Estimation \u00b6 Histograms Logistic","title":"My Appendices"},{"location":"appendix/#my-appendices","text":"","title":"My Appendices"},{"location":"appendix/#concepts","text":"Matrix Tricks Sleeper Concepts","title":"Concepts"},{"location":"appendix/#bayesian","text":"Introduction Regression Variational Inference Monte Carlo Gaussian Distribution","title":"Bayesian"},{"location":"appendix/#gaussian-processes","text":"Introduction Sparse Gaussian Processes Input Uncertainty Taylor Expansion Variational Information","title":"Gaussian Processes"},{"location":"appendix/#kernels","text":"HSIC MMD","title":"Kernels"},{"location":"appendix/#information-theory","text":"Entropy Formulas Variation of Information","title":"Information Theory"},{"location":"appendix/#density-estimation","text":"Histograms Logistic","title":"Density Estimation"},{"location":"appendix/bayesian/exponential/","text":"Exponential Family of Distributions \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle Resources \u00b6 A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Exponential Family of Distributions"},{"location":"appendix/bayesian/exponential/#exponential-family-of-distributions","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle","title":"Exponential Family of Distributions"},{"location":"appendix/bayesian/exponential/#resources","text":"A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Resources"},{"location":"appendix/bayesian/gaussian/","text":"Gaussian Distributions \u00b6 Univariate Gaussian \u00b6 \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) Multivariate Gaussian \u00b6 \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} Joint Gaussian Distribution \u00b6 \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} Marginal Distribution \\mathcal{P}(\\cdot) \\mathcal{P}(\\cdot) \u00b6 We have the marginal distribution of x x \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) and in integral form: \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy and we have the marginal distribution of y y \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) Conditional Distribution \\mathcal{P}(\\cdot | \\cdot) \\mathcal{P}(\\cdot | \\cdot) \u00b6 We have the conditional distribution of x x given y y . \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) where: \\mu_{a|b} = a + BC^{-1}(y-b) \\mu_{a|b} = a + BC^{-1}(y-b) \\Sigma_{a|b} = A - BC^{-1}B^T \\Sigma_{a|b} = A - BC^{-1}B^T and we have the marginal distribution of y y given x x \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) where: \\mu_{b|a} = b + AC^{-1}(x-a) \\mu_{b|a} = b + AC^{-1}(x-a) \\Sigma_{b|a} = B - AC^{-1}A^T \\Sigma_{b|a} = B - AC^{-1}A^T basically mirror opposites of each other. But this might be useful to know later when we deal with trying to find the marginal distributions of Gaussian process functions. Source : Sampling from a Normal Distribution - blog A really nice blog with nice plots of joint distributions. Two was to derive the conditional distributions - stack How to generate Gaussian samples = blog Multivariate Gaussians and Detereminant - Lecturee Notes Bandwidth Selection \u00b6 Scotts sigma = np . power ( n_samples , - 1.0 / ( d_dimensions + 4 )) Silverman sigma = np . power ( n_samples * ( d_dimensions + 2.0 ) / 4.0 , - 1.0 / ( d_dimensions + 4 ) Gaussian Distribution \u00b6 PDF \u00b6 f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) Likelihood \u00b6 - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi Alternative Representation \u00b6 X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda) Reparameterization \u00b6 So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition Entropy \u00b6 1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| KL-Divergence (Relative Entropy) \u00b6 KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"Gaussian"},{"location":"appendix/bayesian/gaussian/#gaussian-distributions","text":"","title":"Gaussian Distributions"},{"location":"appendix/bayesian/gaussian/#univariate-gaussian","text":"\\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) \\mathcal{P}(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right)","title":"Univariate Gaussian"},{"location":"appendix/bayesian/gaussian/#multivariate-gaussian","text":"\\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned} \\begin{aligned} \\mathcal{P}(x | \\mu, \\Sigma) &= \\mathcal{N}(\\mu, \\Sigma) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{\\sqrt{\\text{det}|\\Sigma|}}\\text{exp}\\left( -\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\right) \\end{aligned}","title":"Multivariate Gaussian"},{"location":"appendix/bayesian/gaussian/#joint-gaussian-distribution","text":"\\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned} \\begin{aligned}\\mathcal{P}(x, y) &= \\mathcal{P}\\left(\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) \\\\ &= \\mathcal{N}\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A & B \\\\ B^{\\top} & C \\end{bmatrix} \\right) \\end{aligned}","title":"Joint Gaussian Distribution"},{"location":"appendix/bayesian/gaussian/#marginal-distribution-mathcalpcdotmathcalpcdot","text":"We have the marginal distribution of x x \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) \\mathcal{P}(x) \\sim \\mathcal{N}(a, A) and in integral form: \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy \\mathcal{P}(x) = \\int_y \\mathcal{P}(x,y)dy and we have the marginal distribution of y y \\mathcal{P}(y) \\sim \\mathcal{N}(b, B) \\mathcal{P}(y) \\sim \\mathcal{N}(b, B)","title":"Marginal Distribution \\mathcal{P}(\\cdot)\\mathcal{P}(\\cdot)"},{"location":"appendix/bayesian/gaussian/#conditional-distribution-mathcalpcdot-cdotmathcalpcdot-cdot","text":"We have the conditional distribution of x x given y y . \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) \\mathcal{P}(x|y) \\sim \\mathcal{N}(\\mu_{a|b}, \\Sigma_{a|b}) where: \\mu_{a|b} = a + BC^{-1}(y-b) \\mu_{a|b} = a + BC^{-1}(y-b) \\Sigma_{a|b} = A - BC^{-1}B^T \\Sigma_{a|b} = A - BC^{-1}B^T and we have the marginal distribution of y y given x x \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) \\mathcal{P}(y|x) \\sim \\mathcal{N}(\\mu_{b|a}, \\Sigma_{b|a}) where: \\mu_{b|a} = b + AC^{-1}(x-a) \\mu_{b|a} = b + AC^{-1}(x-a) \\Sigma_{b|a} = B - AC^{-1}A^T \\Sigma_{b|a} = B - AC^{-1}A^T basically mirror opposites of each other. But this might be useful to know later when we deal with trying to find the marginal distributions of Gaussian process functions. Source : Sampling from a Normal Distribution - blog A really nice blog with nice plots of joint distributions. Two was to derive the conditional distributions - stack How to generate Gaussian samples = blog Multivariate Gaussians and Detereminant - Lecturee Notes","title":"Conditional Distribution \\mathcal{P}(\\cdot | \\cdot)\\mathcal{P}(\\cdot | \\cdot)"},{"location":"appendix/bayesian/gaussian/#bandwidth-selection","text":"Scotts sigma = np . power ( n_samples , - 1.0 / ( d_dimensions + 4 )) Silverman sigma = np . power ( n_samples * ( d_dimensions + 2.0 ) / 4.0 , - 1.0 / ( d_dimensions + 4 )","title":"Bandwidth Selection"},{"location":"appendix/bayesian/gaussian/#gaussian-distribution","text":"","title":"Gaussian Distribution"},{"location":"appendix/bayesian/gaussian/#pdf","text":"f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right)","title":"PDF"},{"location":"appendix/bayesian/gaussian/#likelihood","text":"- \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi","title":"Likelihood"},{"location":"appendix/bayesian/gaussian/#alternative-representation","text":"X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda)","title":"Alternative Representation"},{"location":"appendix/bayesian/gaussian/#reparameterization","text":"So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition","title":"Reparameterization"},{"location":"appendix/bayesian/gaussian/#entropy","text":"1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|","title":"Entropy"},{"location":"appendix/bayesian/gaussian/#kl-divergence-relative-entropy","text":"KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"KL-Divergence (Relative Entropy)"},{"location":"appendix/bayesian/inference/","text":"Solving Hard Integral Problems \u00b6 Source | Deisenroth - Sampling Numerical Integration (low dimension) Bayesian Quadrature Expectation Propagation Conjugate Priors (Gaussian Likelihood w/ GP Prior) Subset Methods (Nystrom) Fast Linear Algebra (Krylov, Fast Transforms, KD-Trees) Variational Methods (Laplace, Mean-Field, Expectation Propagation) Monte Carlo Methods (Gibbs, Metropolis-Hashings, Particle Filter) Inference \u00b6 Laplace Approximation \u00b6 This is where we approximate the posterior with a Gaussian distribution \\mathcal{N}(\\mu, A^{-1}) \\mathcal{N}(\\mu, A^{-1}) . w=w_{map} w=w_{map} , finds a mode (local max) of p(w|D) p(w|D) A = \\nabla\\nabla \\log p(D|w) p(w) A = \\nabla\\nabla \\log p(D|w) p(w) - very expensive calculation Only captures a single mode and discards the probability mass similar to the KLD in one direction. Markov Chain Monte Carlo \u00b6 We can produce samples from the exact posterior by defining a specific Monte Carlo chain. We actually do this in practice with NNs because of the stochastic training regimes. We modify the SGD algorithm to define a scalable MCMC sampler. Here is a visual demonstration of some popular MCMC samplers. Variational Inference \u00b6 Definition : We can find the best approximation within a given family w.r.t. KL-Divergence. $$ \\text{KLD}[q||p] = \\int_w q(w) \\log \\frac{q(w)}{p(w|D)}dw $$ Let q(w)=\\mathcal{N}(\\mu, S) q(w)=\\mathcal{N}(\\mu, S) and then we minimize KLD (q||p) (q||p) to find the parameters \\mu, S \\mu, S . \"Approximate the posterior, not the model\" - James Hensman.","title":"Solving Hard Integral Problems"},{"location":"appendix/bayesian/inference/#solving-hard-integral-problems","text":"Source | Deisenroth - Sampling Numerical Integration (low dimension) Bayesian Quadrature Expectation Propagation Conjugate Priors (Gaussian Likelihood w/ GP Prior) Subset Methods (Nystrom) Fast Linear Algebra (Krylov, Fast Transforms, KD-Trees) Variational Methods (Laplace, Mean-Field, Expectation Propagation) Monte Carlo Methods (Gibbs, Metropolis-Hashings, Particle Filter)","title":"Solving Hard Integral Problems"},{"location":"appendix/bayesian/inference/#inference","text":"","title":"Inference"},{"location":"appendix/bayesian/inference/#laplace-approximation","text":"This is where we approximate the posterior with a Gaussian distribution \\mathcal{N}(\\mu, A^{-1}) \\mathcal{N}(\\mu, A^{-1}) . w=w_{map} w=w_{map} , finds a mode (local max) of p(w|D) p(w|D) A = \\nabla\\nabla \\log p(D|w) p(w) A = \\nabla\\nabla \\log p(D|w) p(w) - very expensive calculation Only captures a single mode and discards the probability mass similar to the KLD in one direction.","title":"Laplace Approximation"},{"location":"appendix/bayesian/inference/#markov-chain-monte-carlo","text":"We can produce samples from the exact posterior by defining a specific Monte Carlo chain. We actually do this in practice with NNs because of the stochastic training regimes. We modify the SGD algorithm to define a scalable MCMC sampler. Here is a visual demonstration of some popular MCMC samplers.","title":"Markov Chain Monte Carlo"},{"location":"appendix/bayesian/inference/#variational-inference","text":"Definition : We can find the best approximation within a given family w.r.t. KL-Divergence. $$ \\text{KLD}[q||p] = \\int_w q(w) \\log \\frac{q(w)}{p(w|D)}dw $$ Let q(w)=\\mathcal{N}(\\mu, S) q(w)=\\mathcal{N}(\\mu, S) and then we minimize KLD (q||p) (q||p) to find the parameters \\mu, S \\mu, S . \"Approximate the posterior, not the model\" - James Hensman.","title":"Variational Inference"},{"location":"appendix/bayesian/intro/","text":"Bayesian: Language of Uncertainty \u00b6 Formulation \u00b6 A model is something that links inputs to outputs. If we are given data, X \\in \\mathbb{R}^{NxD} X \\in \\mathbb{R}^{NxD} , and observations, y y , we ideally would want to know these two entities are related. That relationship (or transformation) from the data X X to the observations y y is what we would call a model, \\mathcal{M} \\mathcal{M} . graph LR A[X] --> B[Model]; B --> C[y] More concretely, let X\\in \\mathbb{R}^{NxD} X\\in \\mathbb{R}^{NxD} and y \\in \\mathbb{R}^{N} y \\in \\mathbb{R}^{N} where N N is the number of samples and D D is the number of dimensions/features. In a transformation sense, we could think of it as a function, f f that maps the data from X X to y y , or f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} . To put it simply, we have the following equation to describe our model. y = f(X) y = f(X) But if we put a statistical spin on it and say that X X is a random variabe (r.v.), X \\sim \\mathbb{P} X \\sim \\mathbb{P} . We typically don't know \\mathbb{P} \\mathbb{P} or else there really would not be a problem. Or even worse, let's say that there is actually noise in our observation so we're not entirely 100% sure that each input, x x corresponds to each output, y y . Fortunately, we have mathematics where we can easily find some mathematical framework to transform our problem into a way we can easily solve. In this case, we can use the mathematics of probability theory to express the uncertainty and noise that come with our model, \\mathcal{M} \\mathcal{M} . More specifically, we can use Bayes rule to give us inverse probabilities that allow us to use inference; basically using our data to infer unknown quantities, model aspects and (most importantly) make predictions. Bayes Rule in Words \u00b6 In a Machine Learning problem, we almost always have the following components: Data Model which we believe can describe our data, parameters which can be changed/tuned to fit the data Goal Learn the parameters given the data which points belong to which cluster predict function outputs predict future labels predict the lower dimensional embedding/representation The Bayesian framework works best when you think about it from a probabilistic standpoint. \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} I've seen some people ( here , here ) have some sort of equivalence between Model, \\mathcal{M} \\mathcal{M} and Hypothesis, \\mathcal{H} \\mathcal{H} . In this particular instance, think of the \\mathcal{M} \\mathcal{M} as the best possible outcome that we can achieve to map x x to y y correctly . And think of \\mathcal{H} \\mathcal{H} as a set of possible formulas we could use; like in a Universe where we have all of the possible formulas and collection of parameters. I quite like the term Hypothesis because it adds another level of abstraction when thinking about the problem. But at the same time I feel like this extra layer of abstraction is not something I like to think about all of the time. Let's break down each of these components. P(\\text{ Model }) P(\\text{ Model }) - Prior Probability P(\\text{ Data } | \\text{}) P(\\text{ Data } | \\text{}) - Evidence, Normalization Constant P(\\text{ Model } | \\text{ Data }) P(\\text{ Model } | \\text{ Data }) - Posterior Probability P(\\text{ Data } | \\text{ Model }) P(\\text{ Data } | \\text{ Model }) - Likelihood Let's change the notation to something a bit more common. P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} where: * P(\\mathcal{D}|\\theta, \\mathcal{M}) P(\\mathcal{D}|\\theta, \\mathcal{M}) - Likelihood of the parameters, \\theta \\theta in model \\mathcal{M} \\mathcal{M} Likelihood of the parameters ( not of the data ). For every set of parameters, I can assign a probability to some observable data. * P(\\theta | \\mathcal{M}) P(\\theta | \\mathcal{M}) - prior probability of \\theta \\theta This expresses the distribution and the uncertainty of the parameters that define my model. It's a way of constraining the range of values that can occur. Expert knowledge in this area is crucial if you would like Physics-aware machine learning models. * P(\\mathcal{D}|\\mathcal{M}) P(\\mathcal{D}|\\mathcal{M}) - The normalization constant (the marginal likelihood) This term seems to give us a lot of problems ??? but this is an artifact of Bayes Rule where in order to obtain my Posterior, I need to renormalize. * P(\\theta | \\mathcal{D,M}) P(\\theta | \\mathcal{D,M}) - Posterior of \\theta \\theta given data \\mathcal{D} \\mathcal{D} T There are few things that are different. First of all, every single component is conditioned on a model \\mathcal{M} \\mathcal{M} . This is to say, given that I have described my model, here are the configurations that this model requires. So we're really staying true to the model based Machine Learning instead of the Toolbox method. Also, I've changed the data to be denoted as \\mathcal{D} \\mathcal{D} where \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} . Maximum A Posteriori (MAP) Estimation \u00b6 Notice how this doesn't really help us use or evaluate the parameters for the model that we've learned. That's when it comes to predictions. If we look at the sum for Bayes rule P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) , we can write something similar for actual datapoints, P(x|\\mathcal{D,M}) P(x|\\mathcal{D,M}) . P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta This integral is by d\\theta d\\theta because we want to integrate out the parameters (WHY???) . There is now a joint distribution between x x and \\theta \\theta . We actually don't have access to that. But we do have access to the posterior probability of \\theta \\theta given the data \\mathcal{D} \\mathcal{D} . So using the product rule ( P(x,y)=P(x)P(y|x) P(x,y)=P(x)P(y|x) ), we can split apart that quantity to obtain: P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta Now dissecting this formula a little further, we now have the average predictions for all \\theta \\theta 's which we weight by the posteriors. This arises a natural ensemble scheme where we are averaging models. Also worth noting is that there is no optimization within any of these quantities. Practically, my brain is telling me that that is a bit useless but I guess in the Bayesian framework, that's not really all that necessary. But yes, optimal model parameters would be needed for actually making decisions on future data. We can write a loss function \\mathcal{L} \\mathcal{L} w.r.t. \\theta \\theta to express how one would optimize this quantity: \\mathcal{L}(\\theta^*)= \\mathcal{L}(\\theta^*)= Model Comparison \u00b6 P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} The most interesting one is deriving the maximum likelihood formulation (Ocamz Razer, Model Evidence, Integrated Likelihood): what's the probability that the data I have came from the model P(\\mathcal{D|M}) P(\\mathcal{D|M}) . Again using Bayes sum rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta Once again, we don't have access to this joint distribution, but we do have access to the likelihood and the prior. So, again, we can decompose this joint distribution by using the Bayes product rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta QUESTIONS \u00b6 Marginalize? Normalize? Why is it so hard? Intractability? Posterior Likelihood Prior --- \u00b6 Supplementary \u00b6 Bait and Switch \u00b6 MAP estimate is easy to make it work but can do some weird stuff. Bayesian - hard to make it work but sometimes makes more sense. Maximum Likelihood \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) <span><span class=\"MathJax_Preview\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)</span><script type=\"math/tex\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) MAP \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) <span><span class=\"MathJax_Preview\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w)</span><script type=\"math/tex\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) Type II Maximum Likelihood \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw <span><span class=\"MathJax_Preview\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw</span><script type=\"math/tex\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw Type II MAP: \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) <span><span class=\"MathJax_Preview\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha)</span><script type=\"math/tex\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) Tips and Tricks for Practicioners \u00b6 Set initial hyper-parameters with domain knowledge Standardize input data Set initial length scales to \\sigma_l \\approx 0.5 \\sigma_l \\approx 0.5 Standardize targets \\mathbf{y} \\mathbf{y} Set initial signal variance to \\sigma_f\\approx 1.0 \\sigma_f\\approx 1.0 Set noise level initially high \\sigma_n \\approx 0.5 \\times \\sigma_f \\sigma_n \\approx 0.5 \\times \\sigma_f Random restarts Penalize high signal-to-noise ratios ( \\frac{\\sigma_f}{\\sigma_n} \\frac{\\sigma_f}{\\sigma_n} ) Resources \u00b6 Deisenroth - Model Selection Bayes Theorem Net Probabilistic World - What is Bayes Theorem | The Anatomy of Bayes Theorem Zhoubin Gharamani - YouTube Talk Learning From Data - Website TODO: FlowChart of Learning and Probabilistic Learning Linear Regression Experiments A Probabilistic View of Linear Regression - Keng (2016) - blog A Probabilistic Interpretation of Regularization - Brian Keng (2016) - blog Code : FlowCharts - Mermaid | VSC Ext Definitiions \u00b6 Bayes Rule Inference - Some conclusion reached given by some evidence or reasoning Probability - An expression of belief Probabilistic Modeling","title":"Bayesian: Language of Uncertainty"},{"location":"appendix/bayesian/intro/#bayesian-language-of-uncertainty","text":"","title":"Bayesian: Language of Uncertainty"},{"location":"appendix/bayesian/intro/#formulation","text":"A model is something that links inputs to outputs. If we are given data, X \\in \\mathbb{R}^{NxD} X \\in \\mathbb{R}^{NxD} , and observations, y y , we ideally would want to know these two entities are related. That relationship (or transformation) from the data X X to the observations y y is what we would call a model, \\mathcal{M} \\mathcal{M} . graph LR A[X] --> B[Model]; B --> C[y] More concretely, let X\\in \\mathbb{R}^{NxD} X\\in \\mathbb{R}^{NxD} and y \\in \\mathbb{R}^{N} y \\in \\mathbb{R}^{N} where N N is the number of samples and D D is the number of dimensions/features. In a transformation sense, we could think of it as a function, f f that maps the data from X X to y y , or f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} f:\\mathbb{X}\\rightarrow \\mathbb{Y}, \\mathbb{R}^{NxD}\\rightarrow \\mathbb{R}^{N} . To put it simply, we have the following equation to describe our model. y = f(X) y = f(X) But if we put a statistical spin on it and say that X X is a random variabe (r.v.), X \\sim \\mathbb{P} X \\sim \\mathbb{P} . We typically don't know \\mathbb{P} \\mathbb{P} or else there really would not be a problem. Or even worse, let's say that there is actually noise in our observation so we're not entirely 100% sure that each input, x x corresponds to each output, y y . Fortunately, we have mathematics where we can easily find some mathematical framework to transform our problem into a way we can easily solve. In this case, we can use the mathematics of probability theory to express the uncertainty and noise that come with our model, \\mathcal{M} \\mathcal{M} . More specifically, we can use Bayes rule to give us inverse probabilities that allow us to use inference; basically using our data to infer unknown quantities, model aspects and (most importantly) make predictions.","title":"Formulation"},{"location":"appendix/bayesian/intro/#bayes-rule-in-words","text":"In a Machine Learning problem, we almost always have the following components: Data Model which we believe can describe our data, parameters which can be changed/tuned to fit the data Goal Learn the parameters given the data which points belong to which cluster predict function outputs predict future labels predict the lower dimensional embedding/representation The Bayesian framework works best when you think about it from a probabilistic standpoint. \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} \\begin{aligned}P(\\text{ Model }|\\text{ Data })= \\frac{P(\\text{ Data }|\\text{ Model })P(\\text{ Model })}{P(\\text{ Data })}\\end{aligned} I've seen some people ( here , here ) have some sort of equivalence between Model, \\mathcal{M} \\mathcal{M} and Hypothesis, \\mathcal{H} \\mathcal{H} . In this particular instance, think of the \\mathcal{M} \\mathcal{M} as the best possible outcome that we can achieve to map x x to y y correctly . And think of \\mathcal{H} \\mathcal{H} as a set of possible formulas we could use; like in a Universe where we have all of the possible formulas and collection of parameters. I quite like the term Hypothesis because it adds another level of abstraction when thinking about the problem. But at the same time I feel like this extra layer of abstraction is not something I like to think about all of the time. Let's break down each of these components. P(\\text{ Model }) P(\\text{ Model }) - Prior Probability P(\\text{ Data } | \\text{}) P(\\text{ Data } | \\text{}) - Evidence, Normalization Constant P(\\text{ Model } | \\text{ Data }) P(\\text{ Model } | \\text{ Data }) - Posterior Probability P(\\text{ Data } | \\text{ Model }) P(\\text{ Data } | \\text{ Model }) - Likelihood Let's change the notation to something a bit more common. P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} P(\\theta | \\mathcal{D}, \\mathcal{M})= \\frac{P(\\mathcal{D}|\\theta, \\mathcal{M})P(\\theta | \\mathcal{M})}{P(\\mathcal{D}|\\mathcal{M})} where: * P(\\mathcal{D}|\\theta, \\mathcal{M}) P(\\mathcal{D}|\\theta, \\mathcal{M}) - Likelihood of the parameters, \\theta \\theta in model \\mathcal{M} \\mathcal{M} Likelihood of the parameters ( not of the data ). For every set of parameters, I can assign a probability to some observable data. * P(\\theta | \\mathcal{M}) P(\\theta | \\mathcal{M}) - prior probability of \\theta \\theta This expresses the distribution and the uncertainty of the parameters that define my model. It's a way of constraining the range of values that can occur. Expert knowledge in this area is crucial if you would like Physics-aware machine learning models. * P(\\mathcal{D}|\\mathcal{M}) P(\\mathcal{D}|\\mathcal{M}) - The normalization constant (the marginal likelihood) This term seems to give us a lot of problems ??? but this is an artifact of Bayes Rule where in order to obtain my Posterior, I need to renormalize. * P(\\theta | \\mathcal{D,M}) P(\\theta | \\mathcal{D,M}) - Posterior of \\theta \\theta given data \\mathcal{D} \\mathcal{D} T There are few things that are different. First of all, every single component is conditioned on a model \\mathcal{M} \\mathcal{M} . This is to say, given that I have described my model, here are the configurations that this model requires. So we're really staying true to the model based Machine Learning instead of the Toolbox method. Also, I've changed the data to be denoted as \\mathcal{D} \\mathcal{D} where \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} \\mathcal{D}=\\left\\{ (x_1, y_1), \\ldots, (x_N, y_N) \\right\\}^{N}_{1} .","title":"Bayes Rule in Words"},{"location":"appendix/bayesian/intro/#maximum-a-posteriori-map-estimation","text":"Notice how this doesn't really help us use or evaluate the parameters for the model that we've learned. That's when it comes to predictions. If we look at the sum for Bayes rule P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) P(x)=\\int P(x,y)dy\\approx\\sum_y P(x,y) , we can write something similar for actual datapoints, P(x|\\mathcal{D,M}) P(x|\\mathcal{D,M}) . P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta P(x|\\mathcal{D,M})=\\int P(x, \\theta|\\mathcal{D,M})d\\theta This integral is by d\\theta d\\theta because we want to integrate out the parameters (WHY???) . There is now a joint distribution between x x and \\theta \\theta . We actually don't have access to that. But we do have access to the posterior probability of \\theta \\theta given the data \\mathcal{D} \\mathcal{D} . So using the product rule ( P(x,y)=P(x)P(y|x) P(x,y)=P(x)P(y|x) ), we can split apart that quantity to obtain: P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta P(x|\\mathcal{D},\\mathcal{M})= \\int P(x|\\theta, \\mathcal{D},\\mathcal{M}) P(\\theta|\\mathcal{D}, \\mathcal{M})d\\theta Now dissecting this formula a little further, we now have the average predictions for all \\theta \\theta 's which we weight by the posteriors. This arises a natural ensemble scheme where we are averaging models. Also worth noting is that there is no optimization within any of these quantities. Practically, my brain is telling me that that is a bit useless but I guess in the Bayesian framework, that's not really all that necessary. But yes, optimal model parameters would be needed for actually making decisions on future data. We can write a loss function \\mathcal{L} \\mathcal{L} w.r.t. \\theta \\theta to express how one would optimize this quantity: \\mathcal{L}(\\theta^*)= \\mathcal{L}(\\theta^*)=","title":"Maximum A Posteriori (MAP) Estimation"},{"location":"appendix/bayesian/intro/#model-comparison","text":"P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} P(\\mathcal{M}|\\mathcal{D})=\\frac{P(\\mathcal{D}|\\mathcal{M})P(\\mathcal{M})}{P(\\mathcal{D})} The most interesting one is deriving the maximum likelihood formulation (Ocamz Razer, Model Evidence, Integrated Likelihood): what's the probability that the data I have came from the model P(\\mathcal{D|M}) P(\\mathcal{D|M}) . Again using Bayes sum rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D},\\theta|\\mathcal{M})d\\theta Once again, we don't have access to this joint distribution, but we do have access to the likelihood and the prior. So, again, we can decompose this joint distribution by using the Bayes product rule: P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta P(\\mathcal{D}|\\mathcal{M})=\\int P(\\mathcal{D}|\\theta,\\mathcal{M})P(\\mathcal{\\theta}|\\mathcal{M})d\\theta","title":"Model Comparison"},{"location":"appendix/bayesian/intro/#questions","text":"Marginalize? Normalize? Why is it so hard? Intractability? Posterior Likelihood Prior","title":"QUESTIONS"},{"location":"appendix/bayesian/intro/#-","text":"","title":"---"},{"location":"appendix/bayesian/intro/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/bayesian/intro/#bait-and-switch","text":"MAP estimate is easy to make it work but can do some weird stuff. Bayesian - hard to make it work but sometimes makes more sense. Maximum Likelihood \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) <span><span class=\"MathJax_Preview\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)</span><script type=\"math/tex\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w) MAP \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) \\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) <span><span class=\"MathJax_Preview\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w)</span><script type=\"math/tex\">\\underset{w}{\\text{argmax }} \\mathcal{P}(y|x,w)\\mathcal{P}(w) Type II Maximum Likelihood \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw <span><span class=\"MathJax_Preview\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw</span><script type=\"math/tex\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)=\\int \\mathcal{P}(y|x,w)\\mathcal{P}(w|\\alpha)dw Type II MAP: \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) \\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha) <span><span class=\"MathJax_Preview\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha)</span><script type=\"math/tex\">\\underset{\\alpha}{\\text{argmax }} \\mathcal{P}(y|x,\\alpha)\\mathcal{P}(\\alpha)","title":"Bait and Switch"},{"location":"appendix/bayesian/intro/#tips-and-tricks-for-practicioners","text":"Set initial hyper-parameters with domain knowledge Standardize input data Set initial length scales to \\sigma_l \\approx 0.5 \\sigma_l \\approx 0.5 Standardize targets \\mathbf{y} \\mathbf{y} Set initial signal variance to \\sigma_f\\approx 1.0 \\sigma_f\\approx 1.0 Set noise level initially high \\sigma_n \\approx 0.5 \\times \\sigma_f \\sigma_n \\approx 0.5 \\times \\sigma_f Random restarts Penalize high signal-to-noise ratios ( \\frac{\\sigma_f}{\\sigma_n} \\frac{\\sigma_f}{\\sigma_n} )","title":"Tips and Tricks for Practicioners"},{"location":"appendix/bayesian/intro/#resources","text":"Deisenroth - Model Selection Bayes Theorem Net Probabilistic World - What is Bayes Theorem | The Anatomy of Bayes Theorem Zhoubin Gharamani - YouTube Talk Learning From Data - Website TODO: FlowChart of Learning and Probabilistic Learning Linear Regression Experiments A Probabilistic View of Linear Regression - Keng (2016) - blog A Probabilistic Interpretation of Regularization - Brian Keng (2016) - blog Code : FlowCharts - Mermaid | VSC Ext","title":"Resources"},{"location":"appendix/bayesian/intro/#definitiions","text":"Bayes Rule Inference - Some conclusion reached given by some evidence or reasoning Probability - An expression of belief Probabilistic Modeling","title":"Definitiions"},{"location":"appendix/bayesian/kde/","text":"Kernel Density Estimation \u00b6 Resources \u00b6 Built-In \u00b6 Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/kde/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/kde/#resources","text":"","title":"Resources"},{"location":"appendix/bayesian/kde/#built-in","text":"Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Built-In"},{"location":"appendix/bayesian/kl_divergence/","text":"KL Divergence \u00b6 Typical : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x VI : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x Positive and Reverse KL \u00b6 Density Ratio Estimation for KL Divergence Minimization between Implicit Distributions - Blog Resources : YouTube Aurelien Geron - Short Intro to Entropy, Cross-Entropy and KL-Divergence Ben Lambert - Through Secret Codes Zhoubin - Video > A nice talk where he highlights the asymptotic conditions for MLE. The proof is sketched using the minimization of the KLD function. Blog Anna-Lena Popkes KLD Explained KLD for ML Reverse Vs Forward KL KL-Divergence as an Objective Function NF Slides (MLE context) Edward Class Notes Stanford - MLE | Consistency and Asymptotic Normality of the MLE | Fisher Information, Cramer-Raw LB | MLE Model Mispecification Code KLD py NumPy/SciPy Recipes","title":"KL Divergence"},{"location":"appendix/bayesian/kl_divergence/#kl-divergence","text":"Typical : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x VI : \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x \\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x)\\right] = - \\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} d\\mathbf x","title":"KL Divergence"},{"location":"appendix/bayesian/kl_divergence/#positive-and-reverse-kl","text":"Density Ratio Estimation for KL Divergence Minimization between Implicit Distributions - Blog Resources : YouTube Aurelien Geron - Short Intro to Entropy, Cross-Entropy and KL-Divergence Ben Lambert - Through Secret Codes Zhoubin - Video > A nice talk where he highlights the asymptotic conditions for MLE. The proof is sketched using the minimization of the KLD function. Blog Anna-Lena Popkes KLD Explained KLD for ML Reverse Vs Forward KL KL-Divergence as an Objective Function NF Slides (MLE context) Edward Class Notes Stanford - MLE | Consistency and Asymptotic Normality of the MLE | Fisher Information, Cramer-Raw LB | MLE Model Mispecification Code KLD py NumPy/SciPy Recipes","title":"Positive and Reverse KL"},{"location":"appendix/bayesian/mixtures/","text":"Mixture Models \u00b6 Resources \u00b6 Overview \u00b6 Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog Code \u00b6 Built-in \u00b6 Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn. From Scratch \u00b6 ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"Mixture Models"},{"location":"appendix/bayesian/mixtures/#mixture-models","text":"","title":"Mixture Models"},{"location":"appendix/bayesian/mixtures/#resources","text":"","title":"Resources"},{"location":"appendix/bayesian/mixtures/#overview","text":"Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog","title":"Overview"},{"location":"appendix/bayesian/mixtures/#code","text":"","title":"Code"},{"location":"appendix/bayesian/mixtures/#built-in","text":"Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn.","title":"Built-in"},{"location":"appendix/bayesian/mixtures/#from-scratch","text":"ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"From Scratch"},{"location":"appendix/bayesian/monte_carlo/","text":"Monte Carlo Sampling \u00b6 MC in Python MC in Python Computational Statistics Parallel MC Sampling with multiprocessing Parallel in Python Parallel w. Ray Parallel MC w. Dask Comp Stats Parallel MC Sims in Python pt 1 MC n IS","title":"Monte carlo"},{"location":"appendix/bayesian/monte_carlo/#monte-carlo-sampling","text":"MC in Python MC in Python Computational Statistics Parallel MC Sampling with multiprocessing Parallel in Python Parallel w. Ray Parallel MC w. Dask Comp Stats Parallel MC Sims in Python pt 1 MC n IS","title":"Monte Carlo Sampling"},{"location":"appendix/bayesian/pdf_est/","text":"PDF Estimation \u00b6 Main Idea \u00b6 Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx Likelihood \u00b6 Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] Stochastic Gradient Descent \u00b6 Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages. Example \u00b6 Mixture of Gaussians \u00b6 p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise. Histogram Method \u00b6 Gotchas \u00b6 Search Sorted \u00b6 Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_est/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_est/#main-idea","text":"Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx","title":"Main Idea"},{"location":"appendix/bayesian/pdf_est/#likelihood","text":"Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}]","title":"Likelihood"},{"location":"appendix/bayesian/pdf_est/#stochastic-gradient-descent","text":"Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages.","title":"Stochastic Gradient Descent"},{"location":"appendix/bayesian/pdf_est/#example","text":"","title":"Example"},{"location":"appendix/bayesian/pdf_est/#mixture-of-gaussians","text":"p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise.","title":"Mixture of Gaussians"},{"location":"appendix/bayesian/pdf_est/#histogram-method","text":"","title":"Histogram Method"},{"location":"appendix/bayesian/pdf_est/#gotchas","text":"","title":"Gotchas"},{"location":"appendix/bayesian/pdf_est/#search-sorted","text":"Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"Search Sorted"},{"location":"appendix/bayesian/pdf_estimation/","text":"PDF Estimation \u00b6 Histograms \u00b6 Bayesian Blocks for Histograms \u00b6 Histograms done the right way AstroML Example Dynamic Programming Example - Jake VanderPlas Kernel Density Estimation \u00b6 Comparison of 1D Density Estimators Compares Normal, Bayesian Blocks, Nearest Neighbors, Kernel Density KNN + Sklearn Density Estimation Gives a nice implementation of knn to do density estimation","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_estimation/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"appendix/bayesian/pdf_estimation/#histograms","text":"","title":"Histograms"},{"location":"appendix/bayesian/pdf_estimation/#bayesian-blocks-for-histograms","text":"Histograms done the right way AstroML Example Dynamic Programming Example - Jake VanderPlas","title":"Bayesian Blocks for Histograms"},{"location":"appendix/bayesian/pdf_estimation/#kernel-density-estimation","text":"Comparison of 1D Density Estimators Compares Normal, Bayesian Blocks, Nearest Neighbors, Kernel Density KNN + Sklearn Density Estimation Gives a nice implementation of knn to do density estimation","title":"Kernel Density Estimation"},{"location":"appendix/bayesian/regression/","text":"Regression \u00b6 Bayesian Regression Inference Bayesian Regression \u00b6 Model \u00b6 In typical regression problems we have some data \\mathcal{D} \\mathcal{D} which consists of some input-output pairs X,y X,y . We wish to find a function f(\\cdot) f(\\cdot) that maps the data X X to y y . We also assume that there is some noise in the outputs \\epsilon_y \\epsilon_y . We can also have noise on the inputs X X but we will discuss that at a later time. So concretely, we have: $$ \\begin{aligned} y &= w : x + \\epsilon_y \\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\end{aligned} $$ Let's demonstrate this by generating N data points from the true distribution. As seen from the figure above, the points that we generated line somewhere along the true line. Of course, we are privvy to see the true like but an algorithm might have trouble with such few points. In addition, we can see the weight space is quite large as well. One thing we can do is maximize the likelihood that y y comes from some normal distribution \\mathcal{N} \\mathcal{N} with some mean \\mu \\mu and standard deviation \\sigma^2 \\sigma^2 . $$ \\mathcal{F} = \\underset{w}{\\text{max}} \\log \\mathcal{N} (y_i | w : x_i, \\sigma^2) $$ So we will use the mean squared error (MSE) error as a loss function for our problem as maximizing the likelihood is equivalent to minimizing the MSE. Proof: max MLE = min MSE The likelihood of our model is: $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log p(y_i|x_i,\\theta)$$ And for simplicity, we assume the noise $\\epsilon$ comes from a Gaussian distribution and that it is constant. So we can rewrite our likelihood as $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\mathcal{N}(y_i | \\mathbf{x_i, w}, \\sigma^2)$$ Plugging in the full formula for the Gaussian distribution with some simplifications gives us: $$ \\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_e^2}} \\exp\\left( - \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} \\right) $$ We can use the log rule $\\log ab = \\log a + \\log b$ to rewrite this expression to separate the constant term from the exponential. Also, $\\log e^x = x$. $$ \\log p(y|\\mathbf{X,w}) = - \\frac{N}{2} \\log 2 \\pi \\sigma_e^2 - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} $$ So, the first term is constant so that we can ignore that in our loss function. We can do the same for the denominator for the second term. Let's simplify it to make our life easier. $$ \\log p(y|\\mathbf{X,w}) = - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ So we want to maximize this quantity: in other words, I want to find the parameter $\\mathbf{w}$ s.t. this equation is maximum. $$ \\mathbf{w}_{MLE} = \\argmax_{\\mathbf{w}} - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ We can rewrite this expression because the maximum of a negative quantity is the same as minimizing a positive quantity. $$ \\mathbf{w}_{MLE} = \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ This is the same as the MSE error expression; with the edition of a scalar value $1/N$. $$ \\begin{aligned} \\mathbf{w}_{MLE} &= \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 \\\\ &= \\argmin_{\\mathbf{w}} \\text{MSE} \\end{aligned} $$ **Note**: If we did not know $\\sigma_y^2$ then we would have to optimize this as well. Inference \u00b6","title":"Regression"},{"location":"appendix/bayesian/regression/#regression","text":"Bayesian Regression Inference","title":"Regression"},{"location":"appendix/bayesian/regression/#bayesian-regression","text":"","title":"Bayesian Regression"},{"location":"appendix/bayesian/regression/#model","text":"In typical regression problems we have some data \\mathcal{D} \\mathcal{D} which consists of some input-output pairs X,y X,y . We wish to find a function f(\\cdot) f(\\cdot) that maps the data X X to y y . We also assume that there is some noise in the outputs \\epsilon_y \\epsilon_y . We can also have noise on the inputs X X but we will discuss that at a later time. So concretely, we have: $$ \\begin{aligned} y &= w : x + \\epsilon_y \\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\end{aligned} $$ Let's demonstrate this by generating N data points from the true distribution. As seen from the figure above, the points that we generated line somewhere along the true line. Of course, we are privvy to see the true like but an algorithm might have trouble with such few points. In addition, we can see the weight space is quite large as well. One thing we can do is maximize the likelihood that y y comes from some normal distribution \\mathcal{N} \\mathcal{N} with some mean \\mu \\mu and standard deviation \\sigma^2 \\sigma^2 . $$ \\mathcal{F} = \\underset{w}{\\text{max}} \\log \\mathcal{N} (y_i | w : x_i, \\sigma^2) $$ So we will use the mean squared error (MSE) error as a loss function for our problem as maximizing the likelihood is equivalent to minimizing the MSE. Proof: max MLE = min MSE The likelihood of our model is: $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log p(y_i|x_i,\\theta)$$ And for simplicity, we assume the noise $\\epsilon$ comes from a Gaussian distribution and that it is constant. So we can rewrite our likelihood as $$\\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\mathcal{N}(y_i | \\mathbf{x_i, w}, \\sigma^2)$$ Plugging in the full formula for the Gaussian distribution with some simplifications gives us: $$ \\log p(y|\\mathbf{X,w}) = \\sum_{i=1}^N \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_e^2}} \\exp\\left( - \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} \\right) $$ We can use the log rule $\\log ab = \\log a + \\log b$ to rewrite this expression to separate the constant term from the exponential. Also, $\\log e^x = x$. $$ \\log p(y|\\mathbf{X,w}) = - \\frac{N}{2} \\log 2 \\pi \\sigma_e^2 - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma_e^2} $$ So, the first term is constant so that we can ignore that in our loss function. We can do the same for the denominator for the second term. Let's simplify it to make our life easier. $$ \\log p(y|\\mathbf{X,w}) = - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ So we want to maximize this quantity: in other words, I want to find the parameter $\\mathbf{w}$ s.t. this equation is maximum. $$ \\mathbf{w}_{MLE} = \\argmax_{\\mathbf{w}} - \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ We can rewrite this expression because the maximum of a negative quantity is the same as minimizing a positive quantity. $$ \\mathbf{w}_{MLE} = \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 $$ This is the same as the MSE error expression; with the edition of a scalar value $1/N$. $$ \\begin{aligned} \\mathbf{w}_{MLE} &= \\argmin_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{x_iw})^2 \\\\ &= \\argmin_{\\mathbf{w}} \\text{MSE} \\end{aligned} $$ **Note**: If we did not know $\\sigma_y^2$ then we would have to optimize this as well.","title":"Model"},{"location":"appendix/bayesian/regression/#inference","text":"","title":"Inference"},{"location":"appendix/bayesian/uniform/","text":"Uniform Distribution \u00b6 Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"appendix/bayesian/uniform/#uniform-distribution","text":"Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"appendix/bayesian/variational_inference/","text":"Variational Inference \u00b6 Motivations \u00b6 Variational inference is the most scalable inference method the machine learning community has (as of 2019). Tutorials https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/ https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/ ELBO - Derivation \u00b6 Let's start with the marginal likelihood function. \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using some algebra, let's take the log of the marginal likelihood (evidence): \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution q(\\mathbf x) q(\\mathbf x) . We get the following equation: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now with Jensen's inequality, we have the relationship f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] . We would like to put the \\log \\log function inside of the integral. Jensen's inequality allows us to do this. If we let f(\\cdot)= \\log(\\cdot) f(\\cdot)= \\log(\\cdot) then we get the Jensen's equality for a concave function, f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] . In this case if we match the terms to each component to the inequality, we have \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] So now finally we have both terms in the inequality. Summarizing everything we have the following relationship: log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x I'm going to switch up the terminology just to make it easier aesthetically. I'm going to let \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) be \\log \\mathcal{P}(y|\\theta) \\log \\mathcal{P}(y|\\theta) and \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) . So basically: \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence. \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} The punchline of this (after many calculated manipulations), is that we obtain an optimization equation \\mathcal{F}(\\theta) \\mathcal{F}(\\theta) : \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] where: Approximate posterior distribution: q(x) q(x) The best match to the true posterior \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . This is what we want to calculate. Reconstruction Cost: \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] The expected log-likelihood measure of how well the samples from q(x) q(x) are able to explain the data y y . Penalty: \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] Ensures that the explanation of the data q(x) q(x) doesn't deviate too far from your beliefs \\mathcal{P}(x) \\mathcal{P}(x) . (Okham's razor constraint) Source : VI Tutorial - Shakir Mohamed If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. ### Comments on q(x) q(x) We have now transformed our problem from an integration problem to an optimization problem where we optimize for q(x) q(x) directly. Many people tend to simplify q q but we could easily write some dependencies on the data for example q(x|\\mathcal{D}) q(x|\\mathcal{D}) . We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence. Typically q(x) q(x) is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem. Many people say 'tighten the bound' but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood. ## Pros and Cons ### Why Variational Inference? Applicable to all probabilistic models Transforms a problem from integration to one of optimization Convergence assessment Principled and Scalable approach to model selection Compact representation of posterior distribution Faster to converge Numerically stable Modern Computing Architectures (GPUs) Why Not Variational Inference? \u00b6 Approximate posterior only Difficulty in optimization due to local minima Under-estimates the variance of posterior Limited theory and guarantees for variational mehtods Resources \u00b6 Tutorial Series - Why? | ELBO | MC ELBO | Reparameterization | MC ELBO unBias | MC ELBO PyTorch | Talk Blog Posts: Neural Variational Inference Classical Theory Scaling Up BlackBox Mode VAEs and Helmholtz Machines Importance Weighted AEs Neural Samplers and Hierarchical VI Importance Weighted Hierarchical VI | Video Normal Approximation to the Posterior Distribution - blog Lower Bound Understaing the Variational Lower Bound Deriving the Variational Lower Bound Summaries Advances in Variational Inference Presentations VI Shakir Deisenroth - VI | IT Bayesian Non-Parametrics and Priors over functions here Reviews * From EM to SVI * Variational Inference * VI- Review for Statisticians * Tutorial on VI * VI w/ Code * VI - Mean Field * VI Tutorial * GMM * VI in GMM * GMM Pyro | Pyro * GMM PyTorch | PyTorch | PyTorchy Code Extensions Neural Samplers and Hierarchical Variational Inference From Scratch \u00b6 Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0 Blog Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Algorithm Breakdown: Expectation Maximization - blog Variational Inference \u00b6 Variational Bayes and The Mean-Field Approximation - Keng (2017) - blog","title":"Variational Inference"},{"location":"appendix/bayesian/variational_inference/#variational-inference","text":"","title":"Variational Inference"},{"location":"appendix/bayesian/variational_inference/#motivations","text":"Variational inference is the most scalable inference method the machine learning community has (as of 2019). Tutorials https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/ https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/","title":"Motivations"},{"location":"appendix/bayesian/variational_inference/#elbo-derivation","text":"Let's start with the marginal likelihood function. \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using some algebra, let's take the log of the marginal likelihood (evidence): \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf x So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution q(\\mathbf x) q(\\mathbf x) . We get the following equation: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot \\frac{q(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following: \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x Now with Jensen's inequality, we have the relationship f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\leq \\mathbb{E} [f(x)] . We would like to put the \\log \\log function inside of the integral. Jensen's inequality allows us to do this. If we let f(\\cdot)= \\log(\\cdot) f(\\cdot)= \\log(\\cdot) then we get the Jensen's equality for a concave function, f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] f(\\mathbb{E}[x]) \\geq \\mathbb{E} [f(x)] . In this case if we match the terms to each component to the inequality, we have \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\log \\cdot \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[ \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] \\geq \\mathbb{E}_\\mathcal{q(\\mathbf x)} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] So now finally we have both terms in the inequality. Summarizing everything we have the following relationship: log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x log \\mathcal{P}(y|\\theta) = \\log \\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot q(\\mathbf x) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x \\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x I'm going to switch up the terminology just to make it easier aesthetically. I'm going to let \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) be \\log \\mathcal{P}(y|\\theta) \\log \\mathcal{P}(y|\\theta) and \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) \\mathcal{F}(q, \\theta) \\leq \\mathcal{L}(\\theta) . So basically: \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) \\mathcal{L}(\\theta) =\\log \\mathcal{P}(y|\\theta) \\geq \\int_\\mathcal{X} \\left[\\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\right] q(\\mathbf x) \\cdot d\\mathbf x = \\mathcal{F}(q, \\theta) With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence. \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} \\mathcal{F}(q) = \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\cdot \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot d\\mathbf x}_{\\mathbb{E}_{q(\\mathbf{x})}} + \\underbrace{\\int_\\mathcal{X} q(\\mathbf x) \\log \\frac{\\mathcal{P}(\\mathbf x)}{q(\\mathbf x)} \\cdot d\\mathbf x}_{\\text{KL}} The punchline of this (after many calculated manipulations), is that we obtain an optimization equation \\mathcal{F}(\\theta) \\mathcal{F}(\\theta) : \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] where: Approximate posterior distribution: q(x) q(x) The best match to the true posterior \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . This is what we want to calculate. Reconstruction Cost: \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] \\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] The expected log-likelihood measure of how well the samples from q(x) q(x) are able to explain the data y y . Penalty: \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] Ensures that the explanation of the data q(x) q(x) doesn't deviate too far from your beliefs \\mathcal{P}(x) \\mathcal{P}(x) . (Okham's razor constraint) Source : VI Tutorial - Shakir Mohamed If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. ### Comments on q(x) q(x) We have now transformed our problem from an integration problem to an optimization problem where we optimize for q(x) q(x) directly. Many people tend to simplify q q but we could easily write some dependencies on the data for example q(x|\\mathcal{D}) q(x|\\mathcal{D}) . We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence. Typically q(x) q(x) is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem. Many people say 'tighten the bound' but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood. ## Pros and Cons ### Why Variational Inference? Applicable to all probabilistic models Transforms a problem from integration to one of optimization Convergence assessment Principled and Scalable approach to model selection Compact representation of posterior distribution Faster to converge Numerically stable Modern Computing Architectures (GPUs)","title":"ELBO - Derivation"},{"location":"appendix/bayesian/variational_inference/#why-not-variational-inference","text":"Approximate posterior only Difficulty in optimization due to local minima Under-estimates the variance of posterior Limited theory and guarantees for variational mehtods","title":"Why Not Variational Inference?"},{"location":"appendix/bayesian/variational_inference/#resources","text":"Tutorial Series - Why? | ELBO | MC ELBO | Reparameterization | MC ELBO unBias | MC ELBO PyTorch | Talk Blog Posts: Neural Variational Inference Classical Theory Scaling Up BlackBox Mode VAEs and Helmholtz Machines Importance Weighted AEs Neural Samplers and Hierarchical VI Importance Weighted Hierarchical VI | Video Normal Approximation to the Posterior Distribution - blog Lower Bound Understaing the Variational Lower Bound Deriving the Variational Lower Bound Summaries Advances in Variational Inference Presentations VI Shakir Deisenroth - VI | IT Bayesian Non-Parametrics and Priors over functions here Reviews * From EM to SVI * Variational Inference * VI- Review for Statisticians * Tutorial on VI * VI w/ Code * VI - Mean Field * VI Tutorial * GMM * VI in GMM * GMM Pyro | Pyro * GMM PyTorch | PyTorch | PyTorchy Code Extensions Neural Samplers and Hierarchical Variational Inference","title":"Resources"},{"location":"appendix/bayesian/variational_inference/#from-scratch","text":"Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0 Blog Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Algorithm Breakdown: Expectation Maximization - blog","title":"From Scratch"},{"location":"appendix/bayesian/variational_inference/#variational-inference_1","text":"Variational Bayes and The Mean-Field Approximation - Keng (2017) - blog","title":"Variational Inference"},{"location":"appendix/concepts/frameworks/","text":"Frameworks \u00b6 Principles to Products \u00b6 Shakir Mohammed Applications Assistive Tech. Advanced Science Climate and Energy Healthcare Fairness and Safety Autonomous Systems Reasoning Planning Explanation Rapid Learning World Simulation Objects and Relations Information Uncertainty Information Gain Causality Prediction Principles Probability Theory Bayesian Analysis Hypothesis Testing Estimation Theory Asymptotics","title":"Frameworks"},{"location":"appendix/concepts/frameworks/#frameworks","text":"","title":"Frameworks"},{"location":"appendix/concepts/frameworks/#principles-to-products","text":"Shakir Mohammed Applications Assistive Tech. Advanced Science Climate and Energy Healthcare Fairness and Safety Autonomous Systems Reasoning Planning Explanation Rapid Learning World Simulation Objects and Relations Information Uncertainty Information Gain Causality Prediction Principles Probability Theory Bayesian Analysis Hypothesis Testing Estimation Theory Asymptotics","title":"Principles to Products"},{"location":"appendix/concepts/matrix_tricks/","text":"\u00b6 Multivariate Normal Covariance Matrices and the Cholesky Decomposition","title":"Matrix tricks"},{"location":"appendix/concepts/matrix_tricks/#_1","text":"Multivariate Normal Covariance Matrices and the Cholesky Decomposition","title":""},{"location":"appendix/concepts/sleeper_concepts/","text":"Key Concepts \u00b6 Definitions \u00b6 Mathematics Geometry: Studying Shapes and Spaces Algebra: Studying Relationships Probability: Belief, Uncertainty Calculus: Mathematics of Change Concepts \u00b6 MLE vs MSE vs KLD - jessica yung MLE vs Cross Validation Jensen's Inequality Error Propagation Law of Iterative Expectations Robustness - Symmetry/Equivariance Model Steps/Principles Occam's Razor - selecting less complex models Kolmogrov Complexity Universal Prior Maximum Likelihood Minimum Description Length Uncertainty What is it? Learning Theory, Epistemic\\Bayesian, aleatoric, Out-of-Distribution Bayes Theorem, Similarity","title":"Key Concepts"},{"location":"appendix/concepts/sleeper_concepts/#key-concepts","text":"","title":"Key Concepts"},{"location":"appendix/concepts/sleeper_concepts/#definitions","text":"Mathematics Geometry: Studying Shapes and Spaces Algebra: Studying Relationships Probability: Belief, Uncertainty Calculus: Mathematics of Change","title":"Definitions"},{"location":"appendix/concepts/sleeper_concepts/#concepts","text":"MLE vs MSE vs KLD - jessica yung MLE vs Cross Validation Jensen's Inequality Error Propagation Law of Iterative Expectations Robustness - Symmetry/Equivariance Model Steps/Principles Occam's Razor - selecting less complex models Kolmogrov Complexity Universal Prior Maximum Likelihood Minimum Description Length Uncertainty What is it? Learning Theory, Epistemic\\Bayesian, aleatoric, Out-of-Distribution Bayes Theorem, Similarity","title":"Concepts"},{"location":"appendix/concepts/splines/","text":"Splines \u00b6 Example - CDF Functions \u00b6 Example from the AstroML Book - Code | Text Invert Interpolations - StackOverFlow Maximum Wind Speed Prediction at the Sprogo station - Scipy Lecture Notes Approximation by Spline Functions and Parametric Spline Curves with SciPy - Vadym Pasko Optimization and Root Finding - Computational Statistics","title":"Splines"},{"location":"appendix/concepts/splines/#splines","text":"","title":"Splines"},{"location":"appendix/concepts/splines/#example-cdf-functions","text":"Example from the AstroML Book - Code | Text Invert Interpolations - StackOverFlow Maximum Wind Speed Prediction at the Sprogo station - Scipy Lecture Notes Approximation by Spline Functions and Parametric Spline Curves with SciPy - Vadym Pasko Optimization and Root Finding - Computational Statistics","title":"Example - CDF Functions"},{"location":"appendix/concepts/calculus/autograd/","text":"Automatic Differentiation \u00b6 Tutorials \u00b6 Automatic Differentiation - Matthew James Johnson (2017) - Video","title":"Automatic Differentiation"},{"location":"appendix/concepts/calculus/autograd/#automatic-differentiation","text":"","title":"Automatic Differentiation"},{"location":"appendix/concepts/calculus/autograd/#tutorials","text":"Automatic Differentiation - Matthew James Johnson (2017) - Video","title":"Tutorials"},{"location":"appendix/concepts/calculus/change_of_variables/","text":"Change of Variables \u00b6 This is after making some transformation function, we can find the probability of that function by simply multiplying Normalizing Flows \u00b6 First we will apply the change of variables formula from the perspective of parametric Gaussianization. Recall that we have our original data distribution \\mathcal{x} \\mathcal{x} and we want to find some transformation z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) such that z z is drawn from a Gaussian distribution z\\sim \\mathcal{N}(0, \\mathbf{I}) z\\sim \\mathcal{N}(0, \\mathbf{I}) . graph LR A((X)) -- D --> B((Z)) \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| Let z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) , we can simplify the notation a bit: \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| Now we can rewrite this equation in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} Let's do the same thing as above but from the perspective of normalized flows (at least the original idea). I've seen the perspective of a transformation \\mathcal{G} \\mathcal{G} that maps data from a latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . graph LR A((Z)) -- G --> B((X)) In this instance, we have a generator \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} that transforms the data from the latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . We can describe this as x=\\mathcal{G}_{\\theta}(z) x=\\mathcal{G}_{\\theta}(z) , so therefore going from \\mathcal{Z} \\mathcal{Z} to \\mathcal{X} \\mathcal{X} is given by this equation z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) . So first, let's write out the transformation not including the function values. \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| Now let's add in the function values taking into account that z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) : \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| Here, we have something different because we have the determinant of a function's inverse. We assume that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} is invertible which would allow us to use the inverse function theorem to move the inverse outside of the \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} . \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| And now we can use the fact that the determinant of the inverse of the Jacobian of invertible function is simply the inverse of the determinant of the Jacobian of the invertible function. In words, that's a lot to unpack, but it basically means that: \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} So with this last idea in mind, we can finally construct the final form: \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} Again, we can write this in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| Resources : * Youtube: * Professor Leonard - How to Change Variables in Multiple Integrals (Jacobian) * mrgonzalezWHS - Change of Variables. Jacobian * Kishore Kashyap - Transformations I | Transformations II * MathInsight * Double Integrals | Example * Course * Cambridge * Transforming Density Functions * Transforming Bivariate Density Functions * Pauls Online Math Notes * Change of Variables Teaching Notes Transforming Density Functions Transformation of RVs","title":"Change of Variables"},{"location":"appendix/concepts/calculus/change_of_variables/#change-of-variables","text":"This is after making some transformation function, we can find the probability of that function by simply multiplying","title":"Change of Variables"},{"location":"appendix/concepts/calculus/change_of_variables/#normalizing-flows","text":"First we will apply the change of variables formula from the perspective of parametric Gaussianization. Recall that we have our original data distribution \\mathcal{x} \\mathcal{x} and we want to find some transformation z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) such that z z is drawn from a Gaussian distribution z\\sim \\mathcal{N}(0, \\mathbf{I}) z\\sim \\mathcal{N}(0, \\mathbf{I}) . graph LR A((X)) -- D --> B((Z)) \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| Let z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) , we can simplify the notation a bit: \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( z \\right) \\left| \\frac{\\partial z}{\\partial x} \\right| Now we can rewrite this equation in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} \\mathcal{P}_z(z)= \\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1} Let's do the same thing as above but from the perspective of normalized flows (at least the original idea). I've seen the perspective of a transformation \\mathcal{G} \\mathcal{G} that maps data from a latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . graph LR A((Z)) -- G --> B((X)) In this instance, we have a generator \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} that transforms the data from the latent space \\mathcal{Z} \\mathcal{Z} to the data space \\mathcal{X} \\mathcal{X} . We can describe this as x=\\mathcal{G}_{\\theta}(z) x=\\mathcal{G}_{\\theta}(z) , so therefore going from \\mathcal{Z} \\mathcal{Z} to \\mathcal{X} \\mathcal{X} is given by this equation z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) . So first, let's write out the transformation not including the function values. \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ z \\right] \\left| \\text{det} \\frac{\\partial z}{\\partial x} \\right| Now let's add in the function values taking into account that z = \\mathcal{G}^{-1}_{\\theta}(x) z = \\mathcal{G}^{-1}_{\\theta}(x) : \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}^{-1}(x)}{\\partial x} \\right| Here, we have something different because we have the determinant of a function's inverse. We assume that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} is invertible which would allow us to use the inverse function theorem to move the inverse outside of the \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} . \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| And now we can use the fact that the determinant of the inverse of the Jacobian of invertible function is simply the inverse of the determinant of the Jacobian of the invertible function. In words, that's a lot to unpack, but it basically means that: \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\left| \\text{det} \\left(\\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x}\\right)^{-1} \\right| = \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} So with this last idea in mind, we can finally construct the final form: \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} \\mathcal{P}_x(x)=\\mathcal{P}_z\\left[ \\mathcal{G}_{\\theta}^{-1}(x) \\right] \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right|^{-1} Again, we can write this in terms of \\mathcal{P}_z(z) \\mathcal{P}_z(z) : \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| \\mathcal{P}_z(z)=\\mathcal{P}_x (x) \\left| \\text{det} \\frac{\\partial \\mathcal{G}_{\\theta}(z)}{\\partial x} \\right| Resources : * Youtube: * Professor Leonard - How to Change Variables in Multiple Integrals (Jacobian) * mrgonzalezWHS - Change of Variables. Jacobian * Kishore Kashyap - Transformations I | Transformations II * MathInsight * Double Integrals | Example * Course * Cambridge * Transforming Density Functions * Transforming Bivariate Density Functions * Pauls Online Math Notes * Change of Variables Teaching Notes Transforming Density Functions Transformation of RVs","title":"Normalizing Flows"},{"location":"appendix/concepts/calculus/identity_trick/","text":"Identity Trick \u00b6 I think the slides from the MLSS 2018 meeting is the only place that I have encountered anyone actually explicitly mentioning this Identity trick. Given an integral problem: p(x) = \\int p(x|z)p(z)dz p(x) = \\int p(x|z)p(z)dz I can multiply by an arbitrary distribution which is equivalent to 1. p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz Then I can regroup and reweight the integral p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz This results in a different expectation that we initially had p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] Examples: Importance Sampling Manipulate Stochastic gradients Derive Probability bounds RL for policy corrections","title":"[Identity Trick](https://www.shakirm.com/slides/MLSS2018-Madrid-ProbThinking.pdf)"},{"location":"appendix/concepts/calculus/identity_trick/#identity-trick","text":"I think the slides from the MLSS 2018 meeting is the only place that I have encountered anyone actually explicitly mentioning this Identity trick. Given an integral problem: p(x) = \\int p(x|z)p(z)dz p(x) = \\int p(x|z)p(z)dz I can multiply by an arbitrary distribution which is equivalent to 1. p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz p(x)=\\int p(x|z) p(z) \\frac{q(z)}{q(z)}dz Then I can regroup and reweight the integral p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz p(x) = \\int p(x|z)\\frac{p(z)}{q(z)}q(z)dz This results in a different expectation that we initially had p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] p(x) = \\underset{q(z)}{\\mathbb{E}}\\left[ p(x|z)\\frac{p(z)}{q(z)} \\right] Examples: Importance Sampling Manipulate Stochastic gradients Derive Probability bounds RL for policy corrections","title":"Identity Trick"},{"location":"appendix/concepts/calculus/inverse_function/","text":"Inverse Function Theorem \u00b6 Resources : * Wiki * YouTube * Prof Ghist Math - Inverse Function Theorem * The Infinite Looper - Inv Fun Theorem * Professor Leonard - Fundamental Theorem of Calculus | Derivatives of Inverse Functions Source : Mathematics for Machine Learning - Deisenroth (2019) Change of Variables: A Precursor to Normalizing Flow - Rui Shu Pattern Recognition and Machine Learning - Bishop (2006) Often we are faced with the situation where we do not know the distribution of our data. But perhaps we know the distribution of a transformation of our data, e.g. if we know that X X is a r.v. that is uniformly distributed, then what is the distribution of X^2 + X + c X^2 + X + c ? In this case, we want to understand what's the relationship between the distribution we know and the transformed distribution. One way to do so is to use the inverse transform theorem which directly uses the cumulative distribution function (CDF). Let's say we have u \\sim \\mathcal U(0,1) u \\sim \\mathcal U(0,1) and some invertible function f(\\cdot) f(\\cdot) that maps X \\sim \\mathcal P X \\sim \\mathcal P to u u . x = f(u) x = f(u) Now, we want to know the probability of x x when all we know is the probability of u u . \\mathcal P(x)=\\mathcal P(f(u)=x) \\mathcal P(x)=\\mathcal P(f(u)=x) So solving for u u in that equation gives us: \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) Now we see that u=f^{-1}(x) u=f^{-1}(x) which gives us a direct formulation for moving from the uniform distribution space \\mathcal U \\mathcal U to a different probability distribution space \\mathcal P \\mathcal P . Probability Integral Transform Resources * Brilliant Does a nice example where they talk about the problems with fat-tailed distributions. * Wiki * CrossValidated * How does the inverse transform method work * Help me understand the quantile (inverse CDF) function * Youtube * Ben Hambert - Intro to Inv Transform Sampling * Mathematical Monk * Intro | General Case | Invertible Case * Code Review - Inverse Transform Sampling * R Markdown - Inverse Transform Sampling * Using Chebyshev - Blog | Code * CDFs - Super powerful way to visualize data and also is uniformly distriuted * Histograms and CDFs - blog * Why We Love CDFS so Much and not histograms - Blog * Boundary Issues * Confidence Band from DKW inequality - code * Make Monotonic - code * Matplotlib example of CDF bins versus theoretical (not smooth) - Code * Alternatives * KDE * Statsmodels Implementation - code | Univariate * KDE vs Histograms - blog * Empirical CDF * The Empirical Distribution Function - blog * Plotting an Empirical CDF In python - blog * Scipy histogram - code * Empirical CDF Function - code * ECDFs - notebook Derivative of an Inverse Function \u00b6 MathInsight - Link","title":"Inverse Function Theorem"},{"location":"appendix/concepts/calculus/inverse_function/#inverse-function-theorem","text":"Resources : * Wiki * YouTube * Prof Ghist Math - Inverse Function Theorem * The Infinite Looper - Inv Fun Theorem * Professor Leonard - Fundamental Theorem of Calculus | Derivatives of Inverse Functions Source : Mathematics for Machine Learning - Deisenroth (2019) Change of Variables: A Precursor to Normalizing Flow - Rui Shu Pattern Recognition and Machine Learning - Bishop (2006) Often we are faced with the situation where we do not know the distribution of our data. But perhaps we know the distribution of a transformation of our data, e.g. if we know that X X is a r.v. that is uniformly distributed, then what is the distribution of X^2 + X + c X^2 + X + c ? In this case, we want to understand what's the relationship between the distribution we know and the transformed distribution. One way to do so is to use the inverse transform theorem which directly uses the cumulative distribution function (CDF). Let's say we have u \\sim \\mathcal U(0,1) u \\sim \\mathcal U(0,1) and some invertible function f(\\cdot) f(\\cdot) that maps X \\sim \\mathcal P X \\sim \\mathcal P to u u . x = f(u) x = f(u) Now, we want to know the probability of x x when all we know is the probability of u u . \\mathcal P(x)=\\mathcal P(f(u)=x) \\mathcal P(x)=\\mathcal P(f(u)=x) So solving for u u in that equation gives us: \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) \\mathcal P(x) = \\mathcal P(u=f^{-1}(x)) Now we see that u=f^{-1}(x) u=f^{-1}(x) which gives us a direct formulation for moving from the uniform distribution space \\mathcal U \\mathcal U to a different probability distribution space \\mathcal P \\mathcal P . Probability Integral Transform Resources * Brilliant Does a nice example where they talk about the problems with fat-tailed distributions. * Wiki * CrossValidated * How does the inverse transform method work * Help me understand the quantile (inverse CDF) function * Youtube * Ben Hambert - Intro to Inv Transform Sampling * Mathematical Monk * Intro | General Case | Invertible Case * Code Review - Inverse Transform Sampling * R Markdown - Inverse Transform Sampling * Using Chebyshev - Blog | Code * CDFs - Super powerful way to visualize data and also is uniformly distriuted * Histograms and CDFs - blog * Why We Love CDFS so Much and not histograms - Blog * Boundary Issues * Confidence Band from DKW inequality - code * Make Monotonic - code * Matplotlib example of CDF bins versus theoretical (not smooth) - Code * Alternatives * KDE * Statsmodels Implementation - code | Univariate * KDE vs Histograms - blog * Empirical CDF * The Empirical Distribution Function - blog * Plotting an Empirical CDF In python - blog * Scipy histogram - code * Empirical CDF Function - code * ECDFs - notebook","title":"Inverse Function Theorem"},{"location":"appendix/concepts/calculus/inverse_function/#derivative-of-an-inverse-function","text":"MathInsight - Link","title":"Derivative of an Inverse Function"},{"location":"appendix/concepts/calculus/jensens/","text":"Jensens Inequality \u00b6 This theorem is one of those sleeper theorems which comes up in a big way in many machine learning problems. The Jensen inequality theorem states that for a convex function f f , \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) A convex function (or concave up) is when there exists a minimum to that function. If we take two points on any part of the graph and draw a line between them, we will be above or at (as a limit) the minimum point of the graph. We can flip the signs for a concave function. But we want the convex property because then it means it has a minimum value and this is useful for minimization strategies. Recall from Calculus class 101: let's look at the function f(x)=\\log x f(x)=\\log x . We can use the second derivative test to find out if a function is convex or not. If f'(x) \\geq 0 f'(x) \\geq 0 then it is concave up (or convex). I'll map out the derivatives below: f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} You'll see that -\\frac{1}{x^2}\\leq 0 -\\frac{1}{x^2}\\leq 0 for x \\in [0, \\infty) x \\in [0, \\infty) . This means that \\log x \\log x is a concave function. So, the solution to this if we want a convex function is to take the negative \\log \\log (which adds intuition as to why we typically take the negative log likelihood of many functions). Variational Inference \u00b6 Typically in the VI literature, they add this Jensen inequality property in order to come up with the Evidence Lower Bound (ELBO). But I never understood how it worked because I didn't know if they wanted the convex or concave. If we think of the loss function of the likelihood \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) and the ELBO \\mathcal{F}(q, \\theta) \\mathcal{F}(q, \\theta) . Take a look at the figure from Figure: Showing Typically we use the \\log \\log function when it's used Resources * Computational Statistics * Blog * Sleeper Theorems * DIT Package * Ox Educ - Intuition | Proof * MIT OpenCourseWare - Intro Prob. | Inequalitiese, Convergence and Weak Law of Large Numbers","title":"Jensens Inequality"},{"location":"appendix/concepts/calculus/jensens/#jensens-inequality","text":"This theorem is one of those sleeper theorems which comes up in a big way in many machine learning problems. The Jensen inequality theorem states that for a convex function f f , \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) \\mathbb{E} [f(x)] \\geq f(\\mathbb{E}[x]) A convex function (or concave up) is when there exists a minimum to that function. If we take two points on any part of the graph and draw a line between them, we will be above or at (as a limit) the minimum point of the graph. We can flip the signs for a concave function. But we want the convex property because then it means it has a minimum value and this is useful for minimization strategies. Recall from Calculus class 101: let's look at the function f(x)=\\log x f(x)=\\log x . We can use the second derivative test to find out if a function is convex or not. If f'(x) \\geq 0 f'(x) \\geq 0 then it is concave up (or convex). I'll map out the derivatives below: f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} f'(x) = \\frac{1}{x}$$ $$f''(x) = -\\frac{1}{x^2} You'll see that -\\frac{1}{x^2}\\leq 0 -\\frac{1}{x^2}\\leq 0 for x \\in [0, \\infty) x \\in [0, \\infty) . This means that \\log x \\log x is a concave function. So, the solution to this if we want a convex function is to take the negative \\log \\log (which adds intuition as to why we typically take the negative log likelihood of many functions).","title":"Jensens Inequality"},{"location":"appendix/concepts/calculus/jensens/#variational-inference","text":"Typically in the VI literature, they add this Jensen inequality property in order to come up with the Evidence Lower Bound (ELBO). But I never understood how it worked because I didn't know if they wanted the convex or concave. If we think of the loss function of the likelihood \\mathcal{L}(\\theta) \\mathcal{L}(\\theta) and the ELBO \\mathcal{F}(q, \\theta) \\mathcal{F}(q, \\theta) . Take a look at the figure from Figure: Showing Typically we use the \\log \\log function when it's used Resources * Computational Statistics * Blog * Sleeper Theorems * DIT Package * Ox Educ - Intuition | Proof * MIT OpenCourseWare - Intro Prob. | Inequalitiese, Convergence and Weak Law of Large Numbers","title":"Variational Inference"},{"location":"appendix/density/","text":"","title":"Index"},{"location":"appendix/density/histograms/","text":"Histograms \u00b6","title":"Histograms"},{"location":"appendix/density/histograms/#histograms","text":"","title":"Histograms"},{"location":"appendix/density/logistic/","text":"Logistic Distribution \u00b6 Cheat Sheet \u00b6 PDF Logistic Distribution f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) (-\\infty, \\infty) CDF Logistic Distribution F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) \\rightarrow [0, 1] (-\\infty, \\infty) \\rightarrow [0, 1] Quantile Function Logistic Distribution F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) where p \\sim \\mathcal{U}([0,1]) p \\sim \\mathcal{U}([0,1]) . Inverse sampling Log SoftMax \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) PyTorch Function - Functional.log_softmax Sigmoid \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} PyTorch Function - Functional.sigmoid Log Sigmoid \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) PyTorch Function - Functional.logsigmoid Log Sum Exponential \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) PyTorch Function - torch.logsumexp SoftPlus \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) PyTorch Function - Function.softplus","title":"Logistic Distribution"},{"location":"appendix/density/logistic/#logistic-distribution","text":"","title":"Logistic Distribution"},{"location":"appendix/density/logistic/#cheat-sheet","text":"PDF Logistic Distribution f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} f(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\frac{\\exp(-z)}{\\sigma(1 + \\exp(-z))^2} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) (-\\infty, \\infty) CDF Logistic Distribution F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} F(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{1 + \\exp(-z)} where z = \\frac{(x-\\mu)}{\\sigma} z = \\frac{(x-\\mu)}{\\sigma} . Support: (-\\infty, \\infty) \\rightarrow [0, 1] (-\\infty, \\infty) \\rightarrow [0, 1] Quantile Function Logistic Distribution F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) F^{-1}(x) = \\log\\left(\\frac{p}{1-p}\\right) = \\mu + \\sigma_{\\log} \\log \\left( \\frac{p}{1-p} \\right) where p \\sim \\mathcal{U}([0,1]) p \\sim \\mathcal{U}([0,1]) . Inverse sampling Log SoftMax \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) \\text{LogSoftmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_i)} \\right) PyTorch Function - Functional.log_softmax Sigmoid \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} \\text{Sigmoid}(x) = \\frac{1}{1 +\\exp(-x)} PyTorch Function - Functional.sigmoid Log Sigmoid \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) \\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 +\\exp(-x)} \\right) PyTorch Function - Functional.logsigmoid Log Sum Exponential \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) \\text{LogSumExp}(x)_i = \\log \\sum_j \\exp(x_{ij}) PyTorch Function - torch.logsumexp SoftPlus \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) \\text{SoftPlus}(x) = \\frac{1}{\\beta}\\log \\left(1 + \\exp(\\beta x) \\right) PyTorch Function - Function.softplus","title":"Cheat Sheet"},{"location":"appendix/density/flows/mixture_cdfs/","text":"Continuous Mixture CDFs \u00b6 Author: J. Emmanuel Johnson Paper: Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design - Ho et. al. (2019) We take K Logistics. x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, where \\theta=[\\pi, \\mu, \\beta] \\theta=[\\pi, \\mu, \\beta] are the mixture params. \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) Domain \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) CDF Function F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) Source : Flow++ Code Structure \u00b6 Forward Transform Mixture Log CDF(x) Logit Function Mixture Log PDF Inverse Transformation Sigmoid Function Mixture Inverse CDF Mixture Log PDF Mixture of Logistics Coupling Layer \u00b6","title":"Continuous Mixture CDFs"},{"location":"appendix/density/flows/mixture_cdfs/#continuous-mixture-cdfs","text":"Author: J. Emmanuel Johnson Paper: Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design - Ho et. al. (2019) We take K Logistics. x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, x \\rightarrow \\sigma^{-1}\\left[ \\text{MixLogCDF}_\\theta(x) \\right] \\cdot \\exp(a) + b, where \\theta=[\\pi, \\mu, \\beta] \\theta=[\\pi, \\mu, \\beta] are the mixture params. \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) \\text{MixLogCDF}_\\theta(x) = \\sum_{i=1}^K \\pi_i \\sigma((x-\\mu_i) \\cdot \\exp(-\\beta_i)) Domain \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) \\sigma^{-1}(p) \\rightarrow \\alpha \\in \\mathbf{R}^{+}, p\\in \\mathcal{U}([0,1]) CDF Function F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) F_\\theta(x) = \\sigma^{-1}\\left( \\sum_{j=1}^K \\pi_j \\sigma(\\frac{(x-\\mu_i)}{\\beta_j} \\right) Source : Flow++","title":"Continuous Mixture CDFs"},{"location":"appendix/density/flows/mixture_cdfs/#code-structure","text":"Forward Transform Mixture Log CDF(x) Logit Function Mixture Log PDF Inverse Transformation Sigmoid Function Mixture Inverse CDF Mixture Log PDF","title":"Code Structure"},{"location":"appendix/density/flows/mixture_cdfs/#mixture-of-logistics-coupling-layer","text":"","title":"Mixture of Logistics Coupling Layer"},{"location":"appendix/gps/1_introduction/","text":"Gaussian Processes Definition \u00b6 The first thing to understand about GPs is that we are actively placing a distribution \\mathcal{P}(f) \\mathcal{P}(f) on functions f f where these functions can be infinitely long function values f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . A GP generalizes the multivariate Gaussian distribution to infinitely many variables. A GP is a collection of random variables f_1, f_2, \\ldots f_1, f_2, \\ldots , any finite number of which is Gaussian distributed. A GP defines a distribution over functions p(f) p(f) which can be used for Bayesian regression. (Zhoubin) Another nice definition is: Gaussian Process : Any set of function variables \\{f_n \\}^{N}_{n=1} \\{f_n \\}^{N}_{n=1} has a joint Gaussian distribution with mean function m m . (Deisenroth) The nice thing is that this is provided by a mean function \\mu \\mu and covariance matrix \\mathbf{K} \\mathbf{K} Bayesian Inference Problem \u00b6 Objective Let's have some data set, \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) Model \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} $$ \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} $$ The prior on f f is a GP distribution, the likelihood is Gaussian, therefore the posterior on f f is also a GP, $$ P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} $$ So we can make predictions: $$ P(y_ |x_ , \\mathcal{D}) = \\int P(y_ |x_ , \\mathcal{D})P(f|\\mathcal{D})df $$ We can also do model comparison by way of the marginal likelihood (evidence) so that we can compare and tune the covariance functions $$ P(y|X) = \\int P(y|f,X)P(f)df $$ Bayesian Treatment So now how does this look in terms of the Bayes theorem: \\begin{aligned} \\text{Posterior} &= \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}}\\\\ p(f|X,y) &= \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} \\\\ \\end{aligned} \\begin{aligned} \\text{Posterior} &= \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}}\\\\ p(f|X,y) &= \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} \\\\ \\end{aligned} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) Gaussian Process Regression \u00b6 We only need a few elements to define a Gaussian process in itself. Just a mean function \\mu \\mu , a covariance matrix \\mathbf{K}_\\theta \\mathbf{K}_\\theta and some data, \\mathcal{D} \\mathcal{D} . Code class GPR : def __init__ ( self , mu , kernel , X , y , noise_variance = 1e-6 ): self . mu = mu self . kernel = kernel self . x_train = x_train self . y_train = y_train self . noise_variance = noise_variance Gaussian Process Prior \u00b6 This is the basis of the GP method. Under the assumption that we mentioned above: $$ p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) $$ where: * m_\\theta m_\\theta is a mean function * \\mathbf{K} \\mathbf{K} is a covariance function We kind of treat these functions as a vector of function values up to infinity in theory f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . But in particular we look at the distribution over the function values, for example f_i=f(x_i) f_i=f(x_i) . So let's look at the joint distribution between N N function values f_N f_N and all other function values f_\\infty f_\\infty . This is 'normally distributed' so we can write the joint distribution roughly as: $$ \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) $$ where \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} and \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} (or m\\rightarrow \\infty m\\rightarrow \\infty ) to be more precise. So again, any marginal distribution of a joint Gaussian distribution is still a Gaussian distribution. So if we integrate over all of the functions from the infinite portion, we get: We can even get more specific and split the f_N f_N into training f_{\\text{train}} f_{\\text{train}} and testing f_{\\text{test}} f_{\\text{test}} . It's simply a matter of manipulating joint Gaussian distributions. So again, calculating the marginals: Code $$\\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned}$$ and we arrive at a joint Gaussian distribution of the training and testing which is still normally distributed due to the marginalization. Now, something a bit more practical, generally speaking when we program the sampling portion of the prior, we need data. The kernel function is as is and has already been defined with its appropriate parameters. Furthermore, we already have defined the mean function \\mu \\mu when we initialized the mean function above. So we just need to pass the function through the multivariate normal function along with the number of samples we would like to draw from the prior. def sample_prior ( self , X , n_samples = 1 , random_state = None ): \"\"\"Draws random n_samples from the multivariate normal distribution: X ~ N(mu, K(X,X)) Parameters ---------- X : array, (N x D) The vector data use to draw the random samples from where N is the number of data and D is the dimension. n_samples : int (default = 1) The number of random samples to draw from the normal distribution random_state : int, (default = None) Needed if we want to specify the random seed used to draw the random samples. Returns ------- samples : array, (n_samples x N) The samples drawn from the multivariate normal distribution. \"\"\" # calculate the covariance cov = self . kernel ( X , X ) # handle the random state rng = check_random_state ( random_state ) # draw samples from random multivariate normal samples = rng . multivariate_normal ( self . mu . ravel (), cov , int ( n_samples )) return samples Likelihood (noise model) \u00b6 p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) This comes from our assumption as stated above from y=f(x)+\\epsilon y=f(x)+\\epsilon . Alternative Notation: * y\\sim \\mathcal{N}(f, \\sigma_n^2) y\\sim \\mathcal{N}(f, \\sigma_n^2) * \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) Marginal Likelihood (Evidence) \u00b6 p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df where: * p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) * p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) Note that all we're doing is simply describing each of these elements specifically because all of these quantities are Gaussian distributed. p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df So the product of two Gaussians is simply a Gaussian. That along with the notion that the integral of all the functions is a normal distribution with mean \\mu \\mu and covariance K K . p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) Proof Using the Gaussian identities: $$\\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned}$$ So we can use the same reasoning to combine the prior and the likelihood to get the posterior $$\\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned}$$ **Source**: * Alternative Derivation for Log Likelihood - [blog](http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html) Posterior \u00b6 Alternative Notation: \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) Code def posterior ( self , X ): # K(x,x) K = self . kernel ( self . x_train , self . x_train ) # K(x,x') K_x = self . kernel ( self . x_train , X ) # K(x',x') K_xx = self . kernel ( X , X ) # Inverse of kernel K_inv = np . linalg . inv ( K + self . noise_variance * np . eye ( len ( self . x_train )) ) # Calculate the weights alpha = K_inv @ self . y_train # Calculate the mean function mu = K_x @ alpha # Calculate the covariance function cov = K_xx - K_x . T @ K_inv @ K_x return mu , cov Joint Probability Distribution \u00b6 To make GPs useful, we want to actually make predictions. This stems from the using the joint distribution of the training data and test data with the formula shown above used to condition on multivariate Gaussians. In terms of the GP function space, we have $$ \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\ K_{ x} & K_{* } \\end{bmatrix} \\right) \\end{aligned} $$ Then solving for the marginals, we can come up with the predictive test points. \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) where: \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} Learning in GPs \u00b6 The prior m(x), K m(x), K have hyper-parameters \\theta \\theta . So learning a \\mathcal{GP} \\mathcal{GP} implies inferring hyper-parameters from the model. \\mathcal{P}(Y|X,\\theta)=\\int \\mathcal{P}(Y|f)\\mathcal{P}(f|X, \\theta)df \\mathcal{P}(Y|X,\\theta)=\\int \\mathcal{P}(Y|f)\\mathcal{P}(f|X, \\theta)df However, we are not interested in f f directly. We can marginalize it out via the integral equation. The marginal of a Gaussian is Gaussian. Note : Typically we use the \\log \\log likelihood instead of a pure likelihood. This is purely for computational purposes. The \\log \\log function is monotonic so it doesn't alter the location of the extreme points of the function. Furthermore we typically minimize the -\\log -\\log instead of the maximum \\log \\log for purely practical reasons. One way to train these functions is to use Maximum A Posterior (MAP) of the hyper-parameters \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} Maximum Likelihood \u00b6 \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| In terms of the cholesky decomposition: Let \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) . We can write the log likelihood in terms of the cholesky decomposition. $$ \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\ &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\mathbf{L}^{-1}y||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} $$ This gives us a computational complexity of \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) Code from scipy.linalg.lapack import dtrtrs from scipy.linalg import cholesky , cho_solve def log_likelihood ( self , X , y ): n_samples = self . x_train . shape [ 0 ] K = self . kernel ( X ) K_gp = K + self . noise_variance * np . eye ( n_samples ) L = np . linalg . cholesky ( K_gp ) LinvY = dtrtrs ( L , y , lower = 1 )[ 0 ] # term I - constant logL = - ( n_samples / 2.0 ) * np . log ( 2 * np . pi ) # term II - inverse logL += - ( 1.0 / 2.0 ) * np . square ( LinvY ) . sum () # term III - determinant logL += - np . log ( np . diag ( L )) . sum () return logL source - Dai, [GPSS 2018](http://zhenwendai.github.io/slides/gpss2018_slides.pdf) Minimizing the log-likelihood function: \u00b6 Term I \u00b6 \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)^{-1}=- (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)(K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)^{-1}=- (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)(K+\\sigma^2I)^{-1} Term II \u00b6 \\begin{aligned} \\frac{\\partial}{\\partial \\theta} \\log \\left| K + \\sigma^2I \\right| &=\\text{trace }\\left( \\frac{\\partial}{\\partial \\theta} \\log (K + \\sigma^2 I) \\right) \\\\ &=\\text{trace }\\left( (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} (K + \\sigma^2 I) \\right) \\end{aligned} \\begin{aligned} \\frac{\\partial}{\\partial \\theta} \\log \\left| K + \\sigma^2I \\right| &=\\text{trace }\\left( \\frac{\\partial}{\\partial \\theta} \\log (K + \\sigma^2 I) \\right) \\\\ &=\\text{trace }\\left( (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} (K + \\sigma^2 I) \\right) \\end{aligned} Rule: \\log |\\text{det }A|=\\text{trace }(\\log A) \\log |\\text{det }A|=\\text{trace }(\\log A)","title":"1 introduction"},{"location":"appendix/gps/1_introduction/#definition","text":"The first thing to understand about GPs is that we are actively placing a distribution \\mathcal{P}(f) \\mathcal{P}(f) on functions f f where these functions can be infinitely long function values f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . A GP generalizes the multivariate Gaussian distribution to infinitely many variables. A GP is a collection of random variables f_1, f_2, \\ldots f_1, f_2, \\ldots , any finite number of which is Gaussian distributed. A GP defines a distribution over functions p(f) p(f) which can be used for Bayesian regression. (Zhoubin) Another nice definition is: Gaussian Process : Any set of function variables \\{f_n \\}^{N}_{n=1} \\{f_n \\}^{N}_{n=1} has a joint Gaussian distribution with mean function m m . (Deisenroth) The nice thing is that this is provided by a mean function \\mu \\mu and covariance matrix \\mathbf{K} \\mathbf{K}","title":"Definition"},{"location":"appendix/gps/1_introduction/#bayesian-inference-problem","text":"Objective Let's have some data set, \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) \\mathcal{D}= \\left\\{ (x_i, y_i)^N_{i=1} \\right\\}=(X,y) Model \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} \\begin{aligned} y_i &= f(x_i) + \\epsilon_i \\\\ f &\\sim \\mathcal{GP}(\\cdot | 0, K) \\\\ \\epsilon_i &\\sim \\mathcal{N}(\\cdot | 0, \\sigma^2) \\end{aligned} $$ \\begin{aligned} \\mathcal{P}(f_N) &= \\int_{f_\\infty}\\mathcal{P}(f_N,f_\\infty)df_\\infty \\ &= \\mathcal{N}(\\mu_{f_N},\\Sigma_{NN}) \\end{aligned} $$ The prior on f f is a GP distribution, the likelihood is Gaussian, therefore the posterior on f f is also a GP, $$ P(f|\\mathcal{D}) \\propto P(\\mathcal{D}|f)P(f) = \\mathcal{GP \\propto G \\cdot GP} $$ So we can make predictions: $$ P(y_ |x_ , \\mathcal{D}) = \\int P(y_ |x_ , \\mathcal{D})P(f|\\mathcal{D})df $$ We can also do model comparison by way of the marginal likelihood (evidence) so that we can compare and tune the covariance functions $$ P(y|X) = \\int P(y|f,X)P(f)df $$ Bayesian Treatment So now how does this look in terms of the Bayes theorem: \\begin{aligned} \\text{Posterior} &= \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}}\\\\ p(f|X,y) &= \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} \\\\ \\end{aligned} \\begin{aligned} \\text{Posterior} &= \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}}\\\\ p(f|X,y) &= \\frac{p(y|f, X) \\: p(f|X, \\theta)}{p(y| X)} \\\\ \\end{aligned} where: Prior: p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) p(f|X, \\theta)=\\mathcal{GP}(m_\\theta, \\mathbf{K}_\\theta) Likelihood (noise model): p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f(x), \\sigma_n^2\\mathbf{I}) Marginal Likelihood (Evidence): p(y|X)=\\int_f p(y|f,X)p(f|X)df p(y|X)=\\int_f p(y|f,X)p(f|X)df Posterior: p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*) p(f|X,y) = \\mathcal{GP}(\\mu_*, \\mathbf{K}_*)","title":"Bayesian Inference Problem"},{"location":"appendix/gps/1_introduction/#gaussian-process-regression","text":"We only need a few elements to define a Gaussian process in itself. Just a mean function \\mu \\mu , a covariance matrix \\mathbf{K}_\\theta \\mathbf{K}_\\theta and some data, \\mathcal{D} \\mathcal{D} . Code class GPR : def __init__ ( self , mu , kernel , X , y , noise_variance = 1e-6 ): self . mu = mu self . kernel = kernel self . x_train = x_train self . y_train = y_train self . noise_variance = noise_variance","title":"Gaussian Process Regression"},{"location":"appendix/gps/1_introduction/#gaussian-process-prior","text":"This is the basis of the GP method. Under the assumption that we mentioned above: $$ p(f|X, \\theta)=\\mathcal{GP}(m_\\theta , \\mathbf{K}_\\theta) $$ where: * m_\\theta m_\\theta is a mean function * \\mathbf{K} \\mathbf{K} is a covariance function We kind of treat these functions as a vector of function values up to infinity in theory f=[f_1, f_2, \\ldots] f=[f_1, f_2, \\ldots] . But in particular we look at the distribution over the function values, for example f_i=f(x_i) f_i=f(x_i) . So let's look at the joint distribution between N N function values f_N f_N and all other function values f_\\infty f_\\infty . This is 'normally distributed' so we can write the joint distribution roughly as: $$ \\mathcal{P}(f_N, f_\\infty)=\\mathcal{N} \\left(\\begin{bmatrix} \\mu_N \\ \\mu_\\infty \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{NN} & \\Sigma_{N\\infty} \\ \\Sigma_{N\\infty}^{\\top} & \\Sigma_{\\infty\\infty} \\end{bmatrix}\\right) $$ where \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} \\Sigma_{NN}\\in \\mathbb{R}^{N\\times N} and \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} \\Sigma_{\\infty\\infty} \\in \\mathbb{R}^{\\infty \\times \\infty} (or m\\rightarrow \\infty m\\rightarrow \\infty ) to be more precise. So again, any marginal distribution of a joint Gaussian distribution is still a Gaussian distribution. So if we integrate over all of the functions from the infinite portion, we get: We can even get more specific and split the f_N f_N into training f_{\\text{train}} f_{\\text{train}} and testing f_{\\text{test}} f_{\\text{test}} . It's simply a matter of manipulating joint Gaussian distributions. So again, calculating the marginals: Code $$\\begin{aligned} \\mathcal{P}(f_{\\text{train}}, f_{\\text{test}}) &= \\int_{f_\\infty}\\mathcal{P}(f_{\\text{train}}, f_{\\text{test}},f_\\infty)df_\\infty \\\\ &= \\mathcal{N} \\left(\\begin{bmatrix} f_{\\text{train}} \\\\ f_{\\text{test}} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{\\text{train} \\times \\text{train}} & \\Sigma_{\\text{train} \\times \\text{test}} \\\\ \\Sigma_{\\text{train} \\times \\text{test}}^{\\top} & \\Sigma_{\\text{test} \\times \\text{test}} \\end{bmatrix}\\right) \\end{aligned}$$ and we arrive at a joint Gaussian distribution of the training and testing which is still normally distributed due to the marginalization. Now, something a bit more practical, generally speaking when we program the sampling portion of the prior, we need data. The kernel function is as is and has already been defined with its appropriate parameters. Furthermore, we already have defined the mean function \\mu \\mu when we initialized the mean function above. So we just need to pass the function through the multivariate normal function along with the number of samples we would like to draw from the prior. def sample_prior ( self , X , n_samples = 1 , random_state = None ): \"\"\"Draws random n_samples from the multivariate normal distribution: X ~ N(mu, K(X,X)) Parameters ---------- X : array, (N x D) The vector data use to draw the random samples from where N is the number of data and D is the dimension. n_samples : int (default = 1) The number of random samples to draw from the normal distribution random_state : int, (default = None) Needed if we want to specify the random seed used to draw the random samples. Returns ------- samples : array, (n_samples x N) The samples drawn from the multivariate normal distribution. \"\"\" # calculate the covariance cov = self . kernel ( X , X ) # handle the random state rng = check_random_state ( random_state ) # draw samples from random multivariate normal samples = rng . multivariate_normal ( self . mu . ravel (), cov , int ( n_samples )) return samples","title":"Gaussian Process Prior"},{"location":"appendix/gps/1_introduction/#likelihood-noise-model","text":"p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) p(y|f,X)=\\prod_{i=1}^{N}\\mathcal{N}(y_i|f_i,\\sigma_\\epsilon^2)= \\mathcal{N}(y|f(x), \\sigma_\\epsilon^2\\mathbf{I}_N) This comes from our assumption as stated above from y=f(x)+\\epsilon y=f(x)+\\epsilon . Alternative Notation: * y\\sim \\mathcal{N}(f, \\sigma_n^2) y\\sim \\mathcal{N}(f, \\sigma_n^2) * \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i) \\mathcal{N}(f, \\sigma_n^2) = \\prod_{i=1}^N\\mathcal{P}(y_i, f_i)","title":"Likelihood (noise model)"},{"location":"appendix/gps/1_introduction/#marginal-likelihood-evidence","text":"p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df p(y|X, \\theta)=\\int_f p(y|f,X)\\: p(f|X, \\theta)\\: df where: * p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) p(y|f,X)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) * p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) p(f|X, \\theta)=\\mathcal{N}(f|m_\\theta, K_\\theta) Note that all we're doing is simply describing each of these elements specifically because all of these quantities are Gaussian distributed. p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df p(y|X, \\theta)=\\int_f \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|m_\\theta, K_\\theta) \\: df So the product of two Gaussians is simply a Gaussian. That along with the notion that the integral of all the functions is a normal distribution with mean \\mu \\mu and covariance K K . p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) p(y|X, \\theta)=\\mathcal{N}(y|m_\\theta, K_\\theta + \\sigma_n^2 \\mathbf{I}) Proof Using the Gaussian identities: $$\\begin{aligned} p(x) &= \\mathcal{N} (x | \\mu, \\Lambda^{-1}) \\\\ p(y|x) &= \\mathcal{N} (y | Ax+b, L^{-1}) \\\\ p(y) &= \\mathcal{N} (y|A\\mu + b, L^{-1} + A \\Lambda^{-1}A^T) \\\\ p(x|y) &= \\mathcal{N} (x|\\Sigma \\{ A^T L(y-b) + \\Lambda\\mu \\}, \\Sigma) \\\\ \\Sigma &= (\\Lambda + A^T LA)^{-1} \\end{aligned}$$ So we can use the same reasoning to combine the prior and the likelihood to get the posterior $$\\begin{aligned} p(f) &= \\mathcal{N} (f | m_\\theta, \\mathbf{K}_\\theta) \\\\ p(y|X) &= \\mathcal{N} (y | f(X), \\sigma^2\\mathbf{I}) \\\\ p(y) &= \\mathcal{N} (y|m_\\theta, \\sigma_y^2\\mathbf{I} + \\mathbf{K}_\\theta) \\\\ p(f|y) &= \\mathcal{N} (f|\\Sigma \\{ K^{-1}y + \\mathbf{K}_\\theta m_\\theta \\}, \\Sigma) \\\\ \\Sigma &= (K^{-1} + \\sigma^{-2}\\mathbf{I})^{-1} \\end{aligned}$$ **Source**: * Alternative Derivation for Log Likelihood - [blog](http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html)","title":"Marginal Likelihood (Evidence)"},{"location":"appendix/gps/1_introduction/#posterior","text":"Alternative Notation: \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) \\mathcal{P}(f|y)\\propto \\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I})\\cdot \\mathcal{N}(f|\\mu, \\mathbf{K}_{ff}) Code def posterior ( self , X ): # K(x,x) K = self . kernel ( self . x_train , self . x_train ) # K(x,x') K_x = self . kernel ( self . x_train , X ) # K(x',x') K_xx = self . kernel ( X , X ) # Inverse of kernel K_inv = np . linalg . inv ( K + self . noise_variance * np . eye ( len ( self . x_train )) ) # Calculate the weights alpha = K_inv @ self . y_train # Calculate the mean function mu = K_x @ alpha # Calculate the covariance function cov = K_xx - K_x . T @ K_inv @ K_x return mu , cov","title":"Posterior"},{"location":"appendix/gps/1_introduction/#joint-probability-distribution","text":"To make GPs useful, we want to actually make predictions. This stems from the using the joint distribution of the training data and test data with the formula shown above used to condition on multivariate Gaussians. In terms of the GP function space, we have $$ \\begin{aligned} \\mathcal{P}\\left(\\begin{bmatrix}f \\ f_*\\end{bmatrix} \\right) &= \\mathcal{N}\\left( \\begin{bmatrix} \\mu \\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K_{xx} & K_{x*} \\ K_{ x} & K_{* } \\end{bmatrix} \\right) \\end{aligned} $$ Then solving for the marginals, we can come up with the predictive test points. \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) \\mathcal{P}(f_* |X_*, y, X, \\theta)= \\mathcal{N}(f_* | \\mu_*, \\nu^2_* ) where: \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\mu*=K_* (K + \\sigma^2 I)^{-1}y=K_* \\alpha \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top} \\nu^2_*= K_{**} - K_*(K + \\sigma^2I)^{-1}K_*^{\\top}","title":"Joint Probability Distribution"},{"location":"appendix/gps/1_introduction/#learning-in-gps","text":"The prior m(x), K m(x), K have hyper-parameters \\theta \\theta . So learning a \\mathcal{GP} \\mathcal{GP} implies inferring hyper-parameters from the model. \\mathcal{P}(Y|X,\\theta)=\\int \\mathcal{P}(Y|f)\\mathcal{P}(f|X, \\theta)df \\mathcal{P}(Y|X,\\theta)=\\int \\mathcal{P}(Y|f)\\mathcal{P}(f|X, \\theta)df However, we are not interested in f f directly. We can marginalize it out via the integral equation. The marginal of a Gaussian is Gaussian. Note : Typically we use the \\log \\log likelihood instead of a pure likelihood. This is purely for computational purposes. The \\log \\log function is monotonic so it doesn't alter the location of the extreme points of the function. Furthermore we typically minimize the -\\log -\\log instead of the maximum \\log \\log for purely practical reasons. One way to train these functions is to use Maximum A Posterior (MAP) of the hyper-parameters \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned} \\begin{aligned} \\theta^* &= \\underset{\\theta}{\\text{argmax}}\\log p(y|X,\\theta) \\\\ &= \\underset{\\theta}{\\text{argmax}}\\log \\mathcal{N}(y | 0, K + \\sigma^2 I) \\end{aligned}","title":"Learning in GPs"},{"location":"appendix/gps/1_introduction/#maximum-likelihood","text":"\\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\log p(y|x, \\theta) = - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| In terms of the cholesky decomposition: Let \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) \\mathbf{L}=\\text{cholesky}(\\mathbf{K}+\\sigma_n^2\\mathbf{I}) . We can write the log likelihood in terms of the cholesky decomposition. $$ \\begin{aligned} \\log p(y|x, \\theta) &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2}y^{\\top}(K+\\sigma^2I)^{-1}y - \\frac{1}{2} \\log \\left| K+\\sigma^2I \\right| \\ &= - \\frac{N}{2} \\log 2\\pi - \\frac{1}{2} ||\\mathbf{L}^{-1}y||^2 - \\sum_i \\log \\mathbf{L}_{ii} \\end{aligned} $$ This gives us a computational complexity of \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) \\mathcal{O}(N + N^2 + N^3)=\\mathcal{O}(N^3) Code from scipy.linalg.lapack import dtrtrs from scipy.linalg import cholesky , cho_solve def log_likelihood ( self , X , y ): n_samples = self . x_train . shape [ 0 ] K = self . kernel ( X ) K_gp = K + self . noise_variance * np . eye ( n_samples ) L = np . linalg . cholesky ( K_gp ) LinvY = dtrtrs ( L , y , lower = 1 )[ 0 ] # term I - constant logL = - ( n_samples / 2.0 ) * np . log ( 2 * np . pi ) # term II - inverse logL += - ( 1.0 / 2.0 ) * np . square ( LinvY ) . sum () # term III - determinant logL += - np . log ( np . diag ( L )) . sum () return logL source - Dai, [GPSS 2018](http://zhenwendai.github.io/slides/gpss2018_slides.pdf)","title":"Maximum Likelihood"},{"location":"appendix/gps/1_introduction/#minimizing-the-log-likelihood-function","text":"","title":"Minimizing the log-likelihood function:"},{"location":"appendix/gps/1_introduction/#term-i","text":"\\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)^{-1}=- (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)(K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)^{-1}=- (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} \\left( K + \\sigma^2I \\right)(K+\\sigma^2I)^{-1}","title":"Term I"},{"location":"appendix/gps/1_introduction/#term-ii","text":"\\begin{aligned} \\frac{\\partial}{\\partial \\theta} \\log \\left| K + \\sigma^2I \\right| &=\\text{trace }\\left( \\frac{\\partial}{\\partial \\theta} \\log (K + \\sigma^2 I) \\right) \\\\ &=\\text{trace }\\left( (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} (K + \\sigma^2 I) \\right) \\end{aligned} \\begin{aligned} \\frac{\\partial}{\\partial \\theta} \\log \\left| K + \\sigma^2I \\right| &=\\text{trace }\\left( \\frac{\\partial}{\\partial \\theta} \\log (K + \\sigma^2 I) \\right) \\\\ &=\\text{trace }\\left( (K+\\sigma^2I)^{-1} \\frac{\\partial}{\\partial \\theta} (K + \\sigma^2 I) \\right) \\end{aligned} Rule: \\log |\\text{det }A|=\\text{trace }(\\log A) \\log |\\text{det }A|=\\text{trace }(\\log A)","title":"Term II"},{"location":"appendix/gps/2_sparse_gps/","text":"Sparse Gaussian Processes \u00b6 [toc] Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories: Subset of Data (Transformation, Random Sampling) Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte) Inducing Points (SoR, FITC, DTC, KISS-GP) Linear Algebra (Toeplitz, Kronecker, ) Approximate Inference (Variational Methods) Each of these methods ultimately augment the model so that the largest computation goes from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(MN^2) \\mathcal{O}(MN^2) where M<<N M<<N . Subset of Data \u00b6 This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you're more likely to have. It's an age old rule that says if you want better predictions, it's often better just to have more data. A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space \\mathcal{X} \\in \\mathbb{R}^{N \\times D} \\mathcal{X} \\in \\mathbb{R}^{N \\times D} to subset data space \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} which is found through an eigen decomposition scheme. In GPs we calculate a kernel matrix \\mathbf K \\in \\mathbb{R}^{N \\times N} \\mathbf K \\in \\mathbb{R}^{N \\times N} . If N N is large enough, then throughout the marginal likelihood, we need to calculate \\mathbf K^{-1} \\mathbf K^{-1} and |\\mathbf K| |\\mathbf K| which has \\mathcal{O}(N^3) \\mathcal{O}(N^3) operations and \\mathcal{O}(N^2) \\mathcal{O}(N^2) memory costs. So we make an approximate matrix \\mathbf {\\tilde{K}} \\mathbf {\\tilde{K}} given by the following formula: \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} where: * K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} - the kernel matrix for the subspace \\mathcal{Z} \\mathcal{Z} * K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} - the transformation matrix from the data space \\mathcal{X} \\mathcal{X} to the subspace \\mathcal{Z} \\mathcal{Z} * K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} - the approximate kernel matrix of the data space \\mathcal{X} \\mathcal{X} Below is an example of where this would be applicable where we just implement this method where we just transform the day. from sklearn.kernel_approximation import Nystroem from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_processes.kernels import RBF # Initialize Nystrom transform nystrom_map = Nystrom ( random_state = 1 , n_components = 1 ) # Transform Data X_transformed = nystrom_map . fit_transform ( X ) # initialize GPR model = GPR () # fit GP model model . fit ( X_transformed , y ) Kernel Approximations \u00b6 Pivoting off of the method above, we So now when we calculate the log likelihood term \\log \\mathcal{P}(y|X,\\theta) \\log \\mathcal{P}(y|X,\\theta) we can have an approximation: \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) Notice how we haven't actually changing our formulation because we still have to calculate the inverse of \\tilde{K} \\tilde{K} which is \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} . Using the Woodbury matrix identity for the kernel approximation form ( Sherman-Morrison Formula ): (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} Now the matrix that we need to invert is (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} which is (M \\times M) (M \\times M) which is considerably smaller if M << N M << N . So the overall computational complexity reduces to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) . Inducing Points \u00b6 Deisenroth - GPs for Big Data - MLSS2015 Dai - Scalable GPs - MLSS2018 Sparse GPs - Inducing Points Summary \u00b6 So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data \\mathcal{Z} \\mathcal{Z} from the data space \\mathcal{D} \\mathcal{D} . We typically have some approximate matrix \\mathbf{Q} \\mathbf{Q} which approximates the kernel matrix \\mathbf{K} \\mathbf{K} : \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix \\mathbf{K} \\mathbf{K} . Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down: \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} The data fit term penalizes the data lying outside the covariance ellipse, the complexity penalty is the integral of the data fit term over all possible observations \\mathbf{y} \\mathbf{y} which characterizes the volume of possible datasets, the trace term ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods. Algorithm \\mathbf{G} \\mathbf{G} \\mathbf{T} \\mathbf{T} FITC diag (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} 0 VFE \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} DTC \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} 0 Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will argue for approximating the inference but I think it's important to be pragmatic about these sorts of things. Observations about the Sparse GPs \u00b6 VFE Overestimates noise variance Improves with additional inducing inputs Recovers the full GP Posterior Hindered by local optima FITC Can severly underestimate the noise variance May ignore additional inducing inputs Does not recover the full GP posterior Relies on Local Optima Some parameter initialization strategies: * K-Means * Initially fixing the hyperparameters * Random Restarts An interesting solution to find good hyperparameters for VFE: Find parameters with FITC solution Initialize GP model of VFE with FITC solutions Find parameters with VFE. Source: * Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - Paper * Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - Thesis Variational Compression \u00b6 Figure : This graphical model shows the relationship between the data X X , the labels y y and the augmented labels z z . This is a concept I've came across that seeks to give a stronger argument for using an augmented space \\mathcal Z\\in \\mathbb{R}^{M \\times D} \\mathcal Z\\in \\mathbb{R}^{M \\times D} instead of just the data space \\mathcal X \\in \\mathbb{R}^{N \\times D} \\mathcal X \\in \\mathbb{R}^{N \\times D} . This has allowed us to reduce the computational complexity of all of our most expensive calculations from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) when we are learning the best parameters for our GP models. The term variational compression comes from the notion that we want to suppress the function valuse f f with some auxilary variables u u . It's kind of like reducing the data space \\mathcal X \\mathcal X with the auxilary data space \\mathcal Z \\mathcal Z in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You'll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms. Joint Distribution - Augmented Space \\mathcal{P}(f,u) \\mathcal{P}(f,u) \u00b6 Let's add an additional set of variables u u that's jointly Gaussian with our original function f f . p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) We have a new space where we have introduced some auxilary variables u u to be modeled jointly with f f . Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution \\mathcal{P}(f|u) \\mathcal{P}(f|u) and marginal distribution \\mathcal{P}(u) \\mathcal{P}(u) in terms of the joint distribution \\mathcal P (f,u) \\mathcal P (f,u) using conditional probability rules. \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) where: Conditional Dist.: \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} Augmented space Prior: \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) We could actually marginalize out u u to get back to the standard GP prior \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) . But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain , \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) . We want to 'compress' the data space \\mathcal X \\mathcal X and subsequently the function space of f f . So now let's write the complete joint distribution which includes the data likelihood and the augmented latent variable space: \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} We have a new term which is the familiar GP likelihood term \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) . The rest of the terms we have already defined above. So now you can kind of see how we're attempting to compress the conditional distribution f f . We no longer need the prior for X X or f f in order to obtain the joint distribution for our model. The prior we have is \\mathcal P (u) \\mathcal P (u) which is kind of a made up variable. From henceforth, I will be omitting the dependency on X X and Z Z as they're not important for the argument that follows. But keep it in the back of your mind that that dependency does exist. Conditional Distribution - \\mathcal{P}(y|u) \\mathcal{P}(y|u) \u00b6 The next step would be to try and condition on f f and u u to obtain the conditional distribution of y y given u u , \\mathcal{P}(y|u) \\mathcal{P}(y|u) . We can rearrange the terms of the formula above like so: \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) and using the conditional probability rules P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} we can simplify the formula even further: \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) So, what are we looking at? We are looking at the new joint distribution of y y and f f given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the f f 's. In doing so, we obtain the conditional density that we set off to explore: \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df where: * \\mathcal{P}(y|f) \\mathcal{P}(y|f) - Likelihood * \\mathcal{P}(f|u) \\mathcal{P}(f|u) - Conditional Distribution The last step would be to try and see if we can calculate \\mathcal{P}(y) \\mathcal{P}(y) because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the u u 's I get after grouping the terms: \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du which reduces to: \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data \\mathcal P (y*|y) \\mathcal P (y*|y) . The two important issues are highlighted in that equation alone: We now have the same bottleneck on our parameter for u u as we do for standard Bayesian parametric modeling. The computation of \\mathcal P (y|u) \\mathcal P (y|u) is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior \\mathcal P (u) \\mathcal P (u) . Variational Bound on \\mathcal P (y|u) \\mathcal P (y|u) \u00b6 We've shown the difficulties of actually obtaining the probability density function of \\mathcal{P}(y) \\mathcal{P}(y) but in this section we're just going to show that we can obtain a lower bound for the conditional density function \\mathcal{P}(y|u) \\mathcal{P}(y|u) \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df I'll do the 4.5 classic steps in order to arrive at a variational lower bound: Given an integral problem , take the \\log \\log of both sides of the function. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df Introduce the variational parameter q(f) q(f) as a proposal with the Identity trick. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df Use Jensen's inequality for the log function to rearrange the formula to highlight the importance weight and provide a bound for \\mathcal{F}(q) \\mathcal{F}(q) : \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) Rearrange to look like an expectation and KL divergence using targeted \\log \\log rules: \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df Simplify notation to look like every paper in ML that uses VI to profit and obtain the variational lower bound . \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] Titsias Innovation: et q(f) = \\mathcal{P}(f|u) q(f) = \\mathcal{P}(f|u) . \u00b6 According to Titsias et al. (2009) he looked at what happens if we let q(f)=\\mathcal P (f|u) q(f)=\\mathcal P (f|u) . For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less. \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df As a thought experiment though, what would happen if we had thee true posterior of \\mathcal{P}(f|y,u) \\mathcal{P}(f|y,u) and an approximating density of \\mathcal{P}(f|u) \\mathcal{P}(f|u) ? Well, we can take the KL KL divergence of that quantity and we get the following: \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between \\mathcal{P}(f|u) \\mathcal{P}(f|u) and \\mathcal{P}(f|u) \\mathcal{P}(f|u) . Maximizing the bound will try to find the optimal compression and looks at the information between y y and u u . He does not that there is no bound and it is an exact bound when u=f u=f . I believe that's related to the GPFlow derivation of variational GPs implementation but I don't have more information on this. Sources : Deep Gaussian Processes - MLSS 2019 Gaussian Processes for Big Data - Hensman et. al. (2013) Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Scalable Variational Gaussian Process Classification - Hensman et. al. (2015) ELBOs \u00b6 Let \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} Lower Bound \u00b6 \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) where: \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} Nystrom approximation \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} Uncertainty Based Correction Variational Bound on \\mathcal P (y) \\mathcal P (y) \u00b6 In this scenario, we marginalize out the remaining u u 's and we can get an error bound on the \\mathcal P(y) \\mathcal P(y) \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) * James Hensman - GPSS 2015 | Aweseome Graphical Models The explicit form of the lower bound \\mathcal{P}(y) \\mathcal{P}(y) for is gives us: \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Stochastic Variational Inference \u00b6 Supplementary Material \u00b6 Important Formulas \u00b6 These formulas come up when we're looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix \\mathbf K\\in \\mathbb R^{N\\times N} \\mathbf K\\in \\mathbb R^{N\\times N} which implies we need to calculate the inverse \\mathbf K^{-1} \\mathbf K^{-1} and the determinant | | det \\mathbf K| \\mathbf K| which both require \\mathcal{O}(N^3) \\mathcal{O}(N^3) . These formulas below are useful when we want to avoid those computational complexity counts. Nystrom Approximation \u00b6 \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} Sherman-Morrison-Woodbury Formula \u00b6 (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} Sylvester Determinant Theorem \u00b6 \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| Resources \u00b6 Papers \u00b6 Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. * A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula. Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Presentations \u00b6 Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014) Notes \u00b6 On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014) Blogs \u00b6 Variational Free Energy for Sparse GPs - Gonzalo https://github.com/Alaya-in-Matrix/SparseGP","title":"Sparse Gaussian Processes"},{"location":"appendix/gps/2_sparse_gps/#sparse-gaussian-processes","text":"[toc] Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories: Subset of Data (Transformation, Random Sampling) Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte) Inducing Points (SoR, FITC, DTC, KISS-GP) Linear Algebra (Toeplitz, Kronecker, ) Approximate Inference (Variational Methods) Each of these methods ultimately augment the model so that the largest computation goes from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(MN^2) \\mathcal{O}(MN^2) where M<<N M<<N .","title":"Sparse Gaussian Processes"},{"location":"appendix/gps/2_sparse_gps/#subset-of-data","text":"This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you're more likely to have. It's an age old rule that says if you want better predictions, it's often better just to have more data. A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space \\mathcal{X} \\in \\mathbb{R}^{N \\times D} \\mathcal{X} \\in \\mathbb{R}^{N \\times D} to subset data space \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} \\mathcal{Z} \\in \\mathbb{R}^{M \\times d} which is found through an eigen decomposition scheme. In GPs we calculate a kernel matrix \\mathbf K \\in \\mathbb{R}^{N \\times N} \\mathbf K \\in \\mathbb{R}^{N \\times N} . If N N is large enough, then throughout the marginal likelihood, we need to calculate \\mathbf K^{-1} \\mathbf K^{-1} and |\\mathbf K| |\\mathbf K| which has \\mathcal{O}(N^3) \\mathcal{O}(N^3) operations and \\mathcal{O}(N^2) \\mathcal{O}(N^2) memory costs. So we make an approximate matrix \\mathbf {\\tilde{K}} \\mathbf {\\tilde{K}} given by the following formula: \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} \\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\\top} where: * K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} K_{zz}=K(z,z)\\in \\mathbb{R}^{M\\times M} - the kernel matrix for the subspace \\mathcal{Z} \\mathcal{Z} * K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} K_z=K(x,z)\\in \\mathbb{R}^{N\\times M} - the transformation matrix from the data space \\mathcal{X} \\mathcal{X} to the subspace \\mathcal{Z} \\mathcal{Z} * K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} K \\approx \\tilde{K} \\in \\mathbb{R}^{N \\times N} - the approximate kernel matrix of the data space \\mathcal{X} \\mathcal{X} Below is an example of where this would be applicable where we just implement this method where we just transform the day. from sklearn.kernel_approximation import Nystroem from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_processes.kernels import RBF # Initialize Nystrom transform nystrom_map = Nystrom ( random_state = 1 , n_components = 1 ) # Transform Data X_transformed = nystrom_map . fit_transform ( X ) # initialize GPR model = GPR () # fit GP model model . fit ( X_transformed , y )","title":"Subset of Data"},{"location":"appendix/gps/2_sparse_gps/#kernel-approximations","text":"Pivoting off of the method above, we So now when we calculate the log likelihood term \\log \\mathcal{P}(y|X,\\theta) \\log \\mathcal{P}(y|X,\\theta) we can have an approximation: \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) \\log \\mathcal{N}(y | 0, K + \\sigma^2I) \\approx \\log \\mathcal{N}(y | 0, \\tilde{K} + \\sigma^2I) Notice how we haven't actually changing our formulation because we still have to calculate the inverse of \\tilde{K} \\tilde{K} which is \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} . Using the Woodbury matrix identity for the kernel approximation form ( Sherman-Morrison Formula ): (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} (\\tilde{K} + \\sigma^2 I)^{-1}=\\sigma^{-2}I - \\sigma^{-4}K_z(K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1}K_z^{\\top} Now the matrix that we need to invert is (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} (K_{zz}+\\sigma^{-2}K_z^{\\top}K_z)^{-1} which is (M \\times M) (M \\times M) which is considerably smaller if M << N M << N . So the overall computational complexity reduces to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) .","title":"Kernel Approximations"},{"location":"appendix/gps/2_sparse_gps/#inducing-points","text":"Deisenroth - GPs for Big Data - MLSS2015 Dai - Scalable GPs - MLSS2018","title":"Inducing Points"},{"location":"appendix/gps/2_sparse_gps/#sparse-gps-inducing-points-summary","text":"So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data \\mathcal{Z} \\mathcal{Z} from the data space \\mathcal{D} \\mathcal{D} . We typically have some approximate matrix \\mathbf{Q} \\mathbf{Q} which approximates the kernel matrix \\mathbf{K} \\mathbf{K} : \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} \\mathbf{Q}_{ff}=\\mathbf{K}_{fu}\\mathbf{K}_{uu}^{-1}\\mathbf{K}_{uf} Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix \\mathbf{K} \\mathbf{K} . Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down: \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} \\mathcal{L}(\\theta)= \\frac{N}{2}\\log 2\\pi + \\underbrace{\\frac{1}{2} \\log\\left| \\mathbf{Q}_{ff}+G\\right|}_{\\text{Complexity Penalty}} + \\underbrace{\\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{Q}_{ff}+G)^{-1}\\mathbf{y}}_{\\text{Data Fit}} + \\underbrace{\\frac{1}{2\\sigma_n^2}\\text{tr}(\\mathbf{T})}_{\\text{Trace Term}} The data fit term penalizes the data lying outside the covariance ellipse, the complexity penalty is the integral of the data fit term over all possible observations \\mathbf{y} \\mathbf{y} which characterizes the volume of possible datasets, the trace term ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods. Algorithm \\mathbf{G} \\mathbf{G} \\mathbf{T} \\mathbf{T} FITC diag (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} (\\mathbf{K}_{ff}-\\mathbf{Q}_{ff}) + \\sigma_n^2\\mathbf{I} 0 VFE \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} \\mathbf{K}_{ff}-\\mathbf{Q}_{ff} DTC \\sigma_n^2 \\mathbf{I} \\sigma_n^2 \\mathbf{I} 0 Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will argue for approximating the inference but I think it's important to be pragmatic about these sorts of things.","title":"Sparse GPs - Inducing Points Summary"},{"location":"appendix/gps/2_sparse_gps/#observations-about-the-sparse-gps","text":"VFE Overestimates noise variance Improves with additional inducing inputs Recovers the full GP Posterior Hindered by local optima FITC Can severly underestimate the noise variance May ignore additional inducing inputs Does not recover the full GP posterior Relies on Local Optima Some parameter initialization strategies: * K-Means * Initially fixing the hyperparameters * Random Restarts An interesting solution to find good hyperparameters for VFE: Find parameters with FITC solution Initialize GP model of VFE with FITC solutions Find parameters with VFE. Source: * Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - Paper * Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - Thesis","title":"Observations about the Sparse GPs"},{"location":"appendix/gps/2_sparse_gps/#variational-compression","text":"Figure : This graphical model shows the relationship between the data X X , the labels y y and the augmented labels z z . This is a concept I've came across that seeks to give a stronger argument for using an augmented space \\mathcal Z\\in \\mathbb{R}^{M \\times D} \\mathcal Z\\in \\mathbb{R}^{M \\times D} instead of just the data space \\mathcal X \\in \\mathbb{R}^{N \\times D} \\mathcal X \\in \\mathbb{R}^{N \\times D} . This has allowed us to reduce the computational complexity of all of our most expensive calculations from \\mathcal{O}(N^3) \\mathcal{O}(N^3) to \\mathcal{O}(NM^2) \\mathcal{O}(NM^2) when we are learning the best parameters for our GP models. The term variational compression comes from the notion that we want to suppress the function valuse f f with some auxilary variables u u . It's kind of like reducing the data space \\mathcal X \\mathcal X with the auxilary data space \\mathcal Z \\mathcal Z in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You'll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms.","title":"Variational Compression"},{"location":"appendix/gps/2_sparse_gps/#joint-distribution-augmented-space-mathcalpfumathcalpfu","text":"Let's add an additional set of variables u u that's jointly Gaussian with our original function f f . p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) p(f,u)=\\mathcal{N}\\left( \\begin{bmatrix} f \\\\ u \\end{bmatrix}; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K_{ff} & K_{fu} \\\\ K_{uf} & K_{uu} \\end{bmatrix} \\right) We have a new space where we have introduced some auxilary variables u u to be modeled jointly with f f . Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution \\mathcal{P}(f|u) \\mathcal{P}(f|u) and marginal distribution \\mathcal{P}(u) \\mathcal{P}(u) in terms of the joint distribution \\mathcal P (f,u) \\mathcal P (f,u) using conditional probability rules. \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) \\mathcal P (f,u) = \\mathcal{P}(f|u) \\cdot \\mathcal{P}(u) where: Conditional Dist.: \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mathcal{P}(\\mathbf{f | u}) = \\mathcal N (f| \\mathbf {\\mu_u, \\nu^2_{uu}}) \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\mu_u = \\mathbf{K_{fu}K_{uu}^{-1}u} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} \\nu^2_{uu} = \\mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}} Augmented space Prior: \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) \\mathcal P (\\mathbf u) = \\mathcal N\\left( \\mathbf u | 0, \\mathbf K_{uu} \\right) We could actually marginalize out u u to get back to the standard GP prior \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) \\mathcal P (f) = \\mathcal{GP} (f | \\mathbf{ m, K_{ff}}) . But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain , \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) \\mathcal{O}(N^3) \\rightarrow \\mathcal{O}(NM^2) . We want to 'compress' the data space \\mathcal X \\mathcal X and subsequently the function space of f f . So now let's write the complete joint distribution which includes the data likelihood and the augmented latent variable space: \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} \\mathcal{P}(y,f,u|X,Z)= \\underbrace{\\mathcal{P}(y|f)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P} (f|u, X, Z)}_{\\text{Conditional Dist.}} \\cdot \\underbrace{\\mathcal{P}(u|Z)}_{\\text{Prior}} We have a new term which is the familiar GP likelihood term \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) \\mathcal P (y|f) = \\mathcal{N}(y|f, \\sigma_y^2\\mathbf I) . The rest of the terms we have already defined above. So now you can kind of see how we're attempting to compress the conditional distribution f f . We no longer need the prior for X X or f f in order to obtain the joint distribution for our model. The prior we have is \\mathcal P (u) \\mathcal P (u) which is kind of a made up variable. From henceforth, I will be omitting the dependency on X X and Z Z as they're not important for the argument that follows. But keep it in the back of your mind that that dependency does exist.","title":"Joint Distribution - Augmented Space \\mathcal{P}(f,u)\\mathcal{P}(f,u)"},{"location":"appendix/gps/2_sparse_gps/#conditional-distribution-mathcalpyumathcalpyu","text":"The next step would be to try and condition on f f and u u to obtain the conditional distribution of y y given u u , \\mathcal{P}(y|u) \\mathcal{P}(y|u) . We can rearrange the terms of the formula above like so: \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) \\frac{\\mathcal{P}(y,f,u)}{\\mathcal{P}(u)}= \\mathcal{P}(y|f) \\cdot\\mathcal{P}(f|u) and using the conditional probability rules P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} P(A,B)=P(A|B) \\cdot P(B) \\rightarrow P(A|B)=\\frac{P(A,B)}{P(B)} we can simplify the formula even further: \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\mathcal{P}(y,f|u)=\\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) So, what are we looking at? We are looking at the new joint distribution of y y and f f given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the f f 's. In doing so, we obtain the conditional density that we set off to explore: \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df where: * \\mathcal{P}(y|f) \\mathcal{P}(y|f) - Likelihood * \\mathcal{P}(f|u) \\mathcal{P}(f|u) - Conditional Distribution The last step would be to try and see if we can calculate \\mathcal{P}(y) \\mathcal{P}(y) because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the u u 's I get after grouping the terms: \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\underbrace{\\left[\\int_f \\mathcal{P}(y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\right]}_{\\mathcal{P}(y|u)} \\cdot \\mathcal P(u) \\cdot du which reduces to: \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du \\mathcal{P}(y)=\\int_u \\mathcal{P}(y|u) \\cdot \\mathcal P(u) \\cdot du This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data \\mathcal P (y*|y) \\mathcal P (y*|y) . The two important issues are highlighted in that equation alone: We now have the same bottleneck on our parameter for u u as we do for standard Bayesian parametric modeling. The computation of \\mathcal P (y|u) \\mathcal P (y|u) is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior \\mathcal P (u) \\mathcal P (u) .","title":"Conditional Distribution - \\mathcal{P}(y|u)\\mathcal{P}(y|u)"},{"location":"appendix/gps/2_sparse_gps/#variational-bound-on-mathcal-p-yumathcal-p-yu","text":"We've shown the difficulties of actually obtaining the probability density function of \\mathcal{P}(y) \\mathcal{P}(y) but in this section we're just going to show that we can obtain a lower bound for the conditional density function \\mathcal{P}(y|u) \\mathcal{P}(y|u) \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\mathcal{P}(y|u)=\\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df I'll do the 4.5 classic steps in order to arrive at a variational lower bound: Given an integral problem , take the \\log \\log of both sides of the function. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot df Introduce the variational parameter q(f) q(f) as a proposal with the Identity trick. \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df \\log \\mathcal P (y|u) = \\log \\int_f \\mathcal P (y|f) \\cdot \\mathcal{P}(f|u) \\cdot \\frac{q(f)}{q(f)} \\cdot df Use Jensen's inequality for the log function to rearrange the formula to highlight the importance weight and provide a bound for \\mathcal{F}(q) \\mathcal{F}(q) : \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) \\mathcal L () = \\log \\mathcal P (y|u) \\geq \\int_f q(f) \\cdot \\log \\frac{\\mathcal P (y|f) \\cdot \\mathcal{P}(f|u)}{q(f) } \\cdot df = \\mathcal F (q) Rearrange to look like an expectation and KL divergence using targeted \\log \\log rules: \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df \\mathcal F (q) = \\int_f q(f) \\cdot \\log \\mathcal P(y|f) \\cdot df - \\int_f q(f) \\cdot \\log \\frac{\\mathcal{P}(f|u)}{q(f)} \\cdot df Simplify notation to look like every paper in ML that uses VI to profit and obtain the variational lower bound . \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right] \\mathcal F (q) = \\mathbb E_{q(f)} \\left[ \\log \\mathcal P(y|f) \\right] - \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|u)\\right]","title":"Variational Bound on \\mathcal P (y|u)\\mathcal P (y|u)"},{"location":"appendix/gps/2_sparse_gps/#titsias-innovation-et-qf-mathcalpfuqf-mathcalpfu","text":"According to Titsias et al. (2009) he looked at what happens if we let q(f)=\\mathcal P (f|u) q(f)=\\mathcal P (f|u) . For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less. \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df \\log \\mathcal P (y|u) \\geq \\int_f \\mathcal P (f|u) \\cdot \\log \\mathcal P(y|f) \\cdot df As a thought experiment though, what would happen if we had thee true posterior of \\mathcal{P}(f|y,u) \\mathcal{P}(f|y,u) and an approximating density of \\mathcal{P}(f|u) \\mathcal{P}(f|u) ? Well, we can take the KL KL divergence of that quantity and we get the following: \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du \\text{D}_{\\text{KL}} \\left[ q(f) || \\mathcal{P}(f|y, u)\\right] = \\int_u \\mathcal P (f|u) \\cdot \\log \\frac{\\mathcal P (f|u)}{\\mathcal P (f|y,u)} \\cdot du According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between \\mathcal{P}(f|u) \\mathcal{P}(f|u) and \\mathcal{P}(f|u) \\mathcal{P}(f|u) . Maximizing the bound will try to find the optimal compression and looks at the information between y y and u u . He does not that there is no bound and it is an exact bound when u=f u=f . I believe that's related to the GPFlow derivation of variational GPs implementation but I don't have more information on this. Sources : Deep Gaussian Processes - MLSS 2019 Gaussian Processes for Big Data - Hensman et. al. (2013) Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) Scalable Variational Gaussian Process Classification - Hensman et. al. (2015)","title":"Titsias Innovation: et q(f) = \\mathcal{P}(f|u)q(f) = \\mathcal{P}(f|u)."},{"location":"appendix/gps/2_sparse_gps/#elbos","text":"Let \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top} \\mathbf \\Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\\top}","title":"ELBOs"},{"location":"appendix/gps/2_sparse_gps/#lower-bound","text":"\\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) \\mathcal{F}= \\log \\mathcal{N} \\left(y|0, \\tilde{\\mathbf K}_{ff} + \\sigma_y^2\\mathbf I \\right) - \\frac{1}{2\\sigma_y^2}\\text{tr}\\left( \\mathbf \\Sigma\\right) where: \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} \\tilde{\\mathbf K}_{ff} = \\mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\\top}} Nystrom approximation \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} \\mathbf \\Sigma = \\mathbf K_{ff} - \\tilde{\\mathbf K}_{ff} Uncertainty Based Correction","title":"Lower Bound"},{"location":"appendix/gps/2_sparse_gps/#variational-bound-on-mathcal-p-ymathcal-p-y","text":"In this scenario, we marginalize out the remaining u u 's and we can get an error bound on the \\mathcal P(y) \\mathcal P(y) \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du \\mathcal P (y) = \\int_u \\mathcal P (y|u) \\cdot \\mathcal P (u|Z) du Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014) * James Hensman - GPSS 2015 | Aweseome Graphical Models The explicit form of the lower bound \\mathcal{P}(y) \\mathcal{P}(y) for is gives us: \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) \\log \\mathcal P (y) \\geq \\log \\mathcal{N} (y|\\mathbf{y|K_{fu}^{-1}m, \\sigma_y^2I}) - \\frac{1}{2\\sigma_y^2} \\text{tr}\\left( \\right) Source : * Nested Variational Compression in Deep Gaussian Processes - Hensman et. al. (2014)","title":"Variational Bound on \\mathcal P (y)\\mathcal P (y)"},{"location":"appendix/gps/2_sparse_gps/#stochastic-variational-inference","text":"","title":"Stochastic Variational Inference"},{"location":"appendix/gps/2_sparse_gps/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"appendix/gps/2_sparse_gps/#important-formulas","text":"These formulas come up when we're looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix \\mathbf K\\in \\mathbb R^{N\\times N} \\mathbf K\\in \\mathbb R^{N\\times N} which implies we need to calculate the inverse \\mathbf K^{-1} \\mathbf K^{-1} and the determinant | | det \\mathbf K| \\mathbf K| which both require \\mathcal{O}(N^3) \\mathcal{O}(N^3) . These formulas below are useful when we want to avoid those computational complexity counts.","title":"Important Formulas"},{"location":"appendix/gps/2_sparse_gps/#nystrom-approximation","text":"\\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top} \\mathbf K_{NN} \\approx \\mathbf U_{NM} \\mathbf \\Lambda_{MM} \\mathbf U_{NM}^{\\top}","title":"Nystrom Approximation"},{"location":"appendix/gps/2_sparse_gps/#sherman-morrison-woodbury-formula","text":"(\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top} (\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N)^{-1} \\approx \\sigma_y^{-2}\\mathbf I_N + \\sigma_y^{-2} \\mathbf U_{NM}\\left( \\sigma_y^{-2}\\mathbf \\Lambda_{MM}^{-1} + \\mathbf U_{NM}^{\\top} \\mathbf U_{NM} \\right)^{-1}\\mathbf U_{NM}^{\\top}","title":"Sherman-Morrison-Woodbury Formula"},{"location":"appendix/gps/2_sparse_gps/#sylvester-determinant-theorem","text":"\\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right| \\left|\\mathbf K_{NN} + \\sigma_y^2 \\mathbf I_N \\right| \\approx |\\mathbf \\Lambda_{MM} | \\left|\\sigma_y^{2} \\mathbf \\Lambda_{MM}^{-1} + U_{NM}^{\\top} \\mathbf U_{NM} \\right|","title":"Sylvester Determinant Theorem"},{"location":"appendix/gps/2_sparse_gps/#resources","text":"","title":"Resources"},{"location":"appendix/gps/2_sparse_gps/#papers","text":"Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. * A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.","title":"Papers"},{"location":"appendix/gps/2_sparse_gps/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Thesis Explain"},{"location":"appendix/gps/2_sparse_gps/#presentations","text":"Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014)","title":"Presentations"},{"location":"appendix/gps/2_sparse_gps/#notes","text":"On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014)","title":"Notes"},{"location":"appendix/gps/2_sparse_gps/#blogs","text":"Variational Free Energy for Sparse GPs - Gonzalo https://github.com/Alaya-in-Matrix/SparseGP","title":"Blogs"},{"location":"appendix/gps/3_input_error/","text":"GPs and Uncertain Inputs through the Ages \u00b6 Summary Standard GP Formulation Stochastic Test Points Setup GP Predictive Distribution Numerical Integration Approximate Gaussian Distribution Stochastic Measurements Noisy-Input GP (NIGP) Expected Derivative Moment Matching Variational Strategies My Method - Marriage of Two Strategies Model Inference Supplementary Material Moment Matching Kernel Expectations Propagation of Variances NIGP - Propagating Variances Real Results with Variance Estimates Resources Papers Thesis Explain Important Papers Summary \u00b6 Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution Standard GP Formulation \u00b6 Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood Stochastic Test Points \u00b6 This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance. Setup \u00b6 Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step. GP Predictive Distribution \u00b6 So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation. Numerical Integration \u00b6 The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ). Approximate Gaussian Distribution \u00b6 Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations. Stochastic Measurements \u00b6 A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this. Noisy-Input GP (NIGP) \u00b6 We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings. Expected Derivative \u00b6 Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances . Moment Matching \u00b6 The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion. Variational Strategies \u00b6 What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ). My Method - Marriage of Two Strategies \u00b6 I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below. Model \u00b6 Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic. Inference \u00b6 So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter Supplementary Material \u00b6 Moment Matching \u00b6 In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function. Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Propagation of Variances \u00b6 Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) + NIGP - Propagating Variances \u00b6 Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation Real Results with Variance Estimates \u00b6 Figure : Absolute Error From a GP Model Figure : Standard GP Variance Estimates Figure : GP Variance Estimates account for input errors. Resources \u00b6 Papers \u00b6 Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Important Papers \u00b6","title":"GPs and Uncertain Inputs through the Ages"},{"location":"appendix/gps/3_input_error/#gps-and-uncertain-inputs-through-the-ages","text":"Summary Standard GP Formulation Stochastic Test Points Setup GP Predictive Distribution Numerical Integration Approximate Gaussian Distribution Stochastic Measurements Noisy-Input GP (NIGP) Expected Derivative Moment Matching Variational Strategies My Method - Marriage of Two Strategies Model Inference Supplementary Material Moment Matching Kernel Expectations Propagation of Variances NIGP - Propagating Variances Real Results with Variance Estimates Resources Papers Thesis Explain Important Papers","title":"GPs and Uncertain Inputs through the Ages"},{"location":"appendix/gps/3_input_error/#summary","text":"Figure : Intuition of a GP which takes into account the input error. When applying GPs for regression, we always assume that there is noise \\sigma_y^2 \\sigma_y^2 in the output measurements \\mathbf y \\mathbf y . We rarely every assume that their are errors in the input points \\mathbf x \\mathbf x . This assumption does not hold as we can definitely have errors in the inputs as well. For example, most sensors that take measurements have well calibrated errors which is needed for inputs to the next model. This chain of error moving through models is known as error propagation; something that is well known to any physics 101 student that has ever taken a laboratory class. In this review, I would like to go through some of the more important algorithms and dissect some of the mathematics behind it so that we can arrive at some understanding of uncertain inputs through the ages with GPs. As a quick overview, we will look at the following scenarios: Stochastic Test Data Stochastic Input Data My Contribution","title":"Summary"},{"location":"appendix/gps/3_input_error/#standard-gp-formulation","text":"Given some data \\mathcal{D}(X,y) \\mathcal{D}(X,y) we want to learn the following model. y_n = f(x_n) + \\epsilon_n y_n = f(x_n) + \\epsilon_n \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) f \\sim \\mathcal{GP}(\\mathbf m_\\theta, \\mathbf K_\\theta) So remember, we have the following 3 important quantities: Gaussian Likelihood: \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|\\mathbf{x}, f) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Gaussian Process Prior: \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) \\mathcal{P}(f) \\sim \\mathcal{GP}(\\mathbf m, \\mathbf K_\\theta) Gaussian Posterior: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu, \\mathbf \\nu^2) If you go through the steps of the GP formulation (see other document), then you will arrive at the following predictive distribution: \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) \\mathcal{P}(f_*|X_*, X, y)=\\mathcal{N}(\\mu_*, \\nu^2_{**}) where: \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\mu_* = m(X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}(y-m(X)) \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} \\nu^2_{**} = K(X_*, X_*) + k(X_*,X) \\left[ K(X,X) + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}k(X_*,X)^{\\top} This is the typical formulation which assumes that the output of x x is deterministic. However, what happens when x x is stochastic with some noise variance? We want to account for this in our scheme. Source : * Rasmussen - GPs | GP Posterior | Marginal Likelihood","title":"Standard GP Formulation"},{"location":"appendix/gps/3_input_error/#stochastic-test-points","text":"This is the typical scenario for most of the methods that exist in todays literature (that don't involve variational inference). In this instance, we are looking mainly at noisy test data \\mathbf X_* \\mathbf X_* . This is where most of the research lies as it is closely related to dynamical systems. Imagine you have some function with respect to time \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t \\mathbf x_{t+1}=f_t(\\mathbf x_t) + \\epsilon_t At time step t=0 t=0 we will have some output x_{1} x_{1} which is subject to f_0(x_0) f_0(x_0) and \\epsilon_0 \\epsilon_0 . The problem with this is that now the next input at time t=1 t=1 is a noisy input; by definition f_1(\\mathbf x_1 + \\epsilon_1) f_1(\\mathbf x_1 + \\epsilon_1) . So we can easy imagine how this subsequent models t+1 t+1 can quickly decrease in accuracy because the input error is now un-modeled. In the context of GPs, if a test input point \\mathbf x_* \\mathbf x_* has noise, we can simply integrate over all possible trial points. This will not result in a Gaussian distribution. However, we can approximate this distribution as Gaussian using moment matching methods by analytically (or numerically) calculating the mean and covariance.","title":"Stochastic Test Points"},{"location":"appendix/gps/3_input_error/#setup","text":"Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y y_* = f(\\mathbf{x}_*, \\theta) + \\epsilon_y \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} \\mathbf x_* = \\mathbf{\\bar x}_* + \\epsilon_\\mathbf{x} I've summarized the terms of these equations in words below: Equation I - Training Time y y - noise-corrupted training outputs \\mathbf{x} \\mathbf{x} - noise-free training inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation II - Testing Time y_* y_* - noise-corrupted test outputs \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) Equation III - Test Inputs Relationship \\mathbf x_* \\mathbf x_* - noise-corrupted test inputs \\mathbf{\\bar x}_* \\mathbf{\\bar x}_* - noise-free test inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) It seems like a lot of equations but I just wanted to highlight the distinction between the training procedure where we assume the inputs are noise-free and the testing procedure where we assume the inputs are noisy. Immediately this does not seem correct and we can immediately become skeptical at this decision but as a first approximation (especially applicable to time series), this is a good first step.","title":"Setup"},{"location":"appendix/gps/3_input_error/#gp-predictive-distribution","text":"So assuming that we are OK with this assumption, we can move on and think of this in terms of GPs. The nice thing about this setup is that we only need to care about the posterior of the GP because it only has an influence at test time . So we can train a GP assuming that the points that are being used for training are noise-free. More concretely, let's look at the only function that really matters in this scenario: the posterior function for a GP: \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mathcal{P}(f_*|\\mathbf{x}, y) \\sim \\mathcal{N}(\\mu_*, \\mathbf \\nu^2_{*}) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} Throughout this whole equation we have \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) \\mathbf x_* \\sim \\mathcal{N}(\\bar{\\mathbf x}_*, \\Sigma_{x*}) . This implies that \\mathbf x_* \\mathbf x_* is no longer deterministic; we now have a conditional distribution \\mathcal{P}(f_*|\\mathbf x_*) \\mathcal{P}(f_*|\\mathbf x_*) . We are not interested in this conditional probability distribution, only in the probability distribution of f_* f_* , \\mathcal{P}(f_*) \\mathcal{P}(f_*) . So, to get rid of the \\mathbf x_* \\mathbf x_* 's, we need to integrate them out. Through marginalization, we get the predictive distribution for f_* f_* by this equation: \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* \\mathcal{P}(f_*)= \\int_\\mathcal{X}\\mathcal{P}(f_* | \\mathbf x_*) \\cdot \\mathcal{P}(\\mathbf x_*) \\cdot d\\mathbf x_* (I omitted the conditional dependency on \\mathcal{D} \\mathcal{D} , \\mathbf x_* \\mathbf x_* and \\theta \\theta for brevity). We assume that \\mathbf x_* \\mathbf x_* is normally distributed and f_* f_* is normally distributed. So that means the conditional distribution \\mathcal{P}(f_* | \\mathbf x_*) \\mathcal{P}(f_* | \\mathbf x_*) is also normally distributed. The problem is that integrating out the \\mathbf x_* \\mathbf x_* it's not tractable. For example, for the first term (the most problematic) in the integral equation above we have: \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) \\mathcal{P}(f_* | \\mathbf x_*) = \\frac{1}{\\sqrt{|2\\pi \\cdot \\nu_*^2(\\mathbf x_*})}\\text{exp}\\left( -\\frac{1}{2}(f_* - \\mu(\\mathbf x_*))^{\\top} \\cdot \\nu^{-2}(\\mathbf x_*) \\cdot (f_* - \\mu(\\mathbf x_*) )\\right) It's not trivially how to find the determinant nor the inverse of the variance term inside of the equation.","title":"GP Predictive Distribution"},{"location":"appendix/gps/3_input_error/#numerical-integration","text":"The immediate way of solving some complex integral is to just brute force it with something like Monte-Carlo. \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) \\mathcal{P}(f_*)\\approx \\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(f_*|\\mu_{*t}, \\nu^2_{*t}) where every x_*^t x_*^t is drawn from \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) \\mathcal{N}(\\bar{\\mathbf x}_*|\\Sigma_x) . This will move towards the true distribution as T T gets larger but it can be prohibitive when dealing with high dimensional spaces as thee time need to converge to the true distribution gets longer as well. To get an idea about how this works, we can take a look using a simple numerical calculation ( example ).","title":"Numerical Integration"},{"location":"appendix/gps/3_input_error/#approximate-gaussian-distribution","text":"Another problem is that the resulting distribution may not result in a Gaussian distribution (due to some nonlinear interactions within the terms). We want it to be Gaussian because they're easy to use so it's in our best interest that they're Guassian. We could use Gaussian mixture models or Monte Carlo methods to deal with the non-Gaussian distributions. But in most of the literature, you'll find that we want assume (or force) the distribution to Gaussian by way of moment matching. For any distribution to be approximated as a Gaussian, we just need the expectation \\mathbb{E}[f_*] \\mathbb{E}[f_*] and the variance \\mathbb{V}[f_*] \\mathbb{V}[f_*] of that distribution. The derivation is quite long and cumbersome so I will skip to the final formula: \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} \\begin{aligned} \\tilde{\\mu}_* &= \\mathbb{E}[f_*] \\\\ &= \\int_\\mathcal{X} \\mathbf \\mu_* \\cdot \\mathcal{N}(\\mathbf{ x_*|\\bar{x}, \\Sigma_{x*}})d\\mathbf x_*\\\\ &= \\mathbf{\\Omega \\alpha} \\\\ \\end{aligned} where \\mathbf \\Omega \\mathbf \\Omega is something we call a kernel expectation (or sufficient statistic) in some cases. This involves the expectation given some distribution (usually the Normal distribution) where you need to integrate out the inputs. There are closed forms for some of these with specific kernels (see suppl. section below) but I will omit this information in this discussion. Overall the expression obtained above is a familiar expression with some different parameters. We can calculate the variance using some of the same logic but it's not as easy. So, to save time, I'll skip to the final part of the equation because the derivation is even worse than the mean. \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{V}[f_*] = \\mathbb{E}[\\left(f_* - \\mathbb{E}[f_*]\\right)^2] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_* = \\mathbb{E}[\\nu^2_*] - \\mathbb{E}[\\mu^2_*]-\\mathbb{E}^2[\\mu_*] \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) \\tilde{\\nu}^2_*= \\xi - \\text{tr}\\left(\\left( \\left(\\mathbf K + \\sigma_y^2\\mathbf I \\right)^{-1}-\\alpha\\alpha^{\\top}\\right)\\Phi\\right) - \\text{tr}\\left( \\Omega\\Omega^{\\top}\\alpha\\alpha^{\\top} \\right) where \\xi \\xi and \\Phi \\Phi are also kernel expectations. This final expression is not intuitive but it works fine for many applications; most notably the PILCO problem. The derivation in its entirety can be found here , here , and here if you're interested. It's worth noting that I don't this method is suitable for big data without further approximations to the terms in this equation as at first site it looks very inefficient and with complex and expensive calculations.","title":"Approximate Gaussian Distribution"},{"location":"appendix/gps/3_input_error/#stochastic-measurements","text":"A different; and perhaps more realistic and useful scenario; is if we assume that all of the inputs are stochastic. If all input points X X are stochastic, the above techniques of moment matching don't work well because typically they only look at test time and not training time. So, again, let's redefine the model under the assumption that \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . With equations that look something like: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) So in this scenario, we find that the training points are stochastic and the test points are deterministic. This gives us problems when we try to use the same techniques as mentioned above. As an example, let's look at the posterior function for a GP except it doesn't have to be at test time: \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mathcal{P}(f|\\mathbf{x}, y) \\sim \\mathcal{GP}(\\mu, \\mathbf \\nu^2) \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu(x) = \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} \\nu^2(x) = \\mathbf K(x, x')- \\mathbf K(x)\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K(x)^{\\top} We can skip all of that from the first section because we are assuming that \\mathbf x_* \\mathbf x_* is deterministic ( ???? ). We have to remember that we are assuming that we've already found the parameters \\bar{\\mathbf{x}} \\bar{\\mathbf{x}} and \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . So we just need to try and see if we can calculate the \\mu_* \\mu_* and \\nu^2_* \\nu^2_* . To see why it's not really possible to marginalize by the inputs, let's try to calculate the posterior mean \\mu_* \\mu_* . This equation depends on \\mathbf x_* \\mathbf x_* and \\mathbf x \\mathbf x where \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) \\mathbf x\\sim \\mathcal{N}(\\bar{\\mathbf x}, \\Sigma_x) . So if we want to marginalize over all of the stochastic data we get: \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*})= \\int_\\mathcal{X} \\mu_*(\\mathbf{x_*|x}) \\mathcal{P}(\\mathbf{x})d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x \\mu_* (\\mathbf{x_*}) = \\int_\\mathcal{X} \\left( m(\\mathbf x_*) + \\mathbf K_* \\left[ \\mathbf K + \\sigma_\\epsilon^2\\mathbf{I}_N \\right]^{-1}\\mathbf y \\right) \\mathcal{P}(\\mathbf x)d\\mathbf x Now the integral of the first term alone \\mu(\\mathbf x_*|\\mathbf x) \\mu(\\mathbf x_*|\\mathbf x) is a non-trivial integral especially for the inverse of the kernel matrix \\mathbf K \\mathbf K which does not have a trivial solution. So just looking at the posterior function alone, there are problems with this approach. We will have to somehow augment our function to account for this problem. Below, we will discuss an algorithm that attempts to remedy this.","title":"Stochastic Measurements"},{"location":"appendix/gps/3_input_error/#noisy-input-gp-nigp","text":"We're going to make a slight modification to the above equation for stochastic measurements. We will keep the same assumption of stochastic measurements, \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) \\mathbf x \\sim \\mathcal{N}(\\bar{\\mathbf{x}}, \\Sigma_x) . It doesn't change the properties of the formulation but it does change the perspective. y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x \\mathbf x = \\mathbf{\\bar x} + \\epsilon_x where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) The definitions are exactly the same but we need to think of the model itself as having the \\mathbf x - \\epsilon_x \\mathbf x - \\epsilon_x as the input to the latent function f f . Rewriting this expression does not solve the problem as we would still have difficulties marginalizing by the stochastic inputs. Instead, the author uses a first order Taylor series approximation of the GP latent function f f w.r.t. \\mathbf x \\mathbf x to separate the terms which allows us to have an easier function to approximate. We end up with the following equation: \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &\\approx f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\\\ \\end{aligned} where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\frac{\\partial f (\\cdot)}{\\partial x} \\frac{\\partial f (\\cdot)}{\\partial x} - the derivative of the latent function f f w.r.t. \\mathbf x \\mathbf x \\mathbf x \\mathbf x - noise-corrupted training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) We have replaced our \\mathbf{\\bar x} \\mathbf{\\bar x} with a new derivative term and these brings in some new ideas and questions: what does this mean to have the derivative of our latent function and how does relate to the error in our latent function f f ? Figure : Intuition of the NIGP: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\sigma_y^2 \\sigma_y^2 - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . So, getting back to our Taylor expanded function which encompasses this relationship, we will notice that it is not a Gaussian distribution because of the product of the Gaussian vector \\epsilon_x \\epsilon_x and the derivative of the GP function (it is Gaussian, just not a Gaussian PDF). So we will have problems with the inference so we need to make approximations in order to use standard analytical methods. The authors outline two different approaches which we will look at below; they have very similar outcomes but different reasonings.","title":"Noisy-Input GP (NIGP)"},{"location":"appendix/gps/3_input_error/#expected-derivative","text":"Remember a GP function is defined as : f \\sim \\mathcal{GP}(\\mu, \\nu^2) f \\sim \\mathcal{GP}(\\mu, \\nu^2) <span><span class=\"MathJax_Preview\">f \\sim \\mathcal{GP}(\\mu, \\nu^2)</span><script type=\"math/tex\">f \\sim \\mathcal{GP}(\\mu, \\nu^2) It's a random function defined by it's mean and variance. We can also write the derivative of a GP as follows: \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) \\partial f \\sim \\mathcal{GP}(\\partial\\mu, \\partial\\nu^2) Remember the derivative of a GP is still a GP so the treatment is the same. They suggest we take the expectation over the GP uncertainty, \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right] kind of acting as a first order approximation to the approximation. This equation now becomes Gaussian distributed which means we are simply adding a GP and a Gaussian distribution which is a Gaussian distribution (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} (\\mathcal{G} + \\mathcal{GP}=\\mathcal{G} )( ???? ). Taking the expectation over that GP derivative gives us the mean which is defined as the posterior mean of a GP. \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]= \\frac{\\partial \\mu(\\mathbf x)}{\\partial x} = \\partial{\\bar{f}} So to take the expectation for the derivative of a GP would be taking the derivative w.r.t. to the posterior mean function, \\partial\\mu(\\mathbf x) \\partial\\mu(\\mathbf x) only. So in this instance, we just slightly modified our original equation so that we just need to take the derivative of the GP posterior mean instead of the whole distribution. So now we have a new likelihood function based on this approach of expectations: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x}\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Note: please see the supplementary section for a quick alternative explanation using ideas from the propagation of variances .","title":"Expected Derivative"},{"location":"appendix/gps/3_input_error/#moment-matching","text":"The alternative and more involved approach is to use the moment matching approach again. We use this to compute the moments of this formulation to recover the mean and variance of this distribution. The first moment (the mean) is given by: \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y] = \\mathbb{E}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y]= \\mathbb{E}_{f}\\left[ f(\\mathbf x) \\right] \\mathbb{E}[y] = m(\\mathbf x) \\mathbb{E}[y] = m(\\mathbf x) We still recover the GP prior mean which is the same for the standard GP. The variance is more difficult to calculate and I will omit most of the details as it can get a bit cumbersome. \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y] = \\mathbb{V}_{f, \\epsilon_x, \\epsilon_y}\\left[ f(\\mathbf x) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x)}{\\partial x} + \\epsilon_y \\right] \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\mathbb{V}[y]= f(\\mathbf x) + \\mathbb{E}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\Sigma_x \\mathbb{E}_f\\left[\\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top}\\right] + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y and using some of the notation above, we can simplify this a bit: \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} \\mathbb{V}[y] &= f(\\mathbf x) + \\partial{\\bar{f}}^{\\top}\\Sigma_x \\partial{\\bar{f}} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)+ \\epsilon_y \\\\ \\end{aligned} You're more than welcome to read the thesis which goes through each term and explains how to compute the mean and variance for the derivative of a GP. The expression gets long but then a lot of terms go to zero. I went straight to the punchline because I think that's the most important part to take away. So to wrap it up and be consistent, the new likelihood function for the momemnt matching approach is: \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) \\mathcal{P}(y|f)=\\mathcal{N}\\left(f, \\sigma_y^2 + \\tilde \\Sigma_{\\mathbf x} + \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right)\\right) where \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} \\tilde \\Sigma_{\\mathbf x}=\\partial{\\bar{f}}^{\\top}\\Sigma_x\\partial{\\bar{f}} . Right away you'll notice some similarities between the expectation versus the moment matching approach and that's the variance term \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) \\text{tr} \\left( \\Sigma_x\\mathbb{V}_f\\left[ \\frac{\\partial f(\\mathbf x)}{\\partial x} \\right]\\right) which represents the uncertainty in the derivative as a corrective matrix. Both the authors of the NIGP paper and the SONIG paper both confirm that this additional correction term has a negligible effect on the final result. It's also not trivial to calculate so the code-result ratio doesn't seem worthwhile in my opinion for small problems. This might make a difference in large problems and possibly higher dimensional data. So the final formulation that we get for the posterior is: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|\\mu, \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} The big problem with this approach is that we do not know the function f(\\cdot) f(\\cdot) that we are approximating which means we cannot know the derivative of that function. So we are left with a recursive system where we need to know the function to calculate the derivative and we need to know the derivative to calculate the outputs. The solution is to use multiple iterations which is what was done in the NIGP paper (and similarly in the online version SONIG). Regardless, we are trying to marginalize the log likelihood. The likelihood is given by the normal distribution: \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) \\mathcal{P}(y|\\mathbf x, \\theta) = \\mathcal{N}(y | m, \\mathbf K_\\theta) where \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathcal{K}_\\theta=\\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I . But we need to do a two-step procedure: Train a standard GP (with params \\mathbf K + \\sigma^2_y \\mathbf I \\mathbf K + \\sigma^2_y \\mathbf I ) Evaluate the Derivative terms with the GP ( \\partial{\\bar f} \\partial{\\bar f} ) Add the corrective term ( \\mathbf{\\tilde \\Sigma_\\mathbf{x}} \\mathbf{\\tilde \\Sigma_\\mathbf{x}} ). Train the GP with the corrective terms ( \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I \\mathbf K + \\mathbf{\\tilde \\Sigma_\\mathbf{x}} + \\sigma^2_y \\mathbf I ). Repeat 2-4 until desired convergence. TODO : My opinion.","title":"Moment Matching"},{"location":"appendix/gps/3_input_error/#variational-strategies","text":"What links all of the strategies above is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt approximate the posterior predictive distribution by augmenting the variance method only ( ??? ).","title":"Variational Strategies"},{"location":"appendix/gps/3_input_error/#my-method-marriage-of-two-strategies","text":"I looked at both strategies of stochastic trial points versus stochastic inputs to see how would it work in real applications. One thing that was very limiting in almost all of these methods was how expensive they were. When it comes to real data, calculating higher order derivatives can be very costly. It seemed like the more sophisticated the model, the more expensive the method is. An obvious example the NIGP where it requires multiple iterations in conjunction with multiple restarts to avoid local minimum. I just don't see it happening when dealing with 2,000+ points. However, I support the notion of using posterior information by the use of gradients of the predictive mean function as I think this is valuable information which GPs give you access to. With big data as my limiting factor, I chose to keep the training procedure the same but modify the predictive variance using the methodology from the NIGP paper. I don't really do anything new that cannot be found from the above notions but I tried to take the best of both worlds given my problem. I briefly outline it below.","title":"My Method - Marriage of Two Strategies"},{"location":"appendix/gps/3_input_error/#model","text":"Using a combination of two strategies that we mentioned above: Stochastic trial points Derivative of the posterior mean function. It's using the NIGP reasoning but with assuming only the trial points are stochastic. Let's define the model under the assumption that \\mathbf{\\bar{x}} \\mathbf{\\bar{x}} are noise-free inputs to the f(\\cdot) f(\\cdot) . With equations that looks something like: y = f(\\mathbf{x}, \\theta) + \\epsilon_y y = f(\\mathbf{x}, \\theta) + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y y_* \\approx f(\\mathbf x_*) - \\epsilon_x^{\\top} \\frac{\\partial f (\\mathbf x_*)}{\\partial x} + \\epsilon_y where only \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) \\mathbf x_* \\sim \\mathcal{N}\\left(\\mathbf{\\bar x_*}, \\Sigma_x \\right) and \\mathbf x \\mathbf x is deterministic.","title":"Model"},{"location":"appendix/gps/3_input_error/#inference","text":"So the exact same strategy as listed above. Now we will add the final posterior distribution that we found from the NIGP but only for the test points: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x_*}} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} In my work, I really only looked at the variance function to see if it was different. It didn't make sense to use the mean function with a different learning strategy. In addition, I found that the weights calculated with the correction were almost the same and I didn't see a real difference in accuracy for the experiments I conducted. So, to be complete, we come across my final algorithm: \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mathcal{P}(y|\\mathbf{x}, \\theta) = \\mathcal{N}\\left( y|m(\\mathbf x), \\mathbf K + \\mathbf{\\tilde \\Sigma_{\\mathbf x}} + \\sigma_y^2 \\right) \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\mu_*(\\mathbf x_*) = \\mathbf K_{*}\\left( \\mathbf K + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf y = \\mathbf K_* \\alpha \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} \\nu^2_*(\\mathbf x_*) = \\mathbf K_{**} - \\mathbf K_{*}\\left( \\mathbf K + \\mathbf \\Sigma_\\mathbf{x} + \\sigma^2_y \\mathbf I \\right)^{-1}\\mathbf K_{*}^{\\top} + \\mathbf \\Sigma_\\mathbf{*} I did an experiment where I was trying to predict temperature from radiances for some areas around the globe where the radiances that I received had known input errors. So I used those but only for the predictions. I found that the results I got for this method did highlight some differences in the variance estimates. If you try to correlate the standard deviation of the NIGP method versus the standard deviation from the standard GP, you get a very noticeable difference (see suppl. section below). The results were so convincing that I decided to take my research further in order to investigate some other strategies in order to account for input errors. One immediate obvious change is I could use the sparse GP approximation. But I think the most promising one that I would like to investigate is using variational inference which I outline in another document. Source : * Accounting for Input Noise in GP Parameter Retrieval - letter","title":"Inference"},{"location":"appendix/gps/3_input_error/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"appendix/gps/3_input_error/#moment-matching_1","text":"In a nutshell, we can calculate the approximations of any distribution f f by simply taking the moments of that distribution. Each moment is defined by an important statistic that most of us are familiar with: Mean, \\mathbb{E}[f] \\mathbb{E}[f] Variance, \\mathbb{V}[f] \\mathbb{V}[f] Skew, \\mathbb{S}[f] \\mathbb{S}[f] Kurtosis, \\mathbb{K}[f] \\mathbb{K}[f] Higher moments... With each of these moments, we are able to approximate almost any distribution. For a Gaussian distribution, it can only be defined by the first and second moment because all of the other moments are zero. So we can approximate any distribution as a Gaussian by simply taking the expected value and the variance of that probability distribution function.","title":"Moment Matching"},{"location":"appendix/gps/3_input_error/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x \\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"appendix/gps/3_input_error/#propagation-of-variances","text":"Let's reiterate our problem statement: y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y y = f(\\mathbf x - \\epsilon_x) + \\epsilon_y where: y y - noise-corrupted outputs f(\\cdot, \\theta) f(\\cdot, \\theta) - function parameterized by \\theta \\theta \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\mathbf x \\mathbf x - noise-corrupted training inputs \\mathbf{\\bar x} \\mathbf{\\bar x} - noise-free training inputs \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) First order Taylor Series expansion of f(x) f(x) . y \\approx f(x) + y \\approx f(x) +","title":"Propagation of Variances"},{"location":"appendix/gps/3_input_error/#nigp-propagating-variances","text":"Another explanation of the rational of the Taylor series for the NIGP stems from the error propagation law . Let's take some function f(\\mathbf x) f(\\mathbf x) where x \\sim \\mathcal{P} x \\sim \\mathcal{P} described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f(\\mu_{\\mathbf x})}{\\partial \\mathbf x} \\left( \\mathbf x - \\mu_{\\mathbf x} \\right) This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x})$$ $$\\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\cdot \\Sigma_\\mathbf{x} \\cdot\\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. We can relate the above formula to the logic of the NIGP by thinking in terms of the derivatives (slopes) and the input error. We can actually calculate how much the slope contributes to the noise in the error in our inputs because the derivative of a GP is still a GP. Like above, assume that our noise \\epsilon_x \\epsilon_x comes from a normal distribution with variance \\Sigma_x \\Sigma_x , \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x) . We also assume that the slope of our function is given by \\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial x} . At every infinitesimal point we have a tangent line to the slope, so multiplying the derivative by the error will give us an estimate of how much our variance estimate should change, \\epsilon_x\\frac{\\partial f}{\\partial x} \\epsilon_x\\frac{\\partial f}{\\partial x} . We've assumed a constant slope so we will have a mean of 0, \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 \\mathbb{E}\\left[ \\epsilon_x\\frac{\\partial f}{\\partial x} \\right]=m(\\mathbf x)=0 Now we just need to calculate the variance which is given by: \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\epsilon_x\\right)^2\\right] = \\mathbb{E}\\left[ \\left( \\frac{\\partial f}{\\partial x} \\right)\\epsilon_x \\epsilon_x^{\\top}\\left( \\frac{\\partial f}{\\partial x} \\right)^{\\top} \\right] = \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} So we can replace the \\epsilon_y^2 \\epsilon_y^2 with a new estimate for the output noise: \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} \\epsilon_y^2 \\approx \\epsilon_y^2 + \\frac{\\partial f}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f}{\\partial x}\\right)^{\\top} And we can add this to our formulation: \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} \\begin{aligned} y &= f(\\mathbf x) + \\frac{\\partial f(\\mathbf x)}{\\partial x}\\Sigma_x \\left( \\frac{\\partial f(\\mathbf x)}{\\partial x}\\right)^{\\top} + \\epsilon_y \\\\ \\end{aligned} Source : An Introduction to Error Propagation: Derivation, Meaning and Examples - Doc Shorter Summary - 1D, 2D Example Statistical uncertainty and error propagation","title":"NIGP - Propagating Variances"},{"location":"appendix/gps/3_input_error/#real-results-with-variance-estimates","text":"Figure : Absolute Error From a GP Model Figure : Standard GP Variance Estimates Figure : GP Variance Estimates account for input errors.","title":"Real Results with Variance Estimates"},{"location":"appendix/gps/3_input_error/#resources","text":"","title":"Resources"},{"location":"appendix/gps/3_input_error/#papers","text":"","title":"Papers"},{"location":"appendix/gps/3_input_error/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Efficient Reinforcement Learning Using Gaussian Processes - Deisenroth (2009) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Thesis Explain"},{"location":"appendix/gps/3_input_error/#important-papers","text":"","title":"Important Papers"},{"location":"appendix/gps/3_taylor_expansion/","text":"Uncertainty of GPs: Taylor Expansion \u00b6 In this document, I will be showing how we can use the Taylor Expansion approach to the posterior of Gaussian process algorithm GP Model \u00b6 We have a standard GP model. $$ \\begin{aligned} y &= f(x) + \\epsilon_y\\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\ \\end{aligned} $$ GP Prior : p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) Gaussian Likelihood : p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) Posterior : f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} Taylor Expansion \u00b6 Let's assume we have inputs with an additive noise term \\epsilon_x \\epsilon_x and let's assume that it is Gaussian distributed. We can write some expressions which are very similar to the GP model equations specified above: $$ \\begin{aligned} y &= f(x) + \\epsilon_y \\ x &= \\mu_x + \\epsilon_x \\ \\epsilon_y &\\sim \\mathcal{N} (0, \\sigma_y^2) \\ \\epsilon_x &\\sim \\mathcal{N} (0, \\Sigma_x) \\end{aligned} $$ This is the transformation of a Gaussian random variable x x through another r.v. y y where we have some additive noise \\epsilon_y \\epsilon_y . The biggest difference is that the GP model assumes that x x is deterministic whereas we assume here that x x is a random variable itself. Because we know that integrating out the x x 's is quite difficult to do in practice (because of the nonlinear Kernel functions), we can make an approximation of f(\\cdot) f(\\cdot) via the Taylor expansion. We can take the a 2nd order Taylor expansion of f f to be: $$ \\begin{aligned} f(x) &\\approx f(\\mu_x + \\epsilon_x) \\ &\\approx f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x + \\sum_{i}\\frac{1}{2} \\epsilon_x^\\top \\nabla^2 f(\\mu_x) \\epsilon_x \\end{aligned} $$ where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Now the question is: where to put use the Taylor expansion within the GP model? There are two options: the model or the posterior. We will outline the two approaches below. Approximate the Model \u00b6 Approximate The Posterior \u00b6 We can compute the expectation \\mathbb{E}[\\cdot] \\mathbb{E}[\\cdot] and variance \\mathbb{V}[\\cdot] \\mathbb{V}[\\cdot] of this Taylor expansion to come up with an approximate mean and variance function for our posterior. Expectation \u00b6 This calculation is straight-forward because we are taking the expected value of a mean function f(\\mu_x) f(\\mu_x) , the derivative of a mean function f(\\mu_x) f(\\mu_x) and a Gaussian distribution noise term \\epsilon_x \\epsilon_x with mean 0. $$ \\begin{aligned} \\mathbb{E}[f(x)] &\\approx \\mathbb{E}[f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x] \\ &= f(\\mu_x) + \\nabla f(\\mu_x) \\mathbb{\\epsilon_x} \\ &= f(\\mu_x) \\end{aligned} $$ Variance \u00b6 The variance term is a bit more complex. $$ \\begin{aligned} \\mathbb{E}\\left[(f(x) - \\mathbb{E}[f(x)])^\\top(f(x) - \\mathbb{E}[f(x)])\\right] &\\approx \\mathbb{E}\\left[(f(x) - f(\\mu_x))^\\top(f(x) - f(\\mu_x))\\right] \\ &\\approx \\mathbb{E} \\left[ \\left(f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x \\right) \\left( f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x\\right)^\\top\\right] \\ &= \\mathbb{E} \\left[ \\left(\\nabla f(\\mu_x): \\epsilon_x \\right)^\\top \\left( \\nabla f(\\mu_x): \\epsilon_x \\right) \\right] \\ &= \\nabla f(\\mu_x) \\mathbb{E}[\\epsilon_x\\epsilon_x^\\top]\\nabla f(\\mu_x) \\ &= \\nabla f(\\mu_x): \\Sigma_x :\\nabla f(\\mu_x) \\end{aligned} $$ I: Additive Noise Model ( x,f x,f ) \u00b6 This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{ } K_{GP}^{-1}K_{ }^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top . Other GP Methods \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. II: Non-Additive Noise Model \u00b6 III: Quadratic Approximation \u00b6 Parallels to the Kalman Filter \u00b6 The Kalman Filter (KF) community use this exact formulation to motivate the Extended Kalman Filter (EKF) algorithm and some variants. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Uncertainty of GPs: Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#uncertainty-of-gps-taylor-expansion","text":"In this document, I will be showing how we can use the Taylor Expansion approach to the posterior of Gaussian process algorithm","title":"Uncertainty of GPs: Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#gp-model","text":"We have a standard GP model. $$ \\begin{aligned} y &= f(x) + \\epsilon_y\\ \\epsilon_y &\\sim \\mathcal{N}(0, \\sigma_y^2) \\ \\end{aligned} $$ GP Prior : p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) p(f|X)\\sim\\mathcal{N}(m(X), \\mathbf{K}) Gaussian Likelihood : p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) p(y|f, X)=\\mathcal{N}(y|f(x), \\sigma_y^2\\mathbf{I}) Posterior : f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) f \\sim \\mathcal{GP}(f|\\mu_{GP}, \\nu^2_{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_{GP} &= K_{*} K_{GP}^{-1}y=K_{*} \\alpha \\\\ \\nu^2_{GP} &= K_{**} - K_{*} K_{GP}^{-1}K_{*}^{\\top} \\end{aligned}","title":"GP Model"},{"location":"appendix/gps/3_taylor_expansion/#taylor-expansion","text":"Let's assume we have inputs with an additive noise term \\epsilon_x \\epsilon_x and let's assume that it is Gaussian distributed. We can write some expressions which are very similar to the GP model equations specified above: $$ \\begin{aligned} y &= f(x) + \\epsilon_y \\ x &= \\mu_x + \\epsilon_x \\ \\epsilon_y &\\sim \\mathcal{N} (0, \\sigma_y^2) \\ \\epsilon_x &\\sim \\mathcal{N} (0, \\Sigma_x) \\end{aligned} $$ This is the transformation of a Gaussian random variable x x through another r.v. y y where we have some additive noise \\epsilon_y \\epsilon_y . The biggest difference is that the GP model assumes that x x is deterministic whereas we assume here that x x is a random variable itself. Because we know that integrating out the x x 's is quite difficult to do in practice (because of the nonlinear Kernel functions), we can make an approximation of f(\\cdot) f(\\cdot) via the Taylor expansion. We can take the a 2nd order Taylor expansion of f f to be: $$ \\begin{aligned} f(x) &\\approx f(\\mu_x + \\epsilon_x) \\ &\\approx f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x + \\sum_{i}\\frac{1}{2} \\epsilon_x^\\top \\nabla^2 f(\\mu_x) \\epsilon_x \\end{aligned} $$ where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Now the question is: where to put use the Taylor expansion within the GP model? There are two options: the model or the posterior. We will outline the two approaches below.","title":"Taylor Expansion"},{"location":"appendix/gps/3_taylor_expansion/#approximate-the-model","text":"","title":"Approximate the Model"},{"location":"appendix/gps/3_taylor_expansion/#approximate-the-posterior","text":"We can compute the expectation \\mathbb{E}[\\cdot] \\mathbb{E}[\\cdot] and variance \\mathbb{V}[\\cdot] \\mathbb{V}[\\cdot] of this Taylor expansion to come up with an approximate mean and variance function for our posterior.","title":"Approximate The Posterior"},{"location":"appendix/gps/3_taylor_expansion/#expectation","text":"This calculation is straight-forward because we are taking the expected value of a mean function f(\\mu_x) f(\\mu_x) , the derivative of a mean function f(\\mu_x) f(\\mu_x) and a Gaussian distribution noise term \\epsilon_x \\epsilon_x with mean 0. $$ \\begin{aligned} \\mathbb{E}[f(x)] &\\approx \\mathbb{E}[f(\\mu_x) + \\nabla f(\\mu_x) \\epsilon_x] \\ &= f(\\mu_x) + \\nabla f(\\mu_x) \\mathbb{\\epsilon_x} \\ &= f(\\mu_x) \\end{aligned} $$","title":"Expectation"},{"location":"appendix/gps/3_taylor_expansion/#variance","text":"The variance term is a bit more complex. $$ \\begin{aligned} \\mathbb{E}\\left[(f(x) - \\mathbb{E}[f(x)])^\\top(f(x) - \\mathbb{E}[f(x)])\\right] &\\approx \\mathbb{E}\\left[(f(x) - f(\\mu_x))^\\top(f(x) - f(\\mu_x))\\right] \\ &\\approx \\mathbb{E} \\left[ \\left(f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x \\right) \\left( f(\\mu_x) + \\nabla f(\\mu_x)\\epsilon_x\\right)^\\top\\right] \\ &= \\mathbb{E} \\left[ \\left(\\nabla f(\\mu_x): \\epsilon_x \\right)^\\top \\left( \\nabla f(\\mu_x): \\epsilon_x \\right) \\right] \\ &= \\nabla f(\\mu_x) \\mathbb{E}[\\epsilon_x\\epsilon_x^\\top]\\nabla f(\\mu_x) \\ &= \\nabla f(\\mu_x): \\Sigma_x :\\nabla f(\\mu_x) \\end{aligned} $$","title":"Variance"},{"location":"appendix/gps/3_taylor_expansion/#i-additive-noise-model-xfxf","text":"This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{ } K_{GP}^{-1}K_{ }^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"I: Additive Noise Model (x,fx,f)"},{"location":"appendix/gps/3_taylor_expansion/#other-gp-methods","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Other GP Methods"},{"location":"appendix/gps/3_taylor_expansion/#ii-non-additive-noise-model","text":"","title":"II: Non-Additive Noise Model"},{"location":"appendix/gps/3_taylor_expansion/#iii-quadratic-approximation","text":"","title":"III: Quadratic Approximation"},{"location":"appendix/gps/3_taylor_expansion/#parallels-to-the-kalman-filter","text":"The Kalman Filter (KF) community use this exact formulation to motivate the Extended Kalman Filter (EKF) algorithm and some variants. \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\\\ C^\\top & \\Pi \\end{bmatrix} \\right)","title":"Parallels to the Kalman Filter"},{"location":"appendix/gps/4_variational/","text":"Uncertain Inputs GPs - Variational Strategies \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Uncertain Inputs GPs - Variational Strategies Posterior Approximations Sparse Model Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. VGP Model Posterior Distribution: p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF <span><span class=\"MathJax_Preview\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF</span><script type=\"math/tex\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Variational GP Model w/ Prior Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Sparse Model \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"appendix/gps/4_variational/#uncertain-inputs-gps-variational-strategies","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Uncertain Inputs GPs - Variational Strategies Posterior Approximations Sparse Model Evidence Lower Bound (ELBO) Uncertain Inputs Case I - Strong Prior Case II - Regularized Strong Prior Case III - Prior with Openness Case IV - Bonus, Conservative Freedom Resources Important Papers Summary Thesis Talks","title":"Uncertain Inputs GPs - Variational Strategies"},{"location":"appendix/gps/4_variational/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. VGP Model Posterior Distribution: p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF <span><span class=\"MathJax_Preview\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF</span><script type=\"math/tex\">p(Y|X) = \\int_{\\mathcal F} p(Y|F) P(F|X) dF Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Variational GP Model w/ Prior Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX <span><span class=\"MathJax_Preview\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX</span><script type=\"math/tex\">p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Posterior Approximations"},{"location":"appendix/gps/4_variational/#sparse-model","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) \\mathcal{P}(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) q(f) = \\mathcal{GP}\\left( \\mu, \\nu^2 \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} \\theta = \\{ \\mathbf{m, \\Sigma, Z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior.","title":"Sparse Model"},{"location":"appendix/gps/4_variational/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"appendix/gps/4_variational/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"appendix/gps/4_variational/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"appendix/gps/4_variational/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"appendix/gps/4_variational/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"appendix/gps/4_variational/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"appendix/gps/4_variational/#resources","text":"","title":"Resources"},{"location":"appendix/gps/4_variational/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"appendix/gps/4_variational/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"appendix/gps/4_variational/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"appendix/gps/gps_and_it/","text":"Gaussian Dists. and GPs \u00b6 In this summary, I will be exploring what makes Gaussian distributions so special as well as the relation they have with IT measures. I'll also look at some extensions to Gaussian Processes (GPs) and what connection they have have with IT measures. GPs, Entropy, Residuals \u00b6 Easy to compute stuff out of a GP (which is a joint multivariate Gaussian with covariance K) would be: 1) Differential) entropy from the GP: \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} where K_x is the kernel matrix (a covariance after all in feature space). Wiki | GP Book (see e.g. A.5, eq. A.20, etc) 2) And then I remembered that the LS error estimate could be bounded, and since a GP is after all LS regression in feature space, maybe we could check if the formula is right: MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) https://en.wikipedia.org/wiki/Conditional_entropy that is, MSE obtained with the GP is lower bounded by that conditional entropy estimate from RBIG. Some building blocks to start with connecting dots... :) Jesus Intuition said that \"the bigger the conditional entropy, the bigger the residual uncertainty is -> the bigger the MSE should be (no matter the model)\" Cool wiki formula: MSE >= exp(conditional) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) where: Conditional Variance: \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} Unread Stuff : Conditional Entropy in the Context of GPs - Stack Entripy of a GP (Log(det(Cov))) - stack Get full covariance matrix and find its entropy - stack GPs and Causality \u00b6 Intro to Causal Inference with GPs - blog I | blog II","title":"Gaussian Dists. and GPs"},{"location":"appendix/gps/gps_and_it/#gaussian-dists-and-gps","text":"In this summary, I will be exploring what makes Gaussian distributions so special as well as the relation they have with IT measures. I'll also look at some extensions to Gaussian Processes (GPs) and what connection they have have with IT measures.","title":"Gaussian Dists. and GPs"},{"location":"appendix/gps/gps_and_it/#gps-entropy-residuals","text":"Easy to compute stuff out of a GP (which is a joint multivariate Gaussian with covariance K) would be: 1) Differential) entropy from the GP: \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} \\begin{aligned} H(X) &= 0.5 \\cdot log( (2 \\cdot \\pi \\cdot e)^n \\cdot \\text{det}(K_x) ) \\\\ H(X) &= \\frac{N}{2} \\cdot \\log(2 \\cdot \\pi \\cdot e) + \\log |K_x| \\end{aligned} where K_x is the kernel matrix (a covariance after all in feature space). Wiki | GP Book (see e.g. A.5, eq. A.20, etc) 2) And then I remembered that the LS error estimate could be bounded, and since a GP is after all LS regression in feature space, maybe we could check if the formula is right: MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) MSE = E[(Y-\\hat Y)^2] >= 1/(2 \\cdot pi \\cdot e) * exp(H(Y|X)) https://en.wikipedia.org/wiki/Conditional_entropy that is, MSE obtained with the GP is lower bounded by that conditional entropy estimate from RBIG. Some building blocks to start with connecting dots... :) Jesus Intuition said that \"the bigger the conditional entropy, the bigger the residual uncertainty is -> the bigger the MSE should be (no matter the model)\" Cool wiki formula: MSE >= exp(conditional) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) H(X_y|X_A)=\\frac{1}{2} \\log \\left(\\nu_{X_y|X_A}^2 \\right) + \\frac{1}{2} \\left( \\log(2\\pi) + 1 \\right) where: Conditional Variance: \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} \\nu^2_{y|A}=K_{yy}- \\Sigma_{y|A}\\Sigma_{AA}^{-1}\\Sigma_{Ay} Unread Stuff : Conditional Entropy in the Context of GPs - Stack Entripy of a GP (Log(det(Cov))) - stack Get full covariance matrix and find its entropy - stack","title":"GPs, Entropy, Residuals"},{"location":"appendix/gps/gps_and_it/#gps-and-causality","text":"Intro to Causal Inference with GPs - blog I | blog II","title":"GPs and Causality"},{"location":"appendix/information/entropy/","text":"Entropy \u00b6 Intuition Formulas Code - Step-by-Step Code - Refactored Estimating Entropy Histogram Kernel Density Estimation KNN Approximation Single Variable Multivariate Relative Entropy (KL-Divergence) Intuition \u00b6 Expected uncertainty. H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} Lower bound on the number of bits needed to represent a RV, e.g. a RV that has a unform distribution over 32 outcomes. Lower bound on the average length of the shortest description of X X Self-Information Formulas \u00b6 H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} And we can estimate this empirically by: H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i where p_i = P(\\mathbf{X}) p_i = P(\\mathbf{X}) . Code - Step-by-Step \u00b6 # 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from scipy which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. Code - Refactored \u00b6 # 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above base = 2 H = entropy ( counts , base = base ) Estimating Entropy \u00b6 Histogram \u00b6 import numpy as np from scipy import stats # data s1 = np . random . normal ( 10 , 10 , 1_000 ) # construct histogram hist_pdf , hist_bins = np . histogram ( data , bins = 50 , range = (), density = True ) # calculate the entropy H_data = stats . entropy ( hist_pdf , base = 2 ) Kernel Density Estimation \u00b6 KNN Approximation \u00b6 Single Variable \u00b6 H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) Multivariate \u00b6 H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) Relative Entropy (KL-Divergence) \u00b6 Measure of distance between two distributions D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx aka expected log-likelihood ratio measure of inefficiency of assuming that the distribution is q q when we know the true distribution is p p .","title":"Entropy"},{"location":"appendix/information/entropy/#entropy","text":"Intuition Formulas Code - Step-by-Step Code - Refactored Estimating Entropy Histogram Kernel Density Estimation KNN Approximation Single Variable Multivariate Relative Entropy (KL-Divergence)","title":"Entropy"},{"location":"appendix/information/entropy/#intuition","text":"Expected uncertainty. H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} H(X) = \\log \\frac{\\text{\\# of Outcomes}}{\\text{States}} Lower bound on the number of bits needed to represent a RV, e.g. a RV that has a unform distribution over 32 outcomes. Lower bound on the average length of the shortest description of X X Self-Information","title":"Intuition"},{"location":"appendix/information/entropy/#formulas","text":"H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} H(\\mathbf{X}) = - \\int_\\mathcal{X} p(\\mathbf{x}) \\log p(\\mathbf{x}) d\\mathbf{x} And we can estimate this empirically by: H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i H(\\mathbf{X}) = -\\sum_{i=1}^N p_i \\log p_i where p_i = P(\\mathbf{X}) p_i = P(\\mathbf{X}) .","title":"Formulas"},{"location":"appendix/information/entropy/#code-step-by-step","text":"# 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from scipy which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function.","title":"Code - Step-by-Step"},{"location":"appendix/information/entropy/#code-refactored","text":"# 1. obtain all possible occurrences of the outcomes values , counts = np . unique ( labels , return_counts = True ) # 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () # 3. Calculate the entropy using the formula above base = 2 H = entropy ( counts , base = base )","title":"Code - Refactored"},{"location":"appendix/information/entropy/#estimating-entropy","text":"","title":"Estimating Entropy"},{"location":"appendix/information/entropy/#histogram","text":"import numpy as np from scipy import stats # data s1 = np . random . normal ( 10 , 10 , 1_000 ) # construct histogram hist_pdf , hist_bins = np . histogram ( data , bins = 50 , range = (), density = True ) # calculate the entropy H_data = stats . entropy ( hist_pdf , base = 2 )","title":"Histogram"},{"location":"appendix/information/entropy/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"appendix/information/entropy/#knn-approximation","text":"","title":"KNN Approximation"},{"location":"appendix/information/entropy/#single-variable","text":"H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right) H(X) = \\mathbb{E}_{p(X)} \\left( \\log \\frac{1}{p(X)}\\right)","title":"Single Variable"},{"location":"appendix/information/entropy/#multivariate","text":"H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right) H(X) = \\mathbb{E}_{p(X,Y)} \\left( \\log \\frac{1}{p(X,Y)}\\right)","title":"Multivariate"},{"location":"appendix/information/entropy/#relative-entropy-kl-divergence","text":"Measure of distance between two distributions D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx D_{KL} (P,Q) = \\int_\\mathcal{X} p(x) \\:\\log \\frac{p(x)}{q(x)}\\;dx aka expected log-likelihood ratio measure of inefficiency of assuming that the distribution is q q when we know the true distribution is p p .","title":"Relative Entropy (KL-Divergence)"},{"location":"appendix/information/expf/","text":"Exponential Family of Solutions \u00b6","title":"Exponential Family of Solutions"},{"location":"appendix/information/expf/#exponential-family-of-solutions","text":"","title":"Exponential Family of Solutions"},{"location":"appendix/information/info/","text":"Information \u00b6 Formulation \u00b6 I(X) = - \\log \\frac{1}{p(X)} I(X) = - \\log \\frac{1}{p(X)} Units \u00b6 Base Units Conversion Approximate 2 bits 1 bit = 1 bit 1 bit = 1 bit e nats 1 bit = \\log_e 2 \\log_e 2 1 bit \\approx \\approx 0.693 nats 10 bans 1 bit = \\log_{10}2 \\log_{10}2 1 bit \\approx \\approx 0.301 bans","title":"Information"},{"location":"appendix/information/info/#information","text":"","title":"Information"},{"location":"appendix/information/info/#formulation","text":"I(X) = - \\log \\frac{1}{p(X)} I(X) = - \\log \\frac{1}{p(X)}","title":"Formulation"},{"location":"appendix/information/info/#units","text":"Base Units Conversion Approximate 2 bits 1 bit = 1 bit 1 bit = 1 bit e nats 1 bit = \\log_e 2 \\log_e 2 1 bit \\approx \\approx 0.693 nats 10 bans 1 bit = \\log_{10}2 \\log_{10}2 1 bit \\approx \\approx 0.301 bans","title":"Units"},{"location":"appendix/information/information/","text":"Information \u00b6 Empirical Estimation \u00b6 def information ( p ): return - np . log2 ( p )","title":"Information"},{"location":"appendix/information/information/#information","text":"","title":"Information"},{"location":"appendix/information/information/#empirical-estimation","text":"def information ( p ): return - np . log2 ( p )","title":"Empirical Estimation"},{"location":"appendix/information/information_bottleneck/","text":"Information Bottleneck \u00b6 Blog \u00b6 Steps Towards Understanding Deep Learning: The Information Bottleneck Connection (Part 1) - blog On the information bottleneck theory of deep learning - Adrian Colyer (2017) - blog Discussions \u00b6 What is the status of the \"Information Bottleneck Theory of Deep Learning\"? - reddit On the information bottleneck theory of deep learning - reddit Papers \u00b6 The HSIC Bottleneck: Deep Learning without Back-Propagation - Kurt Ma et al (2019) - [ arxiv ] | paper | code Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle - Amjad & Geiger (2019) - arxiv | paper","title":"Information Bottleneck"},{"location":"appendix/information/information_bottleneck/#information-bottleneck","text":"","title":"Information Bottleneck"},{"location":"appendix/information/information_bottleneck/#blog","text":"Steps Towards Understanding Deep Learning: The Information Bottleneck Connection (Part 1) - blog On the information bottleneck theory of deep learning - Adrian Colyer (2017) - blog","title":"Blog"},{"location":"appendix/information/information_bottleneck/#discussions","text":"What is the status of the \"Information Bottleneck Theory of Deep Learning\"? - reddit On the information bottleneck theory of deep learning - reddit","title":"Discussions"},{"location":"appendix/information/information_bottleneck/#papers","text":"The HSIC Bottleneck: Deep Learning without Back-Propagation - Kurt Ma et al (2019) - [ arxiv ] | paper | code Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle - Amjad & Geiger (2019) - arxiv | paper","title":"Papers"},{"location":"appendix/information/it_estimators/","text":"Information Theory Measures Estimators \u00b6 Information theory measures are potentially very useful in many applications ranging from Earth science to neuroscience. Measures like entropy enable us to summarise the uncertainty of a dataset, total correlation enable us to summarise the redundancy of the data, and mutual information enable to calculate the similarities between two or more datasets. However, all of these measures require us to estimate the probabilidy density function (PDF) and thus we need to create information theory estimators (ITEs). Each measure needs a probability distribution function p(X) p(X) and this is a difficult task especially with large high-dimensional datasets. In this post, I will be outlining a few key methods that are used a lot in the literature as well ones that I am familiar with. As a side note, I would like to stress that most conventional IT estimators work on 1D or 2D variables. Rarely do we see estimators designed to work for datasets with a high number of samples and/or a high number of dimensions. The estimator that our lab uses (RBIG) is designed exactly for that. Convential Estimators \u00b6 Now lets consider the conventional estimators of the measures considered in this work: H, T, I and DKL. While plenty of methods focus on the estimation of the above magnitudes for one dimensional or two-dimensional variables [?], there are few methods that deal with variables of arbitrary dimensions. Here we focus our comparative in the relatively wide family of methods present in this recent toolbox 1 which address the general multivariate case. Specifically, the family of methods in [7] is based on the following literature [8]\u2013[18], which tackles the estimation problems according to different strategies. Gaussian Assumption \u00b6 The simplest ITM estimator is to assume that the data distributions are Gaussian. With this Gaussian assumption, the aforementioned ITMs are straightforward to calculate as the close-form solution just involves the approximated covariance matrix of the distribution(s). This assumption is very common to use for many scientific fields as datasets with a high number of samples tend to have a mean that is Gaussian distributed. However, this is not true for the tails of the distribution. In addition, the Gaussian distribution is in the exponential family of distributions which have attractive properties of mathematical convenience. The Sharma-Mittal entropy yields an analytical expression for the entropy of a Gaussian distribution \\cite{Nielsen_2011} which also includes the derivative of the log-Normalizer of the distribution which acts as a corrective term yielding better estimates of the distribution. Omitting the error due to noise, the experiments will in the following section will illustrate that the Gaussian assumption should perform well on simple distributions but should perform worse for distributions which are characterized by their tails like the T-Student or the Pareto distribution. k-Nearest Neighbours Estimator \u00b6 The next family of methods which are very commonly used are those with binning strategies. These binning methods include algorithms like the ridged histogram estimation, the smooth kernel density estimator (KDE), and the adaptive k-Nearest Neighbours (kNN) estimator \\cite{Goria05}. These methods are non-parametric which allows them to be more flexible and should be able to capture more complex distributions. However, these methods are model-based and hence are sensitive to the parameters chosen and there is no intuitive method to choose the appropriate parameters. It all depends on the data given. We chose the most robust method, the kNN estimator, for our experiments which is adaptive in nature due to the neighbour structure from a distance matrix. The kNN algorithms is known to have problems at scale with added problems of high dimensionality so we also included the scaled version which uses partition trees \\cite{Stowell09} which sacrifices accuracy for speed. Von Mises Expansion \u00b6 Algorithms \u00b6 KnnK \u00b6 This is the most common method to estimate multivariate entropy used by the many different communities is the k-nearestest neighbours (kNN) estimator which was originally proposed by Kozachenko & Leonenko (1987); inspired by Dobrushin (1958). Asymptotically unbiased Weakly consistent Non-parametric strategy which can heavily depend on the number of neighbours k k chosen. There has been some work done on this in ( citation ) but this issue is still an open problem for the community. This method works by taking the distance from the k^{th} k^{th} sample fromDeviations of this method can be seen in common machine learning packages such as sklearn to estimate mutual information as well in large scale global causal graphs ( E : Jakob's paper). This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). In relation to the other standard methods of density estimation, it is the most robust in higher dimensions due to its adaptive-like binning. A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - Paper Nearest neighbor estimates of entropy - Singh et. al. (2003) - paper A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - paper KDP \u00b6 This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume. Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - Paper expF \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper vME \u00b6 This estimates the Shannon differential entropy (H) using the von Mises expansion. Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - Paper Ensemble \u00b6 Estimates the entropy from the average entropy estimations on groups of samples This is a simple implementation with the freedom to choose the estimator estimate_H . # split into groups for igroup in batches : H += estimate_H ( igroup ) H /= len ( batches ) High-dimensional mutual information estimation for image registration - Kybic (2004) - Paper Potential New Experiments \u00b6 Voronoi \u00b6 Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities. A new class of entropy estimators for multi-dimensional densities - Miller (2003) - Paper","title":"Information Theory Measures Estimators"},{"location":"appendix/information/it_estimators/#information-theory-measures-estimators","text":"Information theory measures are potentially very useful in many applications ranging from Earth science to neuroscience. Measures like entropy enable us to summarise the uncertainty of a dataset, total correlation enable us to summarise the redundancy of the data, and mutual information enable to calculate the similarities between two or more datasets. However, all of these measures require us to estimate the probabilidy density function (PDF) and thus we need to create information theory estimators (ITEs). Each measure needs a probability distribution function p(X) p(X) and this is a difficult task especially with large high-dimensional datasets. In this post, I will be outlining a few key methods that are used a lot in the literature as well ones that I am familiar with. As a side note, I would like to stress that most conventional IT estimators work on 1D or 2D variables. Rarely do we see estimators designed to work for datasets with a high number of samples and/or a high number of dimensions. The estimator that our lab uses (RBIG) is designed exactly for that.","title":"Information Theory Measures Estimators"},{"location":"appendix/information/it_estimators/#convential-estimators","text":"Now lets consider the conventional estimators of the measures considered in this work: H, T, I and DKL. While plenty of methods focus on the estimation of the above magnitudes for one dimensional or two-dimensional variables [?], there are few methods that deal with variables of arbitrary dimensions. Here we focus our comparative in the relatively wide family of methods present in this recent toolbox 1 which address the general multivariate case. Specifically, the family of methods in [7] is based on the following literature [8]\u2013[18], which tackles the estimation problems according to different strategies.","title":"Convential Estimators"},{"location":"appendix/information/it_estimators/#gaussian-assumption","text":"The simplest ITM estimator is to assume that the data distributions are Gaussian. With this Gaussian assumption, the aforementioned ITMs are straightforward to calculate as the close-form solution just involves the approximated covariance matrix of the distribution(s). This assumption is very common to use for many scientific fields as datasets with a high number of samples tend to have a mean that is Gaussian distributed. However, this is not true for the tails of the distribution. In addition, the Gaussian distribution is in the exponential family of distributions which have attractive properties of mathematical convenience. The Sharma-Mittal entropy yields an analytical expression for the entropy of a Gaussian distribution \\cite{Nielsen_2011} which also includes the derivative of the log-Normalizer of the distribution which acts as a corrective term yielding better estimates of the distribution. Omitting the error due to noise, the experiments will in the following section will illustrate that the Gaussian assumption should perform well on simple distributions but should perform worse for distributions which are characterized by their tails like the T-Student or the Pareto distribution.","title":"Gaussian Assumption"},{"location":"appendix/information/it_estimators/#k-nearest-neighbours-estimator","text":"The next family of methods which are very commonly used are those with binning strategies. These binning methods include algorithms like the ridged histogram estimation, the smooth kernel density estimator (KDE), and the adaptive k-Nearest Neighbours (kNN) estimator \\cite{Goria05}. These methods are non-parametric which allows them to be more flexible and should be able to capture more complex distributions. However, these methods are model-based and hence are sensitive to the parameters chosen and there is no intuitive method to choose the appropriate parameters. It all depends on the data given. We chose the most robust method, the kNN estimator, for our experiments which is adaptive in nature due to the neighbour structure from a distance matrix. The kNN algorithms is known to have problems at scale with added problems of high dimensionality so we also included the scaled version which uses partition trees \\cite{Stowell09} which sacrifices accuracy for speed.","title":"k-Nearest Neighbours Estimator"},{"location":"appendix/information/it_estimators/#von-mises-expansion","text":"","title":"Von Mises Expansion"},{"location":"appendix/information/it_estimators/#algorithms","text":"","title":"Algorithms"},{"location":"appendix/information/it_estimators/#knnk","text":"This is the most common method to estimate multivariate entropy used by the many different communities is the k-nearestest neighbours (kNN) estimator which was originally proposed by Kozachenko & Leonenko (1987); inspired by Dobrushin (1958). Asymptotically unbiased Weakly consistent Non-parametric strategy which can heavily depend on the number of neighbours k k chosen. There has been some work done on this in ( citation ) but this issue is still an open problem for the community. This method works by taking the distance from the k^{th} k^{th} sample fromDeviations of this method can be seen in common machine learning packages such as sklearn to estimate mutual information as well in large scale global causal graphs ( E : Jakob's paper). This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). In relation to the other standard methods of density estimation, it is the most robust in higher dimensions due to its adaptive-like binning. A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - Paper Nearest neighbor estimates of entropy - Singh et. al. (2003) - paper A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - paper","title":"KnnK"},{"location":"appendix/information/it_estimators/#kdp","text":"This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume. Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - Paper","title":"KDP"},{"location":"appendix/information/it_estimators/#expf","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper","title":"expF"},{"location":"appendix/information/it_estimators/#vme","text":"This estimates the Shannon differential entropy (H) using the von Mises expansion. Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - Paper","title":"vME"},{"location":"appendix/information/it_estimators/#ensemble","text":"Estimates the entropy from the average entropy estimations on groups of samples This is a simple implementation with the freedom to choose the estimator estimate_H . # split into groups for igroup in batches : H += estimate_H ( igroup ) H /= len ( batches ) High-dimensional mutual information estimation for image registration - Kybic (2004) - Paper","title":"Ensemble"},{"location":"appendix/information/it_estimators/#potential-new-experiments","text":"","title":"Potential New Experiments"},{"location":"appendix/information/it_estimators/#voronoi","text":"Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities. A new class of entropy estimators for multi-dimensional densities - Miller (2003) - Paper","title":"Voronoi"},{"location":"appendix/information/it_formulas/","text":"Information Theory Measures \u00b6 References \u00b6 Lecture Notes I - PDF Video Introduction - Youtube Entropy (Shannon) \u00b6 One Random Variable \u00b6 If we have a discrete random variable X with p.m.f. p_x(x) p_x(x) , the entropy is: H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] This measures the expected uncertainty in X X . The entropy is basically how much information we learn on average from one instance of the r.v. X X . The standard definition of Entropy can be written as: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{Q(y)}{P(x)}dx\\\\ &=\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{P(x)}{Q(y)}dx \\end{aligned}$$ and the discrete version: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)}\\\\ &=\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)} \\end{aligned}$$ If we want the viewpoint in terms of expectations, we can do a bit of rearranging to get: $$\\begin{aligned} D_{KLD} &= \\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)}\\\\ &= \\sum_{x\\in\\mathcal{X}} P(x) \\log P(x)- \\sum_{-\\infty}^{\\infty}P(x)\\log Q(y)dx \\\\ &= \\sum_{x\\in\\mathcal{X}} P(x)\\left[\\log P(x) - \\log Q(y) \\right] \\\\ &= \\mathbb{E}_x\\left[ \\log P(x) - \\log Q(y) \\right] \\end{aligned}$$ #### Code - Step-by-Step 1. Obtain all of the possible occurrences of the outcomes. values , counts = np . unique ( labels , return_counts = True ) 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from `scipy` which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. 2. Use Scipy Function H = entropy ( counts , base = base ) Two Random Variables \u00b6 If we have two random variables X, Y X, Y jointly distributed according to the p.m.f. p(x,y) p(x,y) , we can come up with two more quantities for entropy. Joint Entropy \u00b6 This is given by: H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] Definition : how much uncertainty we have between two r.v.s X,Y X,Y . Conditional Entropy \u00b6 This is given by: H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] Definition : how much uncertainty remains about the r.v. X X when we know the value of Y Y . Properties of Entropic Quantities \u00b6 Non-Negativity : H(X) \\geq 0 H(X) \\geq 0 , unless X X is deterministic (i.e. no randomness). Chain Rule : You can decompose the joint entropy measure: H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) where X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} . So the result is: H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) Monotonicity : Conditioning always reduces entropy. Information never hurts . H(X|Y) \\leq H(X) H(X|Y) \\leq H(X) Negentropy \u00b6 It is simply entropy but we restrict the comparison to a Gaussian. Let's say that we have Z Z which comes from a normal distribution z\\sim\\mathcal{N}(0, \\mathbb{I}) z\\sim\\mathcal{N}(0, \\mathbb{I}) . We can write the same standard KLD formulation but with the Entropy (Renyi) \u00b6 Above we looked at Shannon entropy which is a special case of Renyi's Entropy measure. But the generalized entropy formula actually is a generalization on entropy. Below is the given formula. H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) Mutual Information \u00b6 Definition : The mutual information (MI) between two discreet r.v.s X,Y X,Y jointly distributed according to p(x,y) p(x,y) is given by: I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia Total Correlation (Multi-Information) \u00b6 In general, the formula for Total Correlation (TC) between two random variables is as follows: TC(X,Y) = H(X) + H(Y) - H(X,Y) TC(X,Y) = H(X) + H(Y) - H(X,Y) Note : This is the same as the equation for mutual information between two random variables, I(X;Y)=H(X)+H(Y)-H(X,Y) I(X;Y)=H(X)+H(Y)-H(X,Y) . This makes sense because for a Venn Diagram between two r.v.s will only have one part that intersects. This is different for the multivariate case where the number of r.v.s is greater than 2. Let's have D D random variables for X = \\{ X_1, X_2, \\ldots, X_D\\} X = \\{ X_1, X_2, \\ldots, X_D\\} . The TC is: TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) In this case, D D can be a feature for X X . Now, let's say we would like to get the difference in total correlation between two random variables, \\Delta \\Delta TC. \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) Note : There is a special case in RBIG where the two random variables are simply rotations of one another. So each feature will have a difference in entropy but the total overall dataset will not. So our function would be reduced to: \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) which is overall much easier to solve. Cross Entropy (Log-Loss Function) \u00b6 Let P(\\cdot) P(\\cdot) be the true distribution and Q(\\cdot) Q(\\cdot) be the predicted distribution. We can define the cross entropy as: H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) This can be thought of the measure in information length. Note : The original cross-entropy uses \\log_2(\\cdot) \\log_2(\\cdot) but in a supervised setting, we can use \\log_{10} \\log_{10} because if we use log rules, we get the following relation \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} . Kullback-Leibler Divergence (KL) \u00b6 Furthermore, the KL divergence is the difference between the cross-entropy and the entropy. D_{KL}(P||Q) = H(P, Q) - H(P) D_{KL}(P||Q) = H(P, Q) - H(P) So this is how far away our predictions are from our actual distribution. Conditional Information Theory Measures \u00b6 Conditional Entropy \u00b6 Conditional Mutual Information \u00b6 Definition : Let X,Y,Z X,Y,Z be jointly distributed according to some p.m.f. p(x,y,z) p(x,y,z) . The conditional mutual information X,Y X,Y given Z Z is: I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) + H(Y) - H(X,Y) I(X;Y|Z) = H(X) + H(Y) - H(X,Y)","title":"Information Theory Measures"},{"location":"appendix/information/it_formulas/#information-theory-measures","text":"","title":"Information Theory Measures"},{"location":"appendix/information/it_formulas/#references","text":"Lecture Notes I - PDF Video Introduction - Youtube","title":"References"},{"location":"appendix/information/it_formulas/#entropy-shannon","text":"","title":"Entropy (Shannon)"},{"location":"appendix/information/it_formulas/#one-random-variable","text":"If we have a discrete random variable X with p.m.f. p_x(x) p_x(x) , the entropy is: H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] H(X) = - \\sum_x p(x) \\log p(x) = - \\mathbb{E} \\left[ \\log(p(x)) \\right] This measures the expected uncertainty in X X . The entropy is basically how much information we learn on average from one instance of the r.v. X X . The standard definition of Entropy can be written as: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{Q(y)}{P(x)}dx\\\\ &=\\int_{-\\infty}^{\\infty} P(x) \\log \\frac{P(x)}{Q(y)}dx \\end{aligned}$$ and the discrete version: $$\\begin{aligned} D_{KLD}(P||Q) &=-\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)}\\\\ &=\\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)} \\end{aligned}$$ If we want the viewpoint in terms of expectations, we can do a bit of rearranging to get: $$\\begin{aligned} D_{KLD} &= \\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(y)}\\\\ &= \\sum_{x\\in\\mathcal{X}} P(x) \\log P(x)- \\sum_{-\\infty}^{\\infty}P(x)\\log Q(y)dx \\\\ &= \\sum_{x\\in\\mathcal{X}} P(x)\\left[\\log P(x) - \\log Q(y) \\right] \\\\ &= \\mathbb{E}_x\\left[ \\log P(x) - \\log Q(y) \\right] \\end{aligned}$$ #### Code - Step-by-Step 1. Obtain all of the possible occurrences of the outcomes. values , counts = np . unique ( labels , return_counts = True ) 2. Normalize the occurrences to obtain a probability distribution counts /= counts . sum () 3. Calculate the entropy using the formula above H = - ( counts * np . log ( counts , 2 )) . sum () As a general rule-of-thumb, I never try to reinvent the wheel so I look to use whatever other software is available for calculating entropy. The simplest I have found is from `scipy` which has an entropy function. We still need a probability distribution (the counts variable). From there we can just use the entropy function. 2. Use Scipy Function H = entropy ( counts , base = base )","title":"One Random Variable"},{"location":"appendix/information/it_formulas/#two-random-variables","text":"If we have two random variables X, Y X, Y jointly distributed according to the p.m.f. p(x,y) p(x,y) , we can come up with two more quantities for entropy.","title":"Two Random Variables"},{"location":"appendix/information/it_formulas/#joint-entropy","text":"This is given by: H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] H(X,Y) = \\sum_{x,y} p(x,y) \\log p(x,y) = - \\mathbb{E} \\left[ \\log(p(x,y)) \\right] Definition : how much uncertainty we have between two r.v.s X,Y X,Y .","title":"Joint Entropy"},{"location":"appendix/information/it_formulas/#conditional-entropy","text":"This is given by: H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] H(X|Y) = \\sum_{x,y} p(x,y) \\log p(x|y) = - \\mathbb{E} \\left[ \\log ( p(x|y)) \\right] Definition : how much uncertainty remains about the r.v. X X when we know the value of Y Y .","title":"Conditional Entropy"},{"location":"appendix/information/it_formulas/#properties-of-entropic-quantities","text":"Non-Negativity : H(X) \\geq 0 H(X) \\geq 0 , unless X X is deterministic (i.e. no randomness). Chain Rule : You can decompose the joint entropy measure: H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^{n}H(X_i | X^{i-1}) where X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} X^{i-1} = \\{ X_1, X_2, \\ldots, X_{i-1} \\} . So the result is: H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X) Monotonicity : Conditioning always reduces entropy. Information never hurts . H(X|Y) \\leq H(X) H(X|Y) \\leq H(X)","title":"Properties of Entropic Quantities"},{"location":"appendix/information/it_formulas/#negentropy","text":"It is simply entropy but we restrict the comparison to a Gaussian. Let's say that we have Z Z which comes from a normal distribution z\\sim\\mathcal{N}(0, \\mathbb{I}) z\\sim\\mathcal{N}(0, \\mathbb{I}) . We can write the same standard KLD formulation but with the","title":"Negentropy"},{"location":"appendix/information/it_formulas/#entropy-renyi","text":"Above we looked at Shannon entropy which is a special case of Renyi's Entropy measure. But the generalized entropy formula actually is a generalization on entropy. Below is the given formula. H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x) H_\\alpha(x) = \\frac{1}{1-\\alpha} \\log_2 \\sum_{x \\in \\mathcal{X}} p^{\\alpha}(x)","title":"Entropy (Renyi)"},{"location":"appendix/information/it_formulas/#mutual-information","text":"Definition : The mutual information (MI) between two discreet r.v.s X,Y X,Y jointly distributed according to p(x,y) p(x,y) is given by: I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia","title":"Mutual Information"},{"location":"appendix/information/it_formulas/#total-correlation-multi-information","text":"In general, the formula for Total Correlation (TC) between two random variables is as follows: TC(X,Y) = H(X) + H(Y) - H(X,Y) TC(X,Y) = H(X) + H(Y) - H(X,Y) Note : This is the same as the equation for mutual information between two random variables, I(X;Y)=H(X)+H(Y)-H(X,Y) I(X;Y)=H(X)+H(Y)-H(X,Y) . This makes sense because for a Venn Diagram between two r.v.s will only have one part that intersects. This is different for the multivariate case where the number of r.v.s is greater than 2. Let's have D D random variables for X = \\{ X_1, X_2, \\ldots, X_D\\} X = \\{ X_1, X_2, \\ldots, X_D\\} . The TC is: TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) TC(X) = \\sum_{d=1}^{D}H(X_d) - H(X_1, X_2, \\ldots, X_D) In this case, D D can be a feature for X X . Now, let's say we would like to get the difference in total correlation between two random variables, \\Delta \\Delta TC. \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\text{TC}(X) - \\text{TC}(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) - H(X) + H(Y) Note : There is a special case in RBIG where the two random variables are simply rotations of one another. So each feature will have a difference in entropy but the total overall dataset will not. So our function would be reduced to: \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) \\Delta\\text{TC}(X,Y) = \\sum_{d=1}^{D}H(X_d) - \\sum_{d=1}^{D} H(Y_d) which is overall much easier to solve.","title":"Total Correlation (Multi-Information)"},{"location":"appendix/information/it_formulas/#cross-entropy-log-loss-function","text":"Let P(\\cdot) P(\\cdot) be the true distribution and Q(\\cdot) Q(\\cdot) be the predicted distribution. We can define the cross entropy as: H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) H(P, Q) = - \\sum_{i}p_i \\log_2 (q_i) This can be thought of the measure in information length. Note : The original cross-entropy uses \\log_2(\\cdot) \\log_2(\\cdot) but in a supervised setting, we can use \\log_{10} \\log_{10} because if we use log rules, we get the following relation \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} \\log_2(\\cdot) = \\frac{\\log_{10}(\\cdot)}{\\log_{10}(2)} .","title":"Cross Entropy (Log-Loss Function)"},{"location":"appendix/information/it_formulas/#kullback-leibler-divergence-kl","text":"Furthermore, the KL divergence is the difference between the cross-entropy and the entropy. D_{KL}(P||Q) = H(P, Q) - H(P) D_{KL}(P||Q) = H(P, Q) - H(P) So this is how far away our predictions are from our actual distribution.","title":"Kullback-Leibler Divergence (KL)"},{"location":"appendix/information/it_formulas/#conditional-information-theory-measures","text":"","title":"Conditional Information Theory Measures"},{"location":"appendix/information/it_formulas/#conditional-entropy_1","text":"","title":"Conditional Entropy"},{"location":"appendix/information/it_formulas/#conditional-mutual-information","text":"Definition : Let X,Y,Z X,Y,Z be jointly distributed according to some p.m.f. p(x,y,z) p(x,y,z) . The conditional mutual information X,Y X,Y given Z Z is: I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = - \\sum_{x,y,z} p(x,y,z) \\log \\frac{p(x,y|z)}{p(x|z)p(y|z)} I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y|Z) = H(X) + H(Y) - H(X,Y) I(X;Y|Z) = H(X) + H(Y) - H(X,Y)","title":"Conditional Mutual Information"},{"location":"appendix/information/knn/","text":"K-Nearest Neighbors Estimator \u00b6 The full entropy expression: \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} where: * \\psi \\psi - the digamma function. * c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} * \\Gamma \\Gamma - is the gamma function * \\epsilon(i) \\epsilon(i) is the distance to the i^{th} i^{th} sample to its k^{th} k^{th} neighbour.","title":"K-Nearest Neighbors Estimator"},{"location":"appendix/information/knn/#k-nearest-neighbors-estimator","text":"The full entropy expression: \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} \\hat{H}(\\mathbf{X}) = \\psi(N) - \\psi(k) + \\log{c_d} + \\frac{d}{N}\\sum_{i=1}^{N} \\log{\\epsilon(i)} where: * \\psi \\psi - the digamma function. * c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} c_d=\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma(1+\\frac{d}{2})} * \\Gamma \\Gamma - is the gamma function * \\epsilon(i) \\epsilon(i) is the distance to the i^{th} i^{th} sample to its k^{th} k^{th} neighbour.","title":"K-Nearest Neighbors Estimator"},{"location":"appendix/information/main/","text":"Main \u00b6 Resources \u00b6 Prezi - Entropy and Mutual Info > Good formulas, good explanations, Gaussian stuff","title":"Main"},{"location":"appendix/information/main/#main","text":"","title":"Main"},{"location":"appendix/information/main/#resources","text":"Prezi - Entropy and Mutual Info > Good formulas, good explanations, Gaussian stuff","title":"Resources"},{"location":"appendix/information/mi/","text":"Mutual Information \u00b6 How much information one random variable says about another random variable. Intiution Full Definition Code Supplementary Information Intuition Supplementary Intiution \u00b6 Measure of the amount of information that one RV contains about another RV Reduction in the uncertainty of one rv due to knowledge of another The intersection of information in X with information in Y Full Definition \u00b6 I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia Code \u00b6 We need a PDF estimation... Normalize counts to probability values pxy = bin_counts / float ( np . sum ( bin_counts )) Get the marginal distributions px = np . sum ( pxy , axis = 1 ) # marginal for x over y py = np . sum ( pxy , axis = 0 ) # marginal for y over x Joint Probability Supplementary \u00b6 Information \u00b6 Intuition \u00b6 Things that don't normally happen, happen. Supplementary \u00b6 MI w. Numpy Predictions and Correlations in Complex Data","title":"Mutual Information"},{"location":"appendix/information/mi/#mutual-information","text":"How much information one random variable says about another random variable. Intiution Full Definition Code Supplementary Information Intuition Supplementary","title":"Mutual Information"},{"location":"appendix/information/mi/#intiution","text":"Measure of the amount of information that one RV contains about another RV Reduction in the uncertainty of one rv due to knowledge of another The intersection of information in X with information in Y","title":"Intiution"},{"location":"appendix/information/mi/#full-definition","text":"I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) I(X;Y) = H(X) + H(Y) - H(X,Y) I(X;Y) = H(X) + H(Y) - H(X,Y) Sources : * Scholarpedia","title":"Full Definition"},{"location":"appendix/information/mi/#code","text":"We need a PDF estimation... Normalize counts to probability values pxy = bin_counts / float ( np . sum ( bin_counts )) Get the marginal distributions px = np . sum ( pxy , axis = 1 ) # marginal for x over y py = np . sum ( pxy , axis = 0 ) # marginal for y over x Joint Probability","title":"Code"},{"location":"appendix/information/mi/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/information/mi/#information","text":"","title":"Information"},{"location":"appendix/information/mi/#intuition","text":"Things that don't normally happen, happen.","title":"Intuition"},{"location":"appendix/information/mi/#supplementary_1","text":"MI w. Numpy Predictions and Correlations in Complex Data","title":"Supplementary"},{"location":"appendix/information/pdf_est/","text":"Probability Density Function Estimators \u00b6 Resources \u00b6 Histogram Approximations Sebastian Blog - 1 | 2 Non-Parametric k-nearest neighbor entropy estimator - pdf","title":"Probability Density Function Estimators"},{"location":"appendix/information/pdf_est/#probability-density-function-estimators","text":"","title":"Probability Density Function Estimators"},{"location":"appendix/information/pdf_est/#resources","text":"Histogram Approximations Sebastian Blog - 1 | 2 Non-Parametric k-nearest neighbor entropy estimator - pdf","title":"Resources"},{"location":"appendix/information/variation/","text":"Variation of Information \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Correlation Example Root Mean Squared Example Taylor Diagram Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram Example Data \u00b6 We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, (c) linear dataset with an outlier. Standard Methods \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$ Example \u00b6 If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics. Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Example \u00b6 An easier number to interpret. But it will not distinguish the datasets. Root Mean Squared \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} Example \u00b6 Taylor Diagram \u00b6 The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 We see that the points are on top of each other. Makes sense seeing as how all of the other measures were also equivalent. Information Theory \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) Example \u00b6 Now we finally see some differences between the distributions. Normalized Mutual Information \u00b6 The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore. Variation of Information \u00b6 This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this... RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison. VI-Based Diagram \u00b6 This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"Variation of Information"},{"location":"appendix/information/variation/#variation-of-information","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Correlation Example Root Mean Squared Example Taylor Diagram Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram","title":"Variation of Information"},{"location":"appendix/information/variation/#example-data","text":"We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, (c) linear dataset with an outlier.","title":"Example Data"},{"location":"appendix/information/variation/#standard-methods","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared","title":"Standard Methods"},{"location":"appendix/information/variation/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$","title":"Covariance"},{"location":"appendix/information/variation/#example","text":"If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics.","title":"Example"},{"location":"appendix/information/variation/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Correlation"},{"location":"appendix/information/variation/#example_1","text":"An easier number to interpret. But it will not distinguish the datasets.","title":"Example"},{"location":"appendix/information/variation/#root-mean-squared","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2}","title":"Root Mean Squared"},{"location":"appendix/information/variation/#example_2","text":"","title":"Example"},{"location":"appendix/information/variation/#taylor-diagram","text":"The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}(X,Y)^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho=\\frac{C(X,Y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"Taylor Diagram"},{"location":"appendix/information/variation/#example_3","text":"We see that the points are on top of each other. Makes sense seeing as how all of the other measures were also equivalent.","title":"Example"},{"location":"appendix/information/variation/#information-theory","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Information Theory"},{"location":"appendix/information/variation/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform.","title":"Entropy"},{"location":"appendix/information/variation/#mutual-information","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y)","title":"Mutual Information"},{"location":"appendix/information/variation/#example_4","text":"Now we finally see some differences between the distributions.","title":"Example"},{"location":"appendix/information/variation/#normalized-mutual-information","text":"The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore.","title":"Normalized Mutual Information"},{"location":"appendix/information/variation/#variation-of-information_1","text":"This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this...","title":"Variation of Information"},{"location":"appendix/information/variation/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"RVI-Based Diagram"},{"location":"appendix/information/variation/#example_5","text":"The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison.","title":"Example"},{"location":"appendix/information/variation/#vi-based-diagram","text":"This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"VI-Based Diagram"},{"location":"appendix/information/vi/","text":"Variation of Information \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Example Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram Resources Example Data \u00b6 We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, (c) linear dataset with an outlier. Standard Methods \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$ Example \u00b6 If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics. Example \u00b6 An easier number to interpret. But it will not distinguish the datasets. Example \u00b6 Information Theory \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) Example \u00b6 Now we finally see some differences between the distributions. Normalized Mutual Information \u00b6 The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore. Variation of Information \u00b6 This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this... RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example \u00b6 The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison. VI-Based Diagram \u00b6 This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range. Resources \u00b6 Comparing Clusterings by the Variation of Information - Meila (2003) - PDF Comparing Clusterings - An Information Based Distance - Meila (2007) - PDF Paper discussing the variation of information as a distance measure from a clustering perspective. Does all of the proofs that it is a legit distance measure. The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) - PDF | Prezi The VI measure in a Taylor diagram format. Uses the variation of information in addition to the slightly modified version.","title":"Variation of Information"},{"location":"appendix/information/vi/#variation-of-information","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. Example Data Standard Methods Covariance Example Example Example Information Theory Entropy Mutual Information Example Normalized Mutual Information Variation of Information RVI-Based Diagram Example VI-Based Diagram Resources","title":"Variation of Information"},{"location":"appendix/information/vi/#example-data","text":"We will be using Anscombe example. This is a dataset that has the same attributes statistically, but measures like mean, variance and correlation seem to be the same. A classic dataset to show that linear methods will fail for nonlinear datasets. Caption : (a) Obviously linear dataset with noise, (b) Nonlinear dataset, (c) linear dataset with an outlier.","title":"Example Data"},{"location":"appendix/information/vi/#standard-methods","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Covariance Correlation Root Mean Squared","title":"Standard Methods"},{"location":"appendix/information/vi/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. We can remove the loop by doing a matrix multiplication. $$ C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) $$ where $X,Y \\in \\mathbb{R}^{N\\times 1}$","title":"Covariance"},{"location":"appendix/information/vi/#example","text":"If we calculate the covariance for the sample dataset, we get the following: As you can see, we have the same statistics.","title":"Example"},{"location":"appendix/information/vi/#example_1","text":"An easier number to interpret. But it will not distinguish the datasets.","title":"Example"},{"location":"appendix/information/vi/#example_2","text":"","title":"Example"},{"location":"appendix/information/vi/#information-theory","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Information Theory"},{"location":"appendix/information/vi/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform.","title":"Entropy"},{"location":"appendix/information/vi/#mutual-information","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y)","title":"Mutual Information"},{"location":"appendix/information/vi/#example_3","text":"Now we finally see some differences between the distributions.","title":"Example"},{"location":"appendix/information/vi/#normalized-mutual-information","text":"The MI measure is useful but it can also be somewhat difficult to interpret. The value goes off to \\infty \\infty and that value doesn't really have meaning unless we consider the entropy of the distributions from which this measure was calculated from. There are a few variants which I will list below. Pearson The measure that is closest to the Pearson correlation coefficient (thus Shannon's entropy is close to the standard variance estimate) can be defined by: \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\text{NMI}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} This method acts as a pure normalization. Note : one thing that strikes me as a flaw is the idea that we can get negative entropy values for differential entropy. This may cause problems if the entropy measures have opposite signs. This is definitely much easier to interpret. The relative values are also the same. Redundancy This is a symmetric version of the normalized MI measure. R=2\\frac{I(X,Y)}{H(X) + H(Y)} R=2\\frac{I(X,Y)}{H(X) + H(Y)} Interestingly, the relative magnitudes are not as similar anymore.","title":"Normalized Mutual Information"},{"location":"appendix/information/vi/#variation-of-information_1","text":"This quantity is akin to the RMSE for the standard statistics. $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ This is a metric that satisfies the properties such as * non-negativity * symmetry * Triangle Inequality. And because the properties are satisfied, we can use it in the Taylor Diagram scheme. I'm not sure how to interpret this...","title":"Variation of Information"},{"location":"appendix/information/vi/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"RVI-Based Diagram"},{"location":"appendix/information/vi/#example_4","text":"The nice thing is that the relative magnitudes are preserved and it definitely captures the correlations. I just need to figure out the labels of the chart... Relative comaprison.","title":"Example"},{"location":"appendix/information/vi/#vi-based-diagram","text":"This method uses the actual entropy measure instead of the square root. \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X)^2 + H(Y)^2 - 2 H(X)H(Y) \\left( 2 I(X,Y)\\frac{H(X,Y)}{H(X)H(Y)} - 1 \\right) \\\\ &= H(X) + H(Y) - 2 H(X)H(Y) c_{XY} \\end{aligned} So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos c_{XY} \\theta= \\arccos c_{XY} . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} \\sigma_{\\text{sim}}, \\theta=\\arccos c_{XY} . Note : This eliminates the sign problem. However, I wonder if this measure is actually bounded between 0 and 1. In my preliminary experiments, I had this problem. I was unable to plot this because of values obtained from the c_{XY} c_{XY} . They were not between 0 and 1 so the arccos function doesn't work for values outside of that range.","title":"VI-Based Diagram"},{"location":"appendix/information/vi/#resources","text":"Comparing Clusterings by the Variation of Information - Meila (2003) - PDF Comparing Clusterings - An Information Based Distance - Meila (2007) - PDF Paper discussing the variation of information as a distance measure from a clustering perspective. Does all of the proofs that it is a legit distance measure. The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) - PDF | Prezi The VI measure in a Taylor diagram format. Uses the variation of information in addition to the slightly modified version.","title":"Resources"},{"location":"appendix/kernels/distances/","text":"Other Distances \u00b6 Standard Distances Haversine Distances * The Performance Impact of Vectorized Operations - [blog](https://blog.godatadriven.com/the-performance-impact-of-vectorized-operations) * Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - [blog](https://nathanrooy.github.io/posts/2016-09-07/haversine-with-python/) * RBF Formaulas - [code](https://github.com/JeremyLinux/PyTorch-Radial-Basis-Function-Layer/blob/master/Torch%20RBF/torch_rbf.py)","title":"Other Distances"},{"location":"appendix/kernels/distances/#other-distances","text":"Standard Distances Haversine Distances * The Performance Impact of Vectorized Operations - [blog](https://blog.godatadriven.com/the-performance-impact-of-vectorized-operations) * Calculating the Distance Between Two GPS Coordinates with Python (Haversine Formula) - [blog](https://nathanrooy.github.io/posts/2016-09-07/haversine-with-python/) * RBF Formaulas - [code](https://github.com/JeremyLinux/PyTorch-Radial-Basis-Function-Layer/blob/master/Torch%20RBF/torch_rbf.py)","title":"Other Distances"},{"location":"appendix/kernels/kernel/","text":"Kernel Measures of Similarity \u00b6 Notation \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} K \\in \\mathbb{R}^{N \\times N} K \\in \\mathbb{R}^{N \\times N} is a kernel matrix. K_\\mathbf{x} K_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} K_\\mathbf{y} K_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} K_\\mathbf{xy} K_\\mathbf{xy} is the cross kernel matrix for the r.v. \\mathcal{X,Y} \\mathcal{X,Y} \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Observations \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} can have different number of dimensions \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} must have different number of samples Probabilities in Feature Space: The Mean Trick Mean Embedding Maximum Mean Discrepency (MMD) Hilbert-Schmidt Independence Criterion (HSIC) Covariance Measures Uncentered Kernel Centered Kernel Hilbert-Schmidt Independence Criterion (HSIC) Maximum Mean Discrepency (MMD) Kernel Matrix Inversion Sherman-Morrison-Woodbury Kernel Approximation Random Fourier Features Nystrom Approximation Structured Kernel Interpolation Correlation Measures Uncentered Kernel Kernel Alignment (KA) Uncentered Kernel Centered Kernel Alignment (cKA) Supplementary Ideas Feature Map \u00b6 We have a function \\varphi(X) \\varphi(X) to map \\mathcal{X} \\mathcal{X} to some feature space \\mathcal{F} \\mathcal{F} . \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N Function Class \u00b6 Reproducing Kernel Hilbert Space \\mathcal{H} \\mathcal{H} with kernel k k . Evaluation functionals f(x) = \\langle k(x,\\cdot), f \\rangle f(x) = \\langle k(x,\\cdot), f \\rangle We can compute means via linearity \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} And empirically \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} Kernels \u00b6 This allows us to not have to explicitly calculate \\varphi(X) \\varphi(X) . We just need an algorithm that calculates the dot product between them. \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') Reproducing Kernel Hilbert Space Notation \u00b6 Reproducing Property \\langle f, k(x,\\cdot) \\rangle = f(x) \\langle f, k(x,\\cdot) \\rangle = f(x) Equivalence between \\phi(x) \\phi(x) and k(x,\\cdot) k(x,\\cdot) . \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') Probabilities in Feature Space: The Mean Trick \u00b6 Mean Embedding \u00b6 \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k Maximum Mean Discrepency (MMD) \u00b6 \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} Hilbert-Schmidt Independence Criterion (HSIC) \u00b6 \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Given \\mathbb{P} \\mathbb{P} a Borel probability measure on \\mathcal{X} \\mathcal{X} , we can define a feature map \\mu_P \\in \\mathcal{F} \\mu_P \\in \\mathcal{F} . \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] Given a positive definite kernel k(x,x') k(x,x') , we can define the expectation of the cross kernel as: \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} for x \\sim P x \\sim P and q \\sim Q q \\sim Q . We can use the mean trick to define the following: \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} Covariance Measures \u00b6 Uncentered Kernel \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} <span><span class=\"MathJax_Preview\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}</span><script type=\"math/tex\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} Centered Kernel \u00b6 Hilbert-Schmidt Independence Criterion (HSIC) \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Maximum Mean Discrepency (MMD) \u00b6 \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Source Kernel Matrix Inversion \u00b6 Sherman-Morrison-Woodbury \u00b6 (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} Matrix Sketch (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} Kernel Approximation \u00b6 Random Fourier Features \u00b6 K \\approx ZZ^\\top K \\approx ZZ^\\top Nystrom Approximation \u00b6 K \\approx C W^\\dagger C^\\top K \\approx C W^\\dagger C^\\top According to ... the Nystroem approximation works better when you want features that are data dependent. The RFF method assumes a basis function and it is irrelevant to the data. It's merely projecting the data into the independent basis. The Nystroem approximation forms the basis through the data itself. Resources A Practical Guide to Randomized Matrix Computations with MATLAB Implementations - Shusen Wang (2015) - axriv Structured Kernel Interpolation \u00b6 \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} Correlation Measures \u00b6 Uncentered Kernel \u00b6 Kernel Alignment (KA) \u00b6 \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Kernel Alignment Uncentered Kernel \u00b6 Centered Kernel Alignment (cKA) \u00b6 \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} <span><span class=\"MathJax_Preview\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}}</span><script type=\"math/tex\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Centered Kernel Alignment Supplementary \u00b6 Ideas \u00b6 What happens when? HS Norm of Noisy Matrix HS Norm of PCA components","title":"Kernel Measures of Similarity"},{"location":"appendix/kernels/kernel/#kernel-measures-of-similarity","text":"Notation \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} K \\in \\mathbb{R}^{N \\times N} K \\in \\mathbb{R}^{N \\times N} is a kernel matrix. K_\\mathbf{x} K_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} K_\\mathbf{y} K_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} K_\\mathbf{xy} K_\\mathbf{xy} is the cross kernel matrix for the r.v. \\mathcal{X,Y} \\mathcal{X,Y} \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Observations \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} can have different number of dimensions \\mathbf{X},\\mathbf{Y} \\mathbf{X},\\mathbf{Y} must have different number of samples Probabilities in Feature Space: The Mean Trick Mean Embedding Maximum Mean Discrepency (MMD) Hilbert-Schmidt Independence Criterion (HSIC) Covariance Measures Uncentered Kernel Centered Kernel Hilbert-Schmidt Independence Criterion (HSIC) Maximum Mean Discrepency (MMD) Kernel Matrix Inversion Sherman-Morrison-Woodbury Kernel Approximation Random Fourier Features Nystrom Approximation Structured Kernel Interpolation Correlation Measures Uncentered Kernel Kernel Alignment (KA) Uncentered Kernel Centered Kernel Alignment (cKA) Supplementary Ideas","title":"Kernel Measures of Similarity"},{"location":"appendix/kernels/kernel/#feature-map","text":"We have a function \\varphi(X) \\varphi(X) to map \\mathcal{X} \\mathcal{X} to some feature space \\mathcal{F} \\mathcal{F} . \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N \\phi(X) = \\left[ \\cdots, \\varphi_i(x), \\cdots \\right] \\in N","title":"Feature Map"},{"location":"appendix/kernels/kernel/#function-class","text":"Reproducing Kernel Hilbert Space \\mathcal{H} \\mathcal{H} with kernel k k . Evaluation functionals f(x) = \\langle k(x,\\cdot), f \\rangle f(x) = \\langle k(x,\\cdot), f \\rangle We can compute means via linearity \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} \\begin{aligned} \\mathbb{E}_{X \\sim P} \\left[ f(X) \\right] &= \\mathbb{E}_{X \\sim P} \\left[ \\langle k(x, \\cdot), f \\rangle \\right] \\\\ &= \\bigg\\langle \\mathbb{E}_{X \\sim P} \\left[ k(x, \\cdot)\\right], f \\bigg\\rangle \\\\ &= \\langle \\mu_P, f \\rangle \\end{aligned} And empirically \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned} \\begin{aligned} \\frac{1}{N} \\sum_{i=1}^N f(x_i) &= \\frac{1}{N} \\sum_{i=1}^N \\langle k(x, \\cdot), f \\rangle \\\\ &= \\bigg\\langle \\frac{1}{N} \\sum_{i=1}^N k(x, \\cdot), f \\bigg\\rangle \\\\ &= \\langle \\mu_X, f \\rangle \\end{aligned}","title":"Function Class"},{"location":"appendix/kernels/kernel/#kernels","text":"This allows us to not have to explicitly calculate \\varphi(X) \\varphi(X) . We just need an algorithm that calculates the dot product between them. \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X') \\langle \\varphi(X), \\varphi(X') \\rangle_\\mathcal{F} = k(X, X')","title":"Kernels"},{"location":"appendix/kernels/kernel/#reproducing-kernel-hilbert-space-notation","text":"Reproducing Property \\langle f, k(x,\\cdot) \\rangle = f(x) \\langle f, k(x,\\cdot) \\rangle = f(x) Equivalence between \\phi(x) \\phi(x) and k(x,\\cdot) k(x,\\cdot) . \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x') \\langle k(x, \\cdot), k(x', \\cdot) \\rangle = k(x, x')","title":"Reproducing Kernel Hilbert Space Notation"},{"location":"appendix/kernels/kernel/#probabilities-in-feature-space-the-mean-trick","text":"","title":"Probabilities in Feature Space: The Mean Trick"},{"location":"appendix/kernels/kernel/#mean-embedding","text":"\\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k","title":"Mean Embedding"},{"location":"appendix/kernels/kernel/#maximum-mean-discrepency-mmd","text":"\\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k}","title":"Maximum Mean Discrepency (MMD)"},{"location":"appendix/kernels/kernel/#hilbert-schmidt-independence-criterion-hsic","text":"\\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Given \\mathbb{P} \\mathbb{P} a Borel probability measure on \\mathcal{X} \\mathcal{X} , we can define a feature map \\mu_P \\in \\mathcal{F} \\mu_P \\in \\mathcal{F} . \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] \\mu_P = \\left[ \\ldots \\mathbb{E}_P\\left[ \\varphi_i(\\mathbf{x}) \\right] \\right] Given a positive definite kernel k(x,x') k(x,x') , we can define the expectation of the cross kernel as: \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} \\mathbb{E}_{P,Q}k(\\mathbf{x,y}) = \\langle \\mu_P, \\mu_Q \\rangle_\\mathcal{F} for x \\sim P x \\sim P and q \\sim Q q \\sim Q . We can use the mean trick to define the following: \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F} \\mathbb{E}_P (f(X)) = \\langle \\mu_P, f(\\cdot) \\rangle_\\mathcal{F}","title":"Hilbert-Schmidt Independence Criterion (HSIC)"},{"location":"appendix/kernels/kernel/#covariance-measures","text":"","title":"Covariance Measures"},{"location":"appendix/kernels/kernel/#uncentered-kernel","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F} <span><span class=\"MathJax_Preview\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}</span><script type=\"math/tex\">\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||K_{\\mathbf{xy}}||_\\mathcal{F} =\\langle K_\\mathbf{x}, K_\\mathbf{y} \\rangle_\\mathcal{F}","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#centered-kernel","text":"","title":"Centered Kernel"},{"location":"appendix/kernels/kernel/#hilbert-schmidt-independence-criterion-hsic_1","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) =||\\tilde{K}_{\\mathbf{xy}}||_\\mathcal{F} =\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}","title":"Hilbert-Schmidt Independence Criterion (HSIC)"},{"location":"appendix/kernels/kernel/#maximum-mean-discrepency-mmd_1","text":"\\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} \\text{cov}(\\mathbf{X}, \\mathbf{Y}) = ||K_\\mathbf{x}||_\\mathcal{F} + ||K_\\mathbf{y}||_\\mathcal{F} - 2\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F} Source","title":"Maximum Mean Discrepency (MMD)"},{"location":"appendix/kernels/kernel/#kernel-matrix-inversion","text":"","title":"Kernel Matrix Inversion"},{"location":"appendix/kernels/kernel/#sherman-morrison-woodbury","text":"(A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}A^{-1} Matrix Sketch (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top} (LL^\\top + \\sigma I_N)^{-1} = \\sigma^{-1} I_N - \\sigma^{-1} (\\sigma I_{n} + L^\\top L)^{-1} L^{\\top}","title":"Sherman-Morrison-Woodbury"},{"location":"appendix/kernels/kernel/#kernel-approximation","text":"","title":"Kernel Approximation"},{"location":"appendix/kernels/kernel/#random-fourier-features","text":"K \\approx ZZ^\\top K \\approx ZZ^\\top","title":"Random Fourier Features"},{"location":"appendix/kernels/kernel/#nystrom-approximation","text":"K \\approx C W^\\dagger C^\\top K \\approx C W^\\dagger C^\\top According to ... the Nystroem approximation works better when you want features that are data dependent. The RFF method assumes a basis function and it is irrelevant to the data. It's merely projecting the data into the independent basis. The Nystroem approximation forms the basis through the data itself. Resources A Practical Guide to Randomized Matrix Computations with MATLAB Implementations - Shusen Wang (2015) - axriv","title":"Nystrom Approximation"},{"location":"appendix/kernels/kernel/#structured-kernel-interpolation","text":"\\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned} \\begin{aligned} K &\\approx C W^\\dagger C^\\top \\\\ &\\approx (XW) W^\\dagger (XW)^\\top \\\\ &\\approx X W X^\\top \\end{aligned}","title":"Structured Kernel Interpolation"},{"location":"appendix/kernels/kernel/#correlation-measures","text":"","title":"Correlation Measures"},{"location":"appendix/kernels/kernel/#uncentered-kernel_1","text":"","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#kernel-alignment-ka","text":"\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Kernel Alignment","title":"Kernel Alignment (KA)"},{"location":"appendix/kernels/kernel/#uncentered-kernel_2","text":"","title":"Uncentered Kernel"},{"location":"appendix/kernels/kernel/#centered-kernel-alignment-cka","text":"\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} \\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} <span><span class=\"MathJax_Preview\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}}</span><script type=\"math/tex\">\\rho(\\mathbf{X}, \\mathbf{Y}) =\\frac{\\langle \\tilde{K}_\\mathbf{x}, \\tilde{K}_\\mathbf{y} \\rangle_\\mathcal{F}}{||\\tilde{K}_\\mathbf{x}||_\\mathcal{F}||\\tilde{K}_\\mathbf{y}||_\\mathcal{F}} In the Literature Centered Kernel Alignment","title":"Centered Kernel Alignment (cKA)"},{"location":"appendix/kernels/kernel/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/kernels/kernel/#ideas","text":"What happens when? HS Norm of Noisy Matrix HS Norm of PCA components","title":"Ideas"},{"location":"appendix/kernels/similarity/dist/","text":"Distance Correlation \u00b6 Resources \u00b6 Euclidean Distance Matrices: Essential Theory, Algorithms and Applications - arxiv","title":"Distance Correlation"},{"location":"appendix/kernels/similarity/dist/#distance-correlation","text":"","title":"Distance Correlation"},{"location":"appendix/kernels/similarity/dist/#resources","text":"Euclidean Distance Matrices: Essential Theory, Algorithms and Applications - arxiv","title":"Resources"},{"location":"appendix/kernels/similarity/hsic/","text":"Measuring Similarities with the Hilbert-Schmidt Independence Criterion \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 29 Jan, 2020 We use the Hilbert-Schmidt Independence Criterion (HSIC) measure independence between two distributions. It involves constructing an appropriate kernel matrix for each dataset and then using the Frobenius Norm as a way to \"summarize\" the variability of the data. Often times the motivation for this method is lost in the notorious paper of Arthur Gretton (the creator of the method), but actually, this idea was developed long before him with ideas from a covariance matrix perspective. Below are my notes for how to get from a simple covariance matrix to the HSIC method and similar ones. Resources \u00b6 Presentations HSIC, A Measure of Independence - Szabo (2018) Measuring Independence with Kernels - Gustau Motivation \u00b6 A very common mechanism to measure the differences between datasets is to measure the variability. The easiest way is the measure the covariance between the two datasets. However, this is limited to datasets with linear relationships and with not many outliers. Anscombe's classic dataset is an example where we have datasets with the same mean and standard deviation. This means measures like the covariance and correlation become useless because they will yield the same result. This requires us to have more robust methods or to do some really good preprocessing to make models easier. Covariance \u00b6 The first measure we need to consider is the covariance. This can be used for a single variable X X , the covariance, or the cross-covariance between multiple variables X,Y X,Y . Some key properties include: It is a measure of the joint variability between the datasets It is difficult to interpret because it can range from -\\infty -\\infty to \\infty \\infty . The units are dependent upon the inputs. It is affected by isotropic scaling For a univariate dataset we can use the following expression. \\text{C}_{xy} = \\mathbb{E}\\left[(x-\\mu_x)(y-\\mu_y) \\right] \\text{C}_{xy} = \\mathbb{E}\\left[(x-\\mu_x)(y-\\mu_y) \\right] We basically need to subtract the mean between all of the samples and then take the joint expectation of the two datasets. This will give us a scalar value which Multidimensional Datasets \u00b6 In the case of multidimensional datasets, this calculation because a bit more complicated. We don't get a scalar output; we get a covariance matrix D\\times D D\\times D . Note : this means that the matrix is in the feature space. The diagonal elements will be the covariance between each feature d d with itself and the off-diagonal elements will be the cross-covariance between each feature d_i d_i and another feature d_j d_j . We can make some observations about the covariance matrices that can give us more information about the structure of our data. There are two important ones: A completely diagonal matrix means that all features are uncorrelated (orthogonal to each other) Diagonal Covariances are useful to learn (condition your model on) because it means that we have non-redundant features. Empirical Covariance We can empirically calculate this by doing the matrix multiplication of two vectors. Code C_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Empirical Cross-Covariance This is the covariance between different datasets Code c_xy = X . T @ Y Correlation \u00b6 We can use this measure to summarize and normalize our data. It is a measure of relationship It is interpretable because it ranges from -1,1 -1,1 It isn't affected by isotropic scaling due to the normalization It is unitless because of the normalization There are many benefits to using this measure which is why it is often used so widely. \\rho_{xy} = \\frac{C_{xy}}{\\sigma_x \\sigma_y} \\rho_{xy} = \\frac{C_{xy}}{\\sigma_x \\sigma_y} Summarizing Multi-Dimensional Information \u00b6 Let's have the two distributions \\mathcal{X} \\in \\mathbb{R}^{D_x} \\mathcal{X} \\in \\mathbb{R}^{D_x} and \\mathcal{Y} \\in \\mathbb{R}^{D_y} \\mathcal{Y} \\in \\mathbb{R}^{D_y} . Let's also assume that we can sample (x,y) (x,y) from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . We can capture the second order dependencies between X X and Y Y by constructing a covariance matrix in the feature space defined as: C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} We can use the Hilbert-Schmidt Norm (HS-Norm) as a statistic to effectively summarize content within this covariance matrix. It's defined as: ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] Note that this term is zero iff X X and Y Y are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value that summarizes the structure of our data. Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: $tr(C_{xy}^\\top C_{xy})$. So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the `sum` operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () **Refactor** There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro) And also just like the correlation, we can also do a normalization scheme that allows us to have an interpretable scalar value. This is similar to the correlation coefficient except it can now be applied to multi-dimensional data. \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} Connections \u00b6 Mutual Information \u00b6 This is an approximation to kernel mutual information I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_xx||C_yy||} \\right) I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_xx||C_yy||} \\right) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) Samples versus Features \u00b6 One interesting connection is that using the HS norm in the feature space is the sample thing as using it in the sample space. \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} Comparing Features is the same as comparing samples! Note : This is very similar to the dual versus sample space that is often mentioned in the kernel literature. So our equations before will change slightly in notation as we are constructing different matrices. But in the end, they will have the same output. This includes the correlation coefficient \\rho \\rho . \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} Kernel Trick \u00b6 So now, we have only had a linear dot-similarity in the sample space of \\mathcal{X} \\mathcal{X} and \\mathcal{Y} \\mathcal{Y} . This is good but we can easily extend this to a non-linear transformation where we add an additional function \\psi \\psi for each of the kernel functions. \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} Centering \u00b6 A very important but subtle point is that the method with kernels assumes that your data is centered in the kernel space. This isn't necessarily true. Fortunately it is easy to do so. HK_xH = \\tilde{K}_x HK_xH = \\tilde{K}_x where H H is your centering matrix. Normalizing your inputs does not equal centering your kernel matrix. We assume that the kernel function $\\psi(x_i)$ has a zero mean like so: $$\\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r)$$ This holds if the covariance matrix is computed from $\\psi(x_i)$. So the kernel matrix $K_{ij}=\\psi(x_i)^\\top \\psi(x_j)$ needs to be replaced with $\\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s)$ where $\\tilde{K}_{ij}$ is: $$ \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} $$ Code On a more practical note, this can be done easily by: $$H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top$$ H = np . eye ( n_samples ) - ( 1 / n_samples ) * np . ones ( n_samples , n_samples ) **Refactor** There is also a function in the `scikit-learn` library which does it for you. from sklearn.preprocessing import KernelCenterer K_centered = KernelCenterer () . fit_transform ( K ) And like the covariance, we can also summarize the data structures with a correlation-like coefficient \\rho_\\mathbf{xy}=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} \\rho_\\mathbf{xy}=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} Hilbert-Schmidt Criterion \u00b6 Let's assume there exists a nonlinear mapping from our data space to the Hilbert space. So \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} and \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} . We also assume that there is a representation of this mapping via the dot product between the features of the data space; i.e. K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle and K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle . So now the data matrices are \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} and \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} . So we can take the kernelized version of the cross covariance mapping as defined for the covariance matrix: \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} Now after a bit of simplication, we end up with the HSIC-Norm: $$ \\begin{aligned} \\text{HSIC}(\\hat{P} {XY}, \\mathcal{F}, \\mathcal{G}) &= tr (K {\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned}$$ Proof In this section, we will derive the empirical formula for HSIC using the Hilbert-Schmidt Norm of the covariance matrix with the kernel mapping. $$ \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} $$ Details Using the same argument as above, we can also define a cross covariance matrix of the form: $$C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right]$$ where $\\otimes$ is the tensor product, $\\mu_x, \\mu_y$ are the expecations of the mappings $\\mathbb{E}_x [\\phi (x)]$, $\\mathbb{E}_y[\\psi(y)]$ respectively. The HSIC is the cross-covariance operator described above and can be expressed in terms of kernels. $$\\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) = ||C_{xy}||_{\\mathcal{H}}^2$$ $$\\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) = \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] $$ $$+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right]$$ $$- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right]$$ where $\\mathbb{E}_{xx'yy'}$ is the expectation over both $(x,y) \\sim \\mathbb{P}_{xy}$ and we assume that $(x',y')$ can be sampled independently from $\\mathbb{P}_{xy}$. Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) between two kernel matrics that correctly model your data. This boils down to computing the trace of the matrix multiplication of two matrices: $tr(K_x^\\top K_y)$. So in algorithmically that is hsic_score = np . trace ( K_x . T @ K_y ) Notice that this is a 3-part operation. So, of course, we can refactor this to be much easier. A faster way to do this is: hsic_score = np . sum ( K_x * K_y ) This can be orders of magnitude faster because it is a much cheaper operation to compute elementwise products than a sum. And for fun, we can even use the `einsum` notation. hsic_score = np . einsum ( \"ji,ij->\" , K_x , K_y ) Future Outlook \u00b6 Advantages Sample Space - Nice for High Dimensional Problems w/ a low number of samples HSIC can estimate dependence between variables of different dimensions Very flexible: lots of ways to create kernel matices Disadvantages Computationally demanding for large scale problems Non-iid samples, e.g. speech or images Tuning Kernel parameters Why the HS nrm? Practical Equations \u00b6 HSIC \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\coloneqq \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\coloneqq \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) where H H is the centering matrix H=I_n-\\frac{1}{n}1_n1_n^\\top H=I_n-\\frac{1}{n}1_n1_n^\\top . \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) $$$$ Objects of Interest \u00b6 Mean Embedding \u00b6 \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k Maximum Mean Discrepency (MMD) \u00b6 \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} Hilbert-Schmidt Independence Criterion (HSIC) \u00b6 \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) Linear Algebra \u00b6 Norm induced by the inner product: ||f||_{\\mathcal{H}} \\coloneqq \\sqrt{\\langle f,f \\rangle_{\\mathcal{H}}} ||f||_{\\mathcal{H}} \\coloneqq \\sqrt{\\langle f,f \\rangle_{\\mathcal{H}}} Classical Information Theory \u00b6 Kullback-Leibler Divergence D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx Mutual Information I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) Tangent Kernel Alignment \u00b6 HSIC $$A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} $$ Original Kernel Tangent Alignment A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} The alignment can be seen as a similarity score based on the cosine of the angle. For arbitrary matrices, this score ranges between -1 and 1. But using positive semidefinite Gram matrices, the score is lower-bounded by 0. Centered Kernel Tangent Alignment $$A(H K_{x}, H K_{y}) = \\frac{\\left\\langle H K_{x}, H K_{y} \\right\\rangle_{F}}{\\sqrt{|| H K_{x}|| {F}|| H K {y} ||_{F}}} $$ They add a normalization term to deal with some of the shortcomings of the original KTA algorithm which had some benefits e.g. a way to cancel out unbalanced class effects. The improvement over the original algorithm seems minor but there is a critical difference. Without the centering, the alignment does not correlate well to the performance of the learning machine. Literature Review \u00b6 An Overview of Kernel Alignment and its Applications - Wang et al (2012) - PDF This goes over the literature of the kernel alignment method as well as some applications it has been used it. Applications \u00b6 Kerneel Target Alignment Parameter: A New Modelability for Regression Tasks - Marcou et al (2016) - Paper Brain Activity Patterns - Paper Scaling - Paper Textbooks \u00b6 Kernel Methods for Digital Processing - Book Useful Formulas \u00b6 Kernel Alignment Empirical Alignment evaluates the similarity between the corresponding matrices. $$ \\begin{aligned} A(K_1, K_2) &= \\frac{\\langle K_1, K_2 \\rangle_{F}}{\\sqrt{\\langle K_1, K_1 \\rangle_{F}\\langle K_2, K_2 \\rangle_{F}}} \\end{aligned} $$ where: \\langle K_1, K_2 \\rangle_{F} = \\sum_{i=1}^N\\sum_{j=1}^Nk_1(x_i, x_j)k_2(x_i, x_j) \\langle K_1, K_2 \\rangle_{F} = \\sum_{i=1}^N\\sum_{j=1}^Nk_1(x_i, x_j)k_2(x_i, x_j) This can be seen as a similarity score between the cosine of the angle. It has a lower bound of 0 because we typically only use positive semi-definite Gram matrices. Centered Kernel Alignment To counter the unbalanced class distribution: $$ \\begin{aligned} k_c(x,z) = \\left( \\phi(x) - \\mathbb{E} \\left[\\phi(X)\\right] \\right)^\\top \\left( \\phi(z) - \\mathbb{E} \\left[\\phi(Z)\\right] \\right) \\end{aligned} $$ The empirical centered alignment can be written as: $$ \\begin{aligned} A_c(K_{c1}, K_{c2}) &= \\frac{\\langle K_{c1}, K_{c2} \\rangle_{F}}{\\sqrt{\\langle K_{c1}, K_{c1} \\rangle_{F}\\langle K_{c2}, K_{c2} \\rangle_{F}}} \\end{aligned} $$ Frobenius Norm (or Hilbert-Schmidt Norm) a matrix \u00b6 \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Details Let A=U\\Sigma V^\\top A=U\\Sigma V^\\top be the Singular Value Decomposition of A. Then ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 If \\lambda_i^2 \\lambda_i^2 are the eigenvalues of AA^\\top AA^\\top and A^\\top A A^\\top A , then we can show \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned}","title":"Measuring Similarities with the Hilbert-Schmidt Independence Criterion"},{"location":"appendix/kernels/similarity/hsic/#measuring-similarities-with-the-hilbert-schmidt-independence-criterion","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 29 Jan, 2020 We use the Hilbert-Schmidt Independence Criterion (HSIC) measure independence between two distributions. It involves constructing an appropriate kernel matrix for each dataset and then using the Frobenius Norm as a way to \"summarize\" the variability of the data. Often times the motivation for this method is lost in the notorious paper of Arthur Gretton (the creator of the method), but actually, this idea was developed long before him with ideas from a covariance matrix perspective. Below are my notes for how to get from a simple covariance matrix to the HSIC method and similar ones.","title":"Measuring Similarities with the Hilbert-Schmidt Independence Criterion"},{"location":"appendix/kernels/similarity/hsic/#resources","text":"Presentations HSIC, A Measure of Independence - Szabo (2018) Measuring Independence with Kernels - Gustau","title":"Resources"},{"location":"appendix/kernels/similarity/hsic/#motivation","text":"A very common mechanism to measure the differences between datasets is to measure the variability. The easiest way is the measure the covariance between the two datasets. However, this is limited to datasets with linear relationships and with not many outliers. Anscombe's classic dataset is an example where we have datasets with the same mean and standard deviation. This means measures like the covariance and correlation become useless because they will yield the same result. This requires us to have more robust methods or to do some really good preprocessing to make models easier.","title":"Motivation"},{"location":"appendix/kernels/similarity/hsic/#covariance","text":"The first measure we need to consider is the covariance. This can be used for a single variable X X , the covariance, or the cross-covariance between multiple variables X,Y X,Y . Some key properties include: It is a measure of the joint variability between the datasets It is difficult to interpret because it can range from -\\infty -\\infty to \\infty \\infty . The units are dependent upon the inputs. It is affected by isotropic scaling For a univariate dataset we can use the following expression. \\text{C}_{xy} = \\mathbb{E}\\left[(x-\\mu_x)(y-\\mu_y) \\right] \\text{C}_{xy} = \\mathbb{E}\\left[(x-\\mu_x)(y-\\mu_y) \\right] We basically need to subtract the mean between all of the samples and then take the joint expectation of the two datasets. This will give us a scalar value which","title":"Covariance"},{"location":"appendix/kernels/similarity/hsic/#multidimensional-datasets","text":"In the case of multidimensional datasets, this calculation because a bit more complicated. We don't get a scalar output; we get a covariance matrix D\\times D D\\times D . Note : this means that the matrix is in the feature space. The diagonal elements will be the covariance between each feature d d with itself and the off-diagonal elements will be the cross-covariance between each feature d_i d_i and another feature d_j d_j . We can make some observations about the covariance matrices that can give us more information about the structure of our data. There are two important ones: A completely diagonal matrix means that all features are uncorrelated (orthogonal to each other) Diagonal Covariances are useful to learn (condition your model on) because it means that we have non-redundant features. Empirical Covariance We can empirically calculate this by doing the matrix multiplication of two vectors. Code C_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Empirical Cross-Covariance This is the covariance between different datasets Code c_xy = X . T @ Y","title":"Multidimensional Datasets"},{"location":"appendix/kernels/similarity/hsic/#correlation","text":"We can use this measure to summarize and normalize our data. It is a measure of relationship It is interpretable because it ranges from -1,1 -1,1 It isn't affected by isotropic scaling due to the normalization It is unitless because of the normalization There are many benefits to using this measure which is why it is often used so widely. \\rho_{xy} = \\frac{C_{xy}}{\\sigma_x \\sigma_y} \\rho_{xy} = \\frac{C_{xy}}{\\sigma_x \\sigma_y}","title":"Correlation"},{"location":"appendix/kernels/similarity/hsic/#summarizing-multi-dimensional-information","text":"Let's have the two distributions \\mathcal{X} \\in \\mathbb{R}^{D_x} \\mathcal{X} \\in \\mathbb{R}^{D_x} and \\mathcal{Y} \\in \\mathbb{R}^{D_y} \\mathcal{Y} \\in \\mathbb{R}^{D_y} . Let's also assume that we can sample (x,y) (x,y) from \\mathbb{P}_{xy} \\mathbb{P}_{xy} . We can capture the second order dependencies between X X and Y Y by constructing a covariance matrix in the feature space defined as: C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} C_{\\mathbf{xy}} \\in \\mathbb{R}^{D \\times D} We can use the Hilbert-Schmidt Norm (HS-Norm) as a statistic to effectively summarize content within this covariance matrix. It's defined as: ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] ||C_{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left[ C_{xy}^\\top C_{xy} \\right] Note that this term is zero iff X X and Y Y are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value that summarizes the structure of our data. Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: $tr(C_{xy}^\\top C_{xy})$. So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the `sum` operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () **Refactor** There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro) And also just like the correlation, we can also do a normalization scheme that allows us to have an interpretable scalar value. This is similar to the correlation coefficient except it can now be applied to multi-dimensional data. \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}} \\rho_\\mathbf{xy} = \\frac{ ||C_{\\mathbf{xy}}||_\\mathcal{F}^2}{||C_\\mathbf{xx}||_{\\mathcal{F}} ||C_\\mathbf{yy}||_{\\mathcal{F}}}","title":"Summarizing Multi-Dimensional Information"},{"location":"appendix/kernels/similarity/hsic/#connections","text":"","title":"Connections"},{"location":"appendix/kernels/similarity/hsic/#mutual-information","text":"This is an approximation to kernel mutual information I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_xx||C_yy||} \\right) I(X,Y) = - \\frac{1}{2} \\log \\left( \\frac{|C|}{|C_xx||C_yy||} \\right) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2) I(X,Y) = - \\frac{1}{2} \\log (1- \\rho^2)","title":"Mutual Information"},{"location":"appendix/kernels/similarity/hsic/#samples-versus-features","text":"One interesting connection is that using the HS norm in the feature space is the sample thing as using it in the sample space. \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} \\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}} = \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} Comparing Features is the same as comparing samples! Note : This is very similar to the dual versus sample space that is often mentioned in the kernel literature. So our equations before will change slightly in notation as we are constructing different matrices. But in the end, they will have the same output. This includes the correlation coefficient \\rho \\rho . \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}} \\frac{\\langle C_{\\mathbf{x^\\top y}}, C_{\\mathbf{x^\\top y}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{x^\\top x}||_{\\mathcal{F}} ||C_\\mathbf{y^\\top y}||_{\\mathcal{F}}} = \\frac{ \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}}}{||C_\\mathbf{xx^\\top}||_{\\mathcal{F}} ||C_\\mathbf{yy^\\top}||_{\\mathcal{F}}}","title":"Samples versus Features"},{"location":"appendix/kernels/similarity/hsic/#kernel-trick","text":"So now, we have only had a linear dot-similarity in the sample space of \\mathcal{X} \\mathcal{X} and \\mathcal{Y} \\mathcal{Y} . This is good but we can easily extend this to a non-linear transformation where we add an additional function \\psi \\psi for each of the kernel functions. \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F} \\langle C_{\\mathbf{xx^\\top }}, C_{\\mathbf{yy^\\top}}\\rangle_{\\mathcal{F}} = \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}","title":"Kernel Trick"},{"location":"appendix/kernels/similarity/hsic/#centering","text":"A very important but subtle point is that the method with kernels assumes that your data is centered in the kernel space. This isn't necessarily true. Fortunately it is easy to do so. HK_xH = \\tilde{K}_x HK_xH = \\tilde{K}_x where H H is your centering matrix. Normalizing your inputs does not equal centering your kernel matrix. We assume that the kernel function $\\psi(x_i)$ has a zero mean like so: $$\\psi(x_i) = \\psi(x_i) - \\frac{1}{N}\\sum_{r=1}^N \\psi(x_r)$$ This holds if the covariance matrix is computed from $\\psi(x_i)$. So the kernel matrix $K_{ij}=\\psi(x_i)^\\top \\psi(x_j)$ needs to be replaced with $\\tilde{K}_{ij}=\\psi(x_i)^\\top \\psi(x_s)$ where $\\tilde{K}_{ij}$ is: $$ \\begin{aligned} \\tilde{K}_{ij} &= \\psi(x_i)^\\top \\psi(x_j) - \\frac{1}{N} \\sum_{r=1}^N - \\frac{1}{N} \\sum_{r=1}^N \\psi(x_r)^\\top \\psi(x_j) + \\frac{1}{N^2} \\sum_{r,s=1}^N \\psi(x_r))^\\top \\psi(x_s) \\\\ &= K_{ij} - \\frac{1}{N}\\sum_{r=1}^{N}K_{ir} - \\frac{1}{N} K_{rj} + \\frac{1}{N^2} \\sum_{r,s=1}^N K_s \\end{aligned} $$ Code On a more practical note, this can be done easily by: $$H = \\mathbf{I}_N - \\frac{1}{N} \\mathbf{1}_N\\mathbf{1}_N^\\top$$ H = np . eye ( n_samples ) - ( 1 / n_samples ) * np . ones ( n_samples , n_samples ) **Refactor** There is also a function in the `scikit-learn` library which does it for you. from sklearn.preprocessing import KernelCenterer K_centered = KernelCenterer () . fit_transform ( K ) And like the covariance, we can also summarize the data structures with a correlation-like coefficient \\rho_\\mathbf{xy}=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}} \\rho_\\mathbf{xy}=\\frac{ \\langle K_{\\mathbf{x}}, K_{\\mathbf{y}}\\rangle_\\mathcal{F}}{||K_\\mathbf{x}||_{\\mathcal{F}} ||K_\\mathbf{y}||_{\\mathcal{F}}}","title":"Centering"},{"location":"appendix/kernels/similarity/hsic/#hilbert-schmidt-criterion","text":"Let's assume there exists a nonlinear mapping from our data space to the Hilbert space. So \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} \\phi : \\mathcal{X} \\rightarrow \\mathcal{F} and \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} \\psi : \\mathcal{Y} \\rightarrow \\mathcal{G} . We also assume that there is a representation of this mapping via the dot product between the features of the data space; i.e. K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle K_x(x,x') = \\langle \\phi(x), \\phi(x') \\rangle and K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle K_y(y,y') = \\langle \\psi(y), \\psi(y') \\rangle . So now the data matrices are \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} \\Phi \\in \\mathbb{R}^{N\\times N_\\mathcal{F}} and \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} \\Psi \\in \\mathbb{R}^{N \\times N_\\mathcal{G}} . So we can take the kernelized version of the cross covariance mapping as defined for the covariance matrix: \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2_{\\mathcal{F}} \\end{aligned} Now after a bit of simplication, we end up with the HSIC-Norm: $$ \\begin{aligned} \\text{HSIC}(\\hat{P} {XY}, \\mathcal{F}, \\mathcal{G}) &= tr (K {\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned}$$ Proof In this section, we will derive the empirical formula for HSIC using the Hilbert-Schmidt Norm of the covariance matrix with the kernel mapping. $$ \\begin{aligned} ||C_{\\phi(x)\\psi(x)}||_\\mathcal{H}^2 &= ||\\Phi^\\top \\Psi||^2 \\\\ &= tr\\left[ (\\Phi^\\top \\Psi)^\\top (\\Phi^\\top \\Psi)\\right] \\\\ &= tr \\left[ \\Psi^\\top \\Phi \\Phi^\\top \\Psi\\right] \\\\ &= tr \\left[ \\Psi \\Psi^\\top \\Phi \\Phi^\\top \\right] \\\\ &= tr (K_{\\mathbf{x}}K_{\\mathbf{x}}) \\end{aligned} $$ Details Using the same argument as above, we can also define a cross covariance matrix of the form: $$C_{xy} = \\mathbb{E}_{xy} \\left[ (\\phi(x) - \\mu_x) \\otimes (\\psi(y) - \\mu_y)\\right]$$ where $\\otimes$ is the tensor product, $\\mu_x, \\mu_y$ are the expecations of the mappings $\\mathbb{E}_x [\\phi (x)]$, $\\mathbb{E}_y[\\psi(y)]$ respectively. The HSIC is the cross-covariance operator described above and can be expressed in terms of kernels. $$\\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) = ||C_{xy}||_{\\mathcal{H}}^2$$ $$\\text{HSIC}(\\mathcal{F}, \\mathcal{G}, \\mathbb{P}_{xy}) = \\mathbb{E}_{xx',yy'} \\left[ K_x(x,x')K_y(y,y') \\right] $$ $$+ \\mathbb{E}_{xx'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{yy'} \\left[ K_y(y,y')\\right]$$ $$- 2\\mathbb{E}_{xy} \\left[ \\mathbb{E}_{x'} \\left[ K_x(x,x')\\right] \\mathbb{E}_{y'} \\left[ K_y(y,y')\\right] \\right]$$ where $\\mathbb{E}_{xx'yy'}$ is the expectation over both $(x,y) \\sim \\mathbb{P}_{xy}$ and we assume that $(x',y')$ can be sampled independently from $\\mathbb{P}_{xy}$. Code This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) between two kernel matrics that correctly model your data. This boils down to computing the trace of the matrix multiplication of two matrices: $tr(K_x^\\top K_y)$. So in algorithmically that is hsic_score = np . trace ( K_x . T @ K_y ) Notice that this is a 3-part operation. So, of course, we can refactor this to be much easier. A faster way to do this is: hsic_score = np . sum ( K_x * K_y ) This can be orders of magnitude faster because it is a much cheaper operation to compute elementwise products than a sum. And for fun, we can even use the `einsum` notation. hsic_score = np . einsum ( \"ji,ij->\" , K_x , K_y )","title":"Hilbert-Schmidt Criterion"},{"location":"appendix/kernels/similarity/hsic/#future-outlook","text":"Advantages Sample Space - Nice for High Dimensional Problems w/ a low number of samples HSIC can estimate dependence between variables of different dimensions Very flexible: lots of ways to create kernel matices Disadvantages Computationally demanding for large scale problems Non-iid samples, e.g. speech or images Tuning Kernel parameters Why the HS nrm?","title":"Future Outlook"},{"location":"appendix/kernels/similarity/hsic/#practical-equations","text":"HSIC \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\coloneqq \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\coloneqq \\frac{1}{n^2}\\text{tr} \\left( K_x H K_y H \\right) where H H is the centering matrix H=I_n-\\frac{1}{n}1_n1_n^\\top H=I_n-\\frac{1}{n}1_n1_n^\\top . \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) \\text{HSIC}^2(\\hat{P}_{XY}, \\mathcal{F}, \\mathcal{G}) = \\text{MMD}^2(\\hat{P}_{XY}, \\hat{P}_X\\hat{P}_Y, \\mathcal{H}_k) $$$$","title":"Practical Equations"},{"location":"appendix/kernels/similarity/hsic/#objects-of-interest","text":"","title":"Objects of Interest"},{"location":"appendix/kernels/similarity/hsic/#mean-embedding","text":"\\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k \\mu_k(\\mathbb{P}) \\coloneqq \\int_\\mathcal{X} \\underbrace{\\psi(x)}_{k(\\cdot,x)} d\\mathbb{P}(x) \\in \\mathcal{H}_k","title":"Mean Embedding"},{"location":"appendix/kernels/similarity/hsic/#maximum-mean-discrepency-mmd","text":"\\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k} \\text{MMD}_k(\\mathbb{P}, \\mathbb{Q}) \\coloneqq || \\mu_k(\\mathbb{P}) - \\mu_k(\\mathbb{Q}) \\in ||_{\\mathcal{H}_k}","title":"Maximum Mean Discrepency (MMD)"},{"location":"appendix/kernels/similarity/hsic/#hilbert-schmidt-independence-criterion-hsic","text":"\\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m) \\text{HSIC}_k(\\mathbb{P}) \\coloneqq \\text{MMD}_k(\\mathbb{P}, \\otimes_{m=1}^M \\mathbb{P}_m)","title":"Hilbert-Schmidt Independence Criterion (HSIC)"},{"location":"appendix/kernels/similarity/hsic/#linear-algebra","text":"Norm induced by the inner product: ||f||_{\\mathcal{H}} \\coloneqq \\sqrt{\\langle f,f \\rangle_{\\mathcal{H}}} ||f||_{\\mathcal{H}} \\coloneqq \\sqrt{\\langle f,f \\rangle_{\\mathcal{H}}}","title":"Linear Algebra"},{"location":"appendix/kernels/similarity/hsic/#classical-information-theory","text":"Kullback-Leibler Divergence D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx D_{KL}(\\mathbb{P}, \\mathbb{Q}) = \\int_{\\mathbb{R}^d} p(x) \\log \\left[ \\frac{p(x)}{q(x)} \\right]dx Mutual Information I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right) I(\\mathbb{P})=D_{KL}\\left( \\mathbb{P}, \\otimes_{m=1}^{M}\\mathbb{P}_m \\right)","title":"Classical Information Theory"},{"location":"appendix/kernels/similarity/hsic/#tangent-kernel-alignment","text":"HSIC $$A(K_x, K_y) = \\left\\langle H K_x, H K_y \\right\\rangle_{F} $$ Original Kernel Tangent Alignment A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} A(K_x, K_y) = \\frac{\\left\\langle K_x, K_y \\right\\rangle_{F}}{\\sqrt{|| K_x||_{F}|| K_y ||_{F}}} The alignment can be seen as a similarity score based on the cosine of the angle. For arbitrary matrices, this score ranges between -1 and 1. But using positive semidefinite Gram matrices, the score is lower-bounded by 0. Centered Kernel Tangent Alignment $$A(H K_{x}, H K_{y}) = \\frac{\\left\\langle H K_{x}, H K_{y} \\right\\rangle_{F}}{\\sqrt{|| H K_{x}|| {F}|| H K {y} ||_{F}}} $$ They add a normalization term to deal with some of the shortcomings of the original KTA algorithm which had some benefits e.g. a way to cancel out unbalanced class effects. The improvement over the original algorithm seems minor but there is a critical difference. Without the centering, the alignment does not correlate well to the performance of the learning machine.","title":"Tangent Kernel Alignment"},{"location":"appendix/kernels/similarity/hsic/#literature-review","text":"An Overview of Kernel Alignment and its Applications - Wang et al (2012) - PDF This goes over the literature of the kernel alignment method as well as some applications it has been used it.","title":"Literature Review"},{"location":"appendix/kernels/similarity/hsic/#applications","text":"Kerneel Target Alignment Parameter: A New Modelability for Regression Tasks - Marcou et al (2016) - Paper Brain Activity Patterns - Paper Scaling - Paper","title":"Applications"},{"location":"appendix/kernels/similarity/hsic/#textbooks","text":"Kernel Methods for Digital Processing - Book","title":"Textbooks"},{"location":"appendix/kernels/similarity/hsic/#useful-formulas","text":"Kernel Alignment Empirical Alignment evaluates the similarity between the corresponding matrices. $$ \\begin{aligned} A(K_1, K_2) &= \\frac{\\langle K_1, K_2 \\rangle_{F}}{\\sqrt{\\langle K_1, K_1 \\rangle_{F}\\langle K_2, K_2 \\rangle_{F}}} \\end{aligned} $$ where: \\langle K_1, K_2 \\rangle_{F} = \\sum_{i=1}^N\\sum_{j=1}^Nk_1(x_i, x_j)k_2(x_i, x_j) \\langle K_1, K_2 \\rangle_{F} = \\sum_{i=1}^N\\sum_{j=1}^Nk_1(x_i, x_j)k_2(x_i, x_j) This can be seen as a similarity score between the cosine of the angle. It has a lower bound of 0 because we typically only use positive semi-definite Gram matrices. Centered Kernel Alignment To counter the unbalanced class distribution: $$ \\begin{aligned} k_c(x,z) = \\left( \\phi(x) - \\mathbb{E} \\left[\\phi(X)\\right] \\right)^\\top \\left( \\phi(z) - \\mathbb{E} \\left[\\phi(Z)\\right] \\right) \\end{aligned} $$ The empirical centered alignment can be written as: $$ \\begin{aligned} A_c(K_{c1}, K_{c2}) &= \\frac{\\langle K_{c1}, K_{c2} \\rangle_{F}}{\\sqrt{\\langle K_{c1}, K_{c1} \\rangle_{F}\\langle K_{c2}, K_{c2} \\rangle_{F}}} \\end{aligned} $$","title":"Useful Formulas"},{"location":"appendix/kernels/similarity/hsic/#frobenius-norm-or-hilbert-schmidt-norm-a-matrix","text":"\\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Details Let A=U\\Sigma V^\\top A=U\\Sigma V^\\top be the Singular Value Decomposition of A. Then ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 ||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2 If \\lambda_i^2 \\lambda_i^2 are the eigenvalues of AA^\\top AA^\\top and A^\\top A A^\\top A , then we can show \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned}","title":"Frobenius Norm (or Hilbert-Schmidt Norm) a matrix"},{"location":"appendix/kernels/similarity/kernels_it/","text":"Kernels and Information Measures \u00b6 This post will be based off of the paper from the following papers: Measures of Entropy from Data Using Infinitely Divisible Kernels - Giraldo et. al. (2014) Multivariate Extension of Matrix-based Renyi's \\alpha \\alpha -order Entropy Functional - Yu et. al. (2018) Short Overview \u00b6 The following IT measures are possible with the scheme mentioned above: Entropy Joint Entropy Conditional Entropy Mutual Information Kernel Matrices \u00b6 \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} Entropy \u00b6 \\alpha=1 \\alpha=1 \u00b6 In this case, we can show that for kernel matrices, the Renyi entropy formulation becomes the eigenvalue decomposition of the kernel matrix. $$ \\begin{aligned} H_1(x) &= \\log \\int_\\mathcal{X}f^1(x) \\cdot dx \\ \\end{aligned}$$ \\alpha=2 \\alpha=2 \u00b6 In this case, we will have the Kernel Density Estimation procedure. H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} Note : We have to use the convolution theorem for Gaussian functions. Source | Practically \u00b6 We can calculate this above formulation by simply multiplying the kernel matrix K_x K_x by the vector 1_N 1_N . \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N where \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} . The quantity \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N is known as the information potential , V V . Cross Information Potential RKHS \u00b6 \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) Cross Information Potential \\mathcal{V} \\mathcal{V} Joint Entropy \u00b6 This formula uses the above entropy formulation. To incorporate both r.v.'s X,Y X,Y , we construct two kernel matrices A,B A,B respectively S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) Note : * The trace is there for normalization. * The matrices A,B A,B have to be the same size (due to the Hadamard product). Multivariate \u00b6 This extends to multiple variables. Let's say we have L variables, then we can calculate the joint entropy like so: S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) Conditional Entropy \u00b6 This formula respects the traditional formula for conditional entropy; the joint entropy of r.v. X,Y X,Y minus the entropy of r.v. Y Y , ( H(X|Y) = H(X,Y) - H(Y) H(X|Y) = H(X,Y) - H(Y) ). Assume we have the kernel matrix for r.v. X X as A A and the kernel matrix for r.v. Y Y as B B . The following formula shows how this is calculated using kernel functions. S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) Mutual Information \u00b6 The classic Shannon definition is the sum of the marginal entropies mines the intersection between the r.v.'s X,Y X,Y , i.e. MI(X;Y)=H(X)+H(Y)-H(X,Y) MI(X;Y)=H(X)+H(Y)-H(X,Y) . The following formula shows the MI with kernels: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) The definition is the exactly the same and utilizes the entropy and joint entropy formulations above. Multivariate \u00b6 This can be extended to multiple variables. Let's use the same example for multi-variate solutions. Let's assume B B is univariate but A A is multivariate, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . We can write the MI as: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) Total Correlation \u00b6 This is a measure of redundancy for multivariate data. It is basically the entropy of each of the marginals minus the joint entropy of the multivariate distribution. Let's assume we have A A as a multivarate distribution, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . Thus we can write this distribution using Kernel matrices: T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right) T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right)","title":"Kernels and Information Measures"},{"location":"appendix/kernels/similarity/kernels_it/#kernels-and-information-measures","text":"This post will be based off of the paper from the following papers: Measures of Entropy from Data Using Infinitely Divisible Kernels - Giraldo et. al. (2014) Multivariate Extension of Matrix-based Renyi's \\alpha \\alpha -order Entropy Functional - Yu et. al. (2018)","title":"Kernels and Information Measures"},{"location":"appendix/kernels/similarity/kernels_it/#short-overview","text":"The following IT measures are possible with the scheme mentioned above: Entropy Joint Entropy Conditional Entropy Mutual Information","title":"Short Overview"},{"location":"appendix/kernels/similarity/kernels_it/#kernel-matrices","text":"\\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned} \\begin{aligned} \\hat{f}(x) &= \\frac{1}{N} \\sum_{i=1}^N K_\\sigma (x, x_i) \\\\ K_\\sigma(x_i, x_j) &= \\frac{1}{(2\\pi \\sigma^2)^{d/2}}\\exp\\left( - \\frac{||x-x_i||^2_2}{2\\sigma^2} \\right) \\end{aligned}","title":"Kernel Matrices"},{"location":"appendix/kernels/similarity/kernels_it/#entropy","text":"","title":"Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#alpha1alpha1","text":"In this case, we can show that for kernel matrices, the Renyi entropy formulation becomes the eigenvalue decomposition of the kernel matrix. $$ \\begin{aligned} H_1(x) &= \\log \\int_\\mathcal{X}f^1(x) \\cdot dx \\ \\end{aligned}$$","title":"\\alpha=1\\alpha=1"},{"location":"appendix/kernels/similarity/kernels_it/#alpha2alpha2","text":"In this case, we will have the Kernel Density Estimation procedure. H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = \\log \\int_\\mathcal{X}f^2(x)\\cdot dx H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma}(x_i, x_j) H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} H_2(x) = - \\log \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^N K_{\\sqrt{2}\\sigma} Note : We have to use the convolution theorem for Gaussian functions. Source |","title":"\\alpha=2\\alpha=2"},{"location":"appendix/kernels/similarity/kernels_it/#practically","text":"We can calculate this above formulation by simply multiplying the kernel matrix K_x K_x by the vector 1_N 1_N . \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\hat{H}_2(x) = - \\log \\frac{1}{N^2} \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N where \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} \\mathbf{1}_N \\in \\mathbf{R}^{1 \\times N} . The quantity \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N \\mathbf{1}_N^\\top \\mathbf{K}_x \\mathbf{1}_N is known as the information potential , V V .","title":"Practically"},{"location":"appendix/kernels/similarity/kernels_it/#cross-information-potential-rkhs","text":"\\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) \\hat{H}(X) = -log\\left(\\frac{1}{n^2} \\text{tr}(KK) \\right) + C(\\sigma) Cross Information Potential \\mathcal{V} \\mathcal{V}","title":"Cross Information Potential RKHS"},{"location":"appendix/kernels/similarity/kernels_it/#joint-entropy","text":"This formula uses the above entropy formulation. To incorporate both r.v.'s X,Y X,Y , we construct two kernel matrices A,B A,B respectively S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) S_\\alpha(A,B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) Note : * The trace is there for normalization. * The matrices A,B A,B have to be the same size (due to the Hadamard product).","title":"Joint Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#multivariate","text":"This extends to multiple variables. Let's say we have L variables, then we can calculate the joint entropy like so: S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right) S_\\alpha(A_1, A_2, \\ldots, A_L) = S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L)} \\right)","title":"Multivariate"},{"location":"appendix/kernels/similarity/kernels_it/#conditional-entropy","text":"This formula respects the traditional formula for conditional entropy; the joint entropy of r.v. X,Y X,Y minus the entropy of r.v. Y Y , ( H(X|Y) = H(X,Y) - H(Y) H(X|Y) = H(X,Y) - H(Y) ). Assume we have the kernel matrix for r.v. X X as A A and the kernel matrix for r.v. Y Y as B B . The following formula shows how this is calculated using kernel functions. S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B) S_\\alpha(A|B) = S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) - S_\\alpha(B)","title":"Conditional Entropy"},{"location":"appendix/kernels/similarity/kernels_it/#mutual-information","text":"The classic Shannon definition is the sum of the marginal entropies mines the intersection between the r.v.'s X,Y X,Y , i.e. MI(X;Y)=H(X)+H(Y)-H(X,Y) MI(X;Y)=H(X)+H(Y)-H(X,Y) . The following formula shows the MI with kernels: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A \\circ B}{\\text{tr}(A \\circ B)} \\right) The definition is the exactly the same and utilizes the entropy and joint entropy formulations above.","title":"Mutual Information"},{"location":"appendix/kernels/similarity/kernels_it/#multivariate_1","text":"This can be extended to multiple variables. Let's use the same example for multi-variate solutions. Let's assume B B is univariate but A A is multivariate, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . We can write the MI as: I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right) I_\\alpha(A;B) = S_\\alpha(A) + S_\\alpha(B) - S_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_L \\circ B)} \\right)","title":"Multivariate"},{"location":"appendix/kernels/similarity/kernels_it/#total-correlation","text":"This is a measure of redundancy for multivariate data. It is basically the entropy of each of the marginals minus the joint entropy of the multivariate distribution. Let's assume we have A A as a multivarate distribution, i.e. A \\in \\{A_1, A_2, \\ldots, A_L \\} A \\in \\{A_1, A_2, \\ldots, A_L \\} . Thus we can write this distribution using Kernel matrices: T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right) T_\\alpha(\\mathbf{A}) = H_\\alpha(A_1) + H_\\alpha(A_2) + \\ldots + H_\\alpha(A_d) - H_\\alpha \\left( \\frac{A_1 \\circ A_2 \\circ \\ldots \\circ A_d}{\\text{tr}(A_1 \\circ A_2 \\circ \\ldots \\circ A_d)} \\right)","title":"Total Correlation"},{"location":"appendix/kernels/similarity/lin_alg/","text":"Linear Algebra Tricks \u00b6 Linear Algebra Tricks Frobenius Norm (Hilbert-Schmidt Norm) Intiution Formulation Code Frobenius Norm (Hilbert-Schmidt Norm) \u00b6 Intiution \u00b6 The Frobenius norm is the common matrix-based norm. Formulation \u00b6 \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Proof Let $A=U\\Sigma V^\\top$ be the Singular Value Decomposition of A. Then $$||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2$$ If $\\lambda_i^2$ are the eigenvalues of $AA^\\top$ and $A^\\top A$, then we can show $$ \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} $$ Code \u00b6 Eigenvalues sigma_xy = covariance ( X , Y ) eigvals = np . linalg . eigvals ( sigma_xy ) f_norm = np . sum ( eigvals ** 2 ) Trace sigma_xy = covariance ( X , Y ) f_norm = np . trace ( X @ X . T ) ** 2 Einsum X -= np . mean ( X , axis = 1 ) Y -= np . mean ( Y , axis = 1 ) f_norm = np . einsum ( 'ij,ji->' , X @ X . T ) Refactor f_norm = np . linalg . norm ( X @ X . T ) Frobenius Norm \u00b6 ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F","title":"Linear Algebra Tricks"},{"location":"appendix/kernels/similarity/lin_alg/#linear-algebra-tricks","text":"Linear Algebra Tricks Frobenius Norm (Hilbert-Schmidt Norm) Intiution Formulation Code","title":"Linear Algebra Tricks"},{"location":"appendix/kernels/similarity/lin_alg/#frobenius-norm-hilbert-schmidt-norm","text":"","title":"Frobenius Norm (Hilbert-Schmidt Norm)"},{"location":"appendix/kernels/similarity/lin_alg/#intiution","text":"The Frobenius norm is the common matrix-based norm.","title":"Intiution"},{"location":"appendix/kernels/similarity/lin_alg/#formulation","text":"\\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} \\begin{aligned} ||A||_F &= \\sqrt{\\langle A, A \\rangle_F} \\\\ ||A|| &= \\sqrt{\\sum_{i,j}|a_{ij}|^2} \\\\ &= \\sqrt{\\text{tr}(A^\\top A)} \\\\ &= \\sqrt{\\sum_{i=1}\\lambda_i^2} \\end{aligned} Proof Let $A=U\\Sigma V^\\top$ be the Singular Value Decomposition of A. Then $$||A||_{F}^2 = ||\\Sigma||_F^2 = \\sum_{i=1}^r \\lambda_i^2$$ If $\\lambda_i^2$ are the eigenvalues of $AA^\\top$ and $A^\\top A$, then we can show $$ \\begin{aligned} ||A||_F^2 &= tr(AA^\\top) \\\\ &= tr(U\\Lambda V^\\top V\\Lambda^\\top U^\\top) \\\\ &= tr(\\Lambda \\Lambda^\\top U^\\top U) \\\\ &= tr(\\Lambda \\Lambda^\\top) \\\\ &= \\sum_{i}\\lambda_i^2 \\end{aligned} $$","title":"Formulation"},{"location":"appendix/kernels/similarity/lin_alg/#code","text":"Eigenvalues sigma_xy = covariance ( X , Y ) eigvals = np . linalg . eigvals ( sigma_xy ) f_norm = np . sum ( eigvals ** 2 ) Trace sigma_xy = covariance ( X , Y ) f_norm = np . trace ( X @ X . T ) ** 2 Einsum X -= np . mean ( X , axis = 1 ) Y -= np . mean ( Y , axis = 1 ) f_norm = np . einsum ( 'ij,ji->' , X @ X . T ) Refactor f_norm = np . linalg . norm ( X @ X . T )","title":"Code"},{"location":"appendix/kernels/similarity/lin_alg/#frobenius-norm","text":"||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F ||X + Y||^2_F = ||X||_F^2 + ||Y||_F^2 + 2 \\langle X, Y \\rangle_F","title":"Frobenius Norm"},{"location":"appendix/kernels/similarity/mmd/","text":"Maximum Mean Discrepancy (MMD) \u00b6 The Maximum Mean Discrepency (MMD) measurement is a distance measure between feature means. Idea Formulation Proof Kernel Trick Empirical Estimate Code Equivalence Euclidean Distance KL-Divergence Variation of Information HSIC Proof Resources Idea \u00b6 This is done by taking the between dataset similarity of each of the datasets individually and then taking the cross-dataset similarity. Formulation \u00b6 \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} Proof \u00b6 \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} Kernel Trick \u00b6 Let k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} : \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} Source : Stackoverflow Empirical Estimate \u00b6 $$ \\begin{aligned} \\widehat{\\text{MMD}}^2 &= \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(x_i, x_j) + \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(y_i, y_j) - \\frac{2}{n^2} \\sum_{i,j}^N k(x_i, y_j) \\end{aligned} $$ Code \u00b6 # Term 1 c1 = 1 / ( m * ( m - 1 )) A = np . sum ( Kxx - np . diag ( np . diagonal ( Kxx ))) # Term II c2 = 1 / ( n * ( n - 1 )) B = np . sum ( Kyy - np . diag ( np . diagonal ( Kyy ))) # Term III c3 = 1 / ( m * n ) C = np . sum ( Kxy ) # estimate MMD mmd_est = c1 * A + c2 * B - 2 * c3 * C Sources Douglas Sutherland HSIC BottleNeck Eugene Belilovsky Equivalence \u00b6 Euclidean Distance \u00b6 Let's assume that \\mathbf{x,y} \\mathbf{x,y} come from two distributions, so \\mathbf{x} \\sim \\mathbb{P} \\mathbf{x} \\sim \\mathbb{P} and \\mathbf{x} \\sim \\mathbb{Q} \\mathbf{x} \\sim \\mathbb{Q} . We can write the MMD as norm of the difference between the means in feature spaces. \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F Empirical Estimation This is only good for Gaussian kernels. But we can empirically estimate this as: \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) where G is the Gaussian kernel with a standard deviation of \\sigma \\sigma . Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives - Principe KL-Divergence \u00b6 This has some alternative interpretation that is similar to the Kullback-Leibler Divergence. Remember, the MMD is the distance between the joint distribution P=\\mathbb{P}_{x,y} P=\\mathbb{P}_{x,y} and the product of the marginals Q=\\mathbb{P}_x\\mathbb{P}_y Q=\\mathbb{P}_x\\mathbb{P}_y . \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| This is similar to the KLD which has a similar interpretation in terms of the Mutual information: the difference between the joint distribution P(x,y) P(x,y) and the product of the marginal distributions p_x p_y p_x p_y . I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] Variation of Information \u00b6 In informaiton theory, we have a measure of variation of information (aka the shared information distance) which a simple linear expression involving mutual information. However, it is a valid distance metric that obeys the triangle inequality. \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) where H(X) H(X) is the entropy of \\mathcal{X} \\mathcal{X} and I(X,Y) I(X,Y) is the mutual information between \\mathcal{X,Y} \\mathcal{X,Y} . Properties \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) HSIC \u00b6 Similar to the KLD interpretation, this formulation is equivalent to the Hilbert-Schmidt Independence Criterion. If we think of the MMD distance between the joint distribution & the product of the marginals then we get the HSIC measure. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} which is the exact formulation for HSIC. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} where we have some equivalences. Proof \u00b6 First we need to do some equivalences. First the norm of two feature spaces \\varphi(\\cdot, \\cdot) \\varphi(\\cdot, \\cdot) is the same as the kernel of the cross product. \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} The second is the equivalence of the kernel of the cross-product of \\mathcal{X,Y} \\mathcal{X,Y} is equal to the multiplication of the respective kernels for \\mathcal{X,Y} \\mathcal{X,Y} . So, let's say we have a kernel k k on dataset \\mathcal{X} \\mathcal{X} in the feature space \\mathcal{F} \\mathcal{F} . We also have a kernel k k on dataset \\mathcal{Y} \\mathcal{Y} with feature space \\mathcal{G} \\mathcal{G} . The kernel k k on the \\mathcal{X,Y} \\mathcal{X,Y} pairs are similar. \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} Resources \u00b6 Arthur Grettons Lectures - lec 5 | lec 2 Notes on Mean Embedding What is RKHS The Maximum Mean Discrepancy for Training Generative Adversarial Networks From Zero to RKHS RKHS in ML Similarity Loss (TF2) - code MMD Smola","title":"Maximum Mean Discrepancy (MMD)"},{"location":"appendix/kernels/similarity/mmd/#maximum-mean-discrepancy-mmd","text":"The Maximum Mean Discrepency (MMD) measurement is a distance measure between feature means. Idea Formulation Proof Kernel Trick Empirical Estimate Code Equivalence Euclidean Distance KL-Divergence Variation of Information HSIC Proof Resources","title":"Maximum Mean Discrepancy (MMD)"},{"location":"appendix/kernels/similarity/mmd/#idea","text":"This is done by taking the between dataset similarity of each of the datasets individually and then taking the cross-dataset similarity.","title":"Idea"},{"location":"appendix/kernels/similarity/mmd/#formulation","text":"\\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned} \\begin{aligned} MMD^2(P,Q) &= ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P}\\left[ k(x,x')\\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q}\\left[ k(y,y')\\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q}\\left[ k(x,y)\\right] \\end{aligned}","title":"Formulation"},{"location":"appendix/kernels/similarity/mmd/#proof","text":"\\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned} \\begin{aligned} ||\\mu_P - \\mu_Q||_\\mathcal{F}^2 &= \\langle \\mu_P - \\mu_Q, \\mu_P - \\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\langle \\mu_P, \\mu_P \\rangle_\\mathcal{F} + \\langle \\mu_Q, \\mu_Q \\rangle_\\mathcal{F} - 2 \\langle \\mu_P,\\mu_Q \\rangle_\\mathcal{F} \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ \\mu_Q(x) \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ \\mu_P(y) \\right] - 2 \\mathbb{E}_{\\mathcal{X} \\sim P, Y \\sim Q} \\left[ \\mu_P(x) \\right] ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, \\varphi(x) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, \\varphi(y) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\langle \\mu_P, k(x, \\cdot) \\rangle_\\mathcal{F} + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\langle \\mu_Q, k(y, \\cdot) \\rangle_\\mathcal{F} - 2 ... ??? \\\\ &= \\mathbb{E}_{\\mathcal{X} \\sim P} \\left[ k(x,x') \\right] + \\mathbb{E}_{\\mathcal{Y} \\sim Q} \\left[ k(y,y') \\right] - 2 \\mathbb{E}_{\\mathcal{X,Y} \\sim P,Q } \\left[ k(x,y) \\right] \\end{aligned}","title":"Proof"},{"location":"appendix/kernels/similarity/mmd/#kernel-trick","text":"Let k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} k(X,Y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\mathcal{H} : \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} \\begin{aligned} \\text{MMD}^2(P, Q) &= || \\mathbb{E}_{X \\sim P} \\varphi(X) - \\mathbb{E}_{Y \\sim P} \\varphi(Y) ||^2_\\mathcal{H} \\\\ &= \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{X' \\sim P} \\varphi(X')\\rangle_\\mathcal{H} + \\langle \\mathbb{E}_{Y \\sim Q} \\varphi(Y), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} - 2 \\langle \\mathbb{E}_{X \\sim P} \\varphi(X), \\mathbb{E}_{Y' \\sim Q} \\varphi(Y')\\rangle_\\mathcal{H} \\end{aligned} Source : Stackoverflow","title":"Kernel Trick"},{"location":"appendix/kernels/similarity/mmd/#empirical-estimate","text":"$$ \\begin{aligned} \\widehat{\\text{MMD}}^2 &= \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(x_i, x_j) + \\frac{1}{n(n-1)} \\sum_{i\\neq j}^N k(y_i, y_j) - \\frac{2}{n^2} \\sum_{i,j}^N k(x_i, y_j) \\end{aligned} $$","title":"Empirical Estimate"},{"location":"appendix/kernels/similarity/mmd/#code","text":"# Term 1 c1 = 1 / ( m * ( m - 1 )) A = np . sum ( Kxx - np . diag ( np . diagonal ( Kxx ))) # Term II c2 = 1 / ( n * ( n - 1 )) B = np . sum ( Kyy - np . diag ( np . diagonal ( Kyy ))) # Term III c3 = 1 / ( m * n ) C = np . sum ( Kxy ) # estimate MMD mmd_est = c1 * A + c2 * B - 2 * c3 * C Sources Douglas Sutherland HSIC BottleNeck Eugene Belilovsky","title":"Code"},{"location":"appendix/kernels/similarity/mmd/#equivalence","text":"","title":"Equivalence"},{"location":"appendix/kernels/similarity/mmd/#euclidean-distance","text":"Let's assume that \\mathbf{x,y} \\mathbf{x,y} come from two distributions, so \\mathbf{x} \\sim \\mathbb{P} \\mathbf{x} \\sim \\mathbb{P} and \\mathbf{x} \\sim \\mathbb{Q} \\mathbf{x} \\sim \\mathbb{Q} . We can write the MMD as norm of the difference between the means in feature spaces. \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F \\text{D}_{ED}(\\mathbb{P,Q}) = ||\\mu_\\mathbf{x} - \\mu_\\mathbf{y}||^2_F = ||\\mu_\\mathbf{x}||^2_F + ||\\mu_\\mathbf{y}||^2_F - 2 \\langle \\mu_\\mathbf{x}, \\mu_\\mathbf{y}\\rangle_F Empirical Estimation This is only good for Gaussian kernels. But we can empirically estimate this as: \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) \\text{D}_{ED}(\\mathbb{P,Q}) = \\frac{1}{N_x^2} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_x} \\text{G}(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{N_y^2} \\sum_{i=1}^{N_y}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{y}_i, \\mathbf{y}_j) - 2 \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\text{G}(\\mathbf{x}_i, \\mathbf{y}_j) where G is the Gaussian kernel with a standard deviation of \\sigma \\sigma . Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives - Principe","title":"Euclidean Distance"},{"location":"appendix/kernels/similarity/mmd/#kl-divergence","text":"This has some alternative interpretation that is similar to the Kullback-Leibler Divergence. Remember, the MMD is the distance between the joint distribution P=\\mathbb{P}_{x,y} P=\\mathbb{P}_{x,y} and the product of the marginals Q=\\mathbb{P}_x\\mathbb{P}_y Q=\\mathbb{P}_x\\mathbb{P}_y . \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| \\text{MMD}(P_{XY},P_X P_Y, \\mathcal{H}_k) = || \\mu_{PQ} - \\mu_{P}\\mu_{Q}|| This is similar to the KLD which has a similar interpretation in terms of the Mutual information: the difference between the joint distribution P(x,y) P(x,y) and the product of the marginal distributions p_x p_y p_x p_y . I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right] I(X,Y) = D_{KL} \\left[ P(x,y) || p_x p_y \\right]","title":"KL-Divergence"},{"location":"appendix/kernels/similarity/mmd/#variation-of-information","text":"In informaiton theory, we have a measure of variation of information (aka the shared information distance) which a simple linear expression involving mutual information. However, it is a valid distance metric that obeys the triangle inequality. \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) \\text{VI}(X,Y) = H(X) + H(Y) - 2 I (X,Y) where H(X) H(X) is the entropy of \\mathcal{X} \\mathcal{X} and I(X,Y) I(X,Y) is the mutual information between \\mathcal{X,Y} \\mathcal{X,Y} . Properties \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) \\geq 0 \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = 0 \\implies X=Y \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Y) = d(Y,X) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z) \\text{VI}(X,Z) \\leq d(X,Y) + d(Y,Z)","title":"Variation of Information"},{"location":"appendix/kernels/similarity/mmd/#hsic","text":"Similar to the KLD interpretation, this formulation is equivalent to the Hilbert-Schmidt Independence Criterion. If we think of the MMD distance between the joint distribution & the product of the marginals then we get the HSIC measure. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= ||\\mu_{\\mathbb{P}_{XY}} - \\mu_{P_XP_Y}|| \\end{aligned} which is the exact formulation for HSIC. \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} \\begin{aligned} \\text{MMD}^2(P_{XY}, P_XP_Y; \\mathcal{H}_k) &= \\text{HSIC}^2(P_{XY}; \\mathcal{F}, \\mathcal{G}) \\end{aligned} where we have some equivalences.","title":"HSIC"},{"location":"appendix/kernels/similarity/mmd/#proof_1","text":"First we need to do some equivalences. First the norm of two feature spaces \\varphi(\\cdot, \\cdot) \\varphi(\\cdot, \\cdot) is the same as the kernel of the cross product. \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} \\begin{aligned} \\langle \\varphi(x,y), \\varphi(x,y) \\rangle_\\mathcal{F} &= k \\left((x,y),(x',y')\\right) \\end{aligned} The second is the equivalence of the kernel of the cross-product of \\mathcal{X,Y} \\mathcal{X,Y} is equal to the multiplication of the respective kernels for \\mathcal{X,Y} \\mathcal{X,Y} . So, let's say we have a kernel k k on dataset \\mathcal{X} \\mathcal{X} in the feature space \\mathcal{F} \\mathcal{F} . We also have a kernel k k on dataset \\mathcal{Y} \\mathcal{Y} with feature space \\mathcal{G} \\mathcal{G} . The kernel k k on the \\mathcal{X,Y} \\mathcal{X,Y} pairs are similar. \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned} \\begin{aligned} k\\left((x,y),(x',y')\\right) &= k(x,x')\\,k(y,y') \\\\ \\end{aligned}","title":"Proof"},{"location":"appendix/kernels/similarity/mmd/#resources","text":"Arthur Grettons Lectures - lec 5 | lec 2 Notes on Mean Embedding What is RKHS The Maximum Mean Discrepancy for Training Generative Adversarial Networks From Zero to RKHS RKHS in ML Similarity Loss (TF2) - code MMD Smola","title":"Resources"},{"location":"appendix/kernels/similarity/rv/","text":"RV Coefficient \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Lab: Colab Notebook Notation Single Variables Mean, Expectation Empirical Estimate Variance Empirical Estimate Covariance Empirical Estimate Correlation Empirical Estimate Root Mean Squared Error Multi-Dimensional Variance Self-Covariance Empirical Estimation Cross-Covariance Empirical Estimation Root Mean Squared Vector Difference Summarizing Multi-Dimensional Information Sample Space Equivalence Supplementary Notation \u00b6 \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma \\in \\mathbb{R}^{N \\times N} \\Sigma \\in \\mathbb{R}^{N \\times N} is a covariance matrix. \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} \\Sigma_\\mathbf{y} \\Sigma_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} is the population covariance matrix between \\mathcal{X,Y} \\mathcal{X,Y} tr(\\cdot) tr(\\cdot) - the trace operator ||\\cdot||_\\mathcal{F} ||\\cdot||_\\mathcal{F} - Frobenius Norm ||\\cdot||_\\mathcal{HS} ||\\cdot||_\\mathcal{HS} - Hilbert-Schmidt Norm \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix. Single Variables \u00b6 Let's consider a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. Mean, Expectation \u00b6 The first order measurement is the mean. This is the expected/average value that we would expect from a r.v.. This results in a scalar value Empirical Estimate \u00b6 \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i Variance \u00b6 The first measure we need to consider is the variance. This is a measure of spread. Empirical Estimate \u00b6 \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} Code We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. # remove mean from data X_mu = X . mean ( axis = 0 ) # ensure it is 1D var = ( X - X_mu [:, None ]) . T @ ( X - X_mu [:, None ]) Covariance \u00b6 The first measure we need to consider is the covariance. This can be used for a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . This results in a scalar value , \\mathbb{R} \\mathbb{R} . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\end{aligned} $$ This will result in a scalar value \\mathbb{R}^+ \\mathbb{R}^+ that ranges from (-\\infty, \\infty) (-\\infty, \\infty) . This number is affected by scale so we can different values depending upon the scale of our data, i.e. \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Empirical Estimate \u00b6 We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) Code c_xy = X . T @ Y Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} This results in a scalar value \\mathbb{R} \\mathbb{R} that lies in between [-1, 1] [-1, 1] . When \\rho=-1 \\rho=-1 , there is a negative correlation and when \\rho=1 \\rho=1 , there is a positive correlation. When \\rho=0 \\rho=0 there is no correlation. Empirical Estimate \u00b6 So the formulation is: \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} Root Mean Squared Error \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} Multi-Dimensional \u00b6 For all of these measures, we have been under the assumption that \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} . However, we may have the case where we have multivariate datasets in \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} . In this case, we need methods that can handle multivariate inputs. Variance \u00b6 Self-Covariance \u00b6 So now we are considering the case when we have multidimensional vectors. If we think of a variable X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} which represents a set of samples with multiple features. First let's consider the variance for a multidimensional variable. This is also known as the covariance because we are actually finding the cross-covariance between itself. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} $$ To simplify the notation, we can write this as: \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data. Empirical Estimation \u00b6 This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} Code c_xy = X . T @ X Cross-Covariance \u00b6 We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ This results in a scalar value which represents the similarity between the samples. There are some key observations of this measure. Empirical Estimation \u00b6 This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Root Mean Squared Vector Difference \u00b6 A diagram for evaluating multiple aspects of model performance insimulating vector fields - Xu et. al. (2016) Summarizing Multi-Dimensional Information \u00b6 Recall that we now have self-covariance matrices \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} and cross-covariance matrices \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} which are \\mathbb{R}^{D \\times D} \\mathbb{R}^{D \\times D} . This is very useful as it captures the structure of the overall data. However, if we want to summarize the statistics, then we need some methods to do so. The matrix norm, in particular the Frobenius Norm (aka the Hilbert-Schmidt Norm) to effectively summarize content within this covariance matrix. It's defined as: ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) Essentially this is a measure of the covariance matrix power or \"essence\" through its eigenvalue decomposition. Note that this term is zero iff \\mathbf{x,y} \\mathbf{x,y} are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value in \\mathbb{R} \\mathbb{R} that summarizes the structure of our data. In the context of matrices, we can normalize this value by the norm of the self-covariance matrices like so: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_F}{||\\Sigma_\\mathbf{xx}||_F \\; || \\Sigma_\\mathbf{yy}||_F} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_F}{||\\Sigma_\\mathbf{xx}||_F \\; || \\Sigma_\\mathbf{yy}||_F} This results in the \\rho \\rho V-Coefficient which is analogous to the Pearson correlation coefficient \\rho \\rho . !> Note We assume that the data is column centered (aka we have removed the mean from the features). Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured Let's add N N independent realizations to the samples. This gives us a vector for each of the observations. So, let \\mathbf{X} \\in \\mathbb{R}^{N \\times D_x} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_x} and \\mathbf{Y} \\in \\mathbb{R}^{N \\times D_y} \\mathbf{Y} \\in \\mathbb{R}^{N \\times D_y} . We assume that they are column-centered (aka remove the mean from the features). So, we can write the S_{\\mathbf{XY}}= \\frac{1}{n-1}\\mathbf{X^\\top Y} S_{\\mathbf{XY}}= \\frac{1}{n-1}\\mathbf{X^\\top Y} \\begin{aligned} \\text{RV}(\\mathbf{X,Y}) &= \\frac{tr\\left( S_{\\mathbf{XY}}S_{\\mathbf{XY}} \\right)}{\\sqrt{tr\\left( S_{\\mathbf{XX}}^2 \\right) tr\\left( S_{\\mathbf{YY}}^2 \\right)}} \\end{aligned} \\begin{aligned} \\text{RV}(\\mathbf{X,Y}) &= \\frac{tr\\left( S_{\\mathbf{XY}}S_{\\mathbf{XY}} \\right)}{\\sqrt{tr\\left( S_{\\mathbf{XX}}^2 \\right) tr\\left( S_{\\mathbf{YY}}^2 \\right)}} \\end{aligned} Sample Space \u00b6 We can also consider the case where the correlations can be measured between samples and not between features. So we can create cross product matrices: \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} and \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} . Just like the feature space, we can use the Hilbert-Schmidt (HS) norm, ||\\cdot||_{F} ||\\cdot||_{F} to measure proximity. \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} And like the above mentioned \\rho V \\rho V , we can also calculate a correlation measure using the sample space. \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ \\end{aligned} \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ \\end{aligned} Code **Sample Space** Recall that the $\\rho V$ can be written in terms of trace operations. $$\\rho V(\\mathbf{x,y}) = \\frac{tr\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{tr}\\left( \\mathbf{yy}^\\top \\right)^2}}$$ This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: $tr(C_{xy}^\\top C_{xy})$. So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the `sum` operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () **Refactor** There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro) Equivalence \u00b6 It turns out, for the linear case, when using the Frobenius norm to summarize the pairwise comparisons, comparing features is the same as comparing samples. For example, the norm of the covariance operator for the features and samples are equivalent: ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F We get the same for the \\rho V \\rho V case. \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} So what does this mean? Well, either method is fine. But you should probably choose one depending upon the computational resources available. For example, if you have more samples than features, then choose the feature space representation. On the other hand, if you have more features than samples, then choose the sample space representation. !> Linear Only This method only works for the linear case. There are some nonlinear transformations (called kernels) that one can use, but those will yield different values between feature space and sample space. Supplementary \u00b6 Common Statistical Tests are Linear Models (or: How to Teach Stats) - Jonas Kristoffer Lindelov - notebook | rmarkdown Correlation vs Regression - Asim Jana - blog RealPython Numpy, SciPy and Pandas: Correlation with Python - blog Correlation and Lag for Signals - notebook Understanding the Covariance Matrix Numpy Vectorized method for computing covariance with population means Eric Marsden Modeling Correlations in Python Regression Analysis in Python","title":"RV Coefficient"},{"location":"appendix/kernels/similarity/rv/#rv-coefficient","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Lab: Colab Notebook Notation Single Variables Mean, Expectation Empirical Estimate Variance Empirical Estimate Covariance Empirical Estimate Correlation Empirical Estimate Root Mean Squared Error Multi-Dimensional Variance Self-Covariance Empirical Estimation Cross-Covariance Empirical Estimation Root Mean Squared Vector Difference Summarizing Multi-Dimensional Information Sample Space Equivalence Supplementary","title":"RV Coefficient"},{"location":"appendix/kernels/similarity/rv/#notation","text":"\\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{x}} are samples from a multidimentionsal r.v. \\mathcal{X} \\mathcal{X} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_\\mathbf{y}} are samples from a multidimensional r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma \\in \\mathbb{R}^{N \\times N} \\Sigma \\in \\mathbb{R}^{N \\times N} is a covariance matrix. \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} is a kernel matrix for the r.v. \\mathcal{X} \\mathcal{X} \\Sigma_\\mathbf{y} \\Sigma_\\mathbf{y} is a kernel matrix for the r.v. \\mathcal{Y} \\mathcal{Y} \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} is the population covariance matrix between \\mathcal{X,Y} \\mathcal{X,Y} tr(\\cdot) tr(\\cdot) - the trace operator ||\\cdot||_\\mathcal{F} ||\\cdot||_\\mathcal{F} - Frobenius Norm ||\\cdot||_\\mathcal{HS} ||\\cdot||_\\mathcal{HS} - Hilbert-Schmidt Norm \\tilde{K} \\in \\mathbb{R}^{N \\times N} \\tilde{K} \\in \\mathbb{R}^{N \\times N} is the centered kernel matrix.","title":"Notation"},{"location":"appendix/kernels/similarity/rv/#single-variables","text":"Let's consider a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature.","title":"Single Variables"},{"location":"appendix/kernels/similarity/rv/#mean-expectation","text":"The first order measurement is the mean. This is the expected/average value that we would expect from a r.v.. This results in a scalar value","title":"Mean, Expectation"},{"location":"appendix/kernels/similarity/rv/#empirical-estimate","text":"\\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i \\mu(x)=\\frac{1}{N}\\sum_{i=1}x_i","title":"Empirical Estimate"},{"location":"appendix/kernels/similarity/rv/#variance","text":"The first measure we need to consider is the variance. This is a measure of spread.","title":"Variance"},{"location":"appendix/kernels/similarity/rv/#empirical-estimate_1","text":"\\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} \\begin{aligned} \\sigma_x^2 &= \\frac{1}{n-1} \\sum_{i=1}^N(x_i-x_\\mu)^2 \\end{aligned} Code We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. # remove mean from data X_mu = X . mean ( axis = 0 ) # ensure it is 1D var = ( X - X_mu [:, None ]) . T @ ( X - X_mu [:, None ])","title":"Empirical Estimate"},{"location":"appendix/kernels/similarity/rv/#covariance","text":"The first measure we need to consider is the covariance. This can be used for a single variable X \\in \\mathbb{R}^{N \\times 1} X \\in \\mathbb{R}^{N \\times 1} which represents a set of samples of a single feature. We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . This results in a scalar value , \\mathbb{R} \\mathbb{R} . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left((\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right) \\\\ &= \\mathbb{E}\\left[\\mathbf{xy} - \\mu_\\mathbf{x} Y - \\mathbf{x}\\mu_\\mathbf{y} + \\mu_\\mathbf{x}\\mu_y \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x} \\mathbb{E}[\\mathbf{x}] - \\mu_y\\mathbb{E}[\\mathbf{y}] + \\mu_\\mathbf{x}\\mu_y \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_y \\end{aligned} $$ This will result in a scalar value \\mathbb{R}^+ \\mathbb{R}^+ that ranges from (-\\infty, \\infty) (-\\infty, \\infty) . This number is affected by scale so we can different values depending upon the scale of our data, i.e. \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) \\text{cov}(\\mathbf{x,y}) \\neq \\text{cov}(\\alpha \\mathbf{x}, \\beta \\mathbf{x}) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Covariance"},{"location":"appendix/kernels/similarity/rv/#empirical-estimate_2","text":"We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) \\text{cov}(\\mathbf{x,y}) = \\frac{1}{n-1} \\sum_{i=1}^N (x_i - x_\\mu)(y_i - y_\\mu) Code c_xy = X . T @ Y","title":"Empirical Estimate"},{"location":"appendix/kernels/similarity/rv/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y})=\\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} This results in a scalar value \\mathbb{R} \\mathbb{R} that lies in between [-1, 1] [-1, 1] . When \\rho=-1 \\rho=-1 , there is a negative correlation and when \\rho=1 \\rho=1 , there is a positive correlation. When \\rho=0 \\rho=0 there is no correlation.","title":"Correlation"},{"location":"appendix/kernels/similarity/rv/#empirical-estimate_3","text":"So the formulation is: \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} \\rho(\\mathbf{x,y}) = \\frac{\\text{cov}(\\mathbf{x,y}) }{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+}","title":"Empirical Estimate"},{"location":"appendix/kernels/similarity/rv/#root-mean-squared-error","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2} RMSE(X,Y)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left((x_i - \\mu_x)-(y_i - \\mu_i)\\right)^2}","title":"Root Mean Squared Error"},{"location":"appendix/kernels/similarity/rv/#multi-dimensional","text":"For all of these measures, we have been under the assumption that \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} \\mathbf{x,y} \\in \\mathbb{R}^{N \\times 1} . However, we may have the case where we have multivariate datasets in \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} . In this case, we need methods that can handle multivariate inputs.","title":"Multi-Dimensional"},{"location":"appendix/kernels/similarity/rv/#variance_1","text":"","title":"Variance"},{"location":"appendix/kernels/similarity/rv/#self-covariance","text":"So now we are considering the case when we have multidimensional vectors. If we think of a variable X \\in \\mathbb{R}^{N \\times D} X \\in \\mathbb{R}^{N \\times D} which represents a set of samples with multiple features. First let's consider the variance for a multidimensional variable. This is also known as the covariance because we are actually finding the cross-covariance between itself. \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left[(X-\\mu_x)^2 \\right] \\\\ \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} \\text{Var}(X) &= \\mathbb{E}\\left((X-\\mu_x)(X-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XX - \\mu_XX - X\\mu_X + \\mu_X\\mu_X \\right) \\\\ &= \\mathbb{E}(XX) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_X + \\mu_x\\mu_X \\\\ &= \\mathbb{E}(X^2) - \\mu_X^2 \\end{aligned} $$ To simplify the notation, we can write this as: \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) \\Sigma_\\mathbf{x} = \\text{cov}(\\mathbf{x,x}) A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data.","title":"Self-Covariance"},{"location":"appendix/kernels/similarity/rv/#empirical-estimation","text":"This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} \\Sigma_\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} Code c_xy = X . T @ X","title":"Empirical Estimation"},{"location":"appendix/kernels/similarity/rv/#cross-covariance","text":"We can compare the r.v. X X with another r.v. Y \\in \\mathbb{R}^{N \\times 1} Y \\in \\mathbb{R}^{N \\times 1} . the covariance, or the cross-covariance between multiple variables X,Y X,Y . We can write this as: \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} \\begin{aligned} \\text{cov}(\\mathbf{x,y}) &= \\mathbb{E}\\left[(\\mathbf{x}-\\mu_\\mathbf{x})(\\mathbf{y}-\\mu_\\mathbf{y}) \\right] \\\\ &= \\mathbb{E}[\\mathbf{xy}] - \\mu_\\mathbf{x}\\mu_\\mathbf{y} \\end{aligned} Proof We can expand the terms in the parenthesis like normally. Then we take the expectation of each of the terms individually. $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ This results in a scalar value which represents the similarity between the samples. There are some key observations of this measure.","title":"Cross-Covariance"},{"location":"appendix/kernels/similarity/rv/#empirical-estimation_1","text":"This shows the joint variation of all pairs of random variables. \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} \\Sigma_\\mathbf{xy} = \\mathbf{x}^\\top \\mathbf{y} Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features!","title":"Empirical Estimation"},{"location":"appendix/kernels/similarity/rv/#root-mean-squared-vector-difference","text":"A diagram for evaluating multiple aspects of model performance insimulating vector fields - Xu et. al. (2016)","title":"Root Mean Squared Vector Difference"},{"location":"appendix/kernels/similarity/rv/#summarizing-multi-dimensional-information","text":"Recall that we now have self-covariance matrices \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} and cross-covariance matrices \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} which are \\mathbb{R}^{D \\times D} \\mathbb{R}^{D \\times D} . This is very useful as it captures the structure of the overall data. However, if we want to summarize the statistics, then we need some methods to do so. The matrix norm, in particular the Frobenius Norm (aka the Hilbert-Schmidt Norm) to effectively summarize content within this covariance matrix. It's defined as: ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) ||\\Sigma_\\mathbf{xy}||_{\\mathcal{F}}^2 = \\sum_i \\lambda_i^2 = \\text{tr}\\left( \\Sigma_\\mathbf{xy}^\\top \\Sigma_\\mathbf{xy} \\right) Essentially this is a measure of the covariance matrix power or \"essence\" through its eigenvalue decomposition. Note that this term is zero iff \\mathbf{x,y} \\mathbf{x,y} are independent and greater than zero otherwise. Since the covariance matrix is a second-order measure of the relations, we can only summarize the the second order relation information. But at the very least, we now have a scalar value in \\mathbb{R} \\mathbb{R} that summarizes the structure of our data. In the context of matrices, we can normalize this value by the norm of the self-covariance matrices like so: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_F}{||\\Sigma_\\mathbf{xx}||_F \\; || \\Sigma_\\mathbf{yy}||_F} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_F}{||\\Sigma_\\mathbf{xx}||_F \\; || \\Sigma_\\mathbf{yy}||_F} This results in the \\rho \\rho V-Coefficient which is analogous to the Pearson correlation coefficient \\rho \\rho . !> Note We assume that the data is column centered (aka we have removed the mean from the features). Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured Let's add N N independent realizations to the samples. This gives us a vector for each of the observations. So, let \\mathbf{X} \\in \\mathbb{R}^{N \\times D_x} \\mathbf{X} \\in \\mathbb{R}^{N \\times D_x} and \\mathbf{Y} \\in \\mathbb{R}^{N \\times D_y} \\mathbf{Y} \\in \\mathbb{R}^{N \\times D_y} . We assume that they are column-centered (aka remove the mean from the features). So, we can write the S_{\\mathbf{XY}}= \\frac{1}{n-1}\\mathbf{X^\\top Y} S_{\\mathbf{XY}}= \\frac{1}{n-1}\\mathbf{X^\\top Y} \\begin{aligned} \\text{RV}(\\mathbf{X,Y}) &= \\frac{tr\\left( S_{\\mathbf{XY}}S_{\\mathbf{XY}} \\right)}{\\sqrt{tr\\left( S_{\\mathbf{XX}}^2 \\right) tr\\left( S_{\\mathbf{YY}}^2 \\right)}} \\end{aligned} \\begin{aligned} \\text{RV}(\\mathbf{X,Y}) &= \\frac{tr\\left( S_{\\mathbf{XY}}S_{\\mathbf{XY}} \\right)}{\\sqrt{tr\\left( S_{\\mathbf{XX}}^2 \\right) tr\\left( S_{\\mathbf{YY}}^2 \\right)}} \\end{aligned}","title":"Summarizing Multi-Dimensional Information"},{"location":"appendix/kernels/similarity/rv/#sample-space","text":"We can also consider the case where the correlations can be measured between samples and not between features. So we can create cross product matrices: \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{X}=\\mathbf{XX}^\\top \\in \\mathbb{R}^{N \\times N} and \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} \\mathbf{W}_\\mathbf{Y}=\\mathbf{YY}^\\top \\in \\mathbb{R}^{N \\times N} . Just like the feature space, we can use the Hilbert-Schmidt (HS) norm, ||\\cdot||_{F} ||\\cdot||_{F} to measure proximity. \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} \\begin{aligned} \\langle {W}_\\mathbf{x}, {W}_\\mathbf{y} \\rangle &= tr \\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right) \\\\ &= \\sum_{i=1}^{D_x} \\sum_{j=1}^{D_y} cov^2(\\mathbf{x}_{d_i}, \\mathbf{y}_{d_j}) \\end{aligned} And like the above mentioned \\rho V \\rho V , we can also calculate a correlation measure using the sample space. \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ \\end{aligned} \\begin{aligned} \\rho V(\\mathbf{x,y}) &= \\frac{\\langle \\mathbf{W_x, W_y}\\rangle_F}{||\\mathbf{W_x}||_F \\; ||\\mathbf{W_y}||_F} \\\\ \\end{aligned} Code **Sample Space** Recall that the $\\rho V$ can be written in terms of trace operations. $$\\rho V(\\mathbf{x,y}) = \\frac{tr\\left( \\mathbf{xx}^\\top \\mathbf{yy}^\\top \\right)}{\\sqrt{\\text{tr}\\left( \\mathbf{xx}^\\top \\right)^2 \\text{tr}\\left( \\mathbf{yy}^\\top \\right)^2}}$$ This is very easy to compute in practice. One just needs to calculate the Frobenius Norm (Hilbert-Schmidt Norm) of a covariance matrix This boils down to computing the trace of the matrix multiplication of two matrices: $tr(C_{xy}^\\top C_{xy})$. So in algorithmically that is: hsic_score = np . sqrt ( np . trace ( C_xy . T * C_xy )) We can make this faster by using the `sum` operation # Numpy hsic_score = np . sqrt ( np . sum ( C_xy * C_xy )) # PyTorch hsic_score = ( C_xy * C_xy ) . sum () . sum () **Refactor** There is a built-in function to be able to to speed up this calculation by a magnitude. hs_score = np . linalg . norm ( C_xy , ord = 'fro' ) and in PyTorch hs_score = torch . norm ( C_xy , p = 'fro)","title":"Sample Space"},{"location":"appendix/kernels/similarity/rv/#equivalence","text":"It turns out, for the linear case, when using the Frobenius norm to summarize the pairwise comparisons, comparing features is the same as comparing samples. For example, the norm of the covariance operator for the features and samples are equivalent: ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F ||\\Sigma_{\\mathbf{xy}}||_F^2 = \\langle \\mathbf{W_x,W_y} \\rangle_F We get the same for the \\rho V \\rho V case. \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} \\frac{ ||\\Sigma_{\\mathbf{xy}}||_F^2}{||\\Sigma_\\mathbf{x}||_F ||\\Sigma_\\mathbf{y}||_F} = \\frac{ \\langle \\mathbf{W_x,W_y} \\rangle}{||\\mathbf{W_x}||_F ||\\mathbf{W_y}||_F} So what does this mean? Well, either method is fine. But you should probably choose one depending upon the computational resources available. For example, if you have more samples than features, then choose the feature space representation. On the other hand, if you have more features than samples, then choose the sample space representation. !> Linear Only This method only works for the linear case. There are some nonlinear transformations (called kernels) that one can use, but those will yield different values between feature space and sample space.","title":"Equivalence"},{"location":"appendix/kernels/similarity/rv/#supplementary","text":"Common Statistical Tests are Linear Models (or: How to Teach Stats) - Jonas Kristoffer Lindelov - notebook | rmarkdown Correlation vs Regression - Asim Jana - blog RealPython Numpy, SciPy and Pandas: Correlation with Python - blog Correlation and Lag for Signals - notebook Understanding the Covariance Matrix Numpy Vectorized method for computing covariance with population means Eric Marsden Modeling Correlations in Python Regression Analysis in Python","title":"Supplementary"},{"location":"appendix/kernels/similarity/similarity/","text":"Similarity Measures \u00b6 Covariance \u00b6 C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) Alternative Formulations $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ Measures Dependence Unbounded, (-\\infty,\\infty) (-\\infty,\\infty) Isotropic scaling Units depend on inputs Empirical Covariance \u00b6 This shows the joint variation of all pairs of random variables. C_{xy} = X^\\top X C_{xy} = X^\\top X Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features! Empirical Cross-Covariance \u00b6 This is the covariance between different datasets C_{xy} = X^\\top Y C_{xy} = X^\\top Y Code c_xy = X . T @ Y Linear Kernel \u00b6 This measures the covariance between samples. K_{xx} = X X^\\top K_{xx} = X X^\\top Code K_xy = X @ X . T A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data. Summarizing \u00b6 The only thing in the literature where I've seen this Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured Correlation \u00b6","title":"Similarity Measures"},{"location":"appendix/kernels/similarity/similarity/#similarity-measures","text":"","title":"Similarity Measures"},{"location":"appendix/kernels/similarity/similarity/#covariance","text":"C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) C(X,Y)=\\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) Alternative Formulations $$ \\begin{aligned} C(X,Y) &= \\mathbb{E}\\left((X-\\mu_x)(Y-\\mu_y) \\right) \\\\ &= \\mathbb{E}\\left(XY - \\mu_xY - X\\mu_y + \\mu_x\\mu_y \\right) \\\\ &= \\mathbb{E}(XY) - \\mu_x \\mathbb{E}(X) - \\mathbb{E}(X)\\mu_y + \\mu_x\\mu_y \\\\ &= \\mathbb{E}(XY) - \\mu_x\\mu_y \\end{aligned} $$ Measures Dependence Unbounded, (-\\infty,\\infty) (-\\infty,\\infty) Isotropic scaling Units depend on inputs","title":"Covariance"},{"location":"appendix/kernels/similarity/similarity/#empirical-covariance","text":"This shows the joint variation of all pairs of random variables. C_{xy} = X^\\top X C_{xy} = X^\\top X Code c_xy = X . T @ X Observations * A completely diagonal covariance matrix means that all features are uncorrelated (orthogonal to each other). * Diagonal covariances are useful for learning, they mean non-redundant features!","title":"Empirical Covariance"},{"location":"appendix/kernels/similarity/similarity/#empirical-cross-covariance","text":"This is the covariance between different datasets C_{xy} = X^\\top Y C_{xy} = X^\\top Y Code c_xy = X . T @ Y","title":"Empirical Cross-Covariance"},{"location":"appendix/kernels/similarity/similarity/#linear-kernel","text":"This measures the covariance between samples. K_{xx} = X X^\\top K_{xx} = X X^\\top Code K_xy = X @ X . T A completely diagonal linear kernel (Gram) matrix means that all examples are uncorrelated (orthogonal to each other). Diagonal kernels are useless for learning: no structure found in the data.","title":"Linear Kernel"},{"location":"appendix/kernels/similarity/similarity/#summarizing","text":"The only thing in the literature where I've seen this Observations * HSIC norm of the covariance only detects second order relationships. More complex (higher-order, nonlinear) relations cannot be captured","title":"Summarizing"},{"location":"appendix/kernels/similarity/similarity/#correlation","text":"","title":"Correlation"},{"location":"appendix/kernels/similarity/visualization/distances/","text":"Distances \u00b6","title":"Distances"},{"location":"appendix/kernels/similarity/visualization/distances/#distances","text":"","title":"Distances"},{"location":"appendix/kernels/similarity/visualization/taylor/","text":"Visualizing Similarities \u00b6 Motivation Questions Current Ways Cosine Similarity Correlation Distances Law of cosines Taylor Diagram Statistics Metric Space Example 1 - Intuition Example 2 - Model Outputs Multi-Dimensional Data Distances Correlation Sample Space Non-Linear Functions References Motivation \u00b6 Visualizations: * help find similarities between outputs * stats are great, but visual uncertainty quantification Questions \u00b6 Which model is more similar to the reference/observations? Should we look at correlations across seasons or latitudes? Are there large discrepancies in the different outputs Current Ways \u00b6 Trend Plots often do not expose the comparison aspects... Scatter plots become impractical for many outputs Parallel Coordinate Plots are more practical, but only certain pairwise comparisons are possible Plots per ensemble - possible but it can be super cluttered Taylor Diagram - visualize several statistics simultaneously in a statistical metric space. Specific Statistics Mean, Variance, Correlation Box Plots (and variations) Cosine Similarity \u00b6 Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim Correlation \u00b6 There is a relationship between the cosine similarity and correlation coefficient \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} if \\rho(x,y) = 0 \\rho(x,y) = 0 , the spaces are orthogonal if \\rho(x,y) = 1 \\rho(x,y) = 1 , the spaces are equivalent Distances \u00b6 Figure II : The triangle showing the cosine similarity and it's relationship to the euclidean distance. d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle Law of cosines \u00b6 If we recall the law of cosines; an extension of the cosine angle formula but for all angles and sides of the triangle. c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta Notice that this forumala looks very similar to the euclidean distance formula shown above. If we do a simple rearrangement of the final term in the above equation to accommadate the correlation term \\rho(x,y) \\rho(x,y) , then we get d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} simplifying to: d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) This is actually equivalent to the law of cosines. Taylor Diagram \u00b6 The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. It simultaneously plots the standard deviation, the root mean square error and correlation between two variables. Statistics Metric Space \u00b6 They key is that it is possible to find a metric space for these quantities, based on the law of cosines. If you look closely, this identity looks like the cosine law of triangles.we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE as we have expressed above. \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) If we write out the full equation, we have the following: \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) The sides are as follows: a = \\sigma_{x} a = \\sigma_{x} - the standard deviation of x x b = \\sigma_{y} b = \\sigma_{y} - the standard deviation of y y \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho . Example 1 - Intuition \u00b6 Figure III : An example Taylor diagram. Example 2 - Model Outputs \u00b6 Multi-Dimensional Data \u00b6 In the above examples, we assume that \\mathbf{x}, \\mathbf{y} \\mathbf{x}, \\mathbf{y} were both vectors of size \\mathbb{R}^{N \\times 1} \\mathbb{R}^{N \\times 1} . But what happens when we get datasets of size \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} ? Well, the above formulas can generalize using the inner product and the norm of the datasets. Figure I : A visual representation of the cosine similarity generalized to vectors. Distances \u00b6 We still get the same formulation as the above except now it is generalized to vectors. d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle Correlation \u00b6 Let \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} be the empirical covariance matrix between \\mathbf{x,y} \\mathbf{x,y} . \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} See the multidimensional section of this page for more details on the \\rho V \\rho V coefficient. So the same rules apply as done above, we can rewrite the law of cosines to encompass the multidimensional data inputs. d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) Sample Space \u00b6 Sometimes it's convenient to write the \\rho V (\\mathbf{x,y}) \\rho V (\\mathbf{x,y}) coefficient for the sample space for the data. So instead of calculating a cross-covariance matrix \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} , we calculate a self-similarity matrix for each of the datasets, e.g. \\mathbf{XX}^\\top = \\mathbf{W_x} \\mathbf{XX}^\\top = \\mathbf{W_x} and \\mathbf{YY}^\\top = \\mathbf{W_y} \\mathbf{YY}^\\top = \\mathbf{W_y} . This is a different and pairwise representation of the data. To measure the proximity between the two matrices, we can use the Frobenius norm (aka the Hilbert-Schmidt norm). This gives us: \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) Again, the two matrices will have differnt norms so we can renormalize them appropriately to obtain the \\rho V \\rho V coefficient: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} We have effectively computed the cosine angle between the two vectors in \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} cross-product of matrices. Again, we can repeat the same operations above in the law of cosines formulation. d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) Non-Linear Functions \u00b6 Let \\varphi(\\mathbf{X}) = \\mathbf{K_x} \\varphi(\\mathbf{X}) = \\mathbf{K_x} and \\varphi(\\mathbf{Y}) = \\mathbf{K_y} \\varphi(\\mathbf{Y}) = \\mathbf{K_y} . In the kernel community, this is known as the centered kernel alignment (cKA) \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} Rewriting this into the law of cosines: d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) References \u00b6 Le Traitement des Variables Vectorielles - Yves Escoufier (1973) Summarizing multiple aspects of model performance in a single diagram - Karl E. Taylor (2001) Taylor Diagram Primer - Karl E. Taylor (2005) The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) Measuring Multivariate Association and Beyond - Josse & Holmes - Statistics Surveys (2016)","title":"Visualizing Similarities"},{"location":"appendix/kernels/similarity/visualization/taylor/#visualizing-similarities","text":"Motivation Questions Current Ways Cosine Similarity Correlation Distances Law of cosines Taylor Diagram Statistics Metric Space Example 1 - Intuition Example 2 - Model Outputs Multi-Dimensional Data Distances Correlation Sample Space Non-Linear Functions References","title":"Visualizing Similarities"},{"location":"appendix/kernels/similarity/visualization/taylor/#motivation","text":"Visualizations: * help find similarities between outputs * stats are great, but visual uncertainty quantification","title":"Motivation"},{"location":"appendix/kernels/similarity/visualization/taylor/#questions","text":"Which model is more similar to the reference/observations? Should we look at correlations across seasons or latitudes? Are there large discrepancies in the different outputs","title":"Questions"},{"location":"appendix/kernels/similarity/visualization/taylor/#current-ways","text":"Trend Plots often do not expose the comparison aspects... Scatter plots become impractical for many outputs Parallel Coordinate Plots are more practical, but only certain pairwise comparisons are possible Plots per ensemble - possible but it can be super cluttered Taylor Diagram - visualize several statistics simultaneously in a statistical metric space. Specific Statistics Mean, Variance, Correlation Box Plots (and variations)","title":"Current Ways"},{"location":"appendix/kernels/similarity/visualization/taylor/#cosine-similarity","text":"Figure I : A visual representation of the cosine similarity. The cosine similarity function measures the degree of similarity between two vectors. \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} \\begin{aligned} \\text{sim}( x,y) &= cos (\\theta) = \\frac{x\\cdot y}{||x||\\;||y||} \\end{aligned} Code def cosine_similarity ( x : np . ndarray , y : np . ndarray ) -> float : \"\"\"Computes the cosine similarity between two vectors X and Y Reflects the degree of similarity. Parameters ---------- X : np.ndarray, (n_samples) Y : np.ndarray, (n_samples) Returns ------- sim : float the cosine similarity between X and Y \"\"\" # compute the dot product between two vectors dot = np . dot ( x , y ) # compute the L2 norm of x x_norm = np . sqrt ( np . sum ( x ** 2 )) y_norm = np . linalg . norm ( y ) # compute the cosine similarity sim = dot / ( x_norm * y_norm ) return sim","title":"Cosine Similarity"},{"location":"appendix/kernels/similarity/visualization/taylor/#correlation","text":"There is a relationship between the cosine similarity and correlation coefficient \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} \\rho(\\mathbf{x}, \\mathbf{y}) = \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} if \\rho(x,y) = 0 \\rho(x,y) = 0 , the spaces are orthogonal if \\rho(x,y) = 1 \\rho(x,y) = 1 , the spaces are equivalent","title":"Correlation"},{"location":"appendix/kernels/similarity/visualization/taylor/#distances","text":"Figure II : The triangle showing the cosine similarity and it's relationship to the euclidean distance. d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\langle x, y \\rangle","title":"Distances"},{"location":"appendix/kernels/similarity/visualization/taylor/#law-of-cosines","text":"If we recall the law of cosines; an extension of the cosine angle formula but for all angles and sides of the triangle. c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta c^2 = a^2 + b^2 - 2 \\,a \\, b \\,\\cos \\theta Notice that this forumala looks very similar to the euclidean distance formula shown above. If we do a simple rearrangement of the final term in the above equation to accommadate the correlation term \\rho(x,y) \\rho(x,y) , then we get d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\frac{ \\text{cov}(\\mathbf{x},\\mathbf{y})}{\\sigma_\\mathbf{x} \\sigma_\\mathbf{y}} simplifying to: d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) d^2(x,y) = ||x-y||^2=||x||^2 + ||y||^2 - 2 \\, ||x||\\, ||y|| \\, \\rho(x,y) This is actually equivalent to the law of cosines.","title":"Law of cosines"},{"location":"appendix/kernels/similarity/visualization/taylor/#taylor-diagram","text":"The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. It simultaneously plots the standard deviation, the root mean square error and correlation between two variables.","title":"Taylor Diagram"},{"location":"appendix/kernels/similarity/visualization/taylor/#statistics-metric-space","text":"They key is that it is possible to find a metric space for these quantities, based on the law of cosines. If you look closely, this identity looks like the cosine law of triangles.we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE as we have expressed above. \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\cos (\\theta) If we write out the full equation, we have the following: \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) \\text{RMSE}^2(x,y) = \\sigma_{x}^2 + \\sigma_{y}^2 - 2 \\, \\sigma_r \\, \\sigma_t \\, \\rho (x,y) The sides are as follows: a = \\sigma_{x} a = \\sigma_{x} - the standard deviation of x x b = \\sigma_{y} b = \\sigma_{y} - the standard deviation of y y \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\rho=\\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} - the correlation coefficient RMSE - the root mean squared difference between the two datasets So, the important quantities needed to be able to plot points on the Taylor diagram are the \\sigma \\sigma and \\theta= \\arccos \\rho \\theta= \\arccos \\rho . If we assume that the observed data is given by \\sigma_{\\text{obs}}, \\theta=0 \\sigma_{\\text{obs}}, \\theta=0 , then we can plot the rest of the comparisons via \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho \\sigma_{\\text{sim}}, \\theta=\\arccos \\rho .","title":"Statistics Metric Space"},{"location":"appendix/kernels/similarity/visualization/taylor/#example-1-intuition","text":"Figure III : An example Taylor diagram.","title":"Example 1 - Intuition"},{"location":"appendix/kernels/similarity/visualization/taylor/#example-2-model-outputs","text":"","title":"Example 2 - Model Outputs"},{"location":"appendix/kernels/similarity/visualization/taylor/#multi-dimensional-data","text":"In the above examples, we assume that \\mathbf{x}, \\mathbf{y} \\mathbf{x}, \\mathbf{y} were both vectors of size \\mathbb{R}^{N \\times 1} \\mathbb{R}^{N \\times 1} . But what happens when we get datasets of size \\mathbb{R}^{N \\times D} \\mathbb{R}^{N \\times D} ? Well, the above formulas can generalize using the inner product and the norm of the datasets. Figure I : A visual representation of the cosine similarity generalized to vectors.","title":"Multi-Dimensional Data"},{"location":"appendix/kernels/similarity/visualization/taylor/#distances_1","text":"We still get the same formulation as the above except now it is generalized to vectors. d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle d^2(\\mathbf{x,y}) = ||\\mathbf{x-y}||^2=||\\mathbf{x}||^2 + ||\\mathbf{y}||^2 - 2 \\langle \\mathbf{x,y} \\rangle","title":"Distances"},{"location":"appendix/kernels/similarity/visualization/taylor/#correlation_1","text":"Let \\Sigma_\\mathbf{xy} \\Sigma_\\mathbf{xy} be the empirical covariance matrix between \\mathbf{x,y} \\mathbf{x,y} . \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\Sigma_\\mathbf{xy}, \\Sigma_\\mathbf{xy} \\rangle_\\mathbf{F}}{||\\Sigma_\\mathbf{xx}||_\\mathbf{F} \\; || \\Sigma_\\mathbf{yy}||_\\mathbf{F}} See the multidimensional section of this page for more details on the \\rho V \\rho V coefficient. So the same rules apply as done above, we can rewrite the law of cosines to encompass the multidimensional data inputs. d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\Sigma_\\mathbf{x}||_F^2 + ||\\Sigma_\\mathbf{y}||_F^2 - 2 \\, ||\\Sigma_\\mathbf{x}||_F \\, ||\\Sigma_\\mathbf{y}||_F \\, \\rho V (\\mathbf{x,y})","title":"Correlation"},{"location":"appendix/kernels/similarity/visualization/taylor/#sample-space","text":"Sometimes it's convenient to write the \\rho V (\\mathbf{x,y}) \\rho V (\\mathbf{x,y}) coefficient for the sample space for the data. So instead of calculating a cross-covariance matrix \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} \\Sigma_{\\mathbf{xy}}\\mathbb{R}^{D \\times D} , we calculate a self-similarity matrix for each of the datasets, e.g. \\mathbf{XX}^\\top = \\mathbf{W_x} \\mathbf{XX}^\\top = \\mathbf{W_x} and \\mathbf{YY}^\\top = \\mathbf{W_y} \\mathbf{YY}^\\top = \\mathbf{W_y} . This is a different and pairwise representation of the data. To measure the proximity between the two matrices, we can use the Frobenius norm (aka the Hilbert-Schmidt norm). This gives us: \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) \\langle \\mathbf{W_x, W_y} \\rangle_F = \\sum_{i=1}\\sum_{j=1} \\text{cov}^2 (\\mathbf{x}_i, \\mathbf{y}_j) Again, the two matrices will have differnt norms so we can renormalize them appropriately to obtain the \\rho V \\rho V coefficient: \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} \\rho V (\\mathbf{x,y}) = \\frac{\\langle \\mathbf{W_x}, \\mathbf{W_y} \\rangle_\\mathbf{F}}{||\\mathbf{W_x}||_\\mathbf{F} \\; || \\mathbf{W_y}||_\\mathbf{F}} We have effectively computed the cosine angle between the two vectors in \\mathbb{R}^{N \\times N} \\mathbb{R}^{N \\times N} cross-product of matrices. Again, we can repeat the same operations above in the law of cosines formulation. d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{W_x}||_F^2 + ||\\mathbf{W_y}||_F^2 - 2 \\, ||\\mathbf{W_x}||_F \\, ||\\mathbf{W_y}||_F \\, \\rho V (\\mathbf{x,y})","title":"Sample Space"},{"location":"appendix/kernels/similarity/visualization/taylor/#non-linear-functions","text":"Let \\varphi(\\mathbf{X}) = \\mathbf{K_x} \\varphi(\\mathbf{X}) = \\mathbf{K_x} and \\varphi(\\mathbf{Y}) = \\mathbf{K_y} \\varphi(\\mathbf{Y}) = \\mathbf{K_y} . In the kernel community, this is known as the centered kernel alignment (cKA) \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} \\text{cKA}(\\mathbf{x,y}) = \\frac{\\langle \\mathbf{K_x}, \\mathbf{K_y} \\rangle_\\mathbf{F}}{||\\mathbf{K_x}||_\\mathbf{F} \\; || \\mathbf{K_y}||_\\mathbf{F}} Rewriting this into the law of cosines: d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y}) d^2(\\mathbf{x,y}) = ||\\mathbf{K_x}||_F^2 + ||\\mathbf{K_y}||_F^2 - 2 \\, ||\\mathbf{K_x}||_F \\, ||\\mathbf{K_y}||_F \\, \\text{cKA}(\\mathbf{x,y})","title":"Non-Linear Functions"},{"location":"appendix/kernels/similarity/visualization/taylor/#references","text":"Le Traitement des Variables Vectorielles - Yves Escoufier (1973) Summarizing multiple aspects of model performance in a single diagram - Karl E. Taylor (2001) Taylor Diagram Primer - Karl E. Taylor (2005) The Mutual Information Diagram for Uncertainty Visualization - Correa & Lindstrom (2012) Measuring Multivariate Association and Beyond - Josse & Holmes - Statistics Surveys (2016)","title":"References"},{"location":"appendix/neural/rbig/","text":"Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6 Testing \u00b6 Here we have a few tests that we can do: DCT components are orthogonal PCA works Eigenvalues are descending PCA + Whitening Works Eigenvalues are descending Data is white [all(abs(np.cov(pca.transform(data))< 1e-6>)] Test the non-symmetric version (?) def test_dct ( self ): # Initialize Linear Transformation Class and DCT components dct = LinearTransformation ( basis = 'DCT' , conv = 16 ) # Make sure the DCT basis is orthogonal self . assertTrue ( all ( abs ( dct . W , dct . W . T ) - np . eye ( 256 )) < 1e-10 ) # The Jacobian should be zero X_rand = np . random . randn ( 16 , 10 ) self . assertTrue ( all ( abs ( dct . logjacobian ( X_rand ), dct . W . T ) - np . eye ( 256 ) < 1e-10 )) def test_pca ( self ): # Get Test Data X_rand = np . random . randn ( 16 , 256 ) covr = np . cov ( X_rand ) data = np . linalg . cholesky ( covr ) @ np . random . randn ( 16 , 10000 ) # Initialize Linear Transformation with PCA pca = LinearTransformation ( basis = 'PCA' ) # Make sure eigenvalues descend # # Make sure data is white def test_pca_whitening ( self ): Supplementary \u00b6 Boundary Issues \u00b6 PDF Estimation under arbitrary transformation \u00b6 Let \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d be a r.v. with a PDF, \\mathcal P_x (\\mathbf x) \\mathcal P_x (\\mathbf x) . Given some bijective, differentiable transform \\mathbf x \\mathbf x and \\mathbf y \\mathbf y using \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d , \\mathbf y = \\mathcal G(\\mathbf x) \\mathbf y = \\mathcal G(\\mathbf x) , we can use the change of variables formula to calculate the determinant: \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| In the case of Gaussianization, we can calculate \\mathcal P (\\mathbf x) \\mathcal P (\\mathbf x) if the Jacobian is known since Iterative Gaussianization Transform is Invertible \u00b6 Given a Gaussianization transform: \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) by simple manipulation, the inversion transform is: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) Note : If \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal, then \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} . So we can simplify our transformation like so: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) iff \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal or (orthonormal vectors). Note 2 : to ensure that \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is invertible, we need to be sure that the PDF support is connected. So the domain is continuous and there are no disjoint spaces ( ??? ). References \u00b6 Algorithm Multivariate Gaussianization for Data Proceessing - Prezi Nonlineear Extraction of 'IC' of elliptically symmetric densities using radial Gaussianization - Lyu et. al. (2008) - Code Real-NVP Implementation - PyTorch Normalizing Flows with PyTorch Radial Gaussianization - Python PyTorch GDN Good RBIG Implementations - Transforms | Models Histogram Estimation Scipy Use Scipy Function HistogramUniverateDensity","title":"Rbig"},{"location":"appendix/neural/rbig/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"appendix/neural/rbig/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"appendix/neural/rbig/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"appendix/neural/rbig/#jacobian","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"appendix/neural/rbig/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"appendix/neural/rbig/#testing","text":"Here we have a few tests that we can do: DCT components are orthogonal PCA works Eigenvalues are descending PCA + Whitening Works Eigenvalues are descending Data is white [all(abs(np.cov(pca.transform(data))< 1e-6>)] Test the non-symmetric version (?) def test_dct ( self ): # Initialize Linear Transformation Class and DCT components dct = LinearTransformation ( basis = 'DCT' , conv = 16 ) # Make sure the DCT basis is orthogonal self . assertTrue ( all ( abs ( dct . W , dct . W . T ) - np . eye ( 256 )) < 1e-10 ) # The Jacobian should be zero X_rand = np . random . randn ( 16 , 10 ) self . assertTrue ( all ( abs ( dct . logjacobian ( X_rand ), dct . W . T ) - np . eye ( 256 ) < 1e-10 )) def test_pca ( self ): # Get Test Data X_rand = np . random . randn ( 16 , 256 ) covr = np . cov ( X_rand ) data = np . linalg . cholesky ( covr ) @ np . random . randn ( 16 , 10000 ) # Initialize Linear Transformation with PCA pca = LinearTransformation ( basis = 'PCA' ) # Make sure eigenvalues descend # # Make sure data is white def test_pca_whitening ( self ):","title":"Testing"},{"location":"appendix/neural/rbig/#supplementary","text":"","title":"Supplementary"},{"location":"appendix/neural/rbig/#boundary-issues","text":"","title":"Boundary Issues"},{"location":"appendix/neural/rbig/#pdf-estimation-under-arbitrary-transformation","text":"Let \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d be a r.v. with a PDF, \\mathcal P_x (\\mathbf x) \\mathcal P_x (\\mathbf x) . Given some bijective, differentiable transform \\mathbf x \\mathbf x and \\mathbf y \\mathbf y using \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d \\mathcal G:\\mathbb R^d \\rightarrow \\mathbb R^d , \\mathbf y = \\mathcal G(\\mathbf x) \\mathbf y = \\mathcal G(\\mathbf x) , we can use the change of variables formula to calculate the determinant: \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(\\mathbf x)}{\\partial \\mathbf x} \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| \\mathcal{P}_x( \\mathbf x)= \\mathcal{P}_{y}\\left( \\mathcal{G}_{\\theta}( \\mathbf x) \\right) \\cdot \\left| \\nabla_{\\mathbf x} \\cdot \\mathcal{G}_{\\theta}(\\mathbf x) \\right| In the case of Gaussianization, we can calculate \\mathcal P (\\mathbf x) \\mathcal P (\\mathbf x) if the Jacobian is known since","title":"PDF Estimation under arbitrary transformation"},{"location":"appendix/neural/rbig/#iterative-gaussianization-transform-is-invertible","text":"Given a Gaussianization transform: \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot\\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) by simple manipulation, the inversion transform is: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{-1} \\cdot \\mathbf x^{(k)} \\right) Note : If \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal, then \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} \\mathbf R_{(k)}^{-1} = \\mathbf R_{(k)}^{\\top} . So we can simplify our transformation like so: \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) \\mathcal G^{-1}:\\mathbf x^{(k)}=\\mathbf \\Psi_{(k)}^{-1}\\left( \\mathbf R_{(k)}^{\\top} \\cdot \\mathbf x^{(k)} \\right) iff \\mathbf R_{(k)}^{-1} \\mathbf R_{(k)}^{-1} is orthogonal or (orthonormal vectors). Note 2 : to ensure that \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is invertible, we need to be sure that the PDF support is connected. So the domain is continuous and there are no disjoint spaces ( ??? ).","title":"Iterative Gaussianization Transform is Invertible"},{"location":"appendix/neural/rbig/#references","text":"Algorithm Multivariate Gaussianization for Data Proceessing - Prezi Nonlineear Extraction of 'IC' of elliptically symmetric densities using radial Gaussianization - Lyu et. al. (2008) - Code Real-NVP Implementation - PyTorch Normalizing Flows with PyTorch Radial Gaussianization - Python PyTorch GDN Good RBIG Implementations - Transforms | Models Histogram Estimation Scipy Use Scipy Function HistogramUniverateDensity","title":"References"},{"location":"blogs/ideas/","text":"Blog Ideas \u00b6 I would like to do some blog posts for my academic blog. There are tons of things I would like to do but I don't really have time to do it all. So I would like to try a concept: combine scientific exploration with some programming concept exploration. For example, instead of doing a segment on Object-Oriented Programming and then a separate segment on Gaussian Process regression (GPR), I would combine the two. Programming \u00b6 Packages \u00b6 GPyTorch - DKL, Pyro GPFlow geopandas xarray Algorithms \u00b6 Optimized Kernel Ridge Regression (OKRR) Jax Optimized Kernel Entropy Components Analysis (OKECA) Jax Rotation-Based Iterative Gaussianization (RBIG) Gaussian Process Regression Variational Gaussian Process Regression Variational Inference Jax Bayesian Neural Networks Deep Kernel Learning GPyTorch - Gaussian Processes PyTorch - Neural Networks Data - Ocean Water Quality Gaussian Processes Exact Variational Sparse Sparse Variational Deep Packages \u00b6 Edward2 - Bayesian Layers (the future) Unsorted \u00b6 Bayesian Formulas + Plotly OOP + GPR from Scratch Kernel Functions (K, GPR, KRR) + Derivatives + AutoGrad VI + PyTorch RBIG + sklearn API IT Measures + ESDC + RBIG AD + ESDC + Feature Selection AD + ESDC + Feature Selection (Pt II) HyperLabelMe + TPOT + AutoSklearn Flask + Xarray Luigi + SLURM + Experiment Uncertainty + GPs Abstract Classes + Kernel Functions Work Deep Dive \u00b6 These notebooks will be directly related to my thesis and things that I am investigating actively. They should all include some results so that I can show off some of the actual applications. Output Normalized Methods I - Kernel Eigenmap Methods II - Kernel Eigenmap Projection Methods III - Manifold Alignment IV - Nearest Neighbours (Annoy, KDE Trees) V - Eigenvalue Decomposition Scaling (rSVD, Multigrid, Random Projections) VI - Out of Sampling (Nystrom, LLL, Var. Nystrom) Kernel Methods I - Kernel Functions II - Learning with Kernel Functions (Overview of Literature) III - Gradients and Sensitivity Analysis Gaussian Processes and Uncertainty I - GPs II - Sparse GPs III - Uncertain GPs (Literature, NIGP, My Work) IV - Variational Methods Deep Density Destructors I - Density Estimation II - RBIG III - GDN Concept Notebook \u00b6 These are notebooks that I decided to investigate because either I sucked at in the beginning or it was something I needed in order to advance to the next level of whatever I was doing related to my thesis. Bayesian Methods Variational Inference Information Theory Anomaly Detection Explorers Book \u00b6 Normalizing Flows Automatic Machine Learning Neural ODEs Lab Notebook \u00b6 Earth Science Data Cube + Xarray Naive AD Detection Dask Remote Computing xarray Shape Files Region Masks Large Scale ML - PCA, LR, KMeans, XGBoost Dask Jax Kernel methods kernels regression - krr, rff bayesian regression - gp, sgps classification - svm dependence estimation - hsic dimension reduction - okeca) Gaussianization flows","title":"Blog Ideas"},{"location":"blogs/ideas/#blog-ideas","text":"I would like to do some blog posts for my academic blog. There are tons of things I would like to do but I don't really have time to do it all. So I would like to try a concept: combine scientific exploration with some programming concept exploration. For example, instead of doing a segment on Object-Oriented Programming and then a separate segment on Gaussian Process regression (GPR), I would combine the two.","title":"Blog Ideas"},{"location":"blogs/ideas/#programming","text":"","title":"Programming"},{"location":"blogs/ideas/#packages","text":"GPyTorch - DKL, Pyro GPFlow geopandas xarray","title":"Packages"},{"location":"blogs/ideas/#algorithms","text":"Optimized Kernel Ridge Regression (OKRR) Jax Optimized Kernel Entropy Components Analysis (OKECA) Jax Rotation-Based Iterative Gaussianization (RBIG) Gaussian Process Regression Variational Gaussian Process Regression Variational Inference Jax Bayesian Neural Networks Deep Kernel Learning GPyTorch - Gaussian Processes PyTorch - Neural Networks Data - Ocean Water Quality Gaussian Processes Exact Variational Sparse Sparse Variational Deep","title":"Algorithms"},{"location":"blogs/ideas/#packages_1","text":"Edward2 - Bayesian Layers (the future)","title":"Packages"},{"location":"blogs/ideas/#unsorted","text":"Bayesian Formulas + Plotly OOP + GPR from Scratch Kernel Functions (K, GPR, KRR) + Derivatives + AutoGrad VI + PyTorch RBIG + sklearn API IT Measures + ESDC + RBIG AD + ESDC + Feature Selection AD + ESDC + Feature Selection (Pt II) HyperLabelMe + TPOT + AutoSklearn Flask + Xarray Luigi + SLURM + Experiment Uncertainty + GPs Abstract Classes + Kernel Functions","title":"Unsorted"},{"location":"blogs/ideas/#work-deep-dive","text":"These notebooks will be directly related to my thesis and things that I am investigating actively. They should all include some results so that I can show off some of the actual applications. Output Normalized Methods I - Kernel Eigenmap Methods II - Kernel Eigenmap Projection Methods III - Manifold Alignment IV - Nearest Neighbours (Annoy, KDE Trees) V - Eigenvalue Decomposition Scaling (rSVD, Multigrid, Random Projections) VI - Out of Sampling (Nystrom, LLL, Var. Nystrom) Kernel Methods I - Kernel Functions II - Learning with Kernel Functions (Overview of Literature) III - Gradients and Sensitivity Analysis Gaussian Processes and Uncertainty I - GPs II - Sparse GPs III - Uncertain GPs (Literature, NIGP, My Work) IV - Variational Methods Deep Density Destructors I - Density Estimation II - RBIG III - GDN","title":"Work Deep Dive"},{"location":"blogs/ideas/#concept-notebook","text":"These are notebooks that I decided to investigate because either I sucked at in the beginning or it was something I needed in order to advance to the next level of whatever I was doing related to my thesis. Bayesian Methods Variational Inference Information Theory Anomaly Detection","title":"Concept Notebook"},{"location":"blogs/ideas/#explorers-book","text":"Normalizing Flows Automatic Machine Learning Neural ODEs","title":"Explorers Book"},{"location":"blogs/ideas/#lab-notebook","text":"Earth Science Data Cube + Xarray Naive AD Detection Dask Remote Computing xarray Shape Files Region Masks Large Scale ML - PCA, LR, KMeans, XGBoost Dask Jax Kernel methods kernels regression - krr, rff bayesian regression - gp, sgps classification - svm dependence estimation - hsic dimension reduction - okeca) Gaussianization flows","title":"Lab Notebook"},{"location":"projects/","text":"Main Projects \u00b6 Similarity Measures \u00b6 Summary I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links : Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links : Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors Important Links : Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk PySim Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links : Github Repository Uncertainty Quantification \u00b6 Projects Input Uncertainty for Gaussian Processes Gaussianization Models in Eath Science Applications Gaussian Process Model Zoo RBIG 1.1 Python Package RBIG 2.0 Python Package Kernel Methods and Derivatives \u00b6 Projects Kernel Derivatives Applied to Earth Science Data Cubes Derivatives for Sensitivity Analysis in Gaussian Processes applied to Emulation Machine Learning for Ocean Applications \u00b6 Projects ARGO Floats MultiOutput Models Ocean Water Types Regression Ocean Water Types Classification","title":"Overview"},{"location":"projects/#main-projects","text":"","title":"Main Projects"},{"location":"projects/#similarity-measures","text":"Summary I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links : Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links : Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors Important Links : Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk PySim Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links : Github Repository","title":"Similarity Measures"},{"location":"projects/#uncertainty-quantification","text":"Projects Input Uncertainty for Gaussian Processes Gaussianization Models in Eath Science Applications Gaussian Process Model Zoo RBIG 1.1 Python Package RBIG 2.0 Python Package","title":"Uncertainty Quantification"},{"location":"projects/#kernel-methods-and-derivatives","text":"Projects Kernel Derivatives Applied to Earth Science Data Cubes Derivatives for Sensitivity Analysis in Gaussian Processes applied to Emulation","title":"Kernel Methods and Derivatives"},{"location":"projects/#machine-learning-for-ocean-applications","text":"Projects ARGO Floats MultiOutput Models Ocean Water Types Regression Ocean Water Types Classification","title":"Machine Learning for Ocean Applications"},{"location":"projects/BNNs/","text":"Bayesian Neural Networks Working Group \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: isp.uv.es/working_groups/bnn Summary \u00b6 This is the working group page for exploring Bayesian Neural Networks (BNNs). Recently BNNs have started to become popular in the Machine learning literature as well as the applied sciences literature. Most research groups are interested because of the 'principled' approach to handling uncertainty within your data. Many traditional ML approaches don't account for uncertainty and make this The adoption of Deep learning methods and easy-to-use open-source software has also aided in the growning popularity. It is now easier to implement and try various models without having to do things from scratch. It's a good time to see if the Bayesian methodology works for your problem as the field has started to progress. In this working group we will working with the Bayesian methodology from 3 perspectives: Theory , Practice , and Application . Even though there are 3 parts, we will be heavily driven by the application portion. After talking with the lab, we have a list of possible applications where we think BNNs would be appropriate. This could determine the direction of our exploration as a principled approach to dealing with your problem does require us to think about our data and which approaches will be appropriate. We will also adopt a balance between the methods that seem to work in practice (e.g. Drop-Out, Ensembles) and the methods that \"would be nice\" (e.g. VI-based layers, Deep Gaussian Processes, SWAG). This means that we will include things that approximate NNs such as drop-out and architectures that are a mixture of standard NNs and probabilistic models. I outline each of the sections in detail below. Sections Outline \u00b6 Theory We will look at some of the staple papers which started the BNN approach as well as some of the SOTA approaches. In addition to Bayes related material, we will also take a look at some things related to uncertainty and neural networks. Practice We will go over some key probabilistic programming aspects. This is different than the standard Neural network architecture and can be a bit difficult to fully grasp. I think with adequate training in the software side of things, your life will be must easier and you will be able to correctly and efficiently implement algorithms and concepts. Applications This will be somewhat application driven, at least for the practical aspects. In the end, the groups have all come with hopes that they can use some of these techniques in the near future. We currently have the following pending applications: * Emulation Data * Ocean Data (Multi-Output) * Medical Data * Gap Filling Data Format ( TBD ) \u00b6 I would like to balance the 3 things I've mentioned above. I would like to spend time in the meetings discussing the theory and concepts. And then we can have a few sessions discussing some programming concepts to ensure that we can be doing practice on the way. Perhaps the individual groups can work on the applications in their free time. Requirements \u00b6 This is not a beginners working group so there will be some expectations from the people attending if they're going to participate. They are not strictly required, but I suggest them because I think it would make the experience better for everyone . Familiarity with Bayesian Concepts (Prior, Likelihood, Posterior, Evidence, etc) Prior Programming experience; preferably in Python ( practical sessions ) Familiarity with Neural networks and the terminology (gradients, loss, optimization, etc) Logistics \u00b6 When * Start: February, 2020 * Duration: TBD Where * ISP Open Office Leads * J. Emmanuel Johnson * Kristoffer Wickstrom Resources \u00b6 Literature * Papers * SOTA Resources * Videos Software * DL Frameworks * Overview * TensorFlow","title":"Bayesian Neural Networks Working Group"},{"location":"projects/BNNs/#bayesian-neural-networks-working-group","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: isp.uv.es/working_groups/bnn","title":"Bayesian Neural Networks Working Group"},{"location":"projects/BNNs/#summary","text":"This is the working group page for exploring Bayesian Neural Networks (BNNs). Recently BNNs have started to become popular in the Machine learning literature as well as the applied sciences literature. Most research groups are interested because of the 'principled' approach to handling uncertainty within your data. Many traditional ML approaches don't account for uncertainty and make this The adoption of Deep learning methods and easy-to-use open-source software has also aided in the growning popularity. It is now easier to implement and try various models without having to do things from scratch. It's a good time to see if the Bayesian methodology works for your problem as the field has started to progress. In this working group we will working with the Bayesian methodology from 3 perspectives: Theory , Practice , and Application . Even though there are 3 parts, we will be heavily driven by the application portion. After talking with the lab, we have a list of possible applications where we think BNNs would be appropriate. This could determine the direction of our exploration as a principled approach to dealing with your problem does require us to think about our data and which approaches will be appropriate. We will also adopt a balance between the methods that seem to work in practice (e.g. Drop-Out, Ensembles) and the methods that \"would be nice\" (e.g. VI-based layers, Deep Gaussian Processes, SWAG). This means that we will include things that approximate NNs such as drop-out and architectures that are a mixture of standard NNs and probabilistic models. I outline each of the sections in detail below.","title":"Summary"},{"location":"projects/BNNs/#sections-outline","text":"Theory We will look at some of the staple papers which started the BNN approach as well as some of the SOTA approaches. In addition to Bayes related material, we will also take a look at some things related to uncertainty and neural networks. Practice We will go over some key probabilistic programming aspects. This is different than the standard Neural network architecture and can be a bit difficult to fully grasp. I think with adequate training in the software side of things, your life will be must easier and you will be able to correctly and efficiently implement algorithms and concepts. Applications This will be somewhat application driven, at least for the practical aspects. In the end, the groups have all come with hopes that they can use some of these techniques in the near future. We currently have the following pending applications: * Emulation Data * Ocean Data (Multi-Output) * Medical Data * Gap Filling Data","title":"Sections Outline"},{"location":"projects/BNNs/#format-tbd","text":"I would like to balance the 3 things I've mentioned above. I would like to spend time in the meetings discussing the theory and concepts. And then we can have a few sessions discussing some programming concepts to ensure that we can be doing practice on the way. Perhaps the individual groups can work on the applications in their free time.","title":"Format (TBD)"},{"location":"projects/BNNs/#requirements","text":"This is not a beginners working group so there will be some expectations from the people attending if they're going to participate. They are not strictly required, but I suggest them because I think it would make the experience better for everyone . Familiarity with Bayesian Concepts (Prior, Likelihood, Posterior, Evidence, etc) Prior Programming experience; preferably in Python ( practical sessions ) Familiarity with Neural networks and the terminology (gradients, loss, optimization, etc)","title":"Requirements"},{"location":"projects/BNNs/#logistics","text":"When * Start: February, 2020 * Duration: TBD Where * ISP Open Office Leads * J. Emmanuel Johnson * Kristoffer Wickstrom","title":"Logistics"},{"location":"projects/BNNs/#resources","text":"Literature * Papers * SOTA Resources * Videos Software * DL Frameworks * Overview * TensorFlow","title":"Resources"},{"location":"projects/BNNs/sidebar/","text":"Home Page Literature * Papers Resources * Videos Explorers Groups * For not so Dummies Software * Overview * TensorFlow","title":"Sidebar"},{"location":"projects/BNNs/code/jax/","text":"Accelerated ML Research via Composable Function Transformations in Python - neurips 2019 Taylor-Mode AD for Higher-Order Derivatives in Jax - neurips 2019 NumPyro - paper","title":"Jax"},{"location":"projects/BNNs/code/pyro/","text":"Pyro \u00b6 Recommended Resources \u00b6 Tutorials \u00b6 Sample Code \u00b6 Pyro Docs Probabilistic Programming with VI: Under the Hood A Prelude to Pyro Experimenting with Pyro's Hidden Native Support for Bayesian Neural Networks Bayesian Inference: How we are able to chase the Posterior","title":"Pyro"},{"location":"projects/BNNs/code/pyro/#pyro","text":"","title":"Pyro"},{"location":"projects/BNNs/code/pyro/#recommended-resources","text":"","title":"Recommended Resources"},{"location":"projects/BNNs/code/pyro/#tutorials","text":"","title":"Tutorials"},{"location":"projects/BNNs/code/pyro/#sample-code","text":"Pyro Docs Probabilistic Programming with VI: Under the Hood A Prelude to Pyro Experimenting with Pyro's Hidden Native Support for Bayesian Neural Networks Bayesian Inference: How we are able to chase the Posterior","title":"Sample Code"},{"location":"projects/BNNs/code/pytorch/","text":"PyTorch \u00b6 Recommended Resources \u00b6 Tutorials \u00b6 PyTorch - Variables, Functionals and Autograd - blog Research to Production: PyTorch JIT/TorchScript Updates - Michael Suo, 2019 Sample Code \u00b6 Grokking-PyTorch VAE in PyTorch, commended and Annotated","title":"PyTorch"},{"location":"projects/BNNs/code/pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"projects/BNNs/code/pytorch/#recommended-resources","text":"","title":"Recommended Resources"},{"location":"projects/BNNs/code/pytorch/#tutorials","text":"PyTorch - Variables, Functionals and Autograd - blog Research to Production: PyTorch JIT/TorchScript Updates - Michael Suo, 2019","title":"Tutorials"},{"location":"projects/BNNs/code/pytorch/#sample-code","text":"Grokking-PyTorch VAE in PyTorch, commended and Annotated","title":"Sample Code"},{"location":"projects/BNNs/code/resources/","text":"From Scratch \u00b6 Probability & Statistics \u00b6 An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma - Blog Probabilistic Programming Concepts - Computational Statistics (2019) - Notes Bayesian regression with linear basis function models - Martin Krasser - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Multivariate Normal Distribution Primer - blog Neural Networks \u00b6 What is torch.nn reall? - Jeremy Howard - docs Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog Deep Learning Fundamentals - Eric Ma, Scipy 2019 - Video & Notebook | blog Bayesian Neural Networks \u00b6 Weight Uncertainty in Neural Networks Tutorial - Josh Feldman (2018) - blog Weight Uncertainty in Neural Networks - Nitarshan Rajkumar (2018) - blog Variational inference in Bayesian neural networks - Martin Krasser - blog Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 - Brendan Hasz - Blog Inference \u00b6 Expectation Maximization \u00b6 Algorithm Breakdown: Expectation Maximization - Ritchie Vink - blog Latent variable models part 1: Gaussian mixture models and the EM algorithm - Martin Krasser - blog Laplace Approximation \u00b6 Monte Carlo \u00b6 Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Variational Inference \u00b6 Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference - Brendan Hasz - Blog From Expectation Maximization to Stochastic Variational Inference - Martin Krasser - blog Latent variable models part 2 - Stochastic variational inference and variational autoencoders - Martin Krasser - blog Automatic Differentiation Variational Inference - Arthur Lui (2020) - blog Paper Bayesian Linear Regression ADVI using PyTorch - Arthur Lui (2020) - blog | Paper Gaussian Processes \u00b6 Gaussian Processes - Martin Krasser - blog 4-Part Tutorial Understanding Gaussian Processes Fitting a GP GP Kernels","title":"Resources"},{"location":"projects/BNNs/code/resources/#from-scratch","text":"","title":"From Scratch"},{"location":"projects/BNNs/code/resources/#probability-statistics","text":"An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma - Blog Probabilistic Programming Concepts - Computational Statistics (2019) - Notes Bayesian regression with linear basis function models - Martin Krasser - blog Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - blog Multivariate Normal Distribution Primer - blog","title":"Probability &amp; Statistics"},{"location":"projects/BNNs/code/resources/#neural-networks","text":"What is torch.nn reall? - Jeremy Howard - docs Programming a Neural Network from Scratch - Ritchie Vink (2017) - blog Deep Learning Fundamentals - Eric Ma, Scipy 2019 - Video & Notebook | blog","title":"Neural Networks"},{"location":"projects/BNNs/code/resources/#bayesian-neural-networks","text":"Weight Uncertainty in Neural Networks Tutorial - Josh Feldman (2018) - blog Weight Uncertainty in Neural Networks - Nitarshan Rajkumar (2018) - blog Variational inference in Bayesian neural networks - Martin Krasser - blog Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 - Brendan Hasz - Blog","title":"Bayesian Neural Networks"},{"location":"projects/BNNs/code/resources/#inference","text":"","title":"Inference"},{"location":"projects/BNNs/code/resources/#expectation-maximization","text":"Algorithm Breakdown: Expectation Maximization - Ritchie Vink - blog Latent variable models part 1: Gaussian mixture models and the EM algorithm - Martin Krasser - blog","title":"Expectation Maximization"},{"location":"projects/BNNs/code/resources/#laplace-approximation","text":"","title":"Laplace Approximation"},{"location":"projects/BNNs/code/resources/#monte-carlo","text":"Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog","title":"Monte Carlo"},{"location":"projects/BNNs/code/resources/#variational-inference","text":"Variational Inference from Scratch - Ritchie Vink (2019) - blog Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - Brendan Hasz - blog Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference - Brendan Hasz - Blog From Expectation Maximization to Stochastic Variational Inference - Martin Krasser - blog Latent variable models part 2 - Stochastic variational inference and variational autoencoders - Martin Krasser - blog Automatic Differentiation Variational Inference - Arthur Lui (2020) - blog Paper Bayesian Linear Regression ADVI using PyTorch - Arthur Lui (2020) - blog | Paper","title":"Variational Inference"},{"location":"projects/BNNs/code/resources/#gaussian-processes","text":"Gaussian Processes - Martin Krasser - blog 4-Part Tutorial Understanding Gaussian Processes Fitting a GP GP Kernels","title":"Gaussian Processes"},{"location":"projects/BNNs/code/software/","text":"Software \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So what to choose? \u00b6 There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health. List of Software \u00b6 There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning. Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Software"},{"location":"projects/BNNs/code/software/#software","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020","title":"Software"},{"location":"projects/BNNs/code/software/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"projects/BNNs/code/software/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"projects/BNNs/code/software/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.","title":"Convergence of the Libraries"},{"location":"projects/BNNs/code/software/#so-what-to-choose","text":"There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"So what to choose?"},{"location":"projects/BNNs/code/software/#list-of-software","text":"There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning.","title":"List of Software"},{"location":"projects/BNNs/code/software/#core-packages","text":"","title":"Core Packages"},{"location":"projects/BNNs/code/software/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"projects/BNNs/code/software/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"projects/BNNs/code/software/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"projects/BNNs/code/tensorflow/","text":"TensorFlow \u00b6 TensorFlow is by far the most popular deep learning framework to date. It is the preferred choice in production and it is also still widely used in research. TensorFlow itself is actually written in C++ (and CUDA) but there are various APIs that allow you to call those functions with other languages like Python, Swift, Go and JavaScript. From TF 1.X to TF 2.X \u00b6 If you are already familiar with TF1.X, there are some key things you need to know about what has changed since TF2.X. The original TensorFlow (TF1.X) wasn't wasn't very pythonic even though the API was in Python. Among other nasty sharp edges , you had to define the graphs statically which required you to compile your graph before running it. This took us back to the C++ days where we have to compile-run-modify-repeat. In addition if something went wrong, we didn't have the training to be able to decipher the error messages. So on the plus side, if you could gain control of TF back then, you had to know what you were doing. It wasn't so easy. But on the other hand, most people programming were not computer scientists so naturally their code was very messy and very difficult to read. There wasn't a very good standard and so \"reproducible code\" was a nightmare to go through; very similar to \"reproducible MATLAB\" code because people do not tend to follow any set standard...except maybe spaghetti . Later they added eager execution which allowed you to define parts of your graph dynamically and then run them as you add more parts without needing to compile it. This was much better and it became easier to use TF without needing to worry about graphs. In tandem, a library called keras was gaining popularity. This library was basically a wrapper to hide all of the 'boilerplate code' so that a different class of users (beginners) can get started without needing to be bothered with the details. TF2.X is a more Model Building \u00b6 There is an example below to demonstrate the readability aspect for defining a simple linear regression model. ** TF 1.X - Static ** \u00b6 # Create Graph lr_graph = tf . Graph () with lr_graph . as_default (): x = tf . placeholder ( name = \"x\" , dtype = tf . float ) y = tf . placeholder ( name = \"y\" , dtype = tf . float ) # WEIGHTS w_init = tf . random_normal_initializer () w = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = tf . float32 ), name = \"w\" ) # BIAS b = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = tf . float32 ), name = \"b\" ) # MODEL OPERATIONS y_hat = tf . add ( tf . matmul ( x , w ), b ) loss = tf . reduce_mean ( tf . squared ( y - y_hat )) optimizer = tf . train . GradientDescentOptimizer ( lr ) . minimize ( loss ) ** TF 2.X - Dynamic ** \u00b6 class LinearRegression ( tf . keras . model ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # WEIGHTS w_init = tf . random_normal_initializer () self . w = tf . Variable ( initial_value = w_init ( shape = ( input_dim , units ), dtype = 'float32' ), trainable = True ) # BIAS self . b = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = 'float32' ), trainable = True ) def call ( self , x ): return tf . matmul ( inputs , self . w ) + self . b Source : Medium Post Notice that the main difference is that the default TF1.X has to define all of the operations as a graph before doing anything else. Whereas TF2.X, there is no need to do that. In addition, the keras API is closely linked to the TF library so they encourage you to use the standard model creation as shown by the documentation. Model Training \u00b6 Training was a different story. For the original TF1.X you had to create a session and then all of the gradients optimization had to go through that session. It was a pain because it was necessary for everything but it basically had to follow through the code if there were any crazy training procedures. I've seen many cases where there are wild and rogue sessions that I have to keep track of in order to follow what the users were doing. In TF2.X, there are no sessions. Just a gradientTape which tracks the final outputs and the final weights. And if you want the gradients, ask for them. That's it. Simple. I have included a minimal training example to showcase the major difference between TF1.X and TF2.X. ** TF 1.X - Static ** \u00b6 # Create session with tf . Session ( graph = lr_graph ) as sess : # Initialize all variables init = tf . global_variables_initializer () sess . run ( init ) # Training Loop for iepoch in range ( epochs ): # Run optimization in session feed_dict = { x : Xtrain , y : ytrain } sess . run ( optimizer , feed_dict = feed_dict ) # get losses loss , acc = sess . run ( [ loss_func , accuracy_func ], feed_dict = feed_dict ) ** TF 2.X - Dynamic ** \u00b6 for iepoch in range ( epochs ): with tf . GradientTape () as tape : # Forward pass ypreds = lr_model ( Xtrain ) # Loss loss = loss_func ( y , ypreds ) # compute gradients gradients = tape . gradient ( loss , lr_model . trainable_weights ) # update weights of linear layer optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) # update accuracy acc_func . update_state ( y , ypreds ) Source: Easy TensorFlow There are some more changes which you can read here . My Final Thoughts \u00b6 The biggest change I would say is the code standard and the clear spectrum of user cases. They are really promoting the keras way of defining Layers , Models , etc but you could also use Sequential or Functional . The rules are not absolute but this does set a nice \"standard way to do things\". This is a good thing . It's pythonic and readable. Some people are researchers and scientists whereas other people are computer scientists. But typically we'll be reading ML peoples code so we need a standard. That is if we plan on being a community and sharing. So I don't think we should spend time fighting DL libraries and reading sloppy code. We can spend more time devloping and solving more problems. One could argue that all of the details are hidden now and this promotes people just using stuff without understanding. But they're actually just optional to see. You can code from scratch if you want to. And yes, there will be many cases of people using models that they don't understand. But that's a choice and there will be barriers to prevent those people from thriving in the community. I personally think the changes are good and we shall see what the future holds for TF and DL software in general. My Favourite Resources \u00b6 These are my favourite resources. I've gone through almost all of them and I found that they did the best at explaining how to use TensorFlow. !> Remember , I am coming at this from a researchers perspective. So I am biased and the resources I've chosen assumes some prior knowledge about Python programming and Deep learning in general. I will list some resources Francois Chollet \u00b6 The best resource I have found is from the founder of keras ( Francois Chollet ). He is a very outspoken individual who is very proud of keras and how it has changed the community. He also likes to make comparisons between frameworks but overall he is very passionate about his work. He is also super active on twitter and has some interesting opinions from time to time. The first tutorial is basically a notebook on using tf.keras from a deep learning perspective. I think he breaks it down quite nicely and goes through all of the important aspects that a researcher should know when using TensorFlow. If you are already familiar with TensorFlow I think you'll find almost every major point he makes useful (e.g. Callbacks , ) when you construct your neural networks. If you still don't see it after going through it, don't worry, it will come up. TensorFlow 2.0 + Keras Overview for Deep Learning Researchers - Colab Notebook tf.keras for Researchers: Crash Course - Colab Notebook Inside TensorFlow: tf.keras - Video 1 | Video 2 TensorFlow Website \u00b6 There are a few really good tutorials on the TF website that give a really good overview of changes from TF 1.X TF 2.X as well as some more in-depth tutorials. I found the tutorials a bit difficult to navigate as there is a lot of repetition with the 'nuggets of wisdom' scattered everywhere. In addition, I find that the organization isn't really done based on the users level of knowledge. So I tried to outline how I think one should approach the tutorials based on three criteria: absolute beginner , Keras users , and PyTorch users which is my way of saying Beginner , Intermediate and Advanced . 1 Absolute Beginners Honestly, if you're just starting out with deep learning then you should probably just dive into it using a project or take your time and go through a course and/or book. I've listed my favourite books in the next section if you're interested but I will recommend this course which is sponsored by TensorFlow. I went through the first few lectures but I got a bit bored because I had already learned this stuff. But I like the balance of explanations and code. Introduction to TensorFlow for Deep Learning - Udacity If you have a bit more free time and dedication, I would recommend you go through the TensorFlow curriculum. They break the parts necessary for learning Deep learning with TF as your platform. It has books and video courses and I personally think it is organized very well. The course I listed above is included in the curriculum. TensorFlow Curriculums - Learn ML 2 Keras Users (Intermediate) Most people who apply DL models to data will be in this category. They will either want simple models with fairly straightforward setups or highly complex networks. In addition, they also may have complex training regimes. They all fall into this category and from this section, you should be able to get started. Keras Overview This is a fairly long tutorial that goes through keras from top to bottom. I wouldn't recommend reading the whole thing in one go as it is a bit overwhelming. If you want to do simple models, then only look at part 1 . And then for a quick overview of complex models, check out part 2 Keras Functional API I imagine most people who are experimenting with complex models with inputs and outputs in various locations will be here. Train and Evaluate with Keras This is another long guide that goes through how one can train your DL model. If you're not interested in too much training customization, then you'll probably mostly interest in part 1 where you use the built-in training module. Part 2 does things from scratch. !> MATLAB Users : Although I recommend you start in the absolute beginner section to get accustomed to Python, you will probably fall into this category as well. The Sequential API is very similar to the new DL toolbox in the latest versions of MATLAB. Unfortunately there is no GUI yet though... 3 PyTorch Users (Advanced) All my PyTorch and advanced Python users (me included) start here. You should feel right at home with TensorFlow using the subclassing . The distinction between Layer and Model is quite blurry but it's similar to the PyTorch nn.Module . In the end, it's super Pythonic so we should feel right at home. Finally... TF for Experts The is a 2-minute introduction to the language. If you are familiar with PyTorch then you will find this very familiar territory. It really highlights how the two packages converged. Writing Custom Layers and Models with Keras This next tutorial will go into more detail about the subclassing and how to build layers from scratch. It's very similar to PyTorch but there are a few more nuggets and subtleties that are unique to TensorFlow. Train and Evaluate with Keras This is another long guide that goes through how one can train your DL model. Pay special attention to part 2 where you build things from scratch as this is most similar to the PyTorch methods. Books \u00b6 This is a bit old school but there have recently been a lot of good books released in the past 2 years that do a very good job at teaching you Machine learning (including Deep learning) from a programming perspective. They don't skip out on all of the theory but you won't find al of the derivations necessary to write the algorithms from scratch. A nice thing is that most of the books below have code on their github accounts that you can download and run yourself. Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow - Aurelien Geron (2019) - Book | Github This is a best selling book and I found it to be the best resources for getting a really good overview of ML with Python in general as well as a really extensive section on TF2.0 and keras. Deep Learning with Python - Francois Chollet (2018) - Book By the creator of Keras himself. It's a great book that goes step-by-step with lots of examples and lots of explanations. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2 - Raschka & Mirjalili (2019) - Book | Github Another great book that talks about DL as well as ML in general.","title":"TensorFlow"},{"location":"projects/BNNs/code/tensorflow/#tensorflow","text":"TensorFlow is by far the most popular deep learning framework to date. It is the preferred choice in production and it is also still widely used in research. TensorFlow itself is actually written in C++ (and CUDA) but there are various APIs that allow you to call those functions with other languages like Python, Swift, Go and JavaScript.","title":"TensorFlow"},{"location":"projects/BNNs/code/tensorflow/#from-tf-1x-to-tf-2x","text":"If you are already familiar with TF1.X, there are some key things you need to know about what has changed since TF2.X. The original TensorFlow (TF1.X) wasn't wasn't very pythonic even though the API was in Python. Among other nasty sharp edges , you had to define the graphs statically which required you to compile your graph before running it. This took us back to the C++ days where we have to compile-run-modify-repeat. In addition if something went wrong, we didn't have the training to be able to decipher the error messages. So on the plus side, if you could gain control of TF back then, you had to know what you were doing. It wasn't so easy. But on the other hand, most people programming were not computer scientists so naturally their code was very messy and very difficult to read. There wasn't a very good standard and so \"reproducible code\" was a nightmare to go through; very similar to \"reproducible MATLAB\" code because people do not tend to follow any set standard...except maybe spaghetti . Later they added eager execution which allowed you to define parts of your graph dynamically and then run them as you add more parts without needing to compile it. This was much better and it became easier to use TF without needing to worry about graphs. In tandem, a library called keras was gaining popularity. This library was basically a wrapper to hide all of the 'boilerplate code' so that a different class of users (beginners) can get started without needing to be bothered with the details. TF2.X is a more","title":"From TF 1.X to TF 2.X"},{"location":"projects/BNNs/code/tensorflow/#model-building","text":"There is an example below to demonstrate the readability aspect for defining a simple linear regression model.","title":"Model Building"},{"location":"projects/BNNs/code/tensorflow/#tf-1x-static","text":"# Create Graph lr_graph = tf . Graph () with lr_graph . as_default (): x = tf . placeholder ( name = \"x\" , dtype = tf . float ) y = tf . placeholder ( name = \"y\" , dtype = tf . float ) # WEIGHTS w_init = tf . random_normal_initializer () w = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = tf . float32 ), name = \"w\" ) # BIAS b = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = tf . float32 ), name = \"b\" ) # MODEL OPERATIONS y_hat = tf . add ( tf . matmul ( x , w ), b ) loss = tf . reduce_mean ( tf . squared ( y - y_hat )) optimizer = tf . train . GradientDescentOptimizer ( lr ) . minimize ( loss )","title":"** TF 1.X - Static **"},{"location":"projects/BNNs/code/tensorflow/#tf-2x-dynamic","text":"class LinearRegression ( tf . keras . model ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # WEIGHTS w_init = tf . random_normal_initializer () self . w = tf . Variable ( initial_value = w_init ( shape = ( input_dim , units ), dtype = 'float32' ), trainable = True ) # BIAS self . b = tf . Variable ( initial_value = w_init ( shape = ( units ,), dtype = 'float32' ), trainable = True ) def call ( self , x ): return tf . matmul ( inputs , self . w ) + self . b Source : Medium Post Notice that the main difference is that the default TF1.X has to define all of the operations as a graph before doing anything else. Whereas TF2.X, there is no need to do that. In addition, the keras API is closely linked to the TF library so they encourage you to use the standard model creation as shown by the documentation.","title":"** TF 2.X - Dynamic **"},{"location":"projects/BNNs/code/tensorflow/#model-training","text":"Training was a different story. For the original TF1.X you had to create a session and then all of the gradients optimization had to go through that session. It was a pain because it was necessary for everything but it basically had to follow through the code if there were any crazy training procedures. I've seen many cases where there are wild and rogue sessions that I have to keep track of in order to follow what the users were doing. In TF2.X, there are no sessions. Just a gradientTape which tracks the final outputs and the final weights. And if you want the gradients, ask for them. That's it. Simple. I have included a minimal training example to showcase the major difference between TF1.X and TF2.X.","title":"Model Training"},{"location":"projects/BNNs/code/tensorflow/#tf-1x-static_1","text":"# Create session with tf . Session ( graph = lr_graph ) as sess : # Initialize all variables init = tf . global_variables_initializer () sess . run ( init ) # Training Loop for iepoch in range ( epochs ): # Run optimization in session feed_dict = { x : Xtrain , y : ytrain } sess . run ( optimizer , feed_dict = feed_dict ) # get losses loss , acc = sess . run ( [ loss_func , accuracy_func ], feed_dict = feed_dict )","title":"** TF 1.X - Static **"},{"location":"projects/BNNs/code/tensorflow/#tf-2x-dynamic_1","text":"for iepoch in range ( epochs ): with tf . GradientTape () as tape : # Forward pass ypreds = lr_model ( Xtrain ) # Loss loss = loss_func ( y , ypreds ) # compute gradients gradients = tape . gradient ( loss , lr_model . trainable_weights ) # update weights of linear layer optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) # update accuracy acc_func . update_state ( y , ypreds ) Source: Easy TensorFlow There are some more changes which you can read here .","title":"** TF 2.X - Dynamic **"},{"location":"projects/BNNs/code/tensorflow/#my-final-thoughts","text":"The biggest change I would say is the code standard and the clear spectrum of user cases. They are really promoting the keras way of defining Layers , Models , etc but you could also use Sequential or Functional . The rules are not absolute but this does set a nice \"standard way to do things\". This is a good thing . It's pythonic and readable. Some people are researchers and scientists whereas other people are computer scientists. But typically we'll be reading ML peoples code so we need a standard. That is if we plan on being a community and sharing. So I don't think we should spend time fighting DL libraries and reading sloppy code. We can spend more time devloping and solving more problems. One could argue that all of the details are hidden now and this promotes people just using stuff without understanding. But they're actually just optional to see. You can code from scratch if you want to. And yes, there will be many cases of people using models that they don't understand. But that's a choice and there will be barriers to prevent those people from thriving in the community. I personally think the changes are good and we shall see what the future holds for TF and DL software in general.","title":"My Final Thoughts"},{"location":"projects/BNNs/code/tensorflow/#my-favourite-resources","text":"These are my favourite resources. I've gone through almost all of them and I found that they did the best at explaining how to use TensorFlow. !> Remember , I am coming at this from a researchers perspective. So I am biased and the resources I've chosen assumes some prior knowledge about Python programming and Deep learning in general. I will list some resources","title":"My Favourite Resources"},{"location":"projects/BNNs/code/tensorflow/#francois-chollet","text":"The best resource I have found is from the founder of keras ( Francois Chollet ). He is a very outspoken individual who is very proud of keras and how it has changed the community. He also likes to make comparisons between frameworks but overall he is very passionate about his work. He is also super active on twitter and has some interesting opinions from time to time. The first tutorial is basically a notebook on using tf.keras from a deep learning perspective. I think he breaks it down quite nicely and goes through all of the important aspects that a researcher should know when using TensorFlow. If you are already familiar with TensorFlow I think you'll find almost every major point he makes useful (e.g. Callbacks , ) when you construct your neural networks. If you still don't see it after going through it, don't worry, it will come up. TensorFlow 2.0 + Keras Overview for Deep Learning Researchers - Colab Notebook tf.keras for Researchers: Crash Course - Colab Notebook Inside TensorFlow: tf.keras - Video 1 | Video 2","title":"Francois Chollet"},{"location":"projects/BNNs/code/tensorflow/#tensorflow-website","text":"There are a few really good tutorials on the TF website that give a really good overview of changes from TF 1.X TF 2.X as well as some more in-depth tutorials. I found the tutorials a bit difficult to navigate as there is a lot of repetition with the 'nuggets of wisdom' scattered everywhere. In addition, I find that the organization isn't really done based on the users level of knowledge. So I tried to outline how I think one should approach the tutorials based on three criteria: absolute beginner , Keras users , and PyTorch users which is my way of saying Beginner , Intermediate and Advanced . 1 Absolute Beginners Honestly, if you're just starting out with deep learning then you should probably just dive into it using a project or take your time and go through a course and/or book. I've listed my favourite books in the next section if you're interested but I will recommend this course which is sponsored by TensorFlow. I went through the first few lectures but I got a bit bored because I had already learned this stuff. But I like the balance of explanations and code. Introduction to TensorFlow for Deep Learning - Udacity If you have a bit more free time and dedication, I would recommend you go through the TensorFlow curriculum. They break the parts necessary for learning Deep learning with TF as your platform. It has books and video courses and I personally think it is organized very well. The course I listed above is included in the curriculum. TensorFlow Curriculums - Learn ML 2 Keras Users (Intermediate) Most people who apply DL models to data will be in this category. They will either want simple models with fairly straightforward setups or highly complex networks. In addition, they also may have complex training regimes. They all fall into this category and from this section, you should be able to get started. Keras Overview This is a fairly long tutorial that goes through keras from top to bottom. I wouldn't recommend reading the whole thing in one go as it is a bit overwhelming. If you want to do simple models, then only look at part 1 . And then for a quick overview of complex models, check out part 2 Keras Functional API I imagine most people who are experimenting with complex models with inputs and outputs in various locations will be here. Train and Evaluate with Keras This is another long guide that goes through how one can train your DL model. If you're not interested in too much training customization, then you'll probably mostly interest in part 1 where you use the built-in training module. Part 2 does things from scratch. !> MATLAB Users : Although I recommend you start in the absolute beginner section to get accustomed to Python, you will probably fall into this category as well. The Sequential API is very similar to the new DL toolbox in the latest versions of MATLAB. Unfortunately there is no GUI yet though... 3 PyTorch Users (Advanced) All my PyTorch and advanced Python users (me included) start here. You should feel right at home with TensorFlow using the subclassing . The distinction between Layer and Model is quite blurry but it's similar to the PyTorch nn.Module . In the end, it's super Pythonic so we should feel right at home. Finally... TF for Experts The is a 2-minute introduction to the language. If you are familiar with PyTorch then you will find this very familiar territory. It really highlights how the two packages converged. Writing Custom Layers and Models with Keras This next tutorial will go into more detail about the subclassing and how to build layers from scratch. It's very similar to PyTorch but there are a few more nuggets and subtleties that are unique to TensorFlow. Train and Evaluate with Keras This is another long guide that goes through how one can train your DL model. Pay special attention to part 2 where you build things from scratch as this is most similar to the PyTorch methods.","title":"TensorFlow Website"},{"location":"projects/BNNs/code/tensorflow/#books","text":"This is a bit old school but there have recently been a lot of good books released in the past 2 years that do a very good job at teaching you Machine learning (including Deep learning) from a programming perspective. They don't skip out on all of the theory but you won't find al of the derivations necessary to write the algorithms from scratch. A nice thing is that most of the books below have code on their github accounts that you can download and run yourself. Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow - Aurelien Geron (2019) - Book | Github This is a best selling book and I found it to be the best resources for getting a really good overview of ML with Python in general as well as a really extensive section on TF2.0 and keras. Deep Learning with Python - Francois Chollet (2018) - Book By the creator of Keras himself. It's a great book that goes step-by-step with lots of examples and lots of explanations. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2 - Raschka & Mirjalili (2019) - Book | Github Another great book that talks about DL as well as ML in general.","title":"Books"},{"location":"projects/BNNs/code/my_notes/tensorflow/","text":"TensorFlow 2.0 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com These are notes that I took based off of lectures 1 and 2 given by Francois Chollet. Architecture \u00b6 1. Engine Module \u00b6 This is basically the model definition. It has the following parts Layer Network - this contains the DAG of Layers (internal component) Model - this contains the network and is used to do the training and evaluation loops Sequential - wraps a list of layers 2 Various Classes (and subclasses) \u00b6 Layers Metric Loss Callback Optimizer Regularizers, Constraints? Layer Class \u00b6 This is the core abstraction in the API. Everything is a Layer or it at least interacts closely with the Layer . What can it do? \u00b6 Computation This manages the computation. It takes in batch inputs / batch outputs. Assumes no interactions between samples Eager or Graph execution Training and Inference model Masking (e.g. time series, missing features) Manages State This keeps track of what's trainable or not trainable. class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () self . weights = ... trainable self . bias = ... not trainable Track Losses & Metrics Up class Linear ( tf . keras . Layer ): def call ( self , x ): # calculate kl divergence kl_loss = ... # add loss self . add_loss ( ... ) Type Checking Frozen or UnFrozen (fine-tuning, batch-norm, GANS) Can build DAGs - Sequential Form Mixed Precio What do they not do? \u00b6 Gradients Device Placement Distribution-specific logic Only batch-wise computation. Basic Layer \u00b6 We are going to create a base layer # create linear layer class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # weights variable w_init = tf . random_normal_initializer ()( shape = ( input_dim , units )) self . w = tf . Variable ( initial_value = w_init , trainable = True ) # bias parameter b_init = tf . zeros_initializer ()( shape = ( units ,)) self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b # data x_train = tf . ones ( 2 , 2 ) # initialize linear layer linear_layer = Linear ( 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x ) Better Basic Layer \u00b6 class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , ** kwargs ): super () . __init__ () self . units = units Notice how we didn't construct the weights when we initialized the class (constructor). This is nice because now we can construct our layer without having to know what the input dimension will be. We can simply specify the units. Instead we create a build method and that has the weights specified. def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + self . b The rest doesn't change. We can initialize the liner layer just with the units. This is called 'Lazy loading' linear_layer = Linear ( 32 ) It will call .build(x.shape) to get the dimensions of the dataset. y = linear_layer ( x ) Nested Layers \u00b6 We can nest Layers (as many) layers as we want actually. For example: Multi-Layer Perceptron \u00b6 class MLP ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x MLP Block \u00b6 class MLPB ( Layer ): def __init__ ( self ): super () . __init__ () self . mlp_1 = MLPBlock ( 32 ) self . mlp_2 = MLPBlock ( 32 ) self . mlp_3 = MLPBlock ( 1 ) def call ( self , inputs ): x = self . mlp_1 ( x ) x = self . mlp_2 ( x ) return x = self . mlp_3 ( x ) Basic Training \u00b6 So assuming that we have our linear layer, we can do some basic training procedure. # initialize model lr_model = Linear ( 32 ) # loss function loss_fn = tf . keras . losses . MSELoss () # optimizer optimizer = tf . keras . optimizers . Adam () # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = linear_model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , lr_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , lr_model . trainable_weights )) Losses \u00b6 We can add losses on the fly. For example, we can add a small activation regularizer in the call function for the MLP layer that we made above: class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x Now when we call the layer, we get the activation loss. mlp_layer = MLP ( 32 ) y = mlp_layer ( x ) Now it gets reset everytime we call it. Modified Training Loop \u00b6 mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function optimizer = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions for minibatch loss = loss_fn ( y , preds ) # loss value for minibatch loss += sum ( mlp_model . losses ) # extra losses from forward pass # find gradients grads = tape . gradients ( loss , mlp_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , mlp_model . trainable_weights )) Useful for: KL-Divergence Weight Regularization Activation Regularization Note : There is some context. The inner layers are also reset when their parent layer is called. Serialization \u00b6 class Linear ( tf . keras . Layer ): def __init__ () ... def get_config ( self ): config super () . get_config () config . update ({ 'units' : self . units }) return config Training Mode \u00b6 Allows you to do training versus inference mode. You simply need to add an extra argument in the cal() method. ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Some good examples: Batch Normalization Probabilistic Models (MC Variational Inference) Model Class \u00b6 This handles top-level functionality. The Model class does everything the Layer class can do, i.e. it is the same except with more available methods. In the literature, we refer to this as a \"model\", e.g. a deep learning model, a machine learning model, or as a \"network\", e.g. a deep neural network. In the literature, we refer to a Layer as something with a closed sequence of operations. For example a convolutional layer or a recurrent layer. Sometimes we also refer layers within layers as a block. For example a ResNet block or an Attention block. So ultimately, you would define the Layer class to do the inner computation blocks and the Model class to do the outer model with what you do to train and save. Training functionality .compile() .fit() .evaulate() .predict() Saving We have the .save() method which includes: configuration (topology) state (weights) optimiser Summarization & Visualization .summary() plot_model() Compile \u00b6 This option give configurations: optimizer Loss When you have the model class and you run .compile() , you are running the graph in graph execution model. So you are basically compiling the graph. If we want to run it eagerly: we need to set the paramter run_eagerly to be True . mlp = MLP () mlp . compile ( optimizer = Adam (), loss = MSELoss (), run_eagerly = True ) Fit \u00b6 How the data will be fit: The training procedure. Callbacks Data Epochs Functional Model \u00b6","title":"TensorFlow 2.0"},{"location":"projects/BNNs/code/my_notes/tensorflow/#tensorflow-20","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com These are notes that I took based off of lectures 1 and 2 given by Francois Chollet.","title":"TensorFlow 2.0"},{"location":"projects/BNNs/code/my_notes/tensorflow/#architecture","text":"","title":"Architecture"},{"location":"projects/BNNs/code/my_notes/tensorflow/#1-engine-module","text":"This is basically the model definition. It has the following parts Layer Network - this contains the DAG of Layers (internal component) Model - this contains the network and is used to do the training and evaluation loops Sequential - wraps a list of layers","title":"1. Engine Module"},{"location":"projects/BNNs/code/my_notes/tensorflow/#2-various-classes-and-subclasses","text":"Layers Metric Loss Callback Optimizer Regularizers, Constraints?","title":"2 Various Classes (and subclasses)"},{"location":"projects/BNNs/code/my_notes/tensorflow/#layer-class","text":"This is the core abstraction in the API. Everything is a Layer or it at least interacts closely with the Layer .","title":"Layer Class"},{"location":"projects/BNNs/code/my_notes/tensorflow/#what-can-it-do","text":"Computation This manages the computation. It takes in batch inputs / batch outputs. Assumes no interactions between samples Eager or Graph execution Training and Inference model Masking (e.g. time series, missing features) Manages State This keeps track of what's trainable or not trainable. class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () self . weights = ... trainable self . bias = ... not trainable Track Losses & Metrics Up class Linear ( tf . keras . Layer ): def call ( self , x ): # calculate kl divergence kl_loss = ... # add loss self . add_loss ( ... ) Type Checking Frozen or UnFrozen (fine-tuning, batch-norm, GANS) Can build DAGs - Sequential Form Mixed Precio","title":"What can it do?"},{"location":"projects/BNNs/code/my_notes/tensorflow/#what-do-they-not-do","text":"Gradients Device Placement Distribution-specific logic Only batch-wise computation.","title":"What do they not do?"},{"location":"projects/BNNs/code/my_notes/tensorflow/#basic-layer","text":"We are going to create a base layer # create linear layer class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () # weights variable w_init = tf . random_normal_initializer ()( shape = ( input_dim , units )) self . w = tf . Variable ( initial_value = w_init , trainable = True ) # bias parameter b_init = tf . zeros_initializer ()( shape = ( units ,)) self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b # data x_train = tf . ones ( 2 , 2 ) # initialize linear layer linear_layer = Linear ( 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x )","title":"Basic Layer"},{"location":"projects/BNNs/code/my_notes/tensorflow/#better-basic-layer","text":"class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , ** kwargs ): super () . __init__ () self . units = units Notice how we didn't construct the weights when we initialized the class (constructor). This is nice because now we can construct our layer without having to know what the input dimension will be. We can simply specify the units. Instead we create a build method and that has the weights specified. def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + self . b The rest doesn't change. We can initialize the liner layer just with the units. This is called 'Lazy loading' linear_layer = Linear ( 32 ) It will call .build(x.shape) to get the dimensions of the dataset. y = linear_layer ( x )","title":"Better Basic Layer"},{"location":"projects/BNNs/code/my_notes/tensorflow/#nested-layers","text":"We can nest Layers (as many) layers as we want actually. For example:","title":"Nested Layers"},{"location":"projects/BNNs/code/my_notes/tensorflow/#multi-layer-perceptron","text":"class MLP ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x","title":"Multi-Layer Perceptron"},{"location":"projects/BNNs/code/my_notes/tensorflow/#mlp-block","text":"class MLPB ( Layer ): def __init__ ( self ): super () . __init__ () self . mlp_1 = MLPBlock ( 32 ) self . mlp_2 = MLPBlock ( 32 ) self . mlp_3 = MLPBlock ( 1 ) def call ( self , inputs ): x = self . mlp_1 ( x ) x = self . mlp_2 ( x ) return x = self . mlp_3 ( x )","title":"MLP Block"},{"location":"projects/BNNs/code/my_notes/tensorflow/#basic-training","text":"So assuming that we have our linear layer, we can do some basic training procedure. # initialize model lr_model = Linear ( 32 ) # loss function loss_fn = tf . keras . losses . MSELoss () # optimizer optimizer = tf . keras . optimizers . Adam () # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = linear_model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , lr_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , lr_model . trainable_weights ))","title":"Basic Training"},{"location":"projects/BNNs/code/my_notes/tensorflow/#losses","text":"We can add losses on the fly. For example, we can add a small activation regularizer in the call function for the MLP layer that we made above: class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x Now when we call the layer, we get the activation loss. mlp_layer = MLP ( 32 ) y = mlp_layer ( x ) Now it gets reset everytime we call it.","title":"Losses"},{"location":"projects/BNNs/code/my_notes/tensorflow/#modified-training-loop","text":"mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function optimizer = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions for minibatch loss = loss_fn ( y , preds ) # loss value for minibatch loss += sum ( mlp_model . losses ) # extra losses from forward pass # find gradients grads = tape . gradients ( loss , mlp_model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , mlp_model . trainable_weights )) Useful for: KL-Divergence Weight Regularization Activation Regularization Note : There is some context. The inner layers are also reset when their parent layer is called.","title":"Modified Training Loop"},{"location":"projects/BNNs/code/my_notes/tensorflow/#serialization","text":"class Linear ( tf . keras . Layer ): def __init__ () ... def get_config ( self ): config super () . get_config () config . update ({ 'units' : self . units }) return config","title":"Serialization"},{"location":"projects/BNNs/code/my_notes/tensorflow/#training-mode","text":"Allows you to do training versus inference mode. You simply need to add an extra argument in the cal() method. ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Some good examples: Batch Normalization Probabilistic Models (MC Variational Inference)","title":"Training Mode"},{"location":"projects/BNNs/code/my_notes/tensorflow/#model-class","text":"This handles top-level functionality. The Model class does everything the Layer class can do, i.e. it is the same except with more available methods. In the literature, we refer to this as a \"model\", e.g. a deep learning model, a machine learning model, or as a \"network\", e.g. a deep neural network. In the literature, we refer to a Layer as something with a closed sequence of operations. For example a convolutional layer or a recurrent layer. Sometimes we also refer layers within layers as a block. For example a ResNet block or an Attention block. So ultimately, you would define the Layer class to do the inner computation blocks and the Model class to do the outer model with what you do to train and save. Training functionality .compile() .fit() .evaulate() .predict() Saving We have the .save() method which includes: configuration (topology) state (weights) optimiser Summarization & Visualization .summary() plot_model()","title":"Model Class"},{"location":"projects/BNNs/code/my_notes/tensorflow/#compile","text":"This option give configurations: optimizer Loss When you have the model class and you run .compile() , you are running the graph in graph execution model. So you are basically compiling the graph. If we want to run it eagerly: we need to set the paramter run_eagerly to be True . mlp = MLP () mlp . compile ( optimizer = Adam (), loss = MSELoss (), run_eagerly = True )","title":"Compile"},{"location":"projects/BNNs/code/my_notes/tensorflow/#fit","text":"How the data will be fit: The training procedure. Callbacks Data Epochs","title":"Fit"},{"location":"projects/BNNs/code/my_notes/tensorflow/#functional-model","text":"","title":"Functional Model"},{"location":"projects/BNNs/other/explorers_2020_01_tf/","text":"Explorers Group: TF 2.X and PyTorch for not so Dummies \u00b6 Date : Tuesday, 21 Feb 2020 Time : 1600 Location : IPL OE Lead : J. Emmanuel Johnson Blurb \u00b6 TensorFlow (TF) is one of the most popular automatic differentiation (autograd) libraries in the world right now being used in production as well as research. Backed by Google (or Alphabet, Inc.), it has a huge company support system and the open-source community is massive so there is a ton of code available for a lot of the state-of-the-art (SOTA) machine learning algorithms. A more recent autograd library, PyTorch , was founded by Facebook and has risen to be the second most popular autograd library available. It's overtaken TensorFlow in the research industry because it is more Pythonic and it uses dynamic graphs which better suited the research community. TensorFlow recently got an update (TF 2.X) that has integrated keras (a high-level TF wrapper) and incorporated some more PyTorch-like design principles. So we will be going over some of the key features that you need to know to get you started on your Machine Learning and/or Deep Learning journey using TensorFlow and/or PyTorch. What To Expect \u00b6 This will be fairly high level but we will have some code examples. I will be presenting an overview of the nature/status of deep learning software and then we will be going through a google colab notebook outlining some of the key features of TensorFlow and PyTorch and answering any questions people have. It should take more more than 1.5 hours. Some other things: This will be in Python so familiarity with the language is expected and/or a strong familiarity with programming. I expect some level of machine learning and/or deep learning background to be able to keep up with some of the terminology, e.g. optimization, loss function, etc. This will not be a live coding session but you're more than welcome to bring your own laptops. Even if you think this is too high-level (or low-level) but are still interested, check the resources I've listed below. There might be some useful tutorials for you. Resources \u00b6 I have put the materials online. Please go through them if you get a chance. In particular I will be giving a short introduction about the nature of a deep learning library and we will spend the rest of the time walking through the key features of this colab notebook. The resources I've linked will be updated over the weekend and over the next few months so check back for updates if you're still interested. Introduction TensorFlow Spirit Animal \u00b6 Did you know that some Koala bears carry a strand of chlamydia (Herpes)? Some news reports even got so far as to kill them due to potential human infections. A moment of silence for the remaining 70% of the population of our moody, furry, cuddly friends as they deal with the crazy Austrailian wildfires .","title":"Explorers Group: TF 2.X and PyTorch for not so Dummies"},{"location":"projects/BNNs/other/explorers_2020_01_tf/#explorers-group-tf-2x-and-pytorch-for-not-so-dummies","text":"Date : Tuesday, 21 Feb 2020 Time : 1600 Location : IPL OE Lead : J. Emmanuel Johnson","title":"Explorers Group: TF 2.X and PyTorch for not so Dummies"},{"location":"projects/BNNs/other/explorers_2020_01_tf/#blurb","text":"TensorFlow (TF) is one of the most popular automatic differentiation (autograd) libraries in the world right now being used in production as well as research. Backed by Google (or Alphabet, Inc.), it has a huge company support system and the open-source community is massive so there is a ton of code available for a lot of the state-of-the-art (SOTA) machine learning algorithms. A more recent autograd library, PyTorch , was founded by Facebook and has risen to be the second most popular autograd library available. It's overtaken TensorFlow in the research industry because it is more Pythonic and it uses dynamic graphs which better suited the research community. TensorFlow recently got an update (TF 2.X) that has integrated keras (a high-level TF wrapper) and incorporated some more PyTorch-like design principles. So we will be going over some of the key features that you need to know to get you started on your Machine Learning and/or Deep Learning journey using TensorFlow and/or PyTorch.","title":"Blurb"},{"location":"projects/BNNs/other/explorers_2020_01_tf/#what-to-expect","text":"This will be fairly high level but we will have some code examples. I will be presenting an overview of the nature/status of deep learning software and then we will be going through a google colab notebook outlining some of the key features of TensorFlow and PyTorch and answering any questions people have. It should take more more than 1.5 hours. Some other things: This will be in Python so familiarity with the language is expected and/or a strong familiarity with programming. I expect some level of machine learning and/or deep learning background to be able to keep up with some of the terminology, e.g. optimization, loss function, etc. This will not be a live coding session but you're more than welcome to bring your own laptops. Even if you think this is too high-level (or low-level) but are still interested, check the resources I've listed below. There might be some useful tutorials for you.","title":"What To Expect"},{"location":"projects/BNNs/other/explorers_2020_01_tf/#resources","text":"I have put the materials online. Please go through them if you get a chance. In particular I will be giving a short introduction about the nature of a deep learning library and we will spend the rest of the time walking through the key features of this colab notebook. The resources I've linked will be updated over the weekend and over the next few months so check back for updates if you're still interested. Introduction TensorFlow","title":"Resources"},{"location":"projects/BNNs/other/explorers_2020_01_tf/#spirit-animal","text":"Did you know that some Koala bears carry a strand of chlamydia (Herpes)? Some news reports even got so far as to kill them due to potential human infections. A moment of silence for the remaining 70% of the population of our moody, furry, cuddly friends as they deal with the crazy Austrailian wildfires .","title":"Spirit Animal"},{"location":"projects/BNNs/other/people/","text":"Key Figures \u00b6 Yarin Gal - One key person who is behind the popular 'drop-out' technique.","title":"Key Figures"},{"location":"projects/BNNs/other/people/#key-figures","text":"Yarin Gal - One key person who is behind the popular 'drop-out' technique.","title":"Key Figures"},{"location":"projects/BNNs/other/polemica/","text":"Controversy \u00b6 Controversal Opinions \u00b6 Original Tweet Neil Lawrence Backlash Andrew Gordon Wilson [Rebutle]((https://twitter.com/andrewgwils/status/1210354001041969152?s=09) | Essay","title":"Controversy"},{"location":"projects/BNNs/other/polemica/#controversy","text":"","title":"Controversy"},{"location":"projects/BNNs/other/polemica/#controversal-opinions","text":"Original Tweet Neil Lawrence Backlash Andrew Gordon Wilson [Rebutle]((https://twitter.com/andrewgwils/status/1210354001041969152?s=09) | Essay","title":"Controversal Opinions"},{"location":"projects/BNNs/other/videos/","text":"Videos \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 2020-01-20 Talks \u00b6 Bayesian Deep Learning \u00b6 An Attempt at Demystifying Bayesian Deep Learning (2017) - Eric Ma, PyData - Youtube | Notebooks | Slides A nice and simple talk. Gives some background and motivation. Great figures. Bayesian Deep Learning: Primer (2019) - Wilson et. al. - Youtube Good talk. A bit advanced stuff later in the talk. But the beginning is great. Bayesian Neural Networks: A function space Tour - GPSS 2019 - Video | Slides An interesting talk from a GP perspective. TensorFlow Probability: Learning with Confidence - Dillion, TF Dev '19 - Youtube Bayesian Deep Learning - Gal, MLSS '19 - Part I | Part II | Part III Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS '19 - Part I | Part II | Part III Tutorials \u00b6 Bayesian Deep Learning - Mohammad Emtiyaz Khan, NeurIPS '19 - Part I | Part II | Slides Workshops \u00b6 DeepBayes 2019 - Playlist NeurIPS 2019 - Website","title":"Videos"},{"location":"projects/BNNs/other/videos/#videos","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 2020-01-20","title":"Videos"},{"location":"projects/BNNs/other/videos/#talks","text":"","title":"Talks"},{"location":"projects/BNNs/other/videos/#bayesian-deep-learning","text":"An Attempt at Demystifying Bayesian Deep Learning (2017) - Eric Ma, PyData - Youtube | Notebooks | Slides A nice and simple talk. Gives some background and motivation. Great figures. Bayesian Deep Learning: Primer (2019) - Wilson et. al. - Youtube Good talk. A bit advanced stuff later in the talk. But the beginning is great. Bayesian Neural Networks: A function space Tour - GPSS 2019 - Video | Slides An interesting talk from a GP perspective. TensorFlow Probability: Learning with Confidence - Dillion, TF Dev '19 - Youtube Bayesian Deep Learning - Gal, MLSS '19 - Part I | Part II | Part III Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS '19 - Part I | Part II | Part III","title":"Bayesian Deep Learning"},{"location":"projects/BNNs/other/videos/#tutorials","text":"Bayesian Deep Learning - Mohammad Emtiyaz Khan, NeurIPS '19 - Part I | Part II | Slides","title":"Tutorials"},{"location":"projects/BNNs/other/videos/#workshops","text":"DeepBayes 2019 - Playlist NeurIPS 2019 - Website","title":"Workshops"},{"location":"projects/BNNs/prezis/test/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson Second slide \u00b6 Best quote ever. Note: speaker notes FTW!","title":"TF2.X and PyTorch"},{"location":"projects/BNNs/prezis/test/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"projects/BNNs/prezis/test/#second-slide","text":"Best quote ever. Note: speaker notes FTW!","title":"Second slide"},{"location":"projects/BNNs/theory/","text":"","title":"Index"},{"location":"projects/BNNs/theory/optimization/","text":"On Empirical Comparisons of Optimizers for Deep Learning - (2019)","title":"Optimization"},{"location":"projects/BNNs/theory/papers/","text":"Papers \u00b6 This page highlights some of the key papers that we will go over during the working group. I have organized the sections into topics and each section will have the appropriate literature. I will keep the papers that we will go in detail near the top and any supporting papers will go at the bottom. !> Warning : This is not the final list. This is merely a guide that we will pivot off of. Bayesian \u00b6 The prior can generally only be understood in the context of the likelihood - Gelman et. al. (2017) - arxiv | blog Introduction to Bayesian Deep Learning \u00b6 Practical Deep Learning with Bayesian Principles - Warner & Neal (1997) - arxiv Towards Bayesian Deep Learning: A Survey - Wang et. al. (2016) Approximately Bayesian \u00b6 Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - Gal & Ghahramani - Paper | Code | Tutorial | Blog Computer Vision \u00b6 What Uncertainties do We Need for BDL for CV - Kendall & Gal (2017) - arxiv | A Comprehensive guide to Bayesian Convolutional NeuralNetwork with Variational Inference - Shridhar et. al. (2018) - arxiv | code (PyTorch)","title":"Papers"},{"location":"projects/BNNs/theory/papers/#papers","text":"This page highlights some of the key papers that we will go over during the working group. I have organized the sections into topics and each section will have the appropriate literature. I will keep the papers that we will go in detail near the top and any supporting papers will go at the bottom. !> Warning : This is not the final list. This is merely a guide that we will pivot off of.","title":"Papers"},{"location":"projects/BNNs/theory/papers/#bayesian","text":"The prior can generally only be understood in the context of the likelihood - Gelman et. al. (2017) - arxiv | blog","title":"Bayesian"},{"location":"projects/BNNs/theory/papers/#introduction-to-bayesian-deep-learning","text":"Practical Deep Learning with Bayesian Principles - Warner & Neal (1997) - arxiv Towards Bayesian Deep Learning: A Survey - Wang et. al. (2016)","title":"Introduction to Bayesian Deep Learning"},{"location":"projects/BNNs/theory/papers/#approximately-bayesian","text":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - Gal & Ghahramani - Paper | Code | Tutorial | Blog","title":"Approximately Bayesian"},{"location":"projects/BNNs/theory/papers/#computer-vision","text":"What Uncertainties do We Need for BDL for CV - Kendall & Gal (2017) - arxiv | A Comprehensive guide to Bayesian Convolutional NeuralNetwork with Variational Inference - Shridhar et. al. (2018) - arxiv | code (PyTorch)","title":"Computer Vision"},{"location":"projects/BNNs/theory/prob_nn/","text":"Neural Networks with Uncertainty \u00b6 Author: J. Emmanuel Johnson Date: 1st October, 2019 Synopsis \u00b6 This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression, What is Uncertainty? \u00b6 Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Distribution Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input. Uncertainty in the Error Generalization \u00b6 First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model. Uncertainty Over Functions \u00b6 In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 . Aleatoric Uncertainty, \\sigma^2 \\sigma^2 \u00b6 This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day... Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 \u00b6 The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find Overview of Architecture Types \u00b6 So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put Generic Neural Networks (NN) \u00b6 This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence Probabilistic Neural Networks (PNN) \u00b6 This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions. Learning: Maximum Likelihood \u00b6 Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} Final Layer \u00b6 This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric) Heteroscedastic Noise Model \u00b6 This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns . Gaussian Process (Deep Kernel Learning) \u00b6 Bayesian Benchmarks \u00b6 So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni Resources \u00b6 Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Neural Networks with Uncertainty"},{"location":"projects/BNNs/theory/prob_nn/#neural-networks-with-uncertainty","text":"Author: J. Emmanuel Johnson Date: 1st October, 2019","title":"Neural Networks with Uncertainty"},{"location":"projects/BNNs/theory/prob_nn/#synopsis","text":"This document will be my notes on how one can classify different neural network architectures with regards to how they deal with uncertainty measures. My inspiration for this document comes from two factors: My general interest in uncertainty (especially in the inputs which seems to be an unsolved problem) The new tensorflow 2.0, tensorflow probability packages with really good blog posts (e.g. here and here ) showing how one can use them to do probabilistic regression,","title":"Synopsis"},{"location":"projects/BNNs/theory/prob_nn/#what-is-uncertainty","text":"Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Distribution Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input.","title":"What is Uncertainty?"},{"location":"projects/BNNs/theory/prob_nn/#uncertainty-in-the-error-generalization","text":"First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model.","title":"Uncertainty in the Error Generalization"},{"location":"projects/BNNs/theory/prob_nn/#uncertainty-over-functions","text":"In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 .","title":"Uncertainty Over Functions"},{"location":"projects/BNNs/theory/prob_nn/#aleatoric-uncertainty-sigma2sigma2","text":"This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day...","title":"Aleatoric Uncertainty, \\sigma^2\\sigma^2"},{"location":"projects/BNNs/theory/prob_nn/#epistemic-uncertainty-nu_2nu_2","text":"The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find","title":"Epistemic Uncertainty, \\nu_{**}^2\\nu_{**}^2"},{"location":"projects/BNNs/theory/prob_nn/#overview-of-architecture-types","text":"So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well More concretely terms of what has distributions and what doesn't, we could classify them by where we put","title":"Overview of Architecture Types"},{"location":"projects/BNNs/theory/prob_nn/#generic-neural-networks-nn","text":"This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence","title":"Generic Neural Networks (NN)"},{"location":"projects/BNNs/theory/prob_nn/#probabilistic-neural-networks-pnn","text":"This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions.","title":"Probabilistic Neural Networks (PNN)"},{"location":"projects/BNNs/theory/prob_nn/#learning-maximum-likelihood","text":"Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned}","title":"Learning: Maximum Likelihood"},{"location":"projects/BNNs/theory/prob_nn/#final-layer","text":"This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric)","title":"Final Layer"},{"location":"projects/BNNs/theory/prob_nn/#heteroscedastic-noise-model","text":"This network is very similar to the above model except we assume that the noise varies as a function of the inputs. \\mathcal{N}(\\mu(x), \\sigma(x)) \\mathcal{N}(\\mu(x), \\sigma(x)) This accounts for the aleatoric uncertainty. So it's the same addition to the output layer mentioned above.is known as heterscedastic model. Again, in this blog post they classified this as a Known Unknowns .","title":"Heteroscedastic Noise Model"},{"location":"projects/BNNs/theory/prob_nn/#gaussian-process-deep-kernel-learning","text":"","title":"Gaussian Process (Deep Kernel Learning)"},{"location":"projects/BNNs/theory/prob_nn/#bayesian-benchmarks","text":"So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni","title":"Bayesian Benchmarks"},{"location":"projects/BNNs/theory/prob_nn/#resources","text":"Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"projects/BNNs/theory/resources/","text":"Uncertainty Decomposition in BNNs with Latent Variables - arxiv Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Discussions \u00b6 Variational Bayesian Inference vs Monte-Carlo Dropout for Uncertainty Quantification in DL - reddit So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni Resources \u00b6 Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"projects/BNNs/theory/resources/#discussions","text":"Variational Bayesian Inference vs Monte-Carlo Dropout for Uncertainty Quantification in DL - reddit So there are a few Benchmark datasets we can look at to determine Current top: MC Dropout Mean-Field Variational Inference Deep Ensembles Ensemble MC Dropout Benchmark Repos: OATML Hugh Salimbeni","title":"Discussions"},{"location":"projects/BNNs/theory/resources/#resources","text":"Neural Network Diagrams - stack MLSS 2019, Moscow - Yarin Gal - Prezi I | Prezi II Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - Blog Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - Blog How Bayesian Methods Embody Occam's razor - blog DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - blog Uncertainty Estimation in Supervised Learning - Video | Slides Blogs Regression with Probabilistic Layers in TensorFlow Probability Variational Inference for Bayesian Neural Networks (2019) | TensorFlow Brenden Hasz Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0 Yarin Gal What My Deep Model Doesn't Know... High Level Series of Posts Probabilistic Deep Learning: Bayes by Backprop When machine learning meets complexity: why Bayesian deep learning is unavoidable Bayesian Convolutional Neural Networks with Bayes by Backprop Reflections on Bayesian Inference in Probabilistic Deep Learning Software TensorFlow Probability Edward2 PyTorch Pyro Papers DropOut as Bayesian Approximation - Paper | Code | Tutorial Uncertainty Decomposition in BNNs with Latent Variables - arxiv Practical Deep Learning with Bayesian Principles - arxiv Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - Paper Probabilistic Numerics and Uncertainty in Computations - Paper Bayesian Inference of Log Determinants - Paper Code A Regression Master Class with Aboleth BNN Implementations - Github A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - Github","title":"Resources"},{"location":"projects/BNNs/theory/sota/","text":"State-of-the-Art \u00b6 SWAG - Video","title":"State-of-the-Art"},{"location":"projects/BNNs/theory/sota/#state-of-the-art","text":"SWAG - Video","title":"State-of-the-Art"},{"location":"projects/BNNs/theory/videos/","text":"Videos \u00b6 Introduction to Bayesian Deep Learning \u00b6","title":"Videos"},{"location":"projects/BNNs/theory/videos/#videos","text":"","title":"Videos"},{"location":"projects/BNNs/theory/videos/#introduction-to-bayesian-deep-learning","text":"","title":"Introduction to Bayesian Deep Learning"},{"location":"projects/ErrorGPs/","text":"Overview \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Repo: github.com/jejjohnson/uncertain_gps We will do a quick overview to show how we can account for input errors in Gaussian process regression models. Problem Statement \u00b6 Standard y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y where \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . Let \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} . Observe Noisy Estimates y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Observation Means only y = f(\\mu_\\mathbf{x}) + \\epsilon_y y = f(\\mu_\\mathbf{x}) + \\epsilon_y Posterior Predictions \u00b6 \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} Linearized Approximation \u00b6 Where we take the Taylor expansion of the predictive mean and variance function. The mean function stays the same: \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha but the predictive variance term gets changed slightly: \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} with the term in red being the derivative of the predictive mean function multiplied by the variance. Notes : * Assumes known variance * Assumes D\\times D D\\times D covariance matrix for multidimensional data * Quite inexpensive to implement * The 3rd term (the 2nd order component of the Taylor expansion) has been show to not make a huge difference egp_moment1 = jax . jfwd ( posterior , args_num = ( None , 0 )) egp_moment2 = jax . hessian ( posterior , args_num = ( None , 0 )) Moment-Matching \u00b6 Mean Predictions \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha where: q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] Variance Predictions \\nu_{eGP*}^2 \\nu_{eGP*}^2 Variational \u00b6 Assumes we have a variational distribution function \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] Other Resources \u00b6 Gaussian Process Model Zoo","title":"Overview"},{"location":"projects/ErrorGPs/#overview","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Repo: github.com/jejjohnson/uncertain_gps We will do a quick overview to show how we can account for input errors in Gaussian process regression models.","title":"Overview"},{"location":"projects/ErrorGPs/#problem-statement","text":"Standard y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y where \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . Let \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} \\mathbf{x} = \\mu_\\mathbf{x} + \\Sigma_\\mathbf{x} . Observe Noisy Estimates y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Observation Means only y = f(\\mu_\\mathbf{x}) + \\epsilon_y y = f(\\mu_\\mathbf{x}) + \\epsilon_y","title":"Problem Statement"},{"location":"projects/ErrorGPs/#posterior-predictions","text":"\\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\mu(\\mathbf{x}_*) = {\\bf k}_* ({\\bf K}+\\lambda {\\bf I}_N)^{-1}{\\bf y} = {\\bf k}_* \\alpha \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\nu_{GP*}^2 = \\sigma_y^2 + k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top}","title":"Posterior Predictions"},{"location":"projects/ErrorGPs/#linearized-approximation","text":"Where we take the Taylor expansion of the predictive mean and variance function. The mean function stays the same: \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = {\\bf k}_* \\alpha but the predictive variance term gets changed slightly: \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} \\nu_{eGP*}^2 = \\nu_{GP*}^2(\\mathbf{x}_*) + {\\nabla_{\\mu_*}\\Sigma_x\\nabla_{\\mu_*}^\\top} + \\text{Tr} \\left\\{ \\frac{\\partial^2 \\nu^2_{GP*}(\\mathbf{x_*})}{\\partial\\mathbf{x_*}\\partial\\mathbf{x_*}^\\top} \\bigg\\vert_{\\mathbf{x_*}=\\mu_{\\mathbf{x_*}}} \\Sigma_\\mathbf{x_*} \\right\\} with the term in red being the derivative of the predictive mean function multiplied by the variance. Notes : * Assumes known variance * Assumes D\\times D D\\times D covariance matrix for multidimensional data * Quite inexpensive to implement * The 3rd term (the 2nd order component of the Taylor expansion) has been show to not make a huge difference egp_moment1 = jax . jfwd ( posterior , args_num = ( None , 0 )) egp_moment2 = jax . hessian ( posterior , args_num = ( None , 0 ))","title":"Linearized Approximation"},{"location":"projects/ErrorGPs/#moment-matching","text":"Mean Predictions \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha \\mu_{eGP*}(\\mathbf{x_*}) = \\mathbf{q}^\\top \\alpha where: q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] q_i = |\\Lambda^{-1} \\Sigma_\\mathbf{x_*} + \\mathbf{I}|^{-1/2} \\exp\\left[ -\\frac{1}{2}(\\mu_* - \\mathbf{x}_i) (\\Sigma_\\mathbf{x_*}+\\Lambda)^{-1} (\\mu_* - \\mathbf{x}_j) \\right] Variance Predictions \\nu_{eGP*}^2 \\nu_{eGP*}^2","title":"Moment-Matching"},{"location":"projects/ErrorGPs/#variational","text":"Assumes we have a variational distribution function \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right] \\mathcal{L}(\\theta) = \\text{D}_{\\text{KL}}\\left[ q(\\mathbf{f})\\, q(\\mathbf{X}) || p(\\mathbf{f|X})\\, p(\\mathbf{X}) \\right]","title":"Variational"},{"location":"projects/ErrorGPs/#other-resources","text":"Gaussian Process Model Zoo","title":"Other Resources"},{"location":"projects/ErrorGPs/Code/software/","text":"Software \u00b6 GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch. GPy \u00b6 This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems. My Model Zoo \u00b6 Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github GPFlow \u00b6 This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting. Pyro \u00b6 This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research. My Model Zoo \u00b6 Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab GPyTorch \u00b6 This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications. Summary \u00b6 Algorithms Implemented \u00b6 Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Software"},{"location":"projects/ErrorGPs/Code/software/#software","text":"GPy My Model Zoo GPFlow Pyro My Model Zoo GPyTorch Summary Algorithms Implemented Right now there are a few Python packages that do handle uncertain inputs. I try to focus on the libraries that offer the most built-infunctionality but also are the most extensible. !> Note If you want more information regarding the software, then please look at my software guide to GPs located here . For more information specifically related to GPs for uncertain inputs, then keep reading. TLDR : * Like TensorFlow? Use GPFlow. * Like PyTorch? Use Pyro. * Lastest and greatest modern GPs? Use GPyTorch.","title":"Software"},{"location":"projects/ErrorGPs/Code/software/#gpy","text":"This library has a lot of the original algorithms available regarding uncertain inputs. It will host the classics such as the sparse variational GP which offers an argument to specify the input uncertainty. However, the backend is the same as the Bayesian GPLVM. This library hasn't been updated in a while so I don't recommend users to use this regularly outside of small data problems.","title":"GPy"},{"location":"projects/ErrorGPs/Code/software/#my-model-zoo","text":"Exact GP Linearized - github Sparse GP Linearized - github Bayesian GPLVM - github","title":"My Model Zoo"},{"location":"projects/ErrorGPs/Code/software/#gpflow","text":"This library is the successor to GPy that is built on TensorFlow and TensorFlow Probability. It now features more or less most of the original algorithms from the GPy library but it is much cleaner because a lot of the gradients are handled automatically by TensorFlow. It is a good defacto library for working with GPs in the research setting.","title":"GPFlow"},{"location":"projects/ErrorGPs/Code/software/#pyro","text":"This is a probabilistic library uses PyTorch as a backend. It features many inference algorithms such as Monte Carlo and Variational inference schemes. It has a barebones but really extensible GP library available. It is really easy to modify parameters and add prior distributions to whichever components is necessary. I find this library very easy to experiment with in my research.","title":"Pyro"},{"location":"projects/ErrorGPs/Code/software/#my-model-zoo_1","text":"Sparse GP - colab Variational GP - colab Stochastic Variational GP - colab","title":"My Model Zoo"},{"location":"projects/ErrorGPs/Code/software/#gpytorch","text":"This is a dedicated GP library with PyTorch as a backend. It has the most update features for using modern GPs. This also has some shared components with the Pyro library so it is now easier to modify parameters and add prior distributions. Right now, there is a bit of a learning curve if you want to use it outside of the use cases in the documentation. But, as they keep updating it, I'm sure utilizing it will get easier and easier; on par with Pyro or better. I recommend using this library when you want to move towards production or more extreme applications.","title":"GPyTorch"},{"location":"projects/ErrorGPs/Code/software/#summary","text":"","title":"Summary"},{"location":"projects/ErrorGPs/Code/software/#algorithms-implemented","text":"Package GPy GPFlow Pyro GPyTorch Linearized (Taylor) S S S S Exact Moment Matching GP \u2717 \u2717 \u2717 \u2717 Sparse Moment Matching GP \u2713 \u2717 \u2713 \u2717 Uncertain Variational GP \u2713 S S S Bayesian GPLVM \u2713 \u2713 \u2713 S Key Symbol Status \u2713 Implemented \u2717 Not Implemented S Supported","title":"Algorithms Implemented"},{"location":"projects/ErrorGPs/Overview/basics/","text":"Basics \u00b6 Data \u00b6 Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise. Model \u00b6 Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} We specify a mean function, m m and a covariance function k k . Likelihood , \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } which describes the dataset Marginal Likelihood , \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} . Posterior \u00b6 First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F}) Deterministic Inputs \u00b6 In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) Probabilistic Inputs \u00b6 In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} Variational GP Models \u00b6 Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) Sparse GP Models \u00b6 Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Basics"},{"location":"projects/ErrorGPs/Overview/basics/#basics","text":"","title":"Basics"},{"location":"projects/ErrorGPs/Overview/basics/#data","text":"Let's consider that we have the following relationship. y = f(\\mathbf{x}) + \\epsilon_y y = f(\\mathbf{x}) + \\epsilon_y Let's assume we have inputs with an additive noise term \\epsilon_y \\epsilon_y and let's assume that it is Gaussian distributed, \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2) . In this setting, we are not considering any input noise.","title":"Data"},{"location":"projects/ErrorGPs/Overview/basics/#model","text":"Given some training data \\mathbf{X},y \\mathbf{X},y , we are interested in the Bayesian formulation: p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} p(f| \\mathbf{X},y) = \\frac{\\color{blue}{p(y| f, \\mathbf{X})} \\,\\color{darkgreen}{p(f)}}{\\color{red}{ p(y| \\mathbf{X}) }} where we have: GP Prior , \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} \\color{darkgreen}{p(f) = \\mathcal{GP}(m, k)} We specify a mean function, m m and a covariance function k k . Likelihood , \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } \\color{blue}{ p(y| f, \\mathbf{X}) = \\mathcal{N}(f(\\mathbf{X}), \\sigma_y^2\\mathbf{I}) } which describes the dataset Marginal Likelihood , \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } \\color{red}{ p(y| \\mathbf{X}) = \\int_f p(y|f, \\mathbf{X}) \\, p(f|\\mathbf{X}) \\, df } Posterior , p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) p(f| \\mathbf{X},y) = \\mathcal{GP}(\\mu_\\text{GP}, \\nu^2_\\text{GP}) And the predictive functions \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} are: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu^2_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}, \\mathbf{x_*}) - k(\\mathbf{x_*}) \\,\\mathbf{K}_{GP}^{-1} \\, k(\\mathbf{x_*})^{\\top} \\end{aligned} where \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} \\mathbf{K}_\\text{GP}=k(\\mathbf{x,x}) + \\sigma_y^2 \\mathbf{I} .","title":"Model"},{"location":"projects/ErrorGPs/Overview/basics/#posterior","text":"First, let's look at the joint distribution: p(\\mathbf{X,Y,F}) p(\\mathbf{X,Y,F})","title":"Posterior"},{"location":"projects/ErrorGPs/Overview/basics/#deterministic-inputs","text":"In this integral, we don't need to propagate a distribution through the GP function. So it should be the standard and we only have to integrate our the function f and condition on our inputs \\mathbf{X} \\mathbf{X} . \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y|X}) &= \\int_f p(\\mathbf{Y,F|X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, df \\end{aligned} This is a known quantity where we have a closed-form solution to this: p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I}) p(\\mathbf{Y|X}) = \\mathcal{N}(\\mathbf{Y}|\\mathbf{0}, \\mathbf{K}+ \\sigma_y^2 \\mathbf{I})","title":"Deterministic Inputs"},{"location":"projects/ErrorGPs/Overview/basics/#probabilistic-inputs","text":"In this integral, we can no longer condition on the X X 's as they have a probabilistic function. So now we need to integrate them out in addition to the f f 's. \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned} \\begin{aligned} p(\\mathbf{Y}) &= \\int_f p(\\mathbf{Y,F,X})\\,df \\\\ &= \\int_f p(\\mathbf{Y|F}) \\, p(\\mathbf{F|X})\\, p(\\mathbf{X}) \\, df \\end{aligned}","title":"Probabilistic Inputs"},{"location":"projects/ErrorGPs/Overview/basics/#variational-gp-models","text":"Posterior Distribution: p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} <span><span class=\"MathJax_Preview\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F}</span><script type=\"math/tex\">p(\\mathbf{Y|X}) = \\int_{\\mathcal F} p(\\mathbf{Y|F}) p(\\mathbf{F|X}) d\\mathbf{F} Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF \\log p(Y|X) = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) dF importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF = \\log \\int_{\\mathcal F} p(Y|F) P(F|X) \\frac{q(F)}{q(F)}dF rearrange to isolate : p(Y|F) p(Y|F) and shorten notation to \\langle \\cdot \\rangle_{q(F)} \\langle \\cdot \\rangle_{q(F)} . = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} = \\log \\left\\langle \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log \\frac{p(Y|F)p(F|X)}{q(F)} \\right\\rangle_{q(F)} Split the logs \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} \\geq \\left\\langle \\log p(Y|F) + \\log \\frac{p(F|X)}{q(F)} \\right\\rangle_{q(F)} collect terms \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right) \\mathcal{L}_{1}(q)=\\left\\langle \\log p(Y|F)\\right\\rangle_{q(F)} - D_{KL} \\left( q(F) || p(F|X)\\right)","title":"Variational GP Models"},{"location":"projects/ErrorGPs/Overview/basics/#sparse-gp-models","text":"Let's build up the GP model from the variational inference perspective. We have the same GP prior as the standard GP regression model: \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) \\mathcal{P}(f) \\sim \\mathcal{GP}\\left(\\mathbf m_\\theta, \\mathbf K_\\theta \\right) We have the same GP likelihood which stems from the relationship between the inputs and the outputs: y = f(\\mathbf x) + \\epsilon_y y = f(\\mathbf x) + \\epsilon_y p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) p(y|f, \\mathbf{x}) = \\prod_{i=1}^{N}\\mathcal{P}\\left(y_i| f(\\mathbf x_i) \\right) \\sim \\mathcal{N}(f, \\sigma_y^2\\mathbf I) Now we just need an variational approximation to the GP posterior: q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) q(f(\\cdot)) = \\mathcal{GP}\\left( \\mu_\\text{GP}(\\cdot), \\nu^2_\\text{GP}(\\cdot, \\cdot) \\right) where q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) q(f) \\approx \\mathcal{P}(f|y, \\mathbf X) . \\mu \\mu and \\nu^2 \\nu^2 are functions that depend on the augmented space \\mathbf Z \\mathbf Z and possibly other parameters. Now, we can actually choose any \\mu \\mu and \\nu^2 \\nu^2 that we want. Typically people pick this to be Gaussian distributed which is augmented by some variable space \\mathcal{Z} \\mathcal{Z} with kernel functions to move us between spaces by a joint distribution; for example: \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} \\mu(\\mathbf x) = \\mathbf k(\\mathbf{x, Z})\\mathbf{k(Z,Z)}^{-1}\\mathbf m$$ $$\\nu^2(\\mathbf x) = \\mathbf k(\\mathbf{x,x}) - \\mathbf k(\\mathbf{x, Z})\\left( \\mathbf{k(Z,Z)}^{-1} - \\mathbf{k(Z,Z)}^{-1} \\Sigma \\mathbf{k(Z,Z)}^{-1}\\right)\\mathbf k(\\mathbf{x, Z})^{-1} where \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} \\theta = \\{ \\mathbf{Z, m_z, \\Sigma_z} \\} are all variational parameters. This formulation above is just the end result of using augmented values by using variational compression (see here for more details). In the end, all of these variables can be adjusted to reduce the KL divergence criteria KL \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] \\left[ q(f)||\\mathcal{P}(f|y, \\mathbf X)\\right] . There are some advantages to the approximate-prior approach for example: The approximation is non-parametric and mimics the true posterior. As the number of inducing points grow, we arrive closer to the real distribution The pseudo-points \\mathbf Z \\mathbf Z and the amount are also parameters which can protect us from overfitting. The predictions are clear as we just need to evaluate the approximate GP posterior. Sources Sparse GPs: Approximate the Posterior, Not the Model - James Hensman (2017) - blog","title":"Sparse GP Models"},{"location":"projects/ErrorGPs/Overview/literature/","text":"Literature Review \u00b6 Motivation \u00b6 This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs. Algorithms \u00b6 Error-In-Variables Regression \u00b6 This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ] Monte Carlo Sampling \u00b6 So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this. Taylor Expansion \u00b6 Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ] Moment Matching \u00b6 This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end) Covariance Functions \u00b6 Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code Iterative \u00b6 Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR Linearized (Unscented) Approximation \u00b6 This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach. Heteroscedastic Likelihood Models \u00b6 Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code Latent Variable Models \u00b6 Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016) Latent Covariates \u00b6 Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) Variational Strategies \u00b6 Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Appendix \u00b6 Kernel Expectations \u00b6 So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs. Literature \u00b6 Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel Toolboxes \u00b6 Emukit Connecting Concepts \u00b6 Moment Matching \u00b6 Derivatives of GPs \u00b6 Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018) Extended Kalman Filter \u00b6 This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code Uncertain Inputs in other ML fields \u00b6 Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation Key Equations \u00b6 Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Literature Review"},{"location":"projects/ErrorGPs/Overview/literature/#literature-review","text":"","title":"Literature Review"},{"location":"projects/ErrorGPs/Overview/literature/#motivation","text":"This is my complete literature review of all the ways the GPs have been modified to allow for uncertain inputs.","title":"Motivation"},{"location":"projects/ErrorGPs/Overview/literature/#algorithms","text":"","title":"Algorithms"},{"location":"projects/ErrorGPs/Overview/literature/#error-in-variables-regression","text":"This isn't really GPs per say but it is probably the first few papers that actually publish about this problem in the Bayesian community (that we know of). Bayesian Analysis of Error-in-Variables Regression Models - Dellaportas & Stephens (1995) Error in Variables Regression: What is the Appropriate Model? - Gillard et. al. (2007) [ Thesis ]","title":"Error-In-Variables Regression"},{"location":"projects/ErrorGPs/Overview/literature/#monte-carlo-sampling","text":"So almost all of the papers in the first few years mention that you can do this. But I haven't seen a paper explicitly walking through the pros and cons of doing this. However, you can see the most implementations of the PILCO method as well as the Deep GP method do implement some form of this.","title":"Monte Carlo Sampling"},{"location":"projects/ErrorGPs/Overview/literature/#taylor-expansion","text":"Learning a Gaussian Process Model with Uncertain Inputs - Girard & Murray-Smith (2003) [ Technical Report ]","title":"Taylor Expansion"},{"location":"projects/ErrorGPs/Overview/literature/#moment-matching","text":"This is where we approximate the mean function and the predictive variance function to be Gaussian by taking the mean and variance (the moments needed to describe the distribution). $$\\begin{aligned} m(\\mu_{x_*}, \\Sigma_{x_*}) &= \\mu(\\mu_{x_*})\\\\ v(\\mu_{x_*}, \\Sigma_{x_*}) &= \\nu^2(\\mu_{x_*}) + \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*}^\\top \\Sigma_{x_*} \\frac{\\partial \\mu(\\mu_{x_*})}{\\partial x_*} + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned}$$ Gaussian Process Priors With Uncertain Inputs \u2013 Application to Multiple-Step Ahead Time Series Forecasting - Girard et. al. (2003) Approximate Methods for Propagation of Uncertainty in GP Models - Girard (2004) [ Thesis ] Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting - Quinonero-Candela et. al. (2003) [ Technical Report ] Analytic moment-based Gaussian process filtering - Deisenroth et. al. (2009) PILCO: A Model-Based and Data-Efficient Approach to Policy Search - Deisenroth et. al. (2011) Code - TensorFlow | GPyTorch | MXFusion I | MXFusion II Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs (Lit review at the end)","title":"Moment Matching"},{"location":"projects/ErrorGPs/Overview/literature/#covariance-functions","text":"Daillaire constructed a modification to the RBF covariance function that takes into account the input noise. $$K_{ij} = \\left| 2\\Lambda^{-1}\\Sigma_x + I \\right|^{1/2} \\sigma_f^2 \\exp\\left( -\\frac{1}{2}(x_i - x_j)^\\top (\\Lambda + 2\\Sigma_x)^{-1}(x_i - x_j) \\right)$$ for $i\\neq j$ and $$K_{ij}=\\sigma_f^2$$ for $i=j$. This was shown to have bad results if this $\\Sigma_x$ is not known. You can see the full explanation in the thesis of McHutchon (section 2.2.1) which can be found in Iterative section below. An approximate inference with Gaussian process to latent functions from uncertain data - Dallaire et. al. (2011) | Prezi | Code","title":"Covariance Functions"},{"location":"projects/ErrorGPs/Overview/literature/#iterative","text":"Gaussian Process Training with Input Noise - McHutchon & Rasmussen (2011) | Code Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Chapter IV - Finding Uncertain Patterns in GPs System Identification through Online Sparse Gaussian Process Regression with Input Noise - Bijl et. al. (2017) | Code Gaussian Process Regression Techniques - Bijl (2018) [ Thesis ] | Code Chapter V - Noisy Input GPR","title":"Iterative"},{"location":"projects/ErrorGPs/Overview/literature/#linearized-unscented-approximation","text":"This is the linearized version of the Moment-Matching approach mentioned above. Also known as unscented GP. In this approximation, we only change the predictive variance. You can find an example colab notebook here with an example of how to use this with the GPy library. $$\\begin{aligned} \\tilde{\\mu}_f(x_*) &= \\underbrace{k_*^\\top K^{-1}y}_{\\mu_f(x_*)} \\\\ \\tilde{\\nu}^2(x_*) &= \\underbrace{k_{**} - k_*^\\top K^{-1} k_*}_{\\nu^2(x_*)} + \\partial \\mu_f \\text{ } \\Sigma_x \\text{ } \\partial \\mu_f^\\top \\end{aligned}$$ **Note**: The inspiration of this comes from the Extended Kalman Filter (links below) which tries to find an approximation to a non-linear transformation, $f$ of $x$ when $x$ comes from a distribution $x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)$. GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction - Ko and Fox (2008) They originally came up with the linearized (unscented) approximation to the moment-matching method. They used it in the context of the extended Kalman filter which has a few more elaborate steps in addition to the input uncertainty propagation. Expectation Propagation in Gaussian Process Dynamical Systems - Deisenroth & Mohamed (2012) The authors use expectation propagation as a way to propagate the noise through the test points. They mention the two ways to account for the input uncertainty referencing the GP-BayesFilters paper above: explicit moment-matching and the linearized (unscented) version. They also give the interpretation that the Moment-Matching approach with the kernel expectations is analogous to doing the KL-Divergence between prior distribution with the uncertain inputs p(x) p(x) and the approximate distribution q(x) q(x) . Accounting for Input Noise in Gaussian Process Parameter Retrieval - Johnson et. al. (2019) My paper where I use the unscented version to get better predictive uncertainty estimates. Note : I didn't know about the unscented stuff until after the publication...unfortunately. Unscented Gaussian Process Latent Variable Model: learning from uncertain inputs with intractable kernels - Souza et. al. (2019) [ arxiv ] A very recent paper that's been on arxiv for a while. They give a formulation for approximating the linearized (unscented) version of the moment matching approach. Apparently it works better that the quadrature, monte carlo and the kernel expectations approach.","title":"Linearized (Unscented) Approximation"},{"location":"projects/ErrorGPs/Overview/literature/#heteroscedastic-likelihood-models","text":"Heteroscedastic Gaussian Process Regression - Le et. al. (2005) Most Likely Heteroscedastic Gaussian Process Regression - Kersting et al (2007) Variational Heteroscedastic Gaussian Process Regression - L\u00e1zaro-Gredilla & Titsias (2011) Heteroscedastic Gaussian Processes for Uncertain and Incomplete Data - Almosallam (2017) [ Thesis ] Large-scale Heteroscedastic Regression via Gaussian Process - Lui et. al. (2019) [ arxiv ] | Code","title":"Heteroscedastic Likelihood Models"},{"location":"projects/ErrorGPs/Overview/literature/#latent-variable-models","text":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data - Lawrence (2004) Generic Inference in Latent Gaussian Process Models - Bonilla et. al. (2016) A review on Gaussian Process Latent Variable Models - Li & Chen (2016)","title":"Latent Variable Models"},{"location":"projects/ErrorGPs/Overview/literature/#latent-covariates","text":"Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals - Wang & Neal (2012) Gaussian Process Conditional Density Estimation - Dutordoir et. al. (2018) Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models - Martens et. al. (2019) Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019)","title":"Latent Covariates"},{"location":"projects/ErrorGPs/Overview/literature/#variational-strategies","text":"Bayesian Gaussian Process Latent Variable Model - Titsias & Lawrence (2010) Nonlinear Modelling and Control using GPs - McHutchon (2014) [ Thesis ] Variational Inference for Uncertainty on the Inputs of Gaussian Process Models - Damianou et. al. (2014) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) [ Thesis ] Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Processes Non-Stationary Surrogate Modeling with Deep Gaussian - Dutordoir (2016) [ Thesis ] > This is a good thesis that walks through the derivations of the moment matching approach and the Bayesian GPLVM approach. It becomes a little clearer how they are related after going through the derivations once. Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) [ Thesis ] Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Overview/literature/#appendix","text":"","title":"Appendix"},{"location":"projects/ErrorGPs/Overview/literature/#kernel-expectations","text":"So [Girard 2003] came up with a name of something we call kernel expectations \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} \\{\\mathbf{\\xi, \\Omega, \\Phi}\\} -statistics. These are basically calculated by taking the expectation of a kernel or product of two kernels w.r.t. some distribution. Typically this distribution is normal but in the variational literature it is a variational distribution. The three kernel expectations that surface are: $$\\mathbf \\xi(\\mathbf{\\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf x)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Omega(\\mathbf{y, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ $$\\mathbf \\Phi(\\mathbf{y, z, \\mu, \\Sigma}) = \\int_X \\mathbf k(\\mathbf x, \\mathbf y)k(\\mathbf x, \\mathbf z)\\mathcal{N}(\\mathbf x|\\mathbf \\mu,\\mathbf \\Sigma)d\\mathbf x$$ To my knowledge, I only know of the following kernels that have analytically calculated sufficient statistics: Linear, RBF, ARD and Spectral Mixture. And furthermore, the connection is how these kernel statistics show up in many other GP literature than just uncertain inputs of GPs; for example in Bayesian GP-LVMs and Deep GPs.","title":"Kernel Expectations"},{"location":"projects/ErrorGPs/Overview/literature/#literature","text":"Oxford M: Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature - Gunter et. al. (2014) Batch Selection for Parallelisation of Bayesian Quadrature - Code Pr\u00fcher et. al On the use of gradient information in Gaussian process quadratures (2016) > A nice introduction to moments in the context of Gaussian distributions. Gaussian Process Quadrature Moment Transform (2017) Student-t Process Quadratures for Filtering of Non-linear Systems with Heavy-tailed Noise (2017) Code: Nonlinear Sigma-Point Kalman Filters based on Bayesian Quadrature This includes an implementation of the nonlinear Sigma-Point Kalman filter. Includes implementations of the Moment Transform Linearized Moment Transform MC Transform Sigma Point Transform , Spherical Radial Transform Unscented Transform Gaussian Hermite Transform Fully Symmetric Student T Transform And a few experimental transforms: Truncated Transforms: Sigma Point Transform Spherical Radial Unscented Gaussian Hermite Taylor GPQ+D w. RBF Kernel","title":"Literature"},{"location":"projects/ErrorGPs/Overview/literature/#toolboxes","text":"Emukit","title":"Toolboxes"},{"location":"projects/ErrorGPs/Overview/literature/#connecting-concepts","text":"","title":"Connecting Concepts"},{"location":"projects/ErrorGPs/Overview/literature/#moment-matching_1","text":"","title":"Moment Matching"},{"location":"projects/ErrorGPs/Overview/literature/#derivatives-of-gps","text":"Derivative observations in Gaussian Process Models of Dynamic Systems - Solak et. al. (2003) Differentiating GPs - McHutchon (2013) A nice PDF with the step-by-step calculations for taking derivatives of the linear and RBF kernels. Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature - Wu et. al. (2018)","title":"Derivatives of GPs"},{"location":"projects/ErrorGPs/Overview/literature/#extended-kalman-filter","text":"This is the origination of the Unscented transformation applied to GPs. It takes the Taylor approximation of your function Wikipedia Blog Posts by Harveen Singh - Kalman Filter | Unscented Kalman Filter | Extended Kalman Filter Intro to Kalman Filter and Its Applications - Kim & Bang (2018) Tutorial - Terejanu Videos Lecture by Cyrill Stachniss Lecture by Robotics Course | Notes Lecture explained with Python Code","title":"Extended Kalman Filter"},{"location":"projects/ErrorGPs/Overview/literature/#uncertain-inputs-in-other-ml-fields","text":"Statistical Rethinking Course Page Lecture | Slides | PyMC3 Implementation","title":"Uncertain Inputs in other ML fields"},{"location":"projects/ErrorGPs/Overview/literature/#key-equations","text":"Predictive Mean and Variance for Latent Function, f $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$ Predictive Mean and Variance for mean output, y $$\\mu_f(x_*) = k_*^\\top K^{-1}y$$ $$\\sigma^2_f(x_*) = k_{**} - k_*^\\top K^{-1} k_*$$","title":"Key Equations"},{"location":"projects/ErrorGPs/Overview/next/","text":"Next Steps \u00b6 So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen: 1. Apply these algorithms to different problems (other than dynamical systems) \u00b6 It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it? 2. Improve the Kernel Expectation Calculations \u00b6 So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there. 3. Think about the problem differently \u00b6 An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too! 4. Think about pragmatic solutions \u00b6 Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free. 5. Figure Out how to extend it to Deep GPs \u00b6 So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"Next Steps"},{"location":"projects/ErrorGPs/Overview/next/#next-steps","text":"So after all of this literature, what is the next step for the community? I have a few suggestions based on what I've seen:","title":"Next Steps"},{"location":"projects/ErrorGPs/Overview/next/#1-apply-these-algorithms-to-different-problems-other-than-dynamical-systems","text":"It's clear to me that there are a LOT of different algorithms. But in almost every study above, I don't see many applications outside of dynamical systems. I would love to see other people outside (or within) community use these algorithms on different problems. Like Neil Lawrence said in a recent MLSS talk; \"we need to stop jacking around with GPs and actually apply them \" (paraphrased). There are many little goodies to be had from all of these methods; like the linearized GP predictive variance estimate for better variance estimates is something you get almost for free. So why not use it?","title":"1. Apply these algorithms to different problems (other than dynamical systems)"},{"location":"projects/ErrorGPs/Overview/next/#2-improve-the-kernel-expectation-calculations","text":"So how we calculate kernel expectations is costly. A typical sparse GP has a cost of O(NM^2) O(NM^2) . But when we do the calculation of kernel expectations, that order goes back up to O (DNM^2) O (DNM^2) . It's not bad considering but it is still now an order of magnitude larger for high dimensional datasets. This is going backwards in terms of efficiency. Also, many implementations attempt to do this in parallel for speed but then the cost of memory becomes prohibitive (especially on GPUs). There are some other good approximation schemes we might be able to use such as advanced Bayesian Quadrature techniques and the many moment transformation techniques that are present in the Kalman Filter literature. I'm sure there are tricks of the trade to be had there.","title":"2. Improve the Kernel Expectation Calculations"},{"location":"projects/ErrorGPs/Overview/next/#3-think-about-the-problem-differently","text":"An interesting way to approach the method is to perhaps use the idea of covariates. Instead of the noise being additive, perhaps it's another combination where we have to model it separately. That's what Salimbeni did for his latest Deep GP and it's a very interesting way to look at it. It works well too!","title":"3. Think about the problem differently"},{"location":"projects/ErrorGPs/Overview/next/#4-think-about-pragmatic-solutions","text":"Some of these algorithms are super complicated. It makes it less desireable to actually try them because it's so easy to get lost in the mathematics of it all. I like pragmatic solutions. For example, using Drop-Out, Ensembles and Noise Constrastive Priors are easy and pragmatic ways of adding reliable uncertainty estimates in Bayesian Neural Networks. I would like some more pragmatic solutions for some of these methods that have been listed above. Another Shameless Plug : the method I used is very easy to get better predictive variances almost for free.","title":"4. Think about pragmatic solutions"},{"location":"projects/ErrorGPs/Overview/next/#5-figure-out-how-to-extend-it-to-deep-gps","text":"So the original Deep GP is just a stack of BGPLVMs and more recent GPs have regressed back to stacking SVGPs. I would like to know if there is a way to improve the BGPLVM in such a way that we can stack them again and then constrain the solutions with our known prior distributions.","title":"5. Figure Out how to extend it to Deep GPs"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/","text":"Error Propagation \u00b6 Taylor Series Expansion \u00b6 A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) Law of Error Propagation \u00b6 This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} Proof: Mean Function \u00b6 Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} Proof: Variance Function \u00b6 Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself. Resources \u00b6 Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#error-propagation","text":"","title":"Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#taylor-series-expansion","text":"A Taylor series is representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point - Wiki Often times we come across functions that are very difficult to compute analytically. Below we have the simple first-order Taylor series approximation. Let's take some function f(\\mathbf x) f(\\mathbf x) where \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) \\mathbf{x} \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) described by a mean \\mu_\\mathbf{x} \\mu_\\mathbf{x} and covariance \\Sigma_\\mathbf{x} \\Sigma_\\mathbf{x} . The Taylor series expansion around the function f(\\mathbf x) f(\\mathbf x) is: \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\mathbf z = f(\\mathbf x) \\approx f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right)","title":"Taylor Series Expansion"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#law-of-error-propagation","text":"This results in a mean and error covariance of the new distribution \\mathbf z \\mathbf z defined by: \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\mu_{\\mathbf z} = f(\\mu_{\\mathbf x}) \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top} \\Sigma_\\mathbf{z} = \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x}) \\; \\Sigma_\\mathbf{x} \\; \\nabla_\\mathbf{x} f(\\mu_{\\mathbf x})^{\\top}","title":"Law of Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#proof-mean-function","text":"Given the mean function: \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i \\mathbb{E}[\\mathbf{x}] = \\frac{1}{N} \\sum_{i=1} x_i We can simply apply this to the first-order Taylor series function. \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned} \\begin{aligned} \\mu_\\mathbf{z} &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= \\mathbb{E}_{\\mathbf{x}} \\left[ f(\\mu_{\\mathbf x}) \\right] + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\left( \\mathbf x - \\mu_{\\mathbf x} \\right) \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mathbf x \\right]- \\mathbb{E}_{\\mathbf{x}} \\left[ \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\right] \\\\ &= f(\\mu_{\\mathbf x}) + \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}} \\mu_\\mathbf{x} - \\frac{\\partial f}{\\partial \\mathbf x} \\bigg\\vert_{\\mathbf{x} = \\mu_\\mathbf{x}}\\mu_{\\mathbf x} \\\\ &= f(\\mu_{\\mathbf x}) \\\\ \\end{aligned}","title":"Proof: Mean Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#proof-variance-function","text":"Given the variance function \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\mathbb{V}[\\mathbf{x}] = \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x} \\right]^2 \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} \\begin{aligned} \\sigma_\\mathbf{z}^2 &= \\mathbb{E} \\left[ f(\\mu_\\mathbf{x}) - \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x}) - \\mu_\\mathbf{x} \\right] \\\\ &= \\mathbb{E} \\left[ \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} (\\mathbf{x} - \\mu_\\mathbf{x})\\right]^2 \\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\mathbb{E}\\left[ \\mathbf{x} - \\mu_\\mathbf{x}\\right]^2\\\\ &= \\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\bigg\\vert_{\\mathbf{x}=\\mu_\\mathbf{x}} \\right)^2 \\Sigma_\\mathbf{x} \\end{aligned} I've linked a nice tutorial for propagating variances below if you would like to go through the derivations yourself.","title":"Proof: Variance Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/error_propagation/#resources","text":"Essence of Calculus, Chapter 11 | Taylor Series - 3Blue1Brown - youtube Introduction to Error Propagation: Derivation, Meaning and Examples - PDF Statistical uncertainty and error propagation - Vermeer - PDF","title":"Resources"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/","text":"Linearized GP \u00b6 Analytical Moments \u00b6 The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the Mean Function \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} Variance Function \u00b6 The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} Taylor Approximation \u00b6 We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} Linearized Predictive Mean and Variance \u00b6 \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term. Conditional Gaussian Distributions \u00b6 I: Additive Noise Model ( x,f x,f ) \u00b6 This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{ } K_{GP}^{-1}K_{ }^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top . Other GP Methods \u00b6 We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c. II: Non-Additive Noise Model \u00b6 III: Quadratic Approximation \u00b6 Literature \u00b6 Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference. Supplementary \u00b6 Error Propagation \u00b6 To see more about error propagation and the relation to the mean and variance, see here . Fubini's Theorem \u00b6 Law of Iterated Expecations \u00b6 Conditional Variance \u00b6","title":"Linearized GP"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#linearized-gp","text":"","title":"Linearized GP"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#analytical-moments","text":"The posterior of this distribution is non-Gaussian because we have to propagate a probability distribution through a non-linear kernel function. So this integral becomes intractable. We can compute the analytical Gaussian approximation by only computing the mean and the variance of the","title":"Analytical Moments"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#mean-function","text":"\\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_{f_*} \\left[ f_* \\,p(f_* | \\mathbf{x_*}) \\right]\\right]\\\\ &= \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right] \\end{aligned}","title":"Mean Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#variance-function","text":"The variance term is a bit more complex. \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned} \\begin{aligned} v(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) &= \\mathbb{E}_\\mathbf{f_*} \\left[ f_*^2 \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{f_*} \\left[ f_* \\, \\mathbb{E}_\\mathbf{x_*} \\left[ p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_*^2 \\, p(f_*|\\mathbf{x}_*) \\right] \\right] - \\left(\\mathbb{E}_\\mathbf{x_*} \\left[ \\mathbb{E}_\\mathbf{x_*} \\left[ f_* \\, p(f_*|\\mathbf{x}_*) \\right] \\right]\\right)^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) + \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2 \\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{E}_\\mathbf{x_*} \\left[ \\mu_\\text{GP}^2(\\mathbf{x}_*) \\right] - \\mathbb{E}_{x_*}\\left[ \\mu_\\text{GP}(\\mathbf{x_*}) \\right]^2\\\\ &= \\mathbb{E}_\\mathbf{x_*} \\left[ \\sigma_\\text{GP}^2(\\mathbf{x}_*) \\right] + \\mathbb{V}_\\mathbf{x_*} \\left[\\mu_\\text{GP}(\\mathbf{x}_*) \\right] \\end{aligned}","title":"Variance Function"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#taylor-approximation","text":"We will approximate our mean and variance function via a Taylor Expansion. First the mean function: \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\mu = \\mu_\\text{GP}(\\mathbf{x_*})= \\mu_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\mu_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} and then the variance function: \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned} \\begin{aligned} \\mathbf{z}_\\sigma = \\nu^2_\\text{GP}(\\mathbf{x_*})= \\nu^2_\\text{GP}(\\mu_\\mathbf{x_*}) + \\nabla \\nu^2_\\text{GP}\\bigg\\vert_{\\mathbf{x}_* = \\mu_\\mathbf{x}} (\\mathbf{x}_* - \\mu_\\mathbf{x_*}) + \\mathcal{O} (\\mathbf{x_*}^2) \\end{aligned}","title":"Taylor Approximation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#linearized-predictive-mean-and-variance","text":"\\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} \\begin{aligned} m(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\mu_\\text{GP}(\\mu_\\mathbf{x_*})\\\\ v(\\mu_\\mathbf{x_*}, \\Sigma_\\mathbf{x_*}) &= \\nu^2_\\text{GP}(\\mu_{x_*}) + \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*})^\\top \\Sigma_{x_*} \\nabla_\\mathbf{x_*} \\mu_\\text{GP}(\\mu_{x_*}) + \\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\nu^2(\\mu_{x_*})}{\\partial x_* \\partial x_*^\\top} \\Sigma_{x_*}\\right\\} \\end{aligned} where \\nabla_x \\nabla_x is the gradient of the function f(\\mu_x) f(\\mu_x) w.r.t. x x and \\nabla_x^2 f(\\mu_x) \\nabla_x^2 f(\\mu_x) is the second derivative (the Hessian) of the function f(\\mu_x) f(\\mu_x) w.r.t. x x . This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough. Practically speaking, this leaves us with the following predictive mean and variance functions: \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} \\begin{aligned} \\mu_\\text{GP}(\\mathbf{x_*}) &= k(\\mathbf{x_*}) \\, \\mathbf{K}_{GP}^{-1}y=k(\\mathbf{x_*}) \\, \\alpha \\\\ \\nu_{GP}^2(\\mathbf{x_*}) &= \\sigma_y^2 + {\\color{red}{\\nabla_{\\mu_\\text{GP}}\\,\\Sigma_\\mathbf{x_*} \\,\\nabla_{\\mu_\\text{GP}}^\\top} }+ k_{**}- {\\bf k}_* ({\\bf K}+\\sigma_y^2 \\mathbf{I}_N )^{-1} {\\bf k}_{*}^{\\top} \\end{aligned} As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.","title":"Linearized Predictive Mean and Variance"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#conditional-gaussian-distributions","text":"","title":"Conditional Gaussian Distributions"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#i-additive-noise-model-xfxf","text":"This is the noise $$ \\begin{bmatrix} x \\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mu_{x} \\ \\mu_{y} \\end{bmatrix}, \\begin{bmatrix} \\Sigma_x & C \\ C^\\top & \\Pi \\end{bmatrix} \\right) $$ where $$ \\begin{aligned} \\mu_y &= f(\\mu_x) \\ \\Pi &= \\nabla_x f(\\mu_x) : \\Sigma_x : \\nabla_x f(\\mu_x)^\\top + \\nu^2(x) \\ C &= \\Sigma_x : \\nabla_x^\\top f(\\mu_x) \\end{aligned} $$ So if we want to make predictions with our new model, we will have the final equation as: $$ \\begin{aligned} f &\\sim \\mathcal{N}(f|\\mu_{GP}, \\nu^2_{GP}) \\ \\mu_{GP} &= K_{ } K_{GP}^{-1}y=K_{ } \\alpha \\ \\nu^2_{GP} &= K_{**} - K_{ } K_{GP}^{-1}K_{ }^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ where \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top \\tilde{\\Sigma}_x = \\nabla_x \\mu_{GP} \\Sigma_x \\nabla \\mu_{GP}^\\top .","title":"I: Additive Noise Model (x,fx,f)"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#other-gp-methods","text":"We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original \\mu_{GP} \\mu_{GP} and \\nu^2_{GP} \\nu^2_{GP} equations. In a sparse GP we have the following predictive functions $$ \\begin{aligned} \\mu_{SGP} &= K_{ z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{ z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} \\end{aligned} $$ So the new predictive functions will be: $$ \\begin{aligned} \\mu_{SGP} &= K_{*z}K_{zz}^{-1}m \\ \\nu^2_{SGP} &= K_{* } - K_{*z}\\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \\right]K_{*z}^{\\top} + \\tilde{\\Sigma}_x \\end{aligned} $$ As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.","title":"Other GP Methods"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#ii-non-additive-noise-model","text":"","title":"II: Non-Additive Noise Model"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#iii-quadratic-approximation","text":"","title":"III: Quadratic Approximation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#literature","text":"Gaussian Process Priors with Uncertain Inputs: Multiple-Step-Ahead Prediction - Girard et. al. (2002) - Technical Report Does the derivation for taking the expectation and variance for the Taylor series expansion of the predictive mean and variance. Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version - Deisenroth & Mohamed (2012) - NeuRIPS First time the moment matching and linearized version appears in the GP literature. Learning with Uncertainty-Gaussian Processes and Relevance Vector Machines - Candela (2004) - Thesis Full law of iterated expectations and conditional variance. Gaussian Process Training with Input Noise - McHutchon & Rasmussen et. al. (2012) - NeuRIPS Used the same logic but instead of just approximated the posterior, they also applied this to the model which resulted in an iterative procedure. Multi-class Gaussian Process Classification with Noisy Inputs - Villacampa-Calvo et. al. (2020) - axriv Applied the first order approximation using the Taylor expansion for a classification problem. Compared this to the variational inference.","title":"Literature"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#supplementary","text":"","title":"Supplementary"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#error-propagation","text":"To see more about error propagation and the relation to the mean and variance, see here .","title":"Error Propagation"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#fubinis-theorem","text":"","title":"Fubini's Theorem"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#law-of-iterated-expecations","text":"","title":"Law of Iterated Expecations"},{"location":"projects/ErrorGPs/Taylor%20Expansion/taylor/#conditional-variance","text":"","title":"Conditional Variance"},{"location":"projects/ErrorGPs/Variational/vi/","text":"Variational Strategies \u00b6 This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error. Posterior Approximations \u00b6 What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more. Variational GP Model with Latent Inputs \u00b6 Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) Evidence Lower Bound (ELBO) \u00b6 In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian. Uncertain Inputs \u00b6 So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties. Case I - Strong Prior \u00b6 Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes. Case II - Regularized Strong Prior \u00b6 This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) Case III - Prior with Openness \u00b6 The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise. Case IV - Bonus, Conservative Freedom \u00b6 Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options Resources \u00b6 Important Papers \u00b6 These are the important papers that helped me understand what was going on throughout the learning process. Summary Thesis \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Talks \u00b6 Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Variational/vi/#variational-strategies","text":"This post is a follow-up from my previous post where I walk through the literature talking about the different strategies of account for input error in Gaussian processes. In this post, I will be discussing how we can use variational strategies to account for input error.","title":"Variational Strategies"},{"location":"projects/ErrorGPs/Variational/vi/#posterior-approximations","text":"What links all of the strategies from uncertain GPs is how they approach the problem of uncertain inputs: approximating the posterior distribution. The methods that use moment matching on stochastic trial points are all using various strategies to construct some posterior approximation. They define their GP model first and then approximate the posterior by using some approximate scheme to account for uncertainty. The NIGP however does change the model which is a product of the Taylor series expansion employed. From there, the resulting posterior is either evaluated or further approximated. My method actually is related because I also avoid changing the model and just attempt to approximate the posterior predictive distribution by augmenting the predictive variance function only ( ??? ). This approach has similar strategies that stem from the wave of methods that appeared using variational inference (VI). VI consists of creating a variational approximation of the posterior distribution q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) q(\\mathbf u)\\approx \\mathcal{P}(\\mathbf x) . Under some assumptions and a baseline distribution for q(\\mathbf u) q(\\mathbf u) , we can try to approximate the complex distribution \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) by minimizing the distance between the two distributions, D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] D\\left[q(\\mathbf u)||\\mathcal{P}(\\mathbf x)\\right] . Many practioners believe that approximating the posterior and not the model is the better option when doing Bayesian inference; especially for large data ( example blog , VFE paper ). The variational family of methods that are common for GPs use the Kullback-Leibler (KL) divergence criteria between the GP posterior approximation q(\\mathbf u) q(\\mathbf u) and the true GP posterior \\mathcal{P}(\\mathbf x) \\mathcal{P}(\\mathbf x) . From the literature, this has been extended to many different problems related to GPs for regression, classification, dimensionality reduction and more.","title":"Posterior Approximations"},{"location":"projects/ErrorGPs/Variational/vi/#variational-gp-model-with-latent-inputs","text":"Posterior Distribution: p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX p(Y) = \\int_{\\mathcal X} p(Y|X) P(X) dX Derive the Lower Bound (w/ Jensens Inequality): \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX \\log p(Y) = \\log \\int_{\\mathcal X} p(Y|X) P(X) dX importance sampling/identity trick = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF = \\log \\int_{\\mathcal F} p(Y|X) P(X) \\frac{q(X)}{q(X)}dF rearrange to isolate : p(Y|X) p(Y|X) and shorten notation to \\langle \\cdot \\rangle_{q(X)} \\langle \\cdot \\rangle_{q(X)} . = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} = \\log \\left\\langle \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Jensens inequality \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log \\frac{p(Y|X)p(X)}{q(X)} \\right\\rangle_{q(X)} Split the logs \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} \\geq \\left\\langle \\log p(Y|X) + \\log \\frac{p(X)}{q(X)} \\right\\rangle_{q(X)} collect terms \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\log p(Y|X)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) plug in other bound \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right) \\mathcal{L}_{2}(q)=\\left\\langle \\mathcal{L}_{1}(q)\\right\\rangle_{q(F)} - D_{KL} \\left( q(X) || p(X)\\right)","title":"Variational GP Model with Latent Inputs"},{"location":"projects/ErrorGPs/Variational/vi/#evidence-lower-bound-elbo","text":"In VI strategies, we never get an explicit function that will lead us to the best variational approximation but we can come close; we can come up with an upper bound. Traditional marginal likelhood (evidence) function that we're given is given by: \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df \\underbrace{\\mathcal{P}(y|\\mathbf x, \\theta)}_{\\text{Evidence}}=\\int_f \\underbrace{\\mathcal{P}(y|f, \\mathbf x, \\theta)}_{\\text{Likelihood}} \\cdot \\underbrace{\\mathcal{P}(f|\\mathbf x, \\theta)}_{\\text{GP Prior}}df where: \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(y|f, \\mathbf x, \\theta)=\\mathcal{N}(y|f, \\sigma_n^2\\mathbf{I}) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) \\mathcal{P}(f|\\mathbf x, \\theta)=\\mathcal{N}(f|\\mu, K_\\theta) In this case we are marginalizing by the latent functions f f 's. But we no longer consider \\mathbf x \\mathbf x to be uncertain with some probability distribution. Also, we do not want some MAP estimation; we want a fully Bayesian approach. So the only thing to do is marginalize out the \\mathbf x \\mathbf x 's. In doing so we get: \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_f\\int_\\mathcal{X} \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x)\\cdot df \\cdot d\\mathbf{x} We can rearrange this equation to change notation: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\underbrace{\\left[ \\int_f \\mathcal{P}(y|f, \\mathbf x, \\theta)\\cdot\\mathcal{P}(f|\\mathbf x, \\theta)\\cdot df\\right]}_{\\text{Evidence}} \\cdot \\underbrace{\\mathcal{P}(\\mathbf x)}_{\\text{Prior}} \\cdot d\\mathbf{x} where we find that that term is simply the same term as the original likelihood, \\mathcal{P}(y|\\mathbf x, \\theta) \\mathcal{P}(y|\\mathbf x, \\theta) . So our new simplified equation is: \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} \\mathcal{P}(y| \\theta)=\\int_\\mathcal{X} \\mathcal{P}(y|\\mathbf x, \\theta) \\cdot \\mathcal{P}(\\mathbf x) \\cdot d\\mathbf{x} where we have effectively marginalized out the f f 's. We already know that it's difficult to propagate the \\mathbf x \\mathbf x 's through the nonlinear functions \\mathbf K^{-1} \\mathbf K^{-1} and | | det \\mathbf K| \\mathbf K| (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution q(\\mathbf x) q(\\mathbf x) to approximate the posterior distribution \\mathcal{P}(\\mathbf x| y) \\mathcal{P}(\\mathbf x| y) . The distribution is normally chosen to be Gaussian: q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) q(\\mathbf x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf x|\\mathbf \\mu_z, \\mathbf \\Sigma_z) So at this point, we are interested in trying to find a way to measure the difference between the approximate distribution q(\\mathbf x) q(\\mathbf x) and the true posterior distribution \\mathcal{P} (\\mathbf x) \\mathcal{P} (\\mathbf x) . Using the standard derivation for the ELBO, we arrive at the final formula: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x)}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x) || \\mathcal{P}(\\mathbf x) \\right] If we optimize \\mathcal{F} \\mathcal{F} with respect to q(\\mathbf x) q(\\mathbf x) , the KL is minimized and we just get the likelihood. As we've seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the \\mathbf x \\mathbf x 's through. So that's nothing new and we've done nothing useful. If we introduce some special structure in q(f) q(f) by introducing sparsity, then we can achieve something useful with this formulation. But through augmentation of the variable space with \\mathbf u \\mathbf u and \\mathbf Z \\mathbf Z we can bypass this problem. The second term is simple to calculate because they're both chosen to be Gaussian.","title":"Evidence Lower Bound (ELBO)"},{"location":"projects/ErrorGPs/Variational/vi/#uncertain-inputs","text":"So how does this relate to uncertain inputs exactly? Let's look again at our problem setting. \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} \\begin{aligned} y &= f(x) + \\epsilon_y \\\\ x &\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\ \\end{aligned} where: y y - noise-corrupted outputs which have a noise parameter characterized by \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) \\epsilon_y \\sim \\mathcal{N}(0, \\sigma^2_y) f(x) f(x) - is the standard GP function x x - \"latent variables\" but we assume that the come from a normal distribution, x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x) where you have some observations \\mu_x \\mu_x but you also have some prior uncertainty \\Sigma_x \\Sigma_x that you would like to incorporate. Now the ELBO that we want to minimize has the following form: \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] \\mathcal{F}(q)=\\mathbb{E}_{q(\\mathbf x | m_{p_z}, S_{p_z})}\\left[ \\log \\mathcal{P}(y|\\mathbf x, \\theta) \\right] - \\text{D}_\\text{KL}\\left[ q(\\mathbf x | m_{p_z}, S_{p_z}) || \\mathcal{P}(\\mathbf x | m_{p_x}, S_{p_x}) \\right] Notice that I have expanded the parameters for p(X) p(X) and q(X) q(X) so that we are clear about where the parameters. We would like to figure out a way to incorporate our uncertainties in the m_{p_x} m_{p_x} , S_{p_x} S_{p_x} , m_{p_z} m_{p_z} , and S_{p_z} S_{p_z} . The author had two suggestions about how to account for noise in the inputs but the original formulation assumed that these parameters were unknown. In my problem setting, we know that there is noise in the inputs so the problems that the original formulations had will change. I will outline the formulations below for both known and unknown uncertainties.","title":"Uncertain Inputs"},{"location":"projects/ErrorGPs/Variational/vi/#case-i-strong-prior","text":"Prior , p(X) p(X) We can directly assume that we know the parameters for the prior distribution. So we let \\mu_x \\mu_x be our noisy observations and we let \\Sigma_x \\Sigma_x be our known covariance matrix for X X . These parameters are fixed as we assume we know them and we would like this to be our prior. This seems to be the most natural as is where we have information and we would like to use it. So now our prior is in the form of: \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) \\mathcal{P}(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) and this will be our regularization that we use for the KL divergence term. Variational , q(X) q(X) However, the variational parameters m m and S S are also important because that is being directly evaluated with the KL divergence term and the likelihood function \\log p(y|X, \\theta) \\log p(y|X, \\theta) . So ideally we would also like to constrain this as well if we know something. The first thing to do would be to fix them as well to what we know about our data, m=\\mu_x m=\\mu_x and S_{p_z} = \\Sigma_x S_{p_z} = \\Sigma_x . So our prior for our variational distribution will be: q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) q(\\mathbf X|\\mu_x, \\Sigma_x) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf \\mu_{\\mathbf{x}_i},\\mathbf \\Sigma_\\mathbf{x_i}) We now have our variational bound with the assumed parameters: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) Assessment So this is a very strong belief over our parameters. The KL divergence term will be zero because the distributions will be the same and we would have probably done some extra computations for no reason; we do need this in order to make the likelihood tractable but it doesn't make sense if we're not learning anything. But this is absolute because we have no reason to change anything. It's worth testing to see how this goes.","title":"Case I - Strong Prior"},{"location":"projects/ErrorGPs/Variational/vi/#case-ii-regularized-strong-prior","text":"This is similar to the above statement, however we would be reducing the prior parameters to a standard 0 mean and 1 standard deviation. So our prior function will look like this \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) \\mathcal{P}(\\mathbf X|0, 1) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |0, 1) This would allow the KL-Divergence criteria extra penalization for the loss function. It might change some of the parameters learned for the other regions of the ELBO. So our final loss function is: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, \\Sigma_x)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, \\Sigma_x) || p(\\mathbf X|0, 1) \\right)","title":"Case II - Regularized Strong Prior"},{"location":"projects/ErrorGPs/Variational/vi/#case-iii-prior-with-openness","text":"The last option I think is the most interesting. It seems to incorporate the prior but also allow for some flexibility. In this option, We can pivot off of what the factor that the KL divergence term is simply a regularizer. So we could also go with a more conservative approach where we We will introduce a variational constraint to encode the input uncertainty directly into the approximate posterior. q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) q(\\mathbf X|\\mathbf Z) = \\prod_{i=1}^{N}\\mathcal{N}(\\mathbf{x}_i |\\mathbf z_i,\\mathbf \\Sigma_\\mathbf{z_i}) We will have a new variational bound now: \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf X|\\mu_x, S)} - \\text{KL}\\left( q(\\mathbf X|\\mu_x, S) || p(\\mathbf X|\\mu_x, \\Sigma_x) \\right) So the only free parameter in the variational bound is the actual variance of our inputs S S that stems from our variational distribution q(X) q(X) . Again, this seems like a nice balanced approach where we can incorporate prior information within our model but at the same time allow for some freedom to maybe find a better distribution to represent the noise.","title":"Case III - Prior with Openness"},{"location":"projects/ErrorGPs/Variational/vi/#case-iv-bonus-conservative-freedom","text":"Ok, so the true last option would be to try and see how the algorithm thinks the results should be. We \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) \\mathcal{F}=\\langle \\log \\mathcal{P}(\\mathbf{Y|X}) \\rangle_{q(\\mathbf{X|Z})} - \\text{KL}\\left( q(\\mathbf{X|Z}) || \\mathcal{P}(\\mathbf{X}) \\right) Options m_{p_x} m_{p_x} S_{p_x} S_{p_x} m_{p_z} m_{p_z} S_{p_z} S_{p_z} No Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x S_z Strong Conservative Prior \\mu_x \\mu_x 1 \\mu_x \\mu_x \\Sigma_x \\Sigma_x Strong Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x \\Sigma_x \\Sigma_x Bayesian Prior \\mu_x \\mu_x \\Sigma_x \\Sigma_x \\mu_x \\mu_x S_z Caption : Summary of Options","title":"Case IV - Bonus, Conservative Freedom"},{"location":"projects/ErrorGPs/Variational/vi/#resources","text":"","title":"Resources"},{"location":"projects/ErrorGPs/Variational/vi/#important-papers","text":"These are the important papers that helped me understand what was going on throughout the learning process.","title":"Important Papers"},{"location":"projects/ErrorGPs/Variational/vi/#summary-thesis","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review","title":"Summary Thesis"},{"location":"projects/ErrorGPs/Variational/vi/#talks","text":"Damianou - Bayesian LVM with GPs - MLSS2015 Lawrence - Deep GPs - MLSS2019","title":"Talks"},{"location":"projects/ML4OCN/","text":"Overview \u00b6 I present some of my findings for working on machine learning applied to ocean applications. ARGO Project \u00b6 Summary In this project, we are trying to predict profiles of Bbp Bbp . This is a multi-output ML problem with high dimensional inputs and high dimensional outputs. Note : You can find the repository with all of the reproducible code, specific functions and notebooks at github.com/IPL-UV/ml4ocean . Subset Dataset \u00b6 We look at the North Atlantic and SubTropical Gyre regions to try and get an idea about how some ML methods might perform in this scenarios. As a first pass, we did minimal preprocessing with some standard PCA components reduction and we were ambitious and attempted to predict all 273 depths for the profiles. We had some success as we were able to do a decent job with Random forests and MultiLayer Perceptrons. Relevant Materials ISPRS 2020 Publication - Pending Todo 0.0 - Data Exploration Notebook 1.0 - Data Preprocessing Notebook 2.0 - Loading the Preprocessed Data 3.0 - Baseline ML Algorithm 4.0 - Visualization & Summary Statistics Global Dataset \u00b6 After we found success in the subset data, we decided to go for the global dataset and see how we do. We did reduce the problem difficulty by predicting only 19 depth levels instead of \\sim \\sim 273. Notebooks 2.0 - Loading the Preprocessed Data 3.0 - Baseline ML Algorithm Todo 0.0 - Data Exploration Notebook 1.0 - Data Preprocessing Notebook 4.0 - Visualization & Summary Statistics","title":"Overview"},{"location":"projects/ML4OCN/#overview","text":"I present some of my findings for working on machine learning applied to ocean applications.","title":"Overview"},{"location":"projects/ML4OCN/#argo-project","text":"Summary In this project, we are trying to predict profiles of Bbp Bbp . This is a multi-output ML problem with high dimensional inputs and high dimensional outputs. Note : You can find the repository with all of the reproducible code, specific functions and notebooks at github.com/IPL-UV/ml4ocean .","title":"ARGO Project"},{"location":"projects/ML4OCN/#subset-dataset","text":"We look at the North Atlantic and SubTropical Gyre regions to try and get an idea about how some ML methods might perform in this scenarios. As a first pass, we did minimal preprocessing with some standard PCA components reduction and we were ambitious and attempted to predict all 273 depths for the profiles. We had some success as we were able to do a decent job with Random forests and MultiLayer Perceptrons. Relevant Materials ISPRS 2020 Publication - Pending Todo 0.0 - Data Exploration Notebook 1.0 - Data Preprocessing Notebook 2.0 - Loading the Preprocessed Data 3.0 - Baseline ML Algorithm 4.0 - Visualization & Summary Statistics","title":"Subset Dataset"},{"location":"projects/ML4OCN/#global-dataset","text":"After we found success in the subset data, we decided to go for the global dataset and see how we do. We did reduce the problem difficulty by predicting only 19 depth levels instead of \\sim \\sim 273. Notebooks 2.0 - Loading the Preprocessed Data 3.0 - Baseline ML Algorithm Todo 0.0 - Data Exploration Notebook 1.0 - Data Preprocessing Notebook 4.0 - Visualization & Summary Statistics","title":"Global Dataset"},{"location":"projects/ML4OCN/ideas/","text":"Ideas \u00b6 Walk-Throughs \u00b6 These are the fully fledged notebooks which will go through all of the preprocessing steps that were done in order to achieve the results we achieved. Explore the Data Preprocessing - PCA Components Analysis, Column Transformations End-to-End Training Example - RF, LR, MLP scikit-learn Output Maps (Validation Floats) Results Line Plots - Variance in Profiles Residual Plots Joint Distribution Plots BaseLine Models : sklearn models Random Forests Linear Regression MLP SOTA Models : Outside CatBoost XGBoost Bayesian Models Keras: Bayesian Layers (Edward) Linear Regression (TF Probability) Deep Kernel Learning (TF Probability, Bayesian Layers) GPyTorch Explore Exact GP Sparse Variational GP Deep GP DKL - Sparse Tutorials \u00b6 Feature Extraction Column Transformations (scikit-ify everything) PCA Trick CrossValidation Model Only PreProcessing Model Training Cross Validation (Random, GridSearch) Training Tips Validation Curves Learning Curves Assessment Interpretability Permutation Maps Sensitivity Analysis Visualization Matplotlib - Complete Customization GeoPandas XArray HoloViews / GeoViews DataShader","title":"Ideas"},{"location":"projects/ML4OCN/ideas/#ideas","text":"","title":"Ideas"},{"location":"projects/ML4OCN/ideas/#walk-throughs","text":"These are the fully fledged notebooks which will go through all of the preprocessing steps that were done in order to achieve the results we achieved. Explore the Data Preprocessing - PCA Components Analysis, Column Transformations End-to-End Training Example - RF, LR, MLP scikit-learn Output Maps (Validation Floats) Results Line Plots - Variance in Profiles Residual Plots Joint Distribution Plots BaseLine Models : sklearn models Random Forests Linear Regression MLP SOTA Models : Outside CatBoost XGBoost Bayesian Models Keras: Bayesian Layers (Edward) Linear Regression (TF Probability) Deep Kernel Learning (TF Probability, Bayesian Layers) GPyTorch Explore Exact GP Sparse Variational GP Deep GP DKL - Sparse","title":"Walk-Throughs"},{"location":"projects/ML4OCN/ideas/#tutorials","text":"Feature Extraction Column Transformations (scikit-ify everything) PCA Trick CrossValidation Model Only PreProcessing Model Training Cross Validation (Random, GridSearch) Training Tips Validation Curves Learning Curves Assessment Interpretability Permutation Maps Sensitivity Analysis Visualization Matplotlib - Complete Customization GeoPandas XArray HoloViews / GeoViews DataShader","title":"Tutorials"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/","text":"Loading the Data \u00b6 Important - Paths and Directories \u00b6 This is annoying but it needs to be defined otherwise things get confusing. We need a few important paths to be pre-defined: Name Variable Purpose Project PROJECT_PATH top level directory for the project (assuming megatron) Code CODE_PATH folder of any dedicated functions that we use Raw Data RAW_PATH where the raw data is. Ideally, we never touch this ever except to read. Processed Data DATA_PATH where the processed data is stored Interim Data INTERIM_PATH where we save the training, validation and testing data Saved Models MODEL_PATH where we save any trained models Results Data RESULTS_PATH where we save any data results or outputs from ML models Figures FIG_PATH where we store any plotted figures during any part of our ML pipeline This cell checks to see if all of the paths exist. If there is a path missing, it probably means you're not in megatron. If that's the case...well, we'll cross that bridge when we get there. import pathlib import sys # define the top level directory PROJECT_PATH = pathlib . Path ( \"/media/disk/erc/papers/2019_ML_OCN/\" ) CODE_PATH = PROJECT_PATH . joinpath ( \"ml4ocean\" , \"src\" ) # check if path exists and is a directory assert PROJECT_PATH . exists () & PROJECT_PATH . is_dir () assert CODE_PATH . exists () & CODE_PATH . is_dir () # add code and project paths to PYTHONPATH (to call functions) sys . path . append ( str ( PROJECT_PATH )) sys . path . append ( str ( CODE_PATH )) # specific paths FIG_PATH = PROJECT_PATH . joinpath ( \"ml4ocean/reports/figures/global/\" ) RAW_PATH = PROJECT_PATH . joinpath ( \"data/global/raw/\" ) DATA_PATH = PROJECT_PATH . joinpath ( \"data/global/processed/\" ) INTERIM_PATH = PROJECT_PATH . joinpath ( \"data/global/interim/\" ) MODEL_PATH = PROJECT_PATH . joinpath ( \"models/global/\" ) RESULTS_PATH = PROJECT_PATH . joinpath ( \"data/global/results/\" ) # check if path exists and is a directory assert FIG_PATH . exists () & FIG_PATH . is_dir () assert RAW_PATH . exists () & RAW_PATH . is_dir () assert DATA_PATH . exists () & DATA_PATH . is_dir () assert INTERIM_PATH . exists () & INTERIM_PATH . is_dir () assert MODEL_PATH . exists () & MODEL_PATH . is_dir () assert RESULTS_PATH . exists () & RESULTS_PATH . is_dir () Python Packages \u00b6 # Standard packages import numpy as np import pandas as pd 1. Load Processed Global Data \u00b6 In this section, I will load the metadata and the actual data. The steps involved are: Define the filepath (check for existence) Open meta data and real data Check that the samples correspond to each other. Check if # of features are the same 1.1 - Meta Data \u00b6 # name of file meta_name = \"METADATA_20200310.csv\" # get full path meta_file = DATA_PATH . joinpath ( meta_name ) # assert meta file exists error_msg = f \"File ' { meta_file . name } ' doesn't exist. Check name or directory.\" assert meta_file . exists (), error_msg # assert meta file is a file error_msg = f \"File ' { meta_file . name } ' isn't a file. Check name or directory.\" assert meta_file . is_file (), error_msg # open meta data meta_df = pd . read_csv ( f \" { meta_file } \" ) meta_df . head () . to_markdown () wmo n_cycle N lon lat juld date 0 2902086 1 1 88.6957 12.1639 23009.2 2012-12-30 03:58:59 1 2902086 10 10 88.6033 12.4128 23018.1 2013-01-08 03:24:59 2 2902086 100 64 86.2039 13.7915 23432.1 2014-02-26 03:34:59 3 2902086 101 65 86.3116 13.75 23437.1 2014-03-03 03:26:59 4 2902086 102 66 86.3971 13.7588 23442.1 2014-03-08 03:31:59 1.2 - Input Data \u00b6 # name of file data_name = \"SOCA_GLOBAL2_20200310.csv\" # get full path data_file = DATA_PATH . joinpath ( data_name ) # assert exists error_msg = f \"File ' { data_file . name } ' doesn't exist. Check name or directory.\" assert data_file . exists (), error_msg # assert meta file is a file error_msg = f \"File ' { data_file . name } ' isn't a file. Check name or directory.\" assert data_file . is_file (), error_msg # load data data_df = pd . read_csv ( f \" { data_file } \" ) data_df . head () . iloc [:, : 6 ] . to_markdown () N wmo n_cycle sla PAR RHO_WN_412 0 1 2.90209e+06 1 -4.7044 42.6541 0.0254617 1 2 2.90209e+06 2 -4.0382 42.6541 0.0254617 2 3 2.90209e+06 3 -3.4604 44.2927 0.0240942 3 4 2.90209e+06 4 -2.8404 42.7664 0.024917 4 5 2.90209e+06 5 -2.394 42.7664 0.024917 1.3 - Checks \u00b6 I do a number of checks to make sure that our data follows a standard and that I am reproducing the same results. Number of samples are equal for both 7 meta features 48 data features (26 data + 19 levels + 3 meta) check features in columns # same number of samples error_msg = f \"Mismatch between meta and data: { data_df . shape [ 0 ] } =/= { meta_df . shape [ 0 ] } \" assert data_df . shape [ 0 ] == meta_df . shape [ 0 ], error_msg # check number of samples n_samples = 25413 error_msg = f \"Incorrect number of samples: { data_df . shape [ 0 ] } =/= { n_samples } \" assert data_df . shape [ 0 ] == n_samples , error_msg # check meta feature names meta_features = [ 'wmo' , 'n_cycle' , 'N' , 'lon' , 'lat' , 'juld' , 'date' ] error_msg = f \"Missing features in meta data.\" assert meta_df . columns . tolist () == meta_features , error_msg # check data feature names input_meta_features = [ 'N' , 'wmo' , 'n_cycle' ] input_features = [ 'sla' , 'PAR' , 'RHO_WN_412' , 'RHO_WN_443' , 'RHO_WN_490' , 'RHO_WN_555' , 'RHO_WN_670' , 'doy_sin' , 'doy_cos' , 'x_cart' , 'y_cart' , 'z_cart' , 'PC1' , 'PC2' , 'PC3' , 'PC4' , 'PC5' , 'PC6' , 'PC7' , 'PC1.1' , 'PC2.1' , 'PC3.1' , 'PC1.2' , 'PC2.2' , 'PC3.2' , 'PC4.1' ] output_features = [ 'bbp' , 'bbp.1' , 'bbp.2' , 'bbp.3' , 'bbp.4' , 'bbp.5' , 'bbp.6' , 'bbp.7' , 'bbp.8' , 'bbp.9' , 'bbp.10' , 'bbp.11' , 'bbp.12' , 'bbp.13' , 'bbp.14' , 'bbp.15' , 'bbp.16' , 'bbp.17' , 'bbp.18' ] features = input_meta_features + input_features + output_features error_msg = f \"Missing features in input data.\" assert data_df . columns . tolist () == features , error_msg 1.4 - Convert metadata to indices ( Important ) \u00b6 To make our life easier, we're going to eliminate the need to keep track of meta data all of the time. So I'm going to merge the datasets together to form one dataframe. Then I will set the index to be the metadata values. The remaining parts will be columns which will be features. So in the end, we will have a dataframe where: the indices is the metadata (e.g. wmo, n_cycle) the columns are the actual features (e.g. sla, pca components, bbp, etc). # merge meta and data full_df = pd . merge ( meta_df , data_df ) # convert meta information to indices full_df = full_df . set_index ( meta_features ) # checks - check indices match metadata meta_features = [ 'wmo' , 'n_cycle' , 'N' , 'lon' , 'lat' , 'juld' , 'date' ] error_msg = f \"Missing features in input data.\" assert full_df . index . names == meta_features , error_msg # checks - check column names match feature names input_features = [ 'sla' , 'PAR' , 'RHO_WN_412' , 'RHO_WN_443' , 'RHO_WN_490' , 'RHO_WN_555' , 'RHO_WN_670' , 'doy_sin' , 'doy_cos' , 'x_cart' , 'y_cart' , 'z_cart' , 'PC1' , 'PC2' , 'PC3' , 'PC4' , 'PC5' , 'PC6' , 'PC7' , 'PC1.1' , 'PC2.1' , 'PC3.1' , 'PC1.2' , 'PC2.2' , 'PC3.2' , 'PC4.1' ] output_features = [ 'bbp' , 'bbp.1' , 'bbp.2' , 'bbp.3' , 'bbp.4' , 'bbp.5' , 'bbp.6' , 'bbp.7' , 'bbp.8' , 'bbp.9' , 'bbp.10' , 'bbp.11' , 'bbp.12' , 'bbp.13' , 'bbp.14' , 'bbp.15' , 'bbp.16' , 'bbp.17' , 'bbp.18' ] features = input_features + output_features error_msg = f \"Missing features in input data.\" assert full_df . columns . tolist () == features , error_msg print ( 'Dataframe Features:' , full_df . shape ) full_df . columns . tolist ()[: 10 ] 1 2 3 4 5 6 7 8 9 10 11 12 Dataframe Features: (25413, 45) ['sla', 'PAR', 'RHO_WN_412', 'RHO_WN_443', 'RHO_WN_490', 'RHO_WN_555', 'RHO_WN_670', 'doy_sin', 'doy_cos', 'x_cart'] print ( 'Dataframe Indices (meta vars):' , len ( full_df . index . names )) full_df . index . names 1 2 3 Dataframe Indices (meta vars): 7 FrozenList(['wmo', 'n_cycle', 'N', 'lon', 'lat', 'juld', 'date']) 2 - Training and Test Split \u00b6 2.1 - Independent Set I (SOCA2016) \u00b6 This independent set has a set number of independent floats which are not counted in the training or validation phase. These floats were in a paper (Sauzede et. al., 2016) and used during the testing phase to showcase how well the models did. 6901472 6901493 6901523 6901496 So we need to take these away from the data. # soca2016 independent floats soca2016_floats = [ \"6901472\" , \"6901493\" , \"6901523\" , \"6901496\" ] # subset soca2016 floats soca2016_df = full_df [ full_df . index . isin ( soca2016_floats , level = 'wmo' )] Checks \u00b6 # check number of samples (meta, inputs) n_samples = 378 error_msg = f \"Incorrect number of samples for soca2016 floats: { soca2016_df . shape [ 0 ] } =/= { n_samples } \" assert soca2016_df . shape [ 0 ] == n_samples , error_msg 2.2 - Indpendent Set II (ISPRS2020) \u00b6 This independent set was a set of floats taken from the ISPRS paper (Sauzede et. al., 2020 (pending...)). These floats were used as the independent testing set to showcase the performance of the ML methods. 6901486 (North Atlantic?) 3902121 (Subtropical Gyre?) So we need to take these away from the data. # isprs2020 independent floats isprs2020_floats = [ \"6901486\" , \"3902121\" ] # isprs2020 independent floats isprs2020_df = full_df [ full_df . index . isin ( isprs2020_floats , level = 'wmo' )] Checks \u00b6 # check number of samples (meta, inputs) n_samples = 331 error_msg = f \"Incorrect number of samples for isprs2016 floats: { isprs2020_df . shape [ 0 ] } =/= { n_samples } \" assert isprs2020_df . shape [ 0 ] == n_samples , error_msg 2.3 - ML Data \u00b6 Now we want to subset the input data to be used for the ML models. Basically, we can subset all datasets that are not in the independent floats. In addition, we want all of the variables in the input features that we provided earlier. # subset non-independent flows ml_df = full_df [ ~ full_df . index . isin ( isprs2020_floats + soca2016_floats , level = 'wmo' )] Checks \u00b6 # check number of samples (meta, inputs) n_samples = 24704 error_msg = f \"Incorrect number of samples for non-independent floats: { ml_df . shape [ 0 ] } =/= { n_samples } \" assert ml_df . shape [ 0 ] == n_samples , error_msg 2.4 - Inputs, Outputs \u00b6 Lastly, we need to split the data into training, validation (and possibly testing). Recall that all the inputs are already written above and the outputs as well. input_df = ml_df [ input_features ] output_df = ml_df [ output_features ] # checks - Input Features n_input_features = 26 error_msg = f \"Incorrect number of features for input df: { input_df . shape [ 1 ] } =/= { n_input_features } \" assert input_df . shape [ 1 ] == n_input_features , error_msg # checks - Output Features n_output_features = 19 error_msg = f \"Incorrect number of features for output df: { output_df . shape [ 1 ] } =/= { n_output_features } \" assert output_df . shape [ 1 ] == n_output_features , error_msg 3. Final Dataset (saving) \u00b6 3.1 - Print out data dimensions (w. metadata) \u00b6 print ( \"Input Data:\" , input_df . shape ) print ( \"Output Data:\" , output_df . shape ) print ( \"SOCA2016 Independent Data:\" , soca2016_df [ input_features ] . shape , soca2016_df [ output_features ] . shape ) print ( \"ISPRS2016 Independent Data:\" , isprs2020_df [ input_features ] . shape , isprs2020_df [ output_features ] . shape ) 1 2 3 4 Input Data: (24704, 26) Output Data: (24704, 19) SOCA2016 Independent Data: (378, 26) (378, 19) ISPRS2016 Independent Data: (331, 26) (331, 19) 3.2 - Saving \u00b6 We're going to save the data in the global/interim/ path. This is to prevent any overwrites. We also need to index=True for the savefile in order to preserve the metadata indices. input_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" , index = True ) 3.3 - Loading \u00b6 This is a tiny bit tricky if we want to preserve the meta data as the indices. So we need to set the index to be the same meta columns that we used last time via the .set_index(meta_vars) command. test_inputs_df = pd . read_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" ) # add index test_inputs_df = test_inputs_df . set_index ( meta_features ) 3.4 - Checking \u00b6 So curiously, we cannot compare the dataframes directly because there is some numerical error when saving them. But if we calculate the exact differences between them, we find that they are almost equal. See below what happens if we calculate the exact difference between the arrays. # example are they exactly the same? # np.testing.assert_array_equal(test_inputs_df.describe(), input_df.describe()) np . testing . assert_array_equal ( test_inputs_df . values , input_df . values ) AssertionError : Arrays are not equal Mismatched elements : 96056 / 642304 ( 15 % ) Max absolute difference : 1.42108547e-14 Max relative difference : 3.92139227e-16 x : array ([[ - 4.704400e+00 , 4.265410e+01 , 2.546170e-02 , ... , - 3.458944e+00 , - 1.017509e-02 , - 1.025450e+00 ], [ - 9.015000e-01 , 4.455050e+01 , 2.060340e-02 , ... , - 3.691716e+00 , ... y : array ([[ - 4.704400e+00 , 4.265410e+01 , 2.546170e-02 , ... , - 3.458944e+00 , - 1.017509e-02 , - 1.025450e+00 ], [ - 9.015000e-01 , 4.455050e+01 , 2.060340e-02 , ... , - 3.691716e+00 , ... We get an assertion error that they're not equal. There is a mismatch difference of order 1e-15 for the absolute and relative differences. That's numerical error probably due to compression that comes when saving and loading data. Let's check again but with a little less expected precision. np . testing . assert_array_almost_equal ( test_inputs_df . values , input_df . values , decimal = 1e-14 ) so just by reducing the precision by a smidge (1e-14 instead of 1e-15), we find that the arrays are the same. So we can trust it. 3.5 - Save the rest of the data \u00b6 input_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" , index = True ) output_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'outputs.csv' ) } \" , index = True ) soca2016_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'soca2016.csv' ) } \" , index = True ) isprs2020_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'isprs2020.csv' ) } \" , index = True )","title":"2.0 Processed Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#loading-the-data","text":"","title":"Loading the Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#important-paths-and-directories","text":"This is annoying but it needs to be defined otherwise things get confusing. We need a few important paths to be pre-defined: Name Variable Purpose Project PROJECT_PATH top level directory for the project (assuming megatron) Code CODE_PATH folder of any dedicated functions that we use Raw Data RAW_PATH where the raw data is. Ideally, we never touch this ever except to read. Processed Data DATA_PATH where the processed data is stored Interim Data INTERIM_PATH where we save the training, validation and testing data Saved Models MODEL_PATH where we save any trained models Results Data RESULTS_PATH where we save any data results or outputs from ML models Figures FIG_PATH where we store any plotted figures during any part of our ML pipeline This cell checks to see if all of the paths exist. If there is a path missing, it probably means you're not in megatron. If that's the case...well, we'll cross that bridge when we get there. import pathlib import sys # define the top level directory PROJECT_PATH = pathlib . Path ( \"/media/disk/erc/papers/2019_ML_OCN/\" ) CODE_PATH = PROJECT_PATH . joinpath ( \"ml4ocean\" , \"src\" ) # check if path exists and is a directory assert PROJECT_PATH . exists () & PROJECT_PATH . is_dir () assert CODE_PATH . exists () & CODE_PATH . is_dir () # add code and project paths to PYTHONPATH (to call functions) sys . path . append ( str ( PROJECT_PATH )) sys . path . append ( str ( CODE_PATH )) # specific paths FIG_PATH = PROJECT_PATH . joinpath ( \"ml4ocean/reports/figures/global/\" ) RAW_PATH = PROJECT_PATH . joinpath ( \"data/global/raw/\" ) DATA_PATH = PROJECT_PATH . joinpath ( \"data/global/processed/\" ) INTERIM_PATH = PROJECT_PATH . joinpath ( \"data/global/interim/\" ) MODEL_PATH = PROJECT_PATH . joinpath ( \"models/global/\" ) RESULTS_PATH = PROJECT_PATH . joinpath ( \"data/global/results/\" ) # check if path exists and is a directory assert FIG_PATH . exists () & FIG_PATH . is_dir () assert RAW_PATH . exists () & RAW_PATH . is_dir () assert DATA_PATH . exists () & DATA_PATH . is_dir () assert INTERIM_PATH . exists () & INTERIM_PATH . is_dir () assert MODEL_PATH . exists () & MODEL_PATH . is_dir () assert RESULTS_PATH . exists () & RESULTS_PATH . is_dir ()","title":"Important - Paths and Directories"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#python-packages","text":"# Standard packages import numpy as np import pandas as pd","title":"Python Packages"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#1-load-processed-global-data","text":"In this section, I will load the metadata and the actual data. The steps involved are: Define the filepath (check for existence) Open meta data and real data Check that the samples correspond to each other. Check if # of features are the same","title":"1. Load Processed Global Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#11-meta-data","text":"# name of file meta_name = \"METADATA_20200310.csv\" # get full path meta_file = DATA_PATH . joinpath ( meta_name ) # assert meta file exists error_msg = f \"File ' { meta_file . name } ' doesn't exist. Check name or directory.\" assert meta_file . exists (), error_msg # assert meta file is a file error_msg = f \"File ' { meta_file . name } ' isn't a file. Check name or directory.\" assert meta_file . is_file (), error_msg # open meta data meta_df = pd . read_csv ( f \" { meta_file } \" ) meta_df . head () . to_markdown () wmo n_cycle N lon lat juld date 0 2902086 1 1 88.6957 12.1639 23009.2 2012-12-30 03:58:59 1 2902086 10 10 88.6033 12.4128 23018.1 2013-01-08 03:24:59 2 2902086 100 64 86.2039 13.7915 23432.1 2014-02-26 03:34:59 3 2902086 101 65 86.3116 13.75 23437.1 2014-03-03 03:26:59 4 2902086 102 66 86.3971 13.7588 23442.1 2014-03-08 03:31:59","title":"1.1 - Meta Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#12-input-data","text":"# name of file data_name = \"SOCA_GLOBAL2_20200310.csv\" # get full path data_file = DATA_PATH . joinpath ( data_name ) # assert exists error_msg = f \"File ' { data_file . name } ' doesn't exist. Check name or directory.\" assert data_file . exists (), error_msg # assert meta file is a file error_msg = f \"File ' { data_file . name } ' isn't a file. Check name or directory.\" assert data_file . is_file (), error_msg # load data data_df = pd . read_csv ( f \" { data_file } \" ) data_df . head () . iloc [:, : 6 ] . to_markdown () N wmo n_cycle sla PAR RHO_WN_412 0 1 2.90209e+06 1 -4.7044 42.6541 0.0254617 1 2 2.90209e+06 2 -4.0382 42.6541 0.0254617 2 3 2.90209e+06 3 -3.4604 44.2927 0.0240942 3 4 2.90209e+06 4 -2.8404 42.7664 0.024917 4 5 2.90209e+06 5 -2.394 42.7664 0.024917","title":"1.2 - Input Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#13-checks","text":"I do a number of checks to make sure that our data follows a standard and that I am reproducing the same results. Number of samples are equal for both 7 meta features 48 data features (26 data + 19 levels + 3 meta) check features in columns # same number of samples error_msg = f \"Mismatch between meta and data: { data_df . shape [ 0 ] } =/= { meta_df . shape [ 0 ] } \" assert data_df . shape [ 0 ] == meta_df . shape [ 0 ], error_msg # check number of samples n_samples = 25413 error_msg = f \"Incorrect number of samples: { data_df . shape [ 0 ] } =/= { n_samples } \" assert data_df . shape [ 0 ] == n_samples , error_msg # check meta feature names meta_features = [ 'wmo' , 'n_cycle' , 'N' , 'lon' , 'lat' , 'juld' , 'date' ] error_msg = f \"Missing features in meta data.\" assert meta_df . columns . tolist () == meta_features , error_msg # check data feature names input_meta_features = [ 'N' , 'wmo' , 'n_cycle' ] input_features = [ 'sla' , 'PAR' , 'RHO_WN_412' , 'RHO_WN_443' , 'RHO_WN_490' , 'RHO_WN_555' , 'RHO_WN_670' , 'doy_sin' , 'doy_cos' , 'x_cart' , 'y_cart' , 'z_cart' , 'PC1' , 'PC2' , 'PC3' , 'PC4' , 'PC5' , 'PC6' , 'PC7' , 'PC1.1' , 'PC2.1' , 'PC3.1' , 'PC1.2' , 'PC2.2' , 'PC3.2' , 'PC4.1' ] output_features = [ 'bbp' , 'bbp.1' , 'bbp.2' , 'bbp.3' , 'bbp.4' , 'bbp.5' , 'bbp.6' , 'bbp.7' , 'bbp.8' , 'bbp.9' , 'bbp.10' , 'bbp.11' , 'bbp.12' , 'bbp.13' , 'bbp.14' , 'bbp.15' , 'bbp.16' , 'bbp.17' , 'bbp.18' ] features = input_meta_features + input_features + output_features error_msg = f \"Missing features in input data.\" assert data_df . columns . tolist () == features , error_msg","title":"1.3 - Checks"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#14-convert-metadata-to-indices-important","text":"To make our life easier, we're going to eliminate the need to keep track of meta data all of the time. So I'm going to merge the datasets together to form one dataframe. Then I will set the index to be the metadata values. The remaining parts will be columns which will be features. So in the end, we will have a dataframe where: the indices is the metadata (e.g. wmo, n_cycle) the columns are the actual features (e.g. sla, pca components, bbp, etc). # merge meta and data full_df = pd . merge ( meta_df , data_df ) # convert meta information to indices full_df = full_df . set_index ( meta_features ) # checks - check indices match metadata meta_features = [ 'wmo' , 'n_cycle' , 'N' , 'lon' , 'lat' , 'juld' , 'date' ] error_msg = f \"Missing features in input data.\" assert full_df . index . names == meta_features , error_msg # checks - check column names match feature names input_features = [ 'sla' , 'PAR' , 'RHO_WN_412' , 'RHO_WN_443' , 'RHO_WN_490' , 'RHO_WN_555' , 'RHO_WN_670' , 'doy_sin' , 'doy_cos' , 'x_cart' , 'y_cart' , 'z_cart' , 'PC1' , 'PC2' , 'PC3' , 'PC4' , 'PC5' , 'PC6' , 'PC7' , 'PC1.1' , 'PC2.1' , 'PC3.1' , 'PC1.2' , 'PC2.2' , 'PC3.2' , 'PC4.1' ] output_features = [ 'bbp' , 'bbp.1' , 'bbp.2' , 'bbp.3' , 'bbp.4' , 'bbp.5' , 'bbp.6' , 'bbp.7' , 'bbp.8' , 'bbp.9' , 'bbp.10' , 'bbp.11' , 'bbp.12' , 'bbp.13' , 'bbp.14' , 'bbp.15' , 'bbp.16' , 'bbp.17' , 'bbp.18' ] features = input_features + output_features error_msg = f \"Missing features in input data.\" assert full_df . columns . tolist () == features , error_msg print ( 'Dataframe Features:' , full_df . shape ) full_df . columns . tolist ()[: 10 ] 1 2 3 4 5 6 7 8 9 10 11 12 Dataframe Features: (25413, 45) ['sla', 'PAR', 'RHO_WN_412', 'RHO_WN_443', 'RHO_WN_490', 'RHO_WN_555', 'RHO_WN_670', 'doy_sin', 'doy_cos', 'x_cart'] print ( 'Dataframe Indices (meta vars):' , len ( full_df . index . names )) full_df . index . names 1 2 3 Dataframe Indices (meta vars): 7 FrozenList(['wmo', 'n_cycle', 'N', 'lon', 'lat', 'juld', 'date'])","title":"1.4 - Convert metadata to indices (Important)"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#2-training-and-test-split","text":"","title":"2 - Training and Test Split"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#21-independent-set-i-soca2016","text":"This independent set has a set number of independent floats which are not counted in the training or validation phase. These floats were in a paper (Sauzede et. al., 2016) and used during the testing phase to showcase how well the models did. 6901472 6901493 6901523 6901496 So we need to take these away from the data. # soca2016 independent floats soca2016_floats = [ \"6901472\" , \"6901493\" , \"6901523\" , \"6901496\" ] # subset soca2016 floats soca2016_df = full_df [ full_df . index . isin ( soca2016_floats , level = 'wmo' )]","title":"2.1 - Independent Set I (SOCA2016)"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#checks","text":"# check number of samples (meta, inputs) n_samples = 378 error_msg = f \"Incorrect number of samples for soca2016 floats: { soca2016_df . shape [ 0 ] } =/= { n_samples } \" assert soca2016_df . shape [ 0 ] == n_samples , error_msg","title":"Checks"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#22-indpendent-set-ii-isprs2020","text":"This independent set was a set of floats taken from the ISPRS paper (Sauzede et. al., 2020 (pending...)). These floats were used as the independent testing set to showcase the performance of the ML methods. 6901486 (North Atlantic?) 3902121 (Subtropical Gyre?) So we need to take these away from the data. # isprs2020 independent floats isprs2020_floats = [ \"6901486\" , \"3902121\" ] # isprs2020 independent floats isprs2020_df = full_df [ full_df . index . isin ( isprs2020_floats , level = 'wmo' )]","title":"2.2 - Indpendent Set II (ISPRS2020)"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#checks_1","text":"# check number of samples (meta, inputs) n_samples = 331 error_msg = f \"Incorrect number of samples for isprs2016 floats: { isprs2020_df . shape [ 0 ] } =/= { n_samples } \" assert isprs2020_df . shape [ 0 ] == n_samples , error_msg","title":"Checks"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#23-ml-data","text":"Now we want to subset the input data to be used for the ML models. Basically, we can subset all datasets that are not in the independent floats. In addition, we want all of the variables in the input features that we provided earlier. # subset non-independent flows ml_df = full_df [ ~ full_df . index . isin ( isprs2020_floats + soca2016_floats , level = 'wmo' )]","title":"2.3 - ML Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#checks_2","text":"# check number of samples (meta, inputs) n_samples = 24704 error_msg = f \"Incorrect number of samples for non-independent floats: { ml_df . shape [ 0 ] } =/= { n_samples } \" assert ml_df . shape [ 0 ] == n_samples , error_msg","title":"Checks"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#24-inputs-outputs","text":"Lastly, we need to split the data into training, validation (and possibly testing). Recall that all the inputs are already written above and the outputs as well. input_df = ml_df [ input_features ] output_df = ml_df [ output_features ] # checks - Input Features n_input_features = 26 error_msg = f \"Incorrect number of features for input df: { input_df . shape [ 1 ] } =/= { n_input_features } \" assert input_df . shape [ 1 ] == n_input_features , error_msg # checks - Output Features n_output_features = 19 error_msg = f \"Incorrect number of features for output df: { output_df . shape [ 1 ] } =/= { n_output_features } \" assert output_df . shape [ 1 ] == n_output_features , error_msg","title":"2.4 - Inputs, Outputs"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#3-final-dataset-saving","text":"","title":"3. Final Dataset (saving)"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#31-print-out-data-dimensions-w-metadata","text":"print ( \"Input Data:\" , input_df . shape ) print ( \"Output Data:\" , output_df . shape ) print ( \"SOCA2016 Independent Data:\" , soca2016_df [ input_features ] . shape , soca2016_df [ output_features ] . shape ) print ( \"ISPRS2016 Independent Data:\" , isprs2020_df [ input_features ] . shape , isprs2020_df [ output_features ] . shape ) 1 2 3 4 Input Data: (24704, 26) Output Data: (24704, 19) SOCA2016 Independent Data: (378, 26) (378, 19) ISPRS2016 Independent Data: (331, 26) (331, 19)","title":"3.1 - Print out data dimensions (w. metadata)"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#32-saving","text":"We're going to save the data in the global/interim/ path. This is to prevent any overwrites. We also need to index=True for the savefile in order to preserve the metadata indices. input_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" , index = True )","title":"3.2 - Saving"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#33-loading","text":"This is a tiny bit tricky if we want to preserve the meta data as the indices. So we need to set the index to be the same meta columns that we used last time via the .set_index(meta_vars) command. test_inputs_df = pd . read_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" ) # add index test_inputs_df = test_inputs_df . set_index ( meta_features )","title":"3.3 - Loading"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#34-checking","text":"So curiously, we cannot compare the dataframes directly because there is some numerical error when saving them. But if we calculate the exact differences between them, we find that they are almost equal. See below what happens if we calculate the exact difference between the arrays. # example are they exactly the same? # np.testing.assert_array_equal(test_inputs_df.describe(), input_df.describe()) np . testing . assert_array_equal ( test_inputs_df . values , input_df . values ) AssertionError : Arrays are not equal Mismatched elements : 96056 / 642304 ( 15 % ) Max absolute difference : 1.42108547e-14 Max relative difference : 3.92139227e-16 x : array ([[ - 4.704400e+00 , 4.265410e+01 , 2.546170e-02 , ... , - 3.458944e+00 , - 1.017509e-02 , - 1.025450e+00 ], [ - 9.015000e-01 , 4.455050e+01 , 2.060340e-02 , ... , - 3.691716e+00 , ... y : array ([[ - 4.704400e+00 , 4.265410e+01 , 2.546170e-02 , ... , - 3.458944e+00 , - 1.017509e-02 , - 1.025450e+00 ], [ - 9.015000e-01 , 4.455050e+01 , 2.060340e-02 , ... , - 3.691716e+00 , ... We get an assertion error that they're not equal. There is a mismatch difference of order 1e-15 for the absolute and relative differences. That's numerical error probably due to compression that comes when saving and loading data. Let's check again but with a little less expected precision. np . testing . assert_array_almost_equal ( test_inputs_df . values , input_df . values , decimal = 1e-14 ) so just by reducing the precision by a smidge (1e-14 instead of 1e-15), we find that the arrays are the same. So we can trust it.","title":"3.4 - Checking"},{"location":"projects/ML4OCN/ARGO_project/global_data/1_load_processed_data/#35-save-the-rest-of-the-data","text":"input_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'inputs.csv' ) } \" , index = True ) output_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'outputs.csv' ) } \" , index = True ) soca2016_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'soca2016.csv' ) } \" , index = True ) isprs2020_df . to_csv ( f \" { INTERIM_PATH . joinpath ( 'isprs2020.csv' ) } \" , index = True )","title":"3.5 - Save the rest of the data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/","text":"Baseline Algorithms \u00b6 Code Block # install specific version of pandas and progress bar just in case ! pip install tqdm pandas == 1.0 . 3 import pathlib import sys # define the top level directory PROJECT_PATH = pathlib . Path ( \"/media/disk/erc/papers/2019_ML_OCN/\" ) CODE_PATH = PROJECT_PATH . joinpath ( \"ml4ocean\" ) sys . path . append ( str ( CODE_PATH )) # ml4ocean packages from src.utils import get_paths from src.data.world import get_full_data , world_features from src.features.world import subset_independent_floats PATHS = get_paths () # standard pacakges import tqdm import time import numpy as np import pandas as pd from scipy import stats # ML packages from sklearn.neural_network import MLPRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import BaggingRegressor # ML preprocessing from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold # statistics from sklearn.metrics import r2_score , mean_absolute_error , mean_squared_error import statsmodels.api as sm # plotting import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-paper' ]) import warnings warnings . simplefilter ( \"ignore\" ) % load_ext autoreload % autoreload 2 Data \u00b6 Code Block inputs_df = pd . read_csv ( f \" { PATHS . data_interim . joinpath ( 'inputs.csv' ) } \" ) outputs_df = pd . read_csv ( f \" { PATHS . data_interim . joinpath ( 'outputs.csv' ) } \" ) # create meta index inputs_df = inputs_df . set_index ( world_features . meta ) outputs_df = outputs_df . set_index ( world_features . meta ) Train-Test Split \u00b6 Code Block train_size = 0.8 random_state = 42 xtrain , xtest , ytrain , ytest = train_test_split ( inputs_df , outputs_df , train_size = train_size , random_state = random_state ) print ( xtrain . shape , xtest . shape ) print ( ytrain . shape , ytest . shape ) 1 2 (19763, 26) (4941, 26) (19763, 19) (4941, 19) Standardize the Inputs / Outputs \u00b6 Code Block x_scaler = StandardScaler () . fit ( xtrain ) y_scaler = StandardScaler ( with_std = False ) . fit ( ytrain ) # scale inputs xtrain_scaled = pd . DataFrame ( x_scaler . transform ( xtrain ), columns = xtrain . columns , index = xtrain . index ) xtest_scaled = pd . DataFrame ( x_scaler . transform ( xtest ), columns = xtest . columns , index = xtest . index ) # scale outputs ytrain_scaled = pd . DataFrame ( y_scaler . transform ( ytrain ), columns = ytrain . columns , index = ytrain . index ) ytest_scaled = pd . DataFrame ( y_scaler . transform ( ytest ), columns = ytest . columns , index = ytest . index ) sns . distplot ( xtrain_scaled . iloc [:, 14 ], label = 'X' ) sns . distplot ( ytrain_scaled . iloc [:, 10 ], label = 'Y' ) plt . legend ( fontsize = 20 ) 1 <matplotlib.legend.Legend at 0x7fcac3ba3080> ML Algorithms \u00b6 Model - Random Forest Regressor \u00b6 Code Block Define Model model = RandomForestRegressor ( n_estimators = 2_000 , random_state = 42 , n_jobs =- 1 , verbose = 1 , warm_start = True , criterion = 'mse' ) Train Model t0 = time . time () model . fit ( xtrain_scaled , ytrain_scaled ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 } secs\" ) 1 2 3 4 5 6 7 8 9 10 [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 144 tasks | elapsed: 7.8s [Parallel(n_jobs=-1)]: Done 394 tasks | elapsed: 19.4s [Parallel(n_jobs=-1)]: Done 744 tasks | elapsed: 35.4s [Parallel(n_jobs=-1)]: Done 1194 tasks | elapsed: 56.9s [Parallel(n_jobs=-1)]: Done 1744 tasks | elapsed: 1.4min Time Taken: 95.3053548336029 secs [Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed: 1.6min finished Visualizations \u00b6 Training Data \u00b6 Code Block ypred_train = model . predict ( xtrain_scaled ) ypred_train = pd . DataFrame ( ytrain_scaled , columns = ytrain_scaled . columns , index = ytrain_scaled . index ) 1 2 3 4 5 6 7 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.2s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.5s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.9s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 1.3s [Parallel(n_jobs=28)]: Done 1744 tasks | elapsed: 1.8s [Parallel(n_jobs=28)]: Done 2000 out of 2000 | elapsed: 2.1s finished np . minimum ( y ) from src.visualization.world import plot_residuals fig , ax = plot_residuals ( ypred_train . values . ravel (), ytrain_scaled . values . ravel (), 'train' ) Fig : Results for Training Data Testing Data \u00b6 Code Block # Apply MLP to the test dataset ypred_scaled = model . predict ( xtest_scaled ) # Detransform the outputs: ypred = pd . DataFrame ( y_scaler . inverse_transform ( ypred_scaled ), columns = ytest_scaled . columns , index = ytest_scaled . index ) ytest_c = pd . DataFrame ( y_scaler . inverse_transform ( ytest_scaled ), columns = ytest_scaled . columns , index = ytest_scaled . index ) # make vectors ypred_vec = ypred . values . ravel () ytest_vec = ytest_c . values . ravel () 1 2 3 4 5 6 7 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.2s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.4s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.6s [Parallel(n_jobs=28)]: Done 1744 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 2000 out of 2000 | elapsed: 0.8s finished plot_residuals ( ytest_vec , ypred_vec , 'test' ) Fig : Results for Testing Data Test Statistics \u00b6 Code Block def median_absolute_percentage_error ( y_true , y_pred ): y_true , y_pred = np . array ( y_true ), np . array ( y_pred ) return np . median ( np . abs (( y_true - y_pred ) / y_true )) * 100 mapd = median_absolute_percentage_error ( pow ( 10 , ytest_vec ), pow ( 10 , ypred_vec )) mse = mean_squared_error ( pow ( 10 , ytest_vec ), pow ( 10 , ypred_vec )) rmse = np . sqrt ( mse ) r2 = r2_score ( ytest_vec , ypred_vec ) slope_0 , * _ = stats . linregress ( ytest_vec , ypred_vec ) print ( f \"MAPD: { mapd : .2f } %\" ) print ( f \"MSE: { mse : .4f } \" ) print ( f \"RMSE: { rmse : .4f } \" ) print ( f \"R2: { r2 : .4f } \" ) print ( f \"Slope: { slope_0 : .4f } \" ) Statistic Value MAPD 10.99% MSE 0.0000 RMSE 0.0004 R2 0.8701 Slope 0.8572 Cross Validation \u00b6 Method I - From Scratch \u00b6 Code Block Transform Function def standardize_data ( Xtrain , Xtest , ytrain , ytest ): x_scaler = StandardScaler () . fit ( Xtrain ) y_scaler = StandardScaler ( with_std = False ) . fit ( ytrain ) # scale inputs Xtrain_scaled = x_scaler . transform ( Xtrain ) Xtest_scaled = x_scaler . transform ( Xtest ) # scale outputs ytrain_scaled = y_scaler . transform ( ytrain ) ytest_scaled = y_scaler . transform ( ytest ) scalers = { 'x' : x_scaler , 'y' : y_scaler } return Xtrain_scaled , Xtest_scaled , ytrain_scaled , ytest_scaled , scalers Cross Validation # define the model model = RandomForestRegressor ( n_estimators = 2_000 , random_state = 42 , n_jobs =- 1 , verbose = 0 , warm_start = False , criterion = 'mse' ) # define the split n_splits = 10 kf = KFold ( n_splits = n_splits ) predictions_test = list () predictions_train = list () for train_index , test_index in tqdm . tqdm ( kf . split ( inputs_df . values ), total = n_splits ): # segment data Xtr , Xte , ytr , yte , scalers = standardize_data ( inputs_df . values [ train_index , :], inputs_df . values [ test_index , :], outputs_df . values [ train_index , :], outputs_df . values [ test_index , :], ) # train model model . fit ( Xtr , ytr ) # predictions predictions_train . append (( model . predict ( Xtr ), ytr , scalers )) predictions_test . append (( model . predict ( Xte ), yte , scalers )) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [18:10<00:00, 109.07s/it] Training Data \u00b6 [ r2_score ( iscale [ 'y' ] . inverse_transform ( x ) . ravel (), iscale [ 'y' ] . inverse_transform ( y ) . ravel ()) for ( y , x , iscale ) in predictions_train ] 1 2 3 4 5 6 7 8 9 10 [0.9831090704366079, 0.9835754471993815, 0.983328216844481, 0.9833219144776779, 0.983307743737022, 0.9829904097669607, 0.982817987510845, 0.983174345701849, 0.9830150417288622, 0.9828010707842838] Testing Data \u00b6 [ r2_score ( iscale [ 'y' ] . inverse_transform ( x ) . ravel (), iscale [ 'y' ] . inverse_transform ( y ) . ravel ()) for ( y , x , iscale ) in predictions_test ] 1 2 3 4 5 6 7 8 9 10 [0.7186974424983624, 0.7071992892898138, 0.6647674379299724, 0.8125536947385796, 0.7271490083454055, 0.6957871257819145, 0.8164298170713613, 0.6147107331177086, 0.7808035958518491, 0.7484396890420137] Other Ways \u00b6 Tutorial sklearn visualize","title":"3.0 Baseline Models"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#baseline-algorithms","text":"Code Block # install specific version of pandas and progress bar just in case ! pip install tqdm pandas == 1.0 . 3 import pathlib import sys # define the top level directory PROJECT_PATH = pathlib . Path ( \"/media/disk/erc/papers/2019_ML_OCN/\" ) CODE_PATH = PROJECT_PATH . joinpath ( \"ml4ocean\" ) sys . path . append ( str ( CODE_PATH )) # ml4ocean packages from src.utils import get_paths from src.data.world import get_full_data , world_features from src.features.world import subset_independent_floats PATHS = get_paths () # standard pacakges import tqdm import time import numpy as np import pandas as pd from scipy import stats # ML packages from sklearn.neural_network import MLPRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import BaggingRegressor # ML preprocessing from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold # statistics from sklearn.metrics import r2_score , mean_absolute_error , mean_squared_error import statsmodels.api as sm # plotting import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-paper' ]) import warnings warnings . simplefilter ( \"ignore\" ) % load_ext autoreload % autoreload 2","title":"Baseline Algorithms"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#data","text":"Code Block inputs_df = pd . read_csv ( f \" { PATHS . data_interim . joinpath ( 'inputs.csv' ) } \" ) outputs_df = pd . read_csv ( f \" { PATHS . data_interim . joinpath ( 'outputs.csv' ) } \" ) # create meta index inputs_df = inputs_df . set_index ( world_features . meta ) outputs_df = outputs_df . set_index ( world_features . meta )","title":"Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#train-test-split","text":"Code Block train_size = 0.8 random_state = 42 xtrain , xtest , ytrain , ytest = train_test_split ( inputs_df , outputs_df , train_size = train_size , random_state = random_state ) print ( xtrain . shape , xtest . shape ) print ( ytrain . shape , ytest . shape ) 1 2 (19763, 26) (4941, 26) (19763, 19) (4941, 19)","title":"Train-Test Split"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#standardize-the-inputs-outputs","text":"Code Block x_scaler = StandardScaler () . fit ( xtrain ) y_scaler = StandardScaler ( with_std = False ) . fit ( ytrain ) # scale inputs xtrain_scaled = pd . DataFrame ( x_scaler . transform ( xtrain ), columns = xtrain . columns , index = xtrain . index ) xtest_scaled = pd . DataFrame ( x_scaler . transform ( xtest ), columns = xtest . columns , index = xtest . index ) # scale outputs ytrain_scaled = pd . DataFrame ( y_scaler . transform ( ytrain ), columns = ytrain . columns , index = ytrain . index ) ytest_scaled = pd . DataFrame ( y_scaler . transform ( ytest ), columns = ytest . columns , index = ytest . index ) sns . distplot ( xtrain_scaled . iloc [:, 14 ], label = 'X' ) sns . distplot ( ytrain_scaled . iloc [:, 10 ], label = 'Y' ) plt . legend ( fontsize = 20 ) 1 <matplotlib.legend.Legend at 0x7fcac3ba3080>","title":"Standardize the Inputs / Outputs"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#ml-algorithms","text":"","title":"ML Algorithms"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#model-random-forest-regressor","text":"Code Block Define Model model = RandomForestRegressor ( n_estimators = 2_000 , random_state = 42 , n_jobs =- 1 , verbose = 1 , warm_start = True , criterion = 'mse' ) Train Model t0 = time . time () model . fit ( xtrain_scaled , ytrain_scaled ) t1 = time . time () - t0 print ( f \"Time Taken: { t1 } secs\" ) 1 2 3 4 5 6 7 8 9 10 [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 144 tasks | elapsed: 7.8s [Parallel(n_jobs=-1)]: Done 394 tasks | elapsed: 19.4s [Parallel(n_jobs=-1)]: Done 744 tasks | elapsed: 35.4s [Parallel(n_jobs=-1)]: Done 1194 tasks | elapsed: 56.9s [Parallel(n_jobs=-1)]: Done 1744 tasks | elapsed: 1.4min Time Taken: 95.3053548336029 secs [Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed: 1.6min finished","title":"Model - Random Forest Regressor"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#visualizations","text":"","title":"Visualizations"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#training-data","text":"Code Block ypred_train = model . predict ( xtrain_scaled ) ypred_train = pd . DataFrame ( ytrain_scaled , columns = ytrain_scaled . columns , index = ytrain_scaled . index ) 1 2 3 4 5 6 7 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.2s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.5s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.9s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 1.3s [Parallel(n_jobs=28)]: Done 1744 tasks | elapsed: 1.8s [Parallel(n_jobs=28)]: Done 2000 out of 2000 | elapsed: 2.1s finished np . minimum ( y ) from src.visualization.world import plot_residuals fig , ax = plot_residuals ( ypred_train . values . ravel (), ytrain_scaled . values . ravel (), 'train' ) Fig : Results for Training Data","title":"Training Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#testing-data","text":"Code Block # Apply MLP to the test dataset ypred_scaled = model . predict ( xtest_scaled ) # Detransform the outputs: ypred = pd . DataFrame ( y_scaler . inverse_transform ( ypred_scaled ), columns = ytest_scaled . columns , index = ytest_scaled . index ) ytest_c = pd . DataFrame ( y_scaler . inverse_transform ( ytest_scaled ), columns = ytest_scaled . columns , index = ytest_scaled . index ) # make vectors ypred_vec = ypred . values . ravel () ytest_vec = ytest_c . values . ravel () 1 2 3 4 5 6 7 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.2s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.4s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.6s [Parallel(n_jobs=28)]: Done 1744 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 2000 out of 2000 | elapsed: 0.8s finished plot_residuals ( ytest_vec , ypred_vec , 'test' ) Fig : Results for Testing Data","title":"Testing Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#test-statistics","text":"Code Block def median_absolute_percentage_error ( y_true , y_pred ): y_true , y_pred = np . array ( y_true ), np . array ( y_pred ) return np . median ( np . abs (( y_true - y_pred ) / y_true )) * 100 mapd = median_absolute_percentage_error ( pow ( 10 , ytest_vec ), pow ( 10 , ypred_vec )) mse = mean_squared_error ( pow ( 10 , ytest_vec ), pow ( 10 , ypred_vec )) rmse = np . sqrt ( mse ) r2 = r2_score ( ytest_vec , ypred_vec ) slope_0 , * _ = stats . linregress ( ytest_vec , ypred_vec ) print ( f \"MAPD: { mapd : .2f } %\" ) print ( f \"MSE: { mse : .4f } \" ) print ( f \"RMSE: { rmse : .4f } \" ) print ( f \"R2: { r2 : .4f } \" ) print ( f \"Slope: { slope_0 : .4f } \" ) Statistic Value MAPD 10.99% MSE 0.0000 RMSE 0.0004 R2 0.8701 Slope 0.8572","title":"Test Statistics"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#cross-validation","text":"","title":"Cross Validation"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#method-i-from-scratch","text":"Code Block Transform Function def standardize_data ( Xtrain , Xtest , ytrain , ytest ): x_scaler = StandardScaler () . fit ( Xtrain ) y_scaler = StandardScaler ( with_std = False ) . fit ( ytrain ) # scale inputs Xtrain_scaled = x_scaler . transform ( Xtrain ) Xtest_scaled = x_scaler . transform ( Xtest ) # scale outputs ytrain_scaled = y_scaler . transform ( ytrain ) ytest_scaled = y_scaler . transform ( ytest ) scalers = { 'x' : x_scaler , 'y' : y_scaler } return Xtrain_scaled , Xtest_scaled , ytrain_scaled , ytest_scaled , scalers Cross Validation # define the model model = RandomForestRegressor ( n_estimators = 2_000 , random_state = 42 , n_jobs =- 1 , verbose = 0 , warm_start = False , criterion = 'mse' ) # define the split n_splits = 10 kf = KFold ( n_splits = n_splits ) predictions_test = list () predictions_train = list () for train_index , test_index in tqdm . tqdm ( kf . split ( inputs_df . values ), total = n_splits ): # segment data Xtr , Xte , ytr , yte , scalers = standardize_data ( inputs_df . values [ train_index , :], inputs_df . values [ test_index , :], outputs_df . values [ train_index , :], outputs_df . values [ test_index , :], ) # train model model . fit ( Xtr , ytr ) # predictions predictions_train . append (( model . predict ( Xtr ), ytr , scalers )) predictions_test . append (( model . predict ( Xte ), yte , scalers )) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [18:10<00:00, 109.07s/it]","title":"Method I - From Scratch"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#training-data_1","text":"[ r2_score ( iscale [ 'y' ] . inverse_transform ( x ) . ravel (), iscale [ 'y' ] . inverse_transform ( y ) . ravel ()) for ( y , x , iscale ) in predictions_train ] 1 2 3 4 5 6 7 8 9 10 [0.9831090704366079, 0.9835754471993815, 0.983328216844481, 0.9833219144776779, 0.983307743737022, 0.9829904097669607, 0.982817987510845, 0.983174345701849, 0.9830150417288622, 0.9828010707842838]","title":"Training Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#testing-data_1","text":"[ r2_score ( iscale [ 'y' ] . inverse_transform ( x ) . ravel (), iscale [ 'y' ] . inverse_transform ( y ) . ravel ()) for ( y , x , iscale ) in predictions_test ] 1 2 3 4 5 6 7 8 9 10 [0.7186974424983624, 0.7071992892898138, 0.6647674379299724, 0.8125536947385796, 0.7271490083454055, 0.6957871257819145, 0.8164298170713613, 0.6147107331177086, 0.7808035958518491, 0.7484396890420137]","title":"Testing Data"},{"location":"projects/ML4OCN/ARGO_project/global_data/2_ml_algorithms/#other-ways","text":"Tutorial sklearn visualize","title":"Other Ways"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/","text":"Full Walkthrough \u00b6 This notebook will walkthrough the preprocessing steps as well as the ML algorithm training procedure used for the multi-dimensional, multi-output data. Experiment Overview \u00b6 Code \u00b6 Packages \u00b6 import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels # Features from features.build_features import times_2_cycles , geo_2_cartesian , get_geodataframe , CycleTransform , GeoCartTransform from features.pca_features import transform_all , transform_individual from features.analysis import get_stats from sklearn.preprocessing import StandardScaler # ML Models from sklearn.model_selection import train_test_split from models.baseline import train_rf_model # Visualization from visualization.visualize import plot_mo_stats , plot_geolocations from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2 1 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload 1 Load Data \u00b6 1.1 Core Data \u00b6 In this step, we will load the standard data. This includes the following variables: SLA PAR RHO WN 412 RHO WN 443 RHO WN 490 RHO WN 555 RHO WN 670 MLD Lat Lon DOY params = { 'region' : 'na' , 'n_components' : 5 , 'pca_seed' : 123 } dataloader = DataLoader () # load training data X_core = load_standard_data ( params [ 'region' ], training = True ) # Testing Data X_core_te = load_standard_data ( params [ 'region' ], training = False ) X_core_te = X_core_te . iloc [:, 2 :] Plot Training Data \u00b6 X_core_na = load_standard_data ( 'NA' , training = True ) X_core_stg = load_standard_data ( 'STG' , training = True ) # convert dataframe into geopandas dataframe X_core_na_gdf = get_geodataframe ( X_core_na . copy ()) X_core_na_stg = get_geodataframe ( X_core_stg . copy ()) # plot world map with points colors = [ 'blue' , 'red' ] plot_geolocations ([ X_core_na_gdf , X_core_na_stg ], colors = [ 'blue' , 'red' ], save_name = 'both' ) Plot Testing Regions \u00b6 X_core_na = load_standard_data ( 'NA' , training = False ) X_core_stg = load_standard_data ( 'STG' , training = False ) # convert dataframe into geopandas dataframe X_core_na_gdf = get_geodataframe ( X_core_na . copy ()) X_core_na_stg = get_geodataframe ( X_core_stg . copy ()) # plot world map with points colors = [ 'blue' , 'red' ] plot_geolocations ([ X_core_na_gdf , X_core_na_stg ], colors = [ 'blue' , 'red' ], save_name = 'both' ) 1.2 High Dimensional Data \u00b6 In this section, we will extract the high dimensional datasets. They include: Temperature Density Salinity Spiciness # =================== # Training Data # =================== # load data from dataloader X_temp , X_dens , X_sal , X_spicy = load_high_dim_data ( params [ 'region' ], training = True ) # add prefix (Training/Validation) X_temp = X_temp . add_prefix ( \"temp_\" ) X_dens = X_dens . add_prefix ( \"dens_\" ) X_sal = X_sal . add_prefix ( \"sal_\" ) X_spicy = X_spicy . add_prefix ( \"spice_\" ) # =================== # Test Data # =================== # load data from dataloader X_temp_te , X_dens_te , X_sal_te , X_spicy_te = load_high_dim_data ( params [ 'region' ], training = False ) # Subset meta columns X_temp_te = X_temp_te . iloc [:, 2 :] X_dens_te = X_dens_te . iloc [:, 2 :] X_sal_te = X_sal_te . iloc [:, 2 :] X_spicy_te = X_spicy_te . iloc [:, 2 :] # add prefix (Test) X_temp_te = X_temp_te . add_prefix ( \"temp_\" ) X_dens_te = X_dens_te . add_prefix ( \"dens_\" ) X_sal_te = X_sal_te . add_prefix ( \"sal_\" ) X_spicy_te = X_spicy_te . add_prefix ( \"spice_\" ) 1.3 - Concatenate Data \u00b6 # Concatenate Data # Training Data Xtr = pd . concat ([ X_core , X_temp , X_dens , X_sal , X_spicy ], axis = 1 ) # Testing Data Xte = pd . concat ([ X_core_te , X_temp_te , X_dens_te , X_sal_te , X_spicy_te ], axis = 1 ) 1.4 - Multi-Output Data \u00b6 We load the multioutput regression labels. # =================== # Labels # =================== ytr = load_labels ( params [ 'region' ], training = True ) ytest = load_labels ( params [ 'region' ], training = False ) # remove meta-columns ytest = ytest . iloc [:, 2 :] 4 - Post-Split Transformations \u00b6 PCA Transform High Dimensional Variables In this step, we do a PCA transformation on the concatenation for the high dimensional variables temp , sal , dens , and spicy . We will reduce the dimensionality to about 10 features. Normalize Core Variables We will use a standard scaler to make the core variables with a mean of 0 and standard deviation of 1. The ML algorithms tend to perform better with this type of standardization. Coordinate Transformation In this step, we will do a simple coordinate transformation of the lat,lon variables from geospatial to cartesian coordinates. This will increase the dimensionality of our dataset from 11 dimensions to 12 dimensions. Time Transformation In this step, we will transform the doy coordinates to cycles of sines and cosines. This will increase the dimensionality of our data from from 12 to 13. from sklearn.compose import ColumnTransformer from sklearn.base import BaseEstimator , TransformerMixin from sklearn.decomposition import PCA 4.1 - Input Preprocessing \u00b6 # new columns columns dataloader = DataLoader () columns = dataloader . load_columns () n_components = 5 times = [ 'doy' ] # create new columns new_columns = [ * [ \"doy_cos\" , \"doy_sin\" ], * [ 'x' , 'y' , 'z' ,], * [ f \"temperature_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"density_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"salinity_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"spicy_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * columns [ \"core\" ], ] seed = 123 # define transfomer X_pre_transformer = ColumnTransformer ( [ ( \"time\" , CycleTransform ( columns [ \"time\" ]), columns [ \"time\" ]), ( \"location\" , GeoCartTransform (), columns [ \"location\" ]), ( \"temperature\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"temperature\" ], ), ( \"density\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"density\" ], ), ( \"salinity\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"salinity\" ], ), ( \"spicy\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"spicy\" ], ), ( \"core\" , StandardScaler ( with_mean = True , with_std = True ), columns [ \"core\" ], ), ], remainder = \"passthrough\" , ) # transform data X_pre_transformer . fit ( Xtr ) # transform data Xtr = X_pre_transformer . transform ( Xtr ) Xte = X_pre_transformer . transform ( Xte ) Plot PCA Components \u00b6 SAVE_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/reports/figures/\" save_name = 'na' # y_ticks = np.arange(0, len(feature_names)) plt . style . use ([ 'seaborn-talk' ]) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) plt . plot ( X_pre_transformer . named_transformers_ [ 'temperature' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Temperature' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'density' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Density' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'salinity' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Salinity' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'spicy' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Spiciness' ) # ax.set_title(\"Random Forest Feature Importances (MDI)\") ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . grid ( alpha = 0.6 , color = 'gray' , zorder = 0 ) ax . set_ylim ([ 0.8 , 1.01 ]) plt . legend ( fontsize = 20 , loc = 'lower right' ) plt . tight_layout () plt . show () # fig.savefig(SAVE_PATH + f\"evar_{save_name}.png\") 3 - Train,Test Split \u00b6 We split the data into 80% training and 20% training. Note : because the dataset we are dealing with is only ~3,000 data points, we will do some bootstrap techniques in the full experiment to see how well we do with different subsamples of data. # train-test split Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtr , ytr , train_size = 0.8 , random_state = 123 ) 4 - Post Output Transformations \u00b6 The distribution of the outputs are skewed because there is a lot more variability in the upper depths than the lower depths. Because the distribution of the outputs are fairly skewed, we propose to do a log transformation to make them normally distributed. We do the following: Do a log10 transformation Standardize the Outputs from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer , StandardScaler # Inverse log10 function def loginv ( x ): return 10 ** x # create pipeline y_transformer = Pipeline ([ # Log10 transformer ( \"log\" , FunctionTransformer ( func = np . log10 , inverse_func = loginv )), # Standardize the outputs (helps with NN models) ( \"scale\" , StandardScaler ( with_mean = True , with_std = True )) ]) # transform data ytrain = y_transformer . fit_transform ( ytrain ) yvalid = y_transformer . transform ( yvalid ) ytest = y_transformer . fit_transform ( ytest ) 5 - Train ML Model \u00b6 In this section, we will use a standard random forest (RF) regressor to train on our dataset. def save_model ( model , save_name ): # model path MODEL_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/models/control/\" # save model from joblib import dump dump ( model , MODEL_PATH + save_name + '.joblib' ) return None model_name = 'morf' # RF params rf_params = { \"n_estimators\" : 1_500 , \"criterion\" : \"mse\" , \"n_jobs\" : - 1 , \"random_state\" : 123 , \"warm_start\" : False , \"verbose\" : 1 , } # train model model = train_rf_model ( Xtrain , ytrain , params = rf_params ) # save model save_model ( model , f \" { model_name } _na\" ) 1 2 3 4 5 6 7 8 9 [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 144 tasks | elapsed: 4.1s [Parallel(n_jobs=-1)]: Done 394 tasks | elapsed: 10.2s [Parallel(n_jobs=-1)]: Done 744 tasks | elapsed: 18.6s [Parallel(n_jobs=-1)]: Done 1194 tasks | elapsed: 29.4s [Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 36.7s finished Training time: 37.415 secs. 6 - Test ML Model \u00b6 6.1 - Training Data Results \u00b6 This is often not reported but it is very good to check how well a model does on the initial training data because we have no entered a validation set. If we find that the training stats are too high and the testing stats are very low then we know that we're either overfitting and/or our model is not generalizing very well. from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score def get_stats ( y_pred : np . ndarray , y : np . ndarray , each_level : bool = False ): stats = pd . DataFrame () # Get total average statistics if each_level == False : multioutput = 'uniform_average' mae = mean_absolute_error ( y , ypred , multioutput = multioutput ) mse = mean_squared_error ( y , ypred , multioutput = multioutput ) rmse = mse r2 = r2_score ( y , ypred , multioutput = multioutput ) stats = pd . DataFrame ( data = [[ mae , mse , rmse , r2 ]], columns = [ 'mae' , 'mse' , 'rmse' , 'r2' ]) elif each_level == True : multioutput = 'raw_values' stats [ 'mae' ] = mean_absolute_error ( y , ypred , multioutput = multioutput ) stats [ 'mse' ] = mean_squared_error ( y , ypred , multioutput = multioutput ) stats [ 'rmse' ] = np . sqrt ( stats [ 'mse' ]) stats [ 'r2' ] = r2_score ( y , ypred , multioutput = multioutput ) else : raise ValueError ( f \"Unrecognized stat request: { each_level } \" ) return stats First we will look at the statistics on average to see what we get. ypred = model . predict ( Xtrain ) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 1.5s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 2.2s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 2.7s finished min_lim = np . min ( np . concatenate (( ypred , ytrain ))) max_lim = np . max ( np . concatenate (( ypred , ytrain ))) plt . scatter ( ypred , ytrain , s = 0.1 ) plt . plot ( np . linspace ( min_lim , max_lim ), np . linspace ( min_lim , max_lim ), color = 'black' , zorder = 3 ) plt . xlim ([ min_lim , max_lim ]) plt . ylim ([ min_lim , max_lim ]) 1 (-5.863397779903146, 13.594780012746048) Then we can look at the statistics for each level. import statsmodels.api as smi stat_mod = smi . OLS ( ypred . ravel (), ytrain . ravel ()) res = stat_mod . fit () print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: y R-squared (uncentered): 0.973 Model: OLS Adj. R-squared (uncentered): 0.973 Method: Least Squares F-statistic: 2.292e+07 Date: Tue, 25 Feb 2020 Prob (F-statistic): 0.00 Time: 12:54:17 Log-Likelihood: 3.1462e+05 No. Observations: 631488 AIC: -6.292e+05 Df Residuals: 631487 BIC: -6.292e+05 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ x1 0.8858 0.000 4787.641 0.000 0.885 0.886 ============================================================================== Omnibus: 78747.646 Durbin-Watson: 0.067 Prob(Omnibus): 0.000 Jarque-Bera (JB): 731236.476 Skew: -0.264 Prob(JB): 0.00 Kurtosis: 8.245 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. statistic = 'r2' stats_df = get_stats ( ypred , ytrain , each_level = True ) plt . style . use ( 'seaborn-talk' ) plot_mo_stats ( stats_df , stat = statistic , save_name = 'na_train' ) 6.2 - Testing Data Results \u00b6 ypred = model . predict ( Xvalid ) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.5s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 0.9s finished # each level each_level = False stats = get_stats ( ypred , yvalid , each_level = each_level ) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 0.350488 0.269311 0.269311 0.73119 each_level = True stats_df = get_stats ( ypred , yvalid , each_level = each_level ) stats_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 0.342811 0.226361 0.475774 0.784954 1 0.334066 0.221741 0.470894 0.789384 2 0.340962 0.227880 0.477368 0.784288 3 0.336849 0.223797 0.473071 0.787629 4 0.330158 0.219167 0.468153 0.788237 statistic = 'r2' plt . style . use ( 'seaborn-talk' ) plot_mo_stats ( stats_df , stat = 'r2' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'mae' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'mse' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'rmse' , save_name = 'na_test' ) 7 - Post Analysis \u00b6 7.1 - Feature Importance \u00b6 For some algorithms, we can calculate the feature importance based on the inputs dimensions. This is true for the random forest model. rf_model . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1500, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} tree_feature_importances = \\ model . feature_importances_ feature_names = np . asarray ( new_columns ) #np.concatenate((core_columns.values, np.array(pca_columns))) assert feature_names . shape [ 0 ] == tree_feature_importances . shape [ 0 ], \"Shapes don't match\" sorted_idx = tree_feature_importances . argsort () SAVE_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/reports/figures/\" save_name = 'na' y_ticks = np . arange ( 0 , len ( feature_names )) plt . style . use ([ 'seaborn-talk' ]) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax . barh ( y_ticks , tree_feature_importances [ sorted_idx ], zorder = 3 , height = 0.8 ) ax . set_yticklabels ( feature_names [ sorted_idx ]) ax . set_yticks ( y_ticks ) # ax.set_title(\"Random Forest Feature Importances (MDI)\") ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . grid ( alpha = 0.6 , color = 'gray' , zorder = 0 ) plt . tight_layout () plt . show () # fig.savefig(SAVE_PATH + f\"fi_{save_name}.png\")","title":"0.0 Full Walkthrough"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#full-walkthrough","text":"This notebook will walkthrough the preprocessing steps as well as the ML algorithm training procedure used for the multi-dimensional, multi-output data.","title":"Full Walkthrough"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#experiment-overview","text":"","title":"Experiment Overview"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#code","text":"","title":"Code"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#packages","text":"import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels # Features from features.build_features import times_2_cycles , geo_2_cartesian , get_geodataframe , CycleTransform , GeoCartTransform from features.pca_features import transform_all , transform_individual from features.analysis import get_stats from sklearn.preprocessing import StandardScaler # ML Models from sklearn.model_selection import train_test_split from models.baseline import train_rf_model # Visualization from visualization.visualize import plot_mo_stats , plot_geolocations from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2 1 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Packages"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#1-load-data","text":"","title":"1 Load Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#11-core-data","text":"In this step, we will load the standard data. This includes the following variables: SLA PAR RHO WN 412 RHO WN 443 RHO WN 490 RHO WN 555 RHO WN 670 MLD Lat Lon DOY params = { 'region' : 'na' , 'n_components' : 5 , 'pca_seed' : 123 } dataloader = DataLoader () # load training data X_core = load_standard_data ( params [ 'region' ], training = True ) # Testing Data X_core_te = load_standard_data ( params [ 'region' ], training = False ) X_core_te = X_core_te . iloc [:, 2 :]","title":"1.1 Core Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#plot-training-data","text":"X_core_na = load_standard_data ( 'NA' , training = True ) X_core_stg = load_standard_data ( 'STG' , training = True ) # convert dataframe into geopandas dataframe X_core_na_gdf = get_geodataframe ( X_core_na . copy ()) X_core_na_stg = get_geodataframe ( X_core_stg . copy ()) # plot world map with points colors = [ 'blue' , 'red' ] plot_geolocations ([ X_core_na_gdf , X_core_na_stg ], colors = [ 'blue' , 'red' ], save_name = 'both' )","title":"Plot Training Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#plot-testing-regions","text":"X_core_na = load_standard_data ( 'NA' , training = False ) X_core_stg = load_standard_data ( 'STG' , training = False ) # convert dataframe into geopandas dataframe X_core_na_gdf = get_geodataframe ( X_core_na . copy ()) X_core_na_stg = get_geodataframe ( X_core_stg . copy ()) # plot world map with points colors = [ 'blue' , 'red' ] plot_geolocations ([ X_core_na_gdf , X_core_na_stg ], colors = [ 'blue' , 'red' ], save_name = 'both' )","title":"Plot Testing Regions"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#12-high-dimensional-data","text":"In this section, we will extract the high dimensional datasets. They include: Temperature Density Salinity Spiciness # =================== # Training Data # =================== # load data from dataloader X_temp , X_dens , X_sal , X_spicy = load_high_dim_data ( params [ 'region' ], training = True ) # add prefix (Training/Validation) X_temp = X_temp . add_prefix ( \"temp_\" ) X_dens = X_dens . add_prefix ( \"dens_\" ) X_sal = X_sal . add_prefix ( \"sal_\" ) X_spicy = X_spicy . add_prefix ( \"spice_\" ) # =================== # Test Data # =================== # load data from dataloader X_temp_te , X_dens_te , X_sal_te , X_spicy_te = load_high_dim_data ( params [ 'region' ], training = False ) # Subset meta columns X_temp_te = X_temp_te . iloc [:, 2 :] X_dens_te = X_dens_te . iloc [:, 2 :] X_sal_te = X_sal_te . iloc [:, 2 :] X_spicy_te = X_spicy_te . iloc [:, 2 :] # add prefix (Test) X_temp_te = X_temp_te . add_prefix ( \"temp_\" ) X_dens_te = X_dens_te . add_prefix ( \"dens_\" ) X_sal_te = X_sal_te . add_prefix ( \"sal_\" ) X_spicy_te = X_spicy_te . add_prefix ( \"spice_\" )","title":"1.2 High Dimensional Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#13-concatenate-data","text":"# Concatenate Data # Training Data Xtr = pd . concat ([ X_core , X_temp , X_dens , X_sal , X_spicy ], axis = 1 ) # Testing Data Xte = pd . concat ([ X_core_te , X_temp_te , X_dens_te , X_sal_te , X_spicy_te ], axis = 1 )","title":"1.3 - Concatenate Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#14-multi-output-data","text":"We load the multioutput regression labels. # =================== # Labels # =================== ytr = load_labels ( params [ 'region' ], training = True ) ytest = load_labels ( params [ 'region' ], training = False ) # remove meta-columns ytest = ytest . iloc [:, 2 :]","title":"1.4 - Multi-Output Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#4-post-split-transformations","text":"PCA Transform High Dimensional Variables In this step, we do a PCA transformation on the concatenation for the high dimensional variables temp , sal , dens , and spicy . We will reduce the dimensionality to about 10 features. Normalize Core Variables We will use a standard scaler to make the core variables with a mean of 0 and standard deviation of 1. The ML algorithms tend to perform better with this type of standardization. Coordinate Transformation In this step, we will do a simple coordinate transformation of the lat,lon variables from geospatial to cartesian coordinates. This will increase the dimensionality of our dataset from 11 dimensions to 12 dimensions. Time Transformation In this step, we will transform the doy coordinates to cycles of sines and cosines. This will increase the dimensionality of our data from from 12 to 13. from sklearn.compose import ColumnTransformer from sklearn.base import BaseEstimator , TransformerMixin from sklearn.decomposition import PCA","title":"4 - Post-Split Transformations"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#41-input-preprocessing","text":"# new columns columns dataloader = DataLoader () columns = dataloader . load_columns () n_components = 5 times = [ 'doy' ] # create new columns new_columns = [ * [ \"doy_cos\" , \"doy_sin\" ], * [ 'x' , 'y' , 'z' ,], * [ f \"temperature_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"density_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"salinity_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * [ f \"spicy_pc { icomponent + 1 } \" for icomponent in range ( params [ 'n_components' ])], * columns [ \"core\" ], ] seed = 123 # define transfomer X_pre_transformer = ColumnTransformer ( [ ( \"time\" , CycleTransform ( columns [ \"time\" ]), columns [ \"time\" ]), ( \"location\" , GeoCartTransform (), columns [ \"location\" ]), ( \"temperature\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"temperature\" ], ), ( \"density\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"density\" ], ), ( \"salinity\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"salinity\" ], ), ( \"spicy\" , PCA ( n_components = params [ 'n_components' ], random_state = params [ 'pca_seed' ]), columns [ \"spicy\" ], ), ( \"core\" , StandardScaler ( with_mean = True , with_std = True ), columns [ \"core\" ], ), ], remainder = \"passthrough\" , ) # transform data X_pre_transformer . fit ( Xtr ) # transform data Xtr = X_pre_transformer . transform ( Xtr ) Xte = X_pre_transformer . transform ( Xte )","title":"4.1 - Input Preprocessing"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#plot-pca-components","text":"SAVE_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/reports/figures/\" save_name = 'na' # y_ticks = np.arange(0, len(feature_names)) plt . style . use ([ 'seaborn-talk' ]) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) plt . plot ( X_pre_transformer . named_transformers_ [ 'temperature' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Temperature' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'density' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Density' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'salinity' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Salinity' ) plt . plot ( X_pre_transformer . named_transformers_ [ 'spicy' ] . explained_variance_ratio_ [: 25 ] . cumsum (), linewidth = 4 , label = 'Spiciness' ) # ax.set_title(\"Random Forest Feature Importances (MDI)\") ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . grid ( alpha = 0.6 , color = 'gray' , zorder = 0 ) ax . set_ylim ([ 0.8 , 1.01 ]) plt . legend ( fontsize = 20 , loc = 'lower right' ) plt . tight_layout () plt . show () # fig.savefig(SAVE_PATH + f\"evar_{save_name}.png\")","title":"Plot PCA Components"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#3-traintest-split","text":"We split the data into 80% training and 20% training. Note : because the dataset we are dealing with is only ~3,000 data points, we will do some bootstrap techniques in the full experiment to see how well we do with different subsamples of data. # train-test split Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtr , ytr , train_size = 0.8 , random_state = 123 )","title":"3 - Train,Test Split"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#4-post-output-transformations","text":"The distribution of the outputs are skewed because there is a lot more variability in the upper depths than the lower depths. Because the distribution of the outputs are fairly skewed, we propose to do a log transformation to make them normally distributed. We do the following: Do a log10 transformation Standardize the Outputs from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer , StandardScaler # Inverse log10 function def loginv ( x ): return 10 ** x # create pipeline y_transformer = Pipeline ([ # Log10 transformer ( \"log\" , FunctionTransformer ( func = np . log10 , inverse_func = loginv )), # Standardize the outputs (helps with NN models) ( \"scale\" , StandardScaler ( with_mean = True , with_std = True )) ]) # transform data ytrain = y_transformer . fit_transform ( ytrain ) yvalid = y_transformer . transform ( yvalid ) ytest = y_transformer . fit_transform ( ytest )","title":"4 - Post Output Transformations"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#5-train-ml-model","text":"In this section, we will use a standard random forest (RF) regressor to train on our dataset. def save_model ( model , save_name ): # model path MODEL_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/models/control/\" # save model from joblib import dump dump ( model , MODEL_PATH + save_name + '.joblib' ) return None model_name = 'morf' # RF params rf_params = { \"n_estimators\" : 1_500 , \"criterion\" : \"mse\" , \"n_jobs\" : - 1 , \"random_state\" : 123 , \"warm_start\" : False , \"verbose\" : 1 , } # train model model = train_rf_model ( Xtrain , ytrain , params = rf_params ) # save model save_model ( model , f \" { model_name } _na\" ) 1 2 3 4 5 6 7 8 9 [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 144 tasks | elapsed: 4.1s [Parallel(n_jobs=-1)]: Done 394 tasks | elapsed: 10.2s [Parallel(n_jobs=-1)]: Done 744 tasks | elapsed: 18.6s [Parallel(n_jobs=-1)]: Done 1194 tasks | elapsed: 29.4s [Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 36.7s finished Training time: 37.415 secs.","title":"5 - Train ML Model"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#6-test-ml-model","text":"","title":"6 - Test ML Model"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#61-training-data-results","text":"This is often not reported but it is very good to check how well a model does on the initial training data because we have no entered a validation set. If we find that the training stats are too high and the testing stats are very low then we know that we're either overfitting and/or our model is not generalizing very well. from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score def get_stats ( y_pred : np . ndarray , y : np . ndarray , each_level : bool = False ): stats = pd . DataFrame () # Get total average statistics if each_level == False : multioutput = 'uniform_average' mae = mean_absolute_error ( y , ypred , multioutput = multioutput ) mse = mean_squared_error ( y , ypred , multioutput = multioutput ) rmse = mse r2 = r2_score ( y , ypred , multioutput = multioutput ) stats = pd . DataFrame ( data = [[ mae , mse , rmse , r2 ]], columns = [ 'mae' , 'mse' , 'rmse' , 'r2' ]) elif each_level == True : multioutput = 'raw_values' stats [ 'mae' ] = mean_absolute_error ( y , ypred , multioutput = multioutput ) stats [ 'mse' ] = mean_squared_error ( y , ypred , multioutput = multioutput ) stats [ 'rmse' ] = np . sqrt ( stats [ 'mse' ]) stats [ 'r2' ] = r2_score ( y , ypred , multioutput = multioutput ) else : raise ValueError ( f \"Unrecognized stat request: { each_level } \" ) return stats First we will look at the statistics on average to see what we get. ypred = model . predict ( Xtrain ) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 1.5s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 2.2s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 2.7s finished min_lim = np . min ( np . concatenate (( ypred , ytrain ))) max_lim = np . max ( np . concatenate (( ypred , ytrain ))) plt . scatter ( ypred , ytrain , s = 0.1 ) plt . plot ( np . linspace ( min_lim , max_lim ), np . linspace ( min_lim , max_lim ), color = 'black' , zorder = 3 ) plt . xlim ([ min_lim , max_lim ]) plt . ylim ([ min_lim , max_lim ]) 1 (-5.863397779903146, 13.594780012746048) Then we can look at the statistics for each level. import statsmodels.api as smi stat_mod = smi . OLS ( ypred . ravel (), ytrain . ravel ()) res = stat_mod . fit () print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: y R-squared (uncentered): 0.973 Model: OLS Adj. R-squared (uncentered): 0.973 Method: Least Squares F-statistic: 2.292e+07 Date: Tue, 25 Feb 2020 Prob (F-statistic): 0.00 Time: 12:54:17 Log-Likelihood: 3.1462e+05 No. Observations: 631488 AIC: -6.292e+05 Df Residuals: 631487 BIC: -6.292e+05 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ x1 0.8858 0.000 4787.641 0.000 0.885 0.886 ============================================================================== Omnibus: 78747.646 Durbin-Watson: 0.067 Prob(Omnibus): 0.000 Jarque-Bera (JB): 731236.476 Skew: -0.264 Prob(JB): 0.00 Kurtosis: 8.245 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. statistic = 'r2' stats_df = get_stats ( ypred , ytrain , each_level = True ) plt . style . use ( 'seaborn-talk' ) plot_mo_stats ( stats_df , stat = statistic , save_name = 'na_train' )","title":"6.1 - Training Data Results"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#62-testing-data-results","text":"ypred = model . predict ( Xvalid ) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.5s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.8s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 0.9s finished # each level each_level = False stats = get_stats ( ypred , yvalid , each_level = each_level ) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 0.350488 0.269311 0.269311 0.73119 each_level = True stats_df = get_stats ( ypred , yvalid , each_level = each_level ) stats_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 0.342811 0.226361 0.475774 0.784954 1 0.334066 0.221741 0.470894 0.789384 2 0.340962 0.227880 0.477368 0.784288 3 0.336849 0.223797 0.473071 0.787629 4 0.330158 0.219167 0.468153 0.788237 statistic = 'r2' plt . style . use ( 'seaborn-talk' ) plot_mo_stats ( stats_df , stat = 'r2' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'mae' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'mse' , save_name = 'na_test' ) plot_mo_stats ( stats_df , stat = 'rmse' , save_name = 'na_test' )","title":"6.2 - Testing Data Results"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#7-post-analysis","text":"","title":"7 - Post Analysis"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/1.0_experiment_explore/#71-feature-importance","text":"For some algorithms, we can calculate the feature importance based on the inputs dimensions. This is true for the random forest model. rf_model . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1500, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} tree_feature_importances = \\ model . feature_importances_ feature_names = np . asarray ( new_columns ) #np.concatenate((core_columns.values, np.array(pca_columns))) assert feature_names . shape [ 0 ] == tree_feature_importances . shape [ 0 ], \"Shapes don't match\" sorted_idx = tree_feature_importances . argsort () SAVE_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/reports/figures/\" save_name = 'na' y_ticks = np . arange ( 0 , len ( feature_names )) plt . style . use ([ 'seaborn-talk' ]) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax . barh ( y_ticks , tree_feature_importances [ sorted_idx ], zorder = 3 , height = 0.8 ) ax . set_yticklabels ( feature_names [ sorted_idx ]) ax . set_yticks ( y_ticks ) # ax.set_title(\"Random Forest Feature Importances (MDI)\") ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . grid ( alpha = 0.6 , color = 'gray' , zorder = 0 ) plt . tight_layout () plt . show () # fig.savefig(SAVE_PATH + f\"fi_{save_name}.png\")","title":"7.1 - Feature Importance"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/","text":"Validation Figures \u00b6 This notebook goes through and plots the validation figures. These are specific to the plots used in the conference paper. Experiment Overview \u00b6 Code \u00b6 Packages \u00b6 import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels , get_data # Experiments # Features from features.pca_features import transform_all , transform_individual from features.analysis import get_stats from sklearn.preprocessing import StandardScaler from data.make_dataset import ValidationFloats from features.build_features import run_input_preprocess , run_input_postprocess , run_output_preprocess , run_output_postprocess , run_split # ML Models from sklearn.model_selection import train_test_split from models.baseline import train_rf_model import statsmodels.api as smi from sklearn.metrics import r2_score # Visualization from visualization.visualize import plot_mo_stats , plot_geolocations , get_depth_labels from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt from matplotlib.offsetbox import AnchoredText plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2 1 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 def run_experiment (): # Load Data dataset = get_data ( DataParams ) print ( \"Input: \\n \" , dataset [ 'ytrain' ] . min () . min (), dataset [ 'ytrain' ] . max () . max ()) # Run Inputs Preprocessing dataset = run_input_preprocess ( ProcessParams , dataset ) # Run Outputs Preprocessing # Train Test Split dataset = run_split ( ProcessParams , dataset ) # Run Inputs PostProcessing if ProcessParams . input_std == 'after' : dataset = run_input_postprocess ( ProcessParams , dataset ) # Run Outputs Post Processing dataset = run_output_postprocess ( ProcessParams , dataset ) print ( \"Output: \\n \" , dataset [ 'ytrain' ] . min () . min (), dataset [ 'ytrain' ] . max () . max ()) return dataset class DataParams : region = 'na' class ProcessParams : n_components = 5 valid_split = 0.2 input_std = \"before\" pca_seed = 123 bootstrap_seed = 111 std_ouputs = True dataset = run_experiment () region = DataParams . region print ( 'Training:' ) print ( dataset [ 'Xtrain' ] . shape , dataset [ 'ytrain' ] . shape ) print ( 'Validation:' ) print ( dataset [ 'Xvalid' ] . shape , dataset [ 'yvalid' ] . shape ) print ( 'Testing:' ) print ( dataset [ 'Xtest' ] . shape , dataset [ 'ytest' ] . shape ) 1 2 3 4 5 6 7 8 9 10 Input: 9.99999974737875e-06 0.0227799993008375 Output: -8.90301918762965 10.745141882960526 Training: (2288, 33) (2288, 276) Validation: (572, 33) (572, 276) Testing: (162, 33) (162, 276) 2 - Load ML Model \u00b6 def load_model ( save_name ): # model path MODEL_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/models/control/\" # save model from joblib import load model = load ( MODEL_PATH + save_name + '.joblib' ) return model 5.1 - Ridge Regression \u00b6 model_name = 'lr' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name ) 5.3 - (3-Layer) MultiPerceptron Model \u00b6 model_name = 'mlp' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name ) 5.4 - MO Random Forest Model \u00b6 model_name = 'morf' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name ) 3 - Validation Floats \u00b6 We have the following validation floats: NA * 6901486 * 3902123 STG * 6901472 * 3902121 # make predictions ypred = model . predict ( dataset [ 'Xtest' ]) # inverse transform the outputs ypred = dataset [ \"out_post_trans\" ] . inverse_transform ( ypred ) ytest = dataset [ \"out_post_trans\" ] . inverse_transform ( dataset [ 'ytest' ]) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.4s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 0.5s finished Average Validation Plots \u00b6 def plot_validation ( y_mu , y_std , ylabels ): fig , ax = plt . subplots ( figsize = ( 7 , 5 )) plt . plot ( y_mu , ylabels , color = 'black' , linewidth = 5 , label = 'bBP$_1$, $\\mu$' ) plt . fill_betweenx ( ylabels , y_mu - y_std , y_mu + y_std , interpolate = False , alpha = 0.5 , color = 'gray' , label = 'bBP$_1$, $\\sigma$' ) return fig , ax ytest_mu , ypred_mu = ytest . mean ( axis = 0 ), ypred . mean ( axis = 0 ) ytest_std , ypred_std = ytest . std ( axis = 0 ), ypred . std ( axis = 0 ) ylabels = get_depth_labels () Labels \u00b6 fig , ax = plot_validation ( ytest_mu , ytest_std , ylabels ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . ticklabel_format ( style = 'sci' , scilimits = ( - 3 , 4 ), axis = 'both' ) plt . legend ( fontsize = 25 , loc = 'lower right' ) plt . tight_layout () # fig.savefig(SAVE_PATH + f\"stg_y1\" + \".png\", dpi=200, transparent=True) plt . show () Predictions \u00b6 fig , ax = plot_validation ( ypred_mu , ypred_std , ylabels ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . ticklabel_format ( style = 'sci' , scilimits = ( - 3 , 4 ), axis = 'both' ) plt . legend ( fontsize = 25 , loc = 'lower right' ) plt . tight_layout () # fig.savefig(SAVE_PATH + f\"stg_y1\" + \".png\", dpi=200, transparent=True) plt . show () Plot Profiles \u00b6 # load validation floats valid_getter = ValidationFloats ( 'na' ) # get validation floats valid_getter . get_validation_floats ( 'na' ) # get timeseries results_df = valid_getter . get_validation_res ( ytest , ypred , validation_float = 6901486 ) results_df . describe () 1 2 (162, 2) (162, 276) 0.00014170522838878957 6901486.0 0.00016999999934341794 6901486.0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Predictions Labels count 36984.000000 36984.000000 36984.000000 mean 159.947761 0.000500 0.000586 std 95.997047 0.000526 0.000800 min 3.000000 0.000142 0.000170 25% 80.000000 0.000239 0.000260 50% 149.500000 0.000301 0.000320 75% 195.000000 0.000464 0.000480 max 352.000000 0.003840 0.008750 def df_2_xr ( df ): \"\"\"Converts the data from a dataframe to an xarray (netcdf format)\"\"\" # create multiindex data df = df . set_index ([ 'n_cycle' , 'Depth' ]) # convert to xarray data = df . to_xarray () return data # convert dataframe to xarray results_xr = df_2_xr ( results_df ) def plot_profiles ( xr_data , vmin , vmax ): import matplotlib.colors as colors fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # plot colormesh xr_data . T . plot . pcolormesh ( ax = ax , # colorbar type cmap = 'jet' , # colorbar arguments cbar_kwargs = { 'label' : '' }, # log scale colorbar norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), # min,max vmin = vmin , vmax = vmax , # don't deal with outliers robust = False ) return fig , ax Original \u00b6 # plot parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) vmin = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) vmax = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) fig , ax = plot_profiles ( results_xr . Labels , vmin , vmax ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) # fig.savefig(f\"{plot_config['save_path']}{plot_config['region']}_y_{plot_config['data']}_heatmap_{plot_config['float']}_pred_{plot_config['model']}\") plt . tight_layout () plt . show () Predicted \u00b6 # plot parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) vmin = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) vmax = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) fig , ax = plot_profiles ( results_xr [ 'Predictions' ], vmin , vmax ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) # fig.savefig(f\"{plot_config['save_path']}{plot_config['region']}_y_{plot_config['data']}_heatmap_{plot_config['float']}_pred_{plot_config['model']}\") plt . tight_layout () plt . show () Scatter Plot \u00b6 # Plot Parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486 Statistics \u00b6 def mapd ( ypred , y ): return np . median ( np . abs ( ypred - y ) / y ) # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results_df [ 'Predictions' ]), np . log10 ( results_df [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = mapd ( results_df [ 'Predictions' ], results_df [ 'Labels' ]) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results_df [ 'Labels' ]), np . log10 ( results_df [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = lin_res . rsquared # slope coefficient plot_config [ 'slope' ] = lin_res . params [ 0 ] print ( lin_res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.343e+07 Date: Tue, 25 Feb 2020 Prob (F-statistic): 0.00 Time: 10:03:32 Log-Likelihood: 32932. No. Observations: 36984 AIC: -6.586e+04 Df Residuals: 36983 BIC: -6.585e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.004 0.000 0.990 0.990 ============================================================================== Omnibus: 6721.847 Durbin-Watson: 0.773 Prob(Omnibus): 0.000 Jarque-Bera (JB): 71532.255 Skew: 0.562 Prob(JB): 0.00 Kurtosis: 9.720 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Figure \u00b6 def plot_scatter ( results_df ): # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results_df . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) return fig , ax # ================== # Scatter Points # ================== fig , ax = plot_scatter ( results_df ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Modify Limits # ================== ax . set_xlim ( 0.0001 , 0.01 ) ax . set_ylim ( 0.0001 , 0.01 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot # fig.savefig(SAVE_PATH + f'{plot_config[\"region\"]}_m{plot_config[\"model\"]}_f{plot_config[\"float\"]}_depth' + '.png') # Show Plot plt . show ()","title":"1.0 Validation Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#validation-figures","text":"This notebook goes through and plots the validation figures. These are specific to the plots used in the conference paper.","title":"Validation Figures"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#experiment-overview","text":"","title":"Experiment Overview"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#code","text":"","title":"Code"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#packages","text":"import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels , get_data # Experiments # Features from features.pca_features import transform_all , transform_individual from features.analysis import get_stats from sklearn.preprocessing import StandardScaler from data.make_dataset import ValidationFloats from features.build_features import run_input_preprocess , run_input_postprocess , run_output_preprocess , run_output_postprocess , run_split # ML Models from sklearn.model_selection import train_test_split from models.baseline import train_rf_model import statsmodels.api as smi from sklearn.metrics import r2_score # Visualization from visualization.visualize import plot_mo_stats , plot_geolocations , get_depth_labels from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt from matplotlib.offsetbox import AnchoredText plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2 1 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Packages"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#data","text":"def run_experiment (): # Load Data dataset = get_data ( DataParams ) print ( \"Input: \\n \" , dataset [ 'ytrain' ] . min () . min (), dataset [ 'ytrain' ] . max () . max ()) # Run Inputs Preprocessing dataset = run_input_preprocess ( ProcessParams , dataset ) # Run Outputs Preprocessing # Train Test Split dataset = run_split ( ProcessParams , dataset ) # Run Inputs PostProcessing if ProcessParams . input_std == 'after' : dataset = run_input_postprocess ( ProcessParams , dataset ) # Run Outputs Post Processing dataset = run_output_postprocess ( ProcessParams , dataset ) print ( \"Output: \\n \" , dataset [ 'ytrain' ] . min () . min (), dataset [ 'ytrain' ] . max () . max ()) return dataset class DataParams : region = 'na' class ProcessParams : n_components = 5 valid_split = 0.2 input_std = \"before\" pca_seed = 123 bootstrap_seed = 111 std_ouputs = True dataset = run_experiment () region = DataParams . region print ( 'Training:' ) print ( dataset [ 'Xtrain' ] . shape , dataset [ 'ytrain' ] . shape ) print ( 'Validation:' ) print ( dataset [ 'Xvalid' ] . shape , dataset [ 'yvalid' ] . shape ) print ( 'Testing:' ) print ( dataset [ 'Xtest' ] . shape , dataset [ 'ytest' ] . shape ) 1 2 3 4 5 6 7 8 9 10 Input: 9.99999974737875e-06 0.0227799993008375 Output: -8.90301918762965 10.745141882960526 Training: (2288, 33) (2288, 276) Validation: (572, 33) (572, 276) Testing: (162, 33) (162, 276)","title":"Data"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#2-load-ml-model","text":"def load_model ( save_name ): # model path MODEL_PATH = \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/models/control/\" # save model from joblib import load model = load ( MODEL_PATH + save_name + '.joblib' ) return model","title":"2 - Load ML Model"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#51-ridge-regression","text":"model_name = 'lr' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name )","title":"5.1 - Ridge Regression"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#53-3-layer-multiperceptron-model","text":"model_name = 'mlp' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name )","title":"5.3 - (3-Layer) MultiPerceptron Model"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#54-mo-random-forest-model","text":"model_name = 'morf' # save name save_name = f \" { model_name } _ { DataParams . region } \" # load model model = load_model ( save_name )","title":"5.4 - MO Random Forest Model"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#3-validation-floats","text":"We have the following validation floats: NA * 6901486 * 3902123 STG * 6901472 * 3902121 # make predictions ypred = model . predict ( dataset [ 'Xtest' ]) # inverse transform the outputs ypred = dataset [ \"out_post_trans\" ] . inverse_transform ( ypred ) ytest = dataset [ \"out_post_trans\" ] . inverse_transform ( dataset [ 'ytest' ]) 1 2 3 4 5 6 [Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers. [Parallel(n_jobs=28)]: Done 144 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 394 tasks | elapsed: 0.1s [Parallel(n_jobs=28)]: Done 744 tasks | elapsed: 0.3s [Parallel(n_jobs=28)]: Done 1194 tasks | elapsed: 0.4s [Parallel(n_jobs=28)]: Done 1500 out of 1500 | elapsed: 0.5s finished","title":"3 - Validation Floats"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#average-validation-plots","text":"def plot_validation ( y_mu , y_std , ylabels ): fig , ax = plt . subplots ( figsize = ( 7 , 5 )) plt . plot ( y_mu , ylabels , color = 'black' , linewidth = 5 , label = 'bBP$_1$, $\\mu$' ) plt . fill_betweenx ( ylabels , y_mu - y_std , y_mu + y_std , interpolate = False , alpha = 0.5 , color = 'gray' , label = 'bBP$_1$, $\\sigma$' ) return fig , ax ytest_mu , ypred_mu = ytest . mean ( axis = 0 ), ypred . mean ( axis = 0 ) ytest_std , ypred_std = ytest . std ( axis = 0 ), ypred . std ( axis = 0 ) ylabels = get_depth_labels ()","title":"Average Validation Plots"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#labels","text":"fig , ax = plot_validation ( ytest_mu , ytest_std , ylabels ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . ticklabel_format ( style = 'sci' , scilimits = ( - 3 , 4 ), axis = 'both' ) plt . legend ( fontsize = 25 , loc = 'lower right' ) plt . tight_layout () # fig.savefig(SAVE_PATH + f\"stg_y1\" + \".png\", dpi=200, transparent=True) plt . show ()","title":"Labels"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#predictions","text":"fig , ax = plot_validation ( ypred_mu , ypred_std , ylabels ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = 20 ) ax . tick_params ( axis = \"both\" , which = \"minor\" , labelsize = 20 ) ax . ticklabel_format ( style = 'sci' , scilimits = ( - 3 , 4 ), axis = 'both' ) plt . legend ( fontsize = 25 , loc = 'lower right' ) plt . tight_layout () # fig.savefig(SAVE_PATH + f\"stg_y1\" + \".png\", dpi=200, transparent=True) plt . show ()","title":"Predictions"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#plot-profiles","text":"# load validation floats valid_getter = ValidationFloats ( 'na' ) # get validation floats valid_getter . get_validation_floats ( 'na' ) # get timeseries results_df = valid_getter . get_validation_res ( ytest , ypred , validation_float = 6901486 ) results_df . describe () 1 2 (162, 2) (162, 276) 0.00014170522838878957 6901486.0 0.00016999999934341794 6901486.0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Predictions Labels count 36984.000000 36984.000000 36984.000000 mean 159.947761 0.000500 0.000586 std 95.997047 0.000526 0.000800 min 3.000000 0.000142 0.000170 25% 80.000000 0.000239 0.000260 50% 149.500000 0.000301 0.000320 75% 195.000000 0.000464 0.000480 max 352.000000 0.003840 0.008750 def df_2_xr ( df ): \"\"\"Converts the data from a dataframe to an xarray (netcdf format)\"\"\" # create multiindex data df = df . set_index ([ 'n_cycle' , 'Depth' ]) # convert to xarray data = df . to_xarray () return data # convert dataframe to xarray results_xr = df_2_xr ( results_df ) def plot_profiles ( xr_data , vmin , vmax ): import matplotlib.colors as colors fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # plot colormesh xr_data . T . plot . pcolormesh ( ax = ax , # colorbar type cmap = 'jet' , # colorbar arguments cbar_kwargs = { 'label' : '' }, # log scale colorbar norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), # min,max vmin = vmin , vmax = vmax , # don't deal with outliers robust = False ) return fig , ax","title":"Plot Profiles"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#original","text":"# plot parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) vmin = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) vmax = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) fig , ax = plot_profiles ( results_xr . Labels , vmin , vmax ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) # fig.savefig(f\"{plot_config['save_path']}{plot_config['region']}_y_{plot_config['data']}_heatmap_{plot_config['float']}_pred_{plot_config['model']}\") plt . tight_layout () plt . show ()","title":"Original"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#predicted","text":"# plot parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) vmin = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) vmax = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) fig , ax = plot_profiles ( results_xr [ 'Predictions' ], vmin , vmax ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) # fig.savefig(f\"{plot_config['save_path']}{plot_config['region']}_y_{plot_config['data']}_heatmap_{plot_config['float']}_pred_{plot_config['model']}\") plt . tight_layout () plt . show ()","title":"Predicted"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#scatter-plot","text":"# Plot Parameters plot_config = dict () plot_config [ 'region' ] = DataParams . region plot_config [ 'model' ] = model_name plot_config [ 'save_path' ] = '/media/disk/erc/papers/2019_ML_OCN/figures/' plot_config [ 'float' ] = 6901486","title":"Scatter Plot"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#statistics","text":"def mapd ( ypred , y ): return np . median ( np . abs ( ypred - y ) / y ) # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results_df [ 'Predictions' ]), np . log10 ( results_df [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = mapd ( results_df [ 'Predictions' ], results_df [ 'Labels' ]) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results_df [ 'Labels' ]), np . log10 ( results_df [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = lin_res . rsquared # slope coefficient plot_config [ 'slope' ] = lin_res . params [ 0 ] print ( lin_res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.343e+07 Date: Tue, 25 Feb 2020 Prob (F-statistic): 0.00 Time: 10:03:32 Log-Likelihood: 32932. No. Observations: 36984 AIC: -6.586e+04 Df Residuals: 36983 BIC: -6.585e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.004 0.000 0.990 0.990 ============================================================================== Omnibus: 6721.847 Durbin-Watson: 0.773 Prob(Omnibus): 0.000 Jarque-Bera (JB): 71532.255 Skew: 0.562 Prob(JB): 0.00 Kurtosis: 9.720 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","title":"Statistics"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/4.0_validation/#figure","text":"def plot_scatter ( results_df ): # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results_df . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) return fig , ax # ================== # Scatter Points # ================== fig , ax = plot_scatter ( results_df ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Modify Limits # ================== ax . set_xlim ( 0.0001 , 0.01 ) ax . set_ylim ( 0.0001 , 0.01 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot # fig.savefig(SAVE_PATH + f'{plot_config[\"region\"]}_m{plot_config[\"model\"]}_f{plot_config[\"float\"]}_depth' + '.png') # Show Plot plt . show ()","title":"Figure"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/","text":"Algorithm Walk-Through \u00b6 This notebook will walkthrough the preprocessing steps as well as the ML algorithm training procedure used for the multi-dimensional, multi-output data. Experiment Overview \u00b6 Code \u00b6 Packages \u00b6 import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd import xarray as xr # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels , get_data # Experiments # Features # from features.pca_features import transform_all, transform_individual # from features.analysis import get_stats # from sklearn.preprocessing import StandardScaler # from data.make_dataset import ValidationFloats # from features.build_features import run_input_preprocess, run_input_postprocess, run_output_preprocess, run_output_postprocess, run_split # ML Models import statsmodels.api as smi from sklearn.metrics import r2_score # Visualization # from visualization.visualize import plot_mo_stats, plot_geolocations, get_depth_labels # from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2 6.3 - Validation Profile \u00b6 def get_scatter_validation ( df , df_test , plot_config ): # initialize class valid_getter = ValidationFloats ( plot_config [ 'region' ]) # get validation floats valid_getter . get_validation_floats ( plot_config [ 'region' ]) # get timeseries df = valid_getter . get_validation_res ( df_test , df , validation_float = plot_config [ 'float' ]) return df NA * 6901486 * 3902123 STG * 6901472 * 3902121 North Atlantic (6901486) \u00b6 SAVE_PATH = '/media/disk/erc/papers/2019_ML_OCN/figures/' results = pd . read_csv ( 'na_results.csv' , index_col = 0 ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels 0 3.0 0 0.003238 0.003690 1 12.0 0 0.003148 0.003060 2 24.0 0 0.003298 0.003780 3 25.0 0 0.003612 0.003905 4 26.0 0 0.003370 0.003920 results . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels count 36984.000000 36984.000000 36984.000000 36984.000000 mean 159.947761 -398.097826 0.000500 0.000586 std 95.997047 300.901737 0.000525 0.000800 min 3.000000 -1000.000000 0.000143 0.000170 25% 80.000000 -656.250000 0.000239 0.000260 50% 149.500000 -312.500000 0.000301 0.000320 75% 195.000000 -137.500000 0.000463 0.000480 max 352.000000 0.000000 0.003838 0.008750 Profile \u00b6 def df_2_xr ( df ): \"\"\"Converts the data from a dataframe to an xarray (netcdf format)\"\"\" # create multiindex data df = df . set_index ([ 'n_cycle' , 'Depth' ]) # convert to xarray data = df . to_xarray () return data # Convert to xarray results_xr = df_2_xr ( results ) results_xr <xarray.Dataset> Dimensions: (Depth: 276, n_cycle: 134) Coordinates: * n_cycle (n_cycle) float64 3.0 12.0 24.0 25.0 ... 349.0 350.0 352.0 * Depth (Depth) int64 -1000 -995 -990 -985 -980 -975 ... -8 -6 -4 -2 0 Data variables: Predictions (n_cycle, Depth) float64 0.0001746 0.000177 ... 0.001984 Labels (n_cycle, Depth) float64 0.000235 0.000235 ... 0.004638 0.00464 def plot_profiles ( xr_data , plot_config ): import matplotlib.colors as colors fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # plot colormesh xr_data . T . plot . pcolormesh ( ax = ax , # colorbar type cmap = 'jet' , # colorbar arguments cbar_kwargs = { 'label' : '' }, # log scale colorbar norm = colors . LogNorm ( vmin = plot_config [ 'vmin' ], vmax = plot_config [ 'vmax' ]), # min,max vmin = plot_config [ 'vmin' ], vmax = plot_config [ 'vmax' ], # don't deal with outliers robust = False ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) plt . tight_layout () # save figure fig . savefig ( SAVE_PATH + f \" { plot_config [ 'region' ] } _y_ { plot_config [ 'data' ] } _heatmap_ { plot_config [ 'float' ] } _pred_ { plot_config [ 'model' ] } \" ) # show figure plt . show () return None # plot parameters plot_config = dict () plot_config [ 'region' ] = 'na' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) plot_config [ 'vmin' ] = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) plot_config [ 'vmax' ] = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) # plot profiles plot_profiles ( results_xr . Labels , plot_config ) plot_config [ 'vmin' ] <xarray.DataArray ()> array(0.00014334) Scatter Plot \u00b6 plot_config = dict () plot_config [ 'region' ] = 'na' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 6901486 # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results [ 'Predictions' ]), np . log10 ( results [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = np . median ( np . abs (( results [ 'Predictions' ]) - ( results [ 'Labels' ])) / ( results [ 'Labels' ])) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results [ 'Labels' ]), np . log10 ( results [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = res . rsquared print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.344e+07 Date: Wed, 05 Feb 2020 Prob (F-statistic): 0.00 Time: 16:14:04 Log-Likelihood: 32935. No. Observations: 36984 AIC: -6.587e+04 Df Residuals: 36983 BIC: -6.586e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.534 0.000 0.990 0.990 ============================================================================== Omnibus: 6786.424 Durbin-Watson: 0.777 Prob(Omnibus): 0.000 Jarque-Bera (JB): 72315.312 Skew: 0.571 Prob(JB): 0.00 Kurtosis: 9.755 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. # extract coefficient plot_config [ 'slope' ] = res . params [ 0 ] from matplotlib.offsetbox import AnchoredText # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Limites # ================== ax . set_xlim ( 0.0001 , 0.01 ) ax . set_ylim ( 0.0001 , 0.01 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot fig . savefig ( SAVE_PATH + f ' { plot_config [ \"region\" ] } _m { plot_config [ \"model\" ] } _f { plot_config [ \"float\" ] } _depth' + '.png' ) # Show Plot plt . show () SubTropical Gyre \u00b6 SAVE_PATH = '/media/disk/erc/papers/2019_ML_OCN/figures/' results = pd . read_csv ( 'stg_results.csv' , index_col = 0 ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels 0 1.0 0 0.000372 0.000322 1 2.0 0 0.000377 0.000354 2 3.0 0 0.000376 0.000360 3 4.0 0 0.000380 0.000340 4 6.0 0 0.000381 0.000350 Profiles \u00b6 results_xr = df_2_xr ( results ) results_xr <xarray.Dataset> Dimensions: (Depth: 276, n_cycle: 25) Coordinates: * n_cycle (n_cycle) float64 1.0 2.0 3.0 4.0 6.0 ... 23.0 24.0 25.0 26.0 * Depth (Depth) int64 -1000 -995 -990 -985 -980 -975 ... -8 -6 -4 -2 0 Data variables: Predictions (n_cycle, Depth) float64 0.0001254 0.0001255 ... 0.0003705 Labels (n_cycle, Depth) float64 0.00015 0.00015 ... 0.000385 0.00037 # plot parameters plot_config = dict () plot_config [ 'region' ] = 'stg' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 3902121 plot_config [ 'data' ] = 'Predictions' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) plot_config [ 'vmin' ] = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) plot_config [ 'vmax' ] = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) # plot profiles plot_profiles ( results_xr . Predictions , plot_config ) Scatter Plot \u00b6 plot_config = dict () plot_config [ 'region' ] = 'stg' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 3902121 # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results [ 'Predictions' ]), np . log10 ( results [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = np . median ( np . abs (( results [ 'Predictions' ]) - ( results [ 'Labels' ])) / ( results [ 'Labels' ])) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results [ 'Labels' ]), np . log10 ( results [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = res . rsquared print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.344e+07 Date: Wed, 05 Feb 2020 Prob (F-statistic): 0.00 Time: 16:18:01 Log-Likelihood: 32935. No. Observations: 36984 AIC: -6.587e+04 Df Residuals: 36983 BIC: -6.586e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.534 0.000 0.990 0.990 ============================================================================== Omnibus: 6786.424 Durbin-Watson: 0.777 Prob(Omnibus): 0.000 Jarque-Bera (JB): 72315.312 Skew: 0.571 Prob(JB): 0.00 Kurtosis: 9.755 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. # extract coefficient plot_config [ 'slope' ] = res . params [ 0 ] from matplotlib.offsetbox import AnchoredText # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Limites # ================== ax . set_xlim ( 0.0001 , 0.001 ) ax . set_ylim ( 0.0001 , 0.001 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot fig . savefig ( SAVE_PATH + f ' { plot_config [ \"region\" ] } _m { plot_config [ \"model\" ] } _f { plot_config [ \"float\" ] } _depth' + '.png' ) # Show Plot plt . show ()","title":"2.0 ISPRS Figures"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#algorithm-walk-through","text":"This notebook will walkthrough the preprocessing steps as well as the ML algorithm training procedure used for the multi-dimensional, multi-output data.","title":"Algorithm Walk-Through"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#experiment-overview","text":"","title":"Experiment Overview"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#code","text":"","title":"Code"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#packages","text":"import sys sys . path . insert ( 0 , \"/media/disk/erc/papers/2019_ML_OCN/ml4ocean/src\" ) # Standard packages import numpy as np import pandas as pd import xarray as xr # Datasets from data.make_dataset import DataLoader , load_standard_data , load_high_dim_data , load_labels , get_data # Experiments # Features # from features.pca_features import transform_all, transform_individual # from features.analysis import get_stats # from sklearn.preprocessing import StandardScaler # from data.make_dataset import ValidationFloats # from features.build_features import run_input_preprocess, run_input_postprocess, run_output_preprocess, run_output_postprocess, run_split # ML Models import statsmodels.api as smi from sklearn.metrics import r2_score # Visualization # from visualization.visualize import plot_mo_stats, plot_geolocations, get_depth_labels # from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt plt . style . use ( 'seaborn-poster' ) % load_ext autoreload % autoreload 2","title":"Packages"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#63-validation-profile","text":"def get_scatter_validation ( df , df_test , plot_config ): # initialize class valid_getter = ValidationFloats ( plot_config [ 'region' ]) # get validation floats valid_getter . get_validation_floats ( plot_config [ 'region' ]) # get timeseries df = valid_getter . get_validation_res ( df_test , df , validation_float = plot_config [ 'float' ]) return df NA * 6901486 * 3902123 STG * 6901472 * 3902121","title":"6.3 - Validation Profile"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#north-atlantic-6901486","text":"SAVE_PATH = '/media/disk/erc/papers/2019_ML_OCN/figures/' results = pd . read_csv ( 'na_results.csv' , index_col = 0 ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels 0 3.0 0 0.003238 0.003690 1 12.0 0 0.003148 0.003060 2 24.0 0 0.003298 0.003780 3 25.0 0 0.003612 0.003905 4 26.0 0 0.003370 0.003920 results . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels count 36984.000000 36984.000000 36984.000000 36984.000000 mean 159.947761 -398.097826 0.000500 0.000586 std 95.997047 300.901737 0.000525 0.000800 min 3.000000 -1000.000000 0.000143 0.000170 25% 80.000000 -656.250000 0.000239 0.000260 50% 149.500000 -312.500000 0.000301 0.000320 75% 195.000000 -137.500000 0.000463 0.000480 max 352.000000 0.000000 0.003838 0.008750","title":"North Atlantic (6901486)"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#profile","text":"def df_2_xr ( df ): \"\"\"Converts the data from a dataframe to an xarray (netcdf format)\"\"\" # create multiindex data df = df . set_index ([ 'n_cycle' , 'Depth' ]) # convert to xarray data = df . to_xarray () return data # Convert to xarray results_xr = df_2_xr ( results ) results_xr <xarray.Dataset> Dimensions: (Depth: 276, n_cycle: 134) Coordinates: * n_cycle (n_cycle) float64 3.0 12.0 24.0 25.0 ... 349.0 350.0 352.0 * Depth (Depth) int64 -1000 -995 -990 -985 -980 -975 ... -8 -6 -4 -2 0 Data variables: Predictions (n_cycle, Depth) float64 0.0001746 0.000177 ... 0.001984 Labels (n_cycle, Depth) float64 0.000235 0.000235 ... 0.004638 0.00464 def plot_profiles ( xr_data , plot_config ): import matplotlib.colors as colors fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # plot colormesh xr_data . T . plot . pcolormesh ( ax = ax , # colorbar type cmap = 'jet' , # colorbar arguments cbar_kwargs = { 'label' : '' }, # log scale colorbar norm = colors . LogNorm ( vmin = plot_config [ 'vmin' ], vmax = plot_config [ 'vmax' ]), # min,max vmin = plot_config [ 'vmin' ], vmax = plot_config [ 'vmax' ], # don't deal with outliers robust = False ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) plt . tight_layout () # save figure fig . savefig ( SAVE_PATH + f \" { plot_config [ 'region' ] } _y_ { plot_config [ 'data' ] } _heatmap_ { plot_config [ 'float' ] } _pred_ { plot_config [ 'model' ] } \" ) # show figure plt . show () return None # plot parameters plot_config = dict () plot_config [ 'region' ] = 'na' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 6901486 plot_config [ 'data' ] = 'Labels' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) plot_config [ 'vmin' ] = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) plot_config [ 'vmax' ] = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) # plot profiles plot_profiles ( results_xr . Labels , plot_config ) plot_config [ 'vmin' ] <xarray.DataArray ()> array(0.00014334)","title":"Profile"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#scatter-plot","text":"plot_config = dict () plot_config [ 'region' ] = 'na' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 6901486 # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results [ 'Predictions' ]), np . log10 ( results [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = np . median ( np . abs (( results [ 'Predictions' ]) - ( results [ 'Labels' ])) / ( results [ 'Labels' ])) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results [ 'Labels' ]), np . log10 ( results [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = res . rsquared print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.344e+07 Date: Wed, 05 Feb 2020 Prob (F-statistic): 0.00 Time: 16:14:04 Log-Likelihood: 32935. No. Observations: 36984 AIC: -6.587e+04 Df Residuals: 36983 BIC: -6.586e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.534 0.000 0.990 0.990 ============================================================================== Omnibus: 6786.424 Durbin-Watson: 0.777 Prob(Omnibus): 0.000 Jarque-Bera (JB): 72315.312 Skew: 0.571 Prob(JB): 0.00 Kurtosis: 9.755 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. # extract coefficient plot_config [ 'slope' ] = res . params [ 0 ] from matplotlib.offsetbox import AnchoredText # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Limites # ================== ax . set_xlim ( 0.0001 , 0.01 ) ax . set_ylim ( 0.0001 , 0.01 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot fig . savefig ( SAVE_PATH + f ' { plot_config [ \"region\" ] } _m { plot_config [ \"model\" ] } _f { plot_config [ \"float\" ] } _depth' + '.png' ) # Show Plot plt . show ()","title":"Scatter Plot"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#subtropical-gyre","text":"SAVE_PATH = '/media/disk/erc/papers/2019_ML_OCN/figures/' results = pd . read_csv ( 'stg_results.csv' , index_col = 0 ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_cycle Depth Predictions Labels 0 1.0 0 0.000372 0.000322 1 2.0 0 0.000377 0.000354 2 3.0 0 0.000376 0.000360 3 4.0 0 0.000380 0.000340 4 6.0 0 0.000381 0.000350","title":"SubTropical Gyre"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#profiles","text":"results_xr = df_2_xr ( results ) results_xr <xarray.Dataset> Dimensions: (Depth: 276, n_cycle: 25) Coordinates: * n_cycle (n_cycle) float64 1.0 2.0 3.0 4.0 6.0 ... 23.0 24.0 25.0 26.0 * Depth (Depth) int64 -1000 -995 -990 -985 -980 -975 ... -8 -6 -4 -2 0 Data variables: Predictions (n_cycle, Depth) float64 0.0001254 0.0001255 ... 0.0003705 Labels (n_cycle, Depth) float64 0.00015 0.00015 ... 0.000385 0.00037 # plot parameters plot_config = dict () plot_config [ 'region' ] = 'stg' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 3902121 plot_config [ 'data' ] = 'Predictions' plot_config [ 'robust' ] = False # y_val_scat = get_scatter_validation(ypred_, ytest_, plot_config) plot_config [ 'vmin' ] = np . minimum ( results_xr . Predictions . min (), results_xr . Labels . min ()) plot_config [ 'vmax' ] = np . maximum ( results_xr . Predictions . max (), results_xr . Labels . max ()) # plot profiles plot_profiles ( results_xr . Predictions , plot_config )","title":"Profiles"},{"location":"projects/ML4OCN/ARGO_project/subset_dataset/figures_mlsp/#scatter-plot_1","text":"plot_config = dict () plot_config [ 'region' ] = 'stg' plot_config [ 'model' ] = 'rf' plot_config [ 'float' ] = 3902121 # ================= # Statistics # ================= # R2 of log10 transform plot_config [ 'r2' ] = r2_score ( np . log10 ( results [ 'Predictions' ]), np . log10 ( results [ 'Labels' ])) # MAPD% of original data plot_config [ 'mapd' ] = np . median ( np . abs (( results [ 'Predictions' ]) - ( results [ 'Labels' ])) / ( results [ 'Labels' ])) # Linear Regression on log10 results stat_mod = smi . OLS ( np . log10 ( results [ 'Labels' ]), np . log10 ( results [ 'Predictions' ])) lin_res = stat_mod . fit () r2_val = res . rsquared print ( res . summary ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 OLS Regression Results ======================================================================================= Dep. Variable: Labels R-squared (uncentered): 0.999 Model: OLS Adj. R-squared (uncentered): 0.999 Method: Least Squares F-statistic: 4.344e+07 Date: Wed, 05 Feb 2020 Prob (F-statistic): 0.00 Time: 16:18:01 Log-Likelihood: 32935. No. Observations: 36984 AIC: -6.587e+04 Df Residuals: 36983 BIC: -6.586e+04 Df Model: 1 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Predictions 0.9902 0.000 6590.534 0.000 0.990 0.990 ============================================================================== Omnibus: 6786.424 Durbin-Watson: 0.777 Prob(Omnibus): 0.000 Jarque-Bera (JB): 72315.312 Skew: 0.571 Prob(JB): 0.00 Kurtosis: 9.755 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. # extract coefficient plot_config [ 'slope' ] = res . params [ 0 ] from matplotlib.offsetbox import AnchoredText # identity line id_line = np . logspace ( - 4 , - 2 , 100 ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) # ================================= # Plot Data # ================================= # scatter points results . plot . scatter ( ax = ax , x = 'Predictions' , y = 'Labels' , c = 'Depth' , logx = True , logy = True , cmap = 'winter' ) # identity line ax . plot ( id_line , id_line , linewidth = 5 , color = 'black' ) # ==================== # results text # ==================== at = AnchoredText ( f \"R$^2$: { plot_config [ 'r2' ] : .3f } \\n Slope: { plot_config [ 'slope' ] : .3f } \\n MAPD: { plot_config [ 'mapd' ] : .2% } \" , prop = dict ( size = 15 , fontsize = 20 ), frameon = True , loc = 'upper left' , ) at . patch . set_boxstyle ( \"round,pad=0.,rounding_size=0.2\" ) ax . add_artist ( at ) ax . autoscale ( enable = True , axis = 'both' , tight = True ) # ================== # Limites # ================== ax . set_xlim ( 0.0001 , 0.001 ) ax . set_ylim ( 0.0001 , 0.001 ) ax . set_xlabel ( '' ) ax . set_ylabel ( '' ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 20 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) # extras plt . tight_layout () # save plot fig . savefig ( SAVE_PATH + f ' { plot_config [ \"region\" ] } _m { plot_config [ \"model\" ] } _f { plot_config [ \"float\" ] } _depth' + '.png' ) # Show Plot plt . show ()","title":"Scatter Plot"},{"location":"projects/RBIG/","text":"RBIG \u00b6 A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Resources \u00b6 Original Webpage - ISP Original MATLAB Code - webpage Original Python Code - github Paper - Iterative Gaussianization: from ICA to Random Rotations Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation. Software \u00b6 I have spent some time developing a python package to be able to use RBIG for many applications. I tried to break everything up into as many components as possible. It's fairly extensible but within a limited scope. I also tried hard to follow the scikit-learn conventions so that the learning curve is as small as possible. Installation Instructions \u00b6 We can pip install the package directly from the repository. pip install \"git+https://github.com/jejjohnson/rbig.git#egg=rbig\" We can create a conda environment and simply install all the packages manually. conda env create -f environment.yml Features \u00b6 Transformations \u00b6 Uniformization * Scipy Histogram Transformation - The simplest way to do it * Quantile Transformer - Focuses on estimating the CDF with options for estimating the PDF * Histogram Transformation (from scratch) - I do my own version which focuses on extending the CDF to allow for outliers * Mixture of Gaussians - A fully parameterized version Inverse Gaussian CDF Rotations * Principal Components Analysis (PCA) * Random Orthogonal Rotations * ICA ( TODO ) Univariate Entropy Estimators \u00b6 Kernel Density Estimation Histogram Multivariate Gaussian K-Nearest Neighbours Loss Functions \u00b6 Maximum Layers Information Loss - the change in total correlation Neg-Entropy Change in Negentropy Negative Log-Likelihood API \u00b6 Layers Params Models Losses","title":"RBIG"},{"location":"projects/RBIG/#rbig","text":"A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework.","title":"RBIG"},{"location":"projects/RBIG/#resources","text":"Original Webpage - ISP Original MATLAB Code - webpage Original Python Code - github Paper - Iterative Gaussianization: from ICA to Random Rotations Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.","title":"Resources"},{"location":"projects/RBIG/#software","text":"I have spent some time developing a python package to be able to use RBIG for many applications. I tried to break everything up into as many components as possible. It's fairly extensible but within a limited scope. I also tried hard to follow the scikit-learn conventions so that the learning curve is as small as possible.","title":"Software"},{"location":"projects/RBIG/#installation-instructions","text":"We can pip install the package directly from the repository. pip install \"git+https://github.com/jejjohnson/rbig.git#egg=rbig\" We can create a conda environment and simply install all the packages manually. conda env create -f environment.yml","title":"Installation Instructions"},{"location":"projects/RBIG/#features","text":"","title":"Features"},{"location":"projects/RBIG/#transformations","text":"Uniformization * Scipy Histogram Transformation - The simplest way to do it * Quantile Transformer - Focuses on estimating the CDF with options for estimating the PDF * Histogram Transformation (from scratch) - I do my own version which focuses on extending the CDF to allow for outliers * Mixture of Gaussians - A fully parameterized version Inverse Gaussian CDF Rotations * Principal Components Analysis (PCA) * Random Orthogonal Rotations * ICA ( TODO )","title":"Transformations"},{"location":"projects/RBIG/#univariate-entropy-estimators","text":"Kernel Density Estimation Histogram Multivariate Gaussian K-Nearest Neighbours","title":"Univariate Entropy Estimators"},{"location":"projects/RBIG/#loss-functions","text":"Maximum Layers Information Loss - the change in total correlation Neg-Entropy Change in Negentropy Negative Log-Likelihood","title":"Loss Functions"},{"location":"projects/RBIG/#api","text":"Layers Params Models Losses","title":"API"},{"location":"projects/RBIG/Applications/climate/","text":"","title":"Climate"},{"location":"projects/RBIG/Applications/demo_innf/","text":"Demo: Gaussianization \u00b6 Data \u00b6 RBIG Model \u00b6 Initialize Model \u00b6 # rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) Fit Model to Data \u00b6 # run RBIG model rbig_clf . fit ( X ); Visualization \u00b6 1. Marginal Gaussianization \u00b6 # rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V 2. Rotation \u00b6","title":"Demo: Gaussianization"},{"location":"projects/RBIG/Applications/demo_innf/#demo-gaussianization","text":"","title":"Demo: Gaussianization"},{"location":"projects/RBIG/Applications/demo_innf/#data","text":"","title":"Data"},{"location":"projects/RBIG/Applications/demo_innf/#rbig-model","text":"","title":"RBIG Model"},{"location":"projects/RBIG/Applications/demo_innf/#initialize-model","text":"# rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base )","title":"Initialize Model"},{"location":"projects/RBIG/Applications/demo_innf/#fit-model-to-data","text":"# run RBIG model rbig_clf . fit ( X );","title":"Fit Model to Data"},{"location":"projects/RBIG/Applications/demo_innf/#visualization","text":"","title":"Visualization"},{"location":"projects/RBIG/Applications/demo_innf/#1-marginal-gaussianization","text":"# rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V","title":"1. Marginal Gaussianization"},{"location":"projects/RBIG/Applications/demo_innf/#2-rotation","text":"","title":"2. Rotation"},{"location":"projects/RBIG/Applications/drought/","text":"","title":"Drought"},{"location":"projects/RBIG/Applications/spatial_temporal/","text":"","title":"Spatial temporal"},{"location":"projects/RBIG/Applications/earth_observation/","text":"RBIG for Spatial-Temporal Representation Analysis \u00b6 Notes \u00b6 Overview * Representation * My experience, PCA * Valeros previous paper * Schrodinger Eigenmaps * Model vs Density * Intro to Simo Sarkaa * Change of Variables to be Gaussian * Model as Gaussian * Density Estimation * Neural Splines Paper * Intro to Information * Neuroscience paper (why?) * variation of info paper - motivation 4 representation * Intro to RBIG * Gaussianization Flows paper * Intro to GPs * Gustau Paper Examples * Earth Science Data Cubes * Different Regions * Minicubes Visualization * Taylor Diagram - IT Measures * PySim Package * Line Plots - Model vs Density | MSE vs Info * Maps * DataShader * NASA Software? Algorithms * (GP vs RBIG) * GPFlow * pyRBIG * Taylor Diagrams * PySim","title":"RBIG for Spatial-Temporal Representation Analysis"},{"location":"projects/RBIG/Applications/earth_observation/#rbig-for-spatial-temporal-representation-analysis","text":"","title":"RBIG for Spatial-Temporal Representation Analysis"},{"location":"projects/RBIG/Applications/earth_observation/#notes","text":"Overview * Representation * My experience, PCA * Valeros previous paper * Schrodinger Eigenmaps * Model vs Density * Intro to Simo Sarkaa * Change of Variables to be Gaussian * Model as Gaussian * Density Estimation * Neural Splines Paper * Intro to Information * Neuroscience paper (why?) * variation of info paper - motivation 4 representation * Intro to RBIG * Gaussianization Flows paper * Intro to GPs * Gustau Paper Examples * Earth Science Data Cubes * Different Regions * Minicubes Visualization * Taylor Diagram - IT Measures * PySim Package * Line Plots - Model vs Density | MSE vs Info * Maps * DataShader * NASA Software? Algorithms * (GP vs RBIG) * GPFlow * pyRBIG * Taylor Diagrams * PySim","title":"Notes"},{"location":"projects/RBIG/Software/coupling/","text":"Coupling Layers \u00b6 A univariate bijective differentiable function \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} , parameterized by \\theta \\theta . Note : It needs to be strictly monotonic. Non-Linear Squared Flows \u00b6 \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} where \\theta=[a,b,c,d,h] \\theta=[a,b,c,d,h] Invertible Inverse is analytically computable (root of cubic polynomial) Paper : Latent Normalizing Flows for Discrete Sequences - Ziegler & Rush (2019) Continuous Mixture CDFs \u00b6 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 where \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K The function F_{\\theta_2}(x, \\pi, \\mu, \\sigma) F_{\\theta_2}(x, \\pi, \\mu, \\sigma) is a CDF mixture distribution of K K logistic functions, post-composed with an inverse Sigmoid function, logit = \\log p / (1-p) = \\log p / (1-p) . So the full function is: F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) Some notes: * \\text{logit}:[0,1] \\rightarrow \\mathbb{R} \\text{logit}:[0,1] \\rightarrow \\mathbb{R} - ensure the right range for \\hat{f} \\hat{f} * Inverse: done numerically w/ the bisection algorithm * \\nabla_x F(\\cdot) \\nabla_x F(\\cdot) - it's a mixture of PDFs of logistic mixture distribution (i.e. linear combination of hyperbolic secant functions) Paper : Flow++ - Ho et. al. (2019) Splines \u00b6 A spline is a piece-wise polynomial or a piece-rational function which is specified by K+1 K+1 points (x_i,y_i)_{i=0}^K (x_i,y_i)_{i=0}^K called knots which a spline is passed. In particular, I am interested in rational-quadratic splines. Models a coupling layer \\hat{f}_\\theta(x) \\hat{f}_\\theta(x) as a monotone rational-quadratic spline on the interval [-B, B] [-B, B] , and outside the interval as an identity function. Paper : Neural Spline Flows, Durkan et. al. (2019)","title":"Coupling Layers"},{"location":"projects/RBIG/Software/coupling/#coupling-layers","text":"A univariate bijective differentiable function \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} \\hat{f}_\\theta(x): \\mathbb{R} \\rightarrow \\mathbb{R} , parameterized by \\theta \\theta . Note : It needs to be strictly monotonic.","title":"Coupling Layers"},{"location":"projects/RBIG/Software/coupling/#non-linear-squared-flows","text":"\\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} \\hat{f}_\\theta(x) = ax + b + \\frac{c}{1+(dx+h)^2} where \\theta=[a,b,c,d,h] \\theta=[a,b,c,d,h] Invertible Inverse is analytically computable (root of cubic polynomial) Paper : Latent Normalizing Flows for Discrete Sequences - Ziegler & Rush (2019)","title":"Non-Linear Squared Flows"},{"location":"projects/RBIG/Software/coupling/#continuous-mixture-cdfs","text":"\\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 \\hat{f}_\\theta(x) = \\theta_1 F_{\\theta_3}(x) + \\theta_2 where \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K \\theta_1 \\neq \\theta, \\theta_3\\in \\mathbb{R}, \\theta_2=[\\pi, \\mu, \\sigma]\\in \\mathbb{R}^K \\times \\mathbb{R}^K \\times \\mathbb{R}^K The function F_{\\theta_2}(x, \\pi, \\mu, \\sigma) F_{\\theta_2}(x, \\pi, \\mu, \\sigma) is a CDF mixture distribution of K K logistic functions, post-composed with an inverse Sigmoid function, logit = \\log p / (1-p) = \\log p / (1-p) . So the full function is: F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) F(x, \\pi, \\mu, \\sigma) = \\text{logit}\\left( \\sum_{j=1}^{K} \\pi_j \\text{ logistic}\\left( \\frac{x-\\mu_j}{\\sigma} \\right) \\right) Some notes: * \\text{logit}:[0,1] \\rightarrow \\mathbb{R} \\text{logit}:[0,1] \\rightarrow \\mathbb{R} - ensure the right range for \\hat{f} \\hat{f} * Inverse: done numerically w/ the bisection algorithm * \\nabla_x F(\\cdot) \\nabla_x F(\\cdot) - it's a mixture of PDFs of logistic mixture distribution (i.e. linear combination of hyperbolic secant functions) Paper : Flow++ - Ho et. al. (2019)","title":"Continuous Mixture CDFs"},{"location":"projects/RBIG/Software/coupling/#splines","text":"A spline is a piece-wise polynomial or a piece-rational function which is specified by K+1 K+1 points (x_i,y_i)_{i=0}^K (x_i,y_i)_{i=0}^K called knots which a spline is passed. In particular, I am interested in rational-quadratic splines. Models a coupling layer \\hat{f}_\\theta(x) \\hat{f}_\\theta(x) as a monotone rational-quadratic spline on the interval [-B, B] [-B, B] , and outside the interval as an identity function. Paper : Neural Spline Flows, Durkan et. al. (2019)","title":"Splines"},{"location":"projects/RBIG/Software/losses/","text":"Loss Functions \u00b6 Recall the change of variables formulation to calculate the probability: p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| and we can also calculate the log probability like so: \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| where z=\\mathcal{G}_\\theta(x) z=\\mathcal{G}_\\theta(x) . Negative Log-Likelihood \u00b6 -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] Empirically, this can be calculated by: -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| Non-Gaussianity \u00b6 Another perspective is the \"Non-Gaussianity\" of your data. J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] If we assume that the probability of p_x(x)=c p_x(x)=c because it will never change, it means that the only thing we have to do is minimize the 2nd and 3rd terms. \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} which we can find empirically: J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) ! Question : What's the difference between the two equations? Perhaps part 1, you fit a Gaussian... Change in Total Correlation \u00b6 Change in Non-Gaussianity \u00b6 \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] Empirically, we can calculate this by: \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)|","title":"Loss Functions"},{"location":"projects/RBIG/Software/losses/#loss-functions","text":"Recall the change of variables formulation to calculate the probability: p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| p_\\theta(x) = p_z(z) \\; |\\nabla_x \\mathcal{G}_\\theta(x)| and we can also calculate the log probability like so: \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| \\log p_\\theta(x) = \\log p_z(z) + \\log |\\nabla_x \\mathcal{G}_\\theta(x)| where z=\\mathcal{G}_\\theta(x) z=\\mathcal{G}_\\theta(x) .","title":"Loss Functions"},{"location":"projects/RBIG/Software/losses/#negative-log-likelihood","text":"-\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = - \\mathbb{E}_x \\left[ \\log p_z(\\mathcal{G}_\\theta(x)) + \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\right] Empirically, this can be calculated by: -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)| -\\mathbb{E}_\\mathbf{x}\\left[ \\log p_\\theta(x)\\right] = -\\frac{1}{N} \\sum_{i=1}^N \\log p_z(\\mathcal{G}_\\theta(x_i)) - \\frac{1}{N} \\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x_i)|","title":"Negative Log-Likelihood"},{"location":"projects/RBIG/Software/losses/#non-gaussianity","text":"Another perspective is the \"Non-Gaussianity\" of your data. J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] J(p_y) = \\mathbb{E}_x \\left[ \\log p_x(x) - \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right)\\right] If we assume that the probability of p_x(x)=c p_x(x)=c because it will never change, it means that the only thing we have to do is minimize the 2nd and 3rd terms. \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} \\begin{aligned} J(p_y) &= - \\mathbb{E}_x \\left[ \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| \\right] - \\mathbb{E}_x \\left[ \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x)\\right) \\right] \\\\ \\end{aligned} which we can find empirically: J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) J(p_y) = \\sum_{i=1}^N \\log \\left| \\nabla_x \\mathcal{G}_\\theta(x) \\right| - \\sum_{i=1}^N \\log \\mathcal{N}\\left(\\mathcal{G}_\\theta(x_i)\\right) ! Question : What's the difference between the two equations? Perhaps part 1, you fit a Gaussian...","title":"Non-Gaussianity"},{"location":"projects/RBIG/Software/losses/#change-in-total-correlation","text":"","title":"Change in Total Correlation"},{"location":"projects/RBIG/Software/losses/#change-in-non-gaussianity","text":"\\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = J(p_y) - J(p_x) \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] \\Delta J(p_y) = \\mathbb{E}_x \\left[ \\frac{1}{2} ||y||_2^2 - \\log |\\nabla_x \\mathcal{G}_\\theta (x)| - \\frac{1}{2} ||x||_2^2 \\right] Empirically, we can calculate this by: \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)| \\Delta J(p_y) = \\frac{1}{2} ||y||_2^2 - \\frac{1}{2} ||x||_2^2 - \\frac{1}{N}\\sum_{i=1}^N \\log |\\nabla_x \\mathcal{G}_\\theta (x)|","title":"Change in Non-Gaussianity"},{"location":"projects/RBIG/Software/marginal_gauss/","text":"Marginal Gaussianization \u00b6 A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform Idea \u00b6 The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration. High-Level Instructions \u00b6 Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d)) Mathematical Details \u00b6 For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian. Data \u00b6 In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution. Marginal Uniformization \u00b6 The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' Histogram Estimation \u00b6 Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1. Gaussianization of Uniform Variable \u00b6 In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' Log Determinant Jacobian \u00b6 \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} Log-Likelihood of the Data \u00b6 \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) Quantile Transform \u00b6 Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch KDE Transform \u00b6 Spline Functions \u00b6 Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper Gaussian Transform \u00b6","title":"Marginal Gaussianization"},{"location":"projects/RBIG/Software/marginal_gauss/#marginal-gaussianization","text":"A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform","title":"Marginal Gaussianization"},{"location":"projects/RBIG/Software/marginal_gauss/#idea","text":"The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration.","title":"Idea"},{"location":"projects/RBIG/Software/marginal_gauss/#high-level-instructions","text":"Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d))","title":"High-Level Instructions"},{"location":"projects/RBIG/Software/marginal_gauss/#mathematical-details","text":"For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian.","title":"Mathematical Details"},{"location":"projects/RBIG/Software/marginal_gauss/#data","text":"In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution.","title":"Data"},{"location":"projects/RBIG/Software/marginal_gauss/#marginal-uniformization","text":"The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d'","title":"Marginal Uniformization"},{"location":"projects/RBIG/Software/marginal_gauss/#histogram-estimation","text":"Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1.","title":"Histogram Estimation"},{"location":"projects/RBIG/Software/marginal_gauss/#gaussianization-of-uniform-variable","text":"In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d'","title":"Gaussianization of Uniform Variable"},{"location":"projects/RBIG/Software/marginal_gauss/#log-determinant-jacobian","text":"\\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)}","title":"Log Determinant Jacobian"},{"location":"projects/RBIG/Software/marginal_gauss/#log-likelihood-of-the-data","text":"\\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N})","title":"Log-Likelihood of the Data"},{"location":"projects/RBIG/Software/marginal_gauss/#quantile-transform","text":"Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch","title":"Quantile Transform"},{"location":"projects/RBIG/Software/marginal_gauss/#kde-transform","text":"","title":"KDE Transform"},{"location":"projects/RBIG/Software/marginal_gauss/#spline-functions","text":"Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper","title":"Spline Functions"},{"location":"projects/RBIG/Software/marginal_gauss/#gaussian-transform","text":"","title":"Gaussian Transform"},{"location":"projects/RBIG/Software/marginal_uni/","text":"Marginal Uniformization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020 Forward Transformation \u00b6 In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) . Boundary Issues \u00b6 The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails. Reverse Transformation \u00b6 This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems. Derivative \u00b6 In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} Log Abs Determinant Jacobian \u00b6 This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values. Probability (Computing the Density) \u00b6 So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Marginal Uniformization"},{"location":"projects/RBIG/Software/marginal_uni/#marginal-uniformization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020","title":"Marginal Uniformization"},{"location":"projects/RBIG/Software/marginal_uni/#forward-transformation","text":"In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) .","title":"Forward Transformation"},{"location":"projects/RBIG/Software/marginal_uni/#boundary-issues","text":"The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails.","title":"Boundary Issues"},{"location":"projects/RBIG/Software/marginal_uni/#reverse-transformation","text":"This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems.","title":"Reverse Transformation"},{"location":"projects/RBIG/Software/marginal_uni/#derivative","text":"In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned}","title":"Derivative"},{"location":"projects/RBIG/Software/marginal_uni/#log-abs-determinant-jacobian","text":"This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values.","title":"Log Abs Determinant Jacobian"},{"location":"projects/RBIG/Software/marginal_uni/#probability-computing-the-density","text":"So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Probability (Computing the Density)"},{"location":"projects/RBIG/Software/refactor/","text":"Refactoring RBIG (RBIG 1.1) \u00b6 Components \u00b6 Flow \u00b6 Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian Normalizing Flow \u00b6 This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian Normalizing Flow Model \u00b6 This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform Ideal Case \u00b6 Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"projects/RBIG/Software/refactor/#refactoring-rbig-rbig-11","text":"","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"projects/RBIG/Software/refactor/#components","text":"","title":"Components"},{"location":"projects/RBIG/Software/refactor/#flow","text":"Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian","title":"Flow"},{"location":"projects/RBIG/Software/refactor/#normalizing-flow","text":"This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian","title":"Normalizing Flow"},{"location":"projects/RBIG/Software/refactor/#normalizing-flow-model","text":"This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform","title":"Normalizing Flow Model"},{"location":"projects/RBIG/Software/refactor/#ideal-case","text":"Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Ideal Case"},{"location":"projects/RBIG/Software/rotation/","text":"Rotation \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook Main Idea \u00b6 Rotation Matrix \u00b6 Foward Transformation \u00b6 Reverse Transformation \u00b6 Jacobian \u00b6 The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 . Log Jacobian \u00b6 As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0 Decompositions \u00b6 QR Decomposition \u00b6 A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular Singular Value Decomposition \u00b6 Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary Eigendecomposition \u00b6 Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Polar Decomposition \u00b6 A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6","title":"Rotation"},{"location":"projects/RBIG/Software/rotation/#rotation","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook","title":"Rotation"},{"location":"projects/RBIG/Software/rotation/#main-idea","text":"","title":"Main Idea"},{"location":"projects/RBIG/Software/rotation/#rotation-matrix","text":"","title":"Rotation Matrix"},{"location":"projects/RBIG/Software/rotation/#foward-transformation","text":"","title":"Foward Transformation"},{"location":"projects/RBIG/Software/rotation/#reverse-transformation","text":"","title":"Reverse Transformation"},{"location":"projects/RBIG/Software/rotation/#jacobian","text":"The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 .","title":"Jacobian"},{"location":"projects/RBIG/Software/rotation/#log-jacobian","text":"As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0","title":"Log Jacobian"},{"location":"projects/RBIG/Software/rotation/#decompositions","text":"","title":"Decompositions"},{"location":"projects/RBIG/Software/rotation/#qr-decomposition","text":"A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular","title":"QR Decomposition"},{"location":"projects/RBIG/Software/rotation/#singular-value-decomposition","text":"Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary","title":"Singular Value Decomposition"},{"location":"projects/RBIG/Software/rotation/#eigendecomposition","text":"Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Eigendecomposition"},{"location":"projects/RBIG/Software/rotation/#polar-decomposition","text":"A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Polar Decomposition"},{"location":"projects/RBIG/Software/rotation/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"projects/RBIG/Software/rotation/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"projects/RBIG/Software/rotation/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"projects/RBIG/Software/rotation/#jacobian_1","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"projects/RBIG/Software/rotation/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"projects/RBIG/Software/RBIG2.0/","text":"","title":"Index"},{"location":"projects/RBIG/Software/RBIG2.0/ideas/","text":"RBIG 2.0 Ideas \u00b6 Neural Spline Flows - github Linear Rational Splines - github \\mathcal{X} \\rightarrow \\mathcal{Z} \\mathcal{X} \\rightarrow \\mathcal{Z} Option I F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] - CDF \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Logit Function Option II F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Quantile Gaussian Function CDF Function \u00b6 Mixture of Gaussians Mixture of Logistics Linear Quadratic Splines Rational Quadratic Splines NonLinear Squared Flows Logit Function \u00b6 Logit Function Gaussian Quantile Function Rotation \u00b6 HouseHolder Transforms SVD QR Decomposition LU Decomposition","title":"RBIG 2.0 Ideas"},{"location":"projects/RBIG/Software/RBIG2.0/ideas/#rbig-20-ideas","text":"Neural Spline Flows - github Linear Rational Splines - github \\mathcal{X} \\rightarrow \\mathcal{Z} \\mathcal{X} \\rightarrow \\mathcal{Z} Option I F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] - CDF \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\sigma^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Logit Function Option II F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] F_\\theta: (-\\infty, \\infty) \\rightarrow [0,1] \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) \\text{Quantile}_{\\mathcal{G}}^{-1}: [0,1] \\rightarrow (-\\infty, \\infty) - Quantile Gaussian Function","title":"RBIG 2.0 Ideas"},{"location":"projects/RBIG/Software/RBIG2.0/ideas/#cdf-function","text":"Mixture of Gaussians Mixture of Logistics Linear Quadratic Splines Rational Quadratic Splines NonLinear Squared Flows","title":"CDF Function"},{"location":"projects/RBIG/Software/RBIG2.0/ideas/#logit-function","text":"Logit Function Gaussian Quantile Function","title":"Logit Function"},{"location":"projects/RBIG/Software/RBIG2.0/ideas/#rotation","text":"HouseHolder Transforms SVD QR Decomposition LU Decomposition","title":"Rotation"},{"location":"projects/RBIG/Theory/gaussianization/","text":"Gaussianization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization Rotation-Based Iterative Gaussianization Rotation-Based Iterative Gaussianization Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence References Why Gaussianization? \u00b6 Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . Loss Function \u00b6 as shown in the equation from the original paper . Negentropy \u00b6 Methods \u00b6 Projection Pursuit \u00b6 Gaussianization \u00b6 Rotation-Based Iterative Gaussianization \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Rotation-Based Iterative Gaussianization \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space. Information Theory Measures \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} KL-Divergence \u00b6 Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} References \u00b6","title":"Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#gaussianization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization Rotation-Based Iterative Gaussianization Rotation-Based Iterative Gaussianization Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence References","title":"Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#why-gaussianization","text":"Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"projects/RBIG/Theory/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"projects/RBIG/Theory/gaussianization/#loss-function","text":"as shown in the equation from the original paper .","title":"Loss Function"},{"location":"projects/RBIG/Theory/gaussianization/#negentropy","text":"","title":"Negentropy"},{"location":"projects/RBIG/Theory/gaussianization/#methods","text":"","title":"Methods"},{"location":"projects/RBIG/Theory/gaussianization/#projection-pursuit","text":"","title":"Projection Pursuit"},{"location":"projects/RBIG/Theory/gaussianization/#gaussianization_1","text":"","title":"Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#rotation-based-iterative-gaussianization","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Rotation-Based Iterative Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#rotation-based-iterative-gaussianization_1","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Rotation-Based Iterative Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"projects/RBIG/Theory/gaussianization/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"projects/RBIG/Theory/gaussianization/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"projects/RBIG/Theory/gaussianization/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"projects/RBIG/Theory/gaussianization/#information-theory-measures","text":"Caption : Information Theory measures in a nutshell.","title":"Information Theory Measures"},{"location":"projects/RBIG/Theory/gaussianization/#information","text":"","title":"Information"},{"location":"projects/RBIG/Theory/gaussianization/#entropy","text":"","title":"Entropy"},{"location":"projects/RBIG/Theory/gaussianization/#mutual-information","text":"Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned}","title":"Mutual Information"},{"location":"projects/RBIG/Theory/gaussianization/#kl-divergence","text":"Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"KL-Divergence"},{"location":"projects/RBIG/Theory/gaussianization/#references","text":"","title":"References"},{"location":"projects/RBIG/Theory/itm/","text":"Information Theory Measures \u00b6 Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD) Summary \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Total Correlation (Mutual Information) \u00b6 This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF Kullback-Leibler Divergence (KLD) \u00b6","title":"Information Theory Measures"},{"location":"projects/RBIG/Theory/itm/#information-theory-measures","text":"Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD)","title":"Information Theory Measures"},{"location":"projects/RBIG/Theory/itm/#summary","text":"Caption : Information Theory measures in a nutshell.","title":"Summary"},{"location":"projects/RBIG/Theory/itm/#information","text":"","title":"Information"},{"location":"projects/RBIG/Theory/itm/#entropy","text":"","title":"Entropy"},{"location":"projects/RBIG/Theory/itm/#mutual-information","text":"","title":"Mutual Information"},{"location":"projects/RBIG/Theory/itm/#total-correlation-mutual-information","text":"This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF","title":"Total Correlation (Mutual Information)"},{"location":"projects/RBIG/Theory/itm/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"projects/RBIG/Theory/literature/","text":"Literature Review \u00b6 Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization Theory \u00b6 Gaussianization \u00b6 The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks RBIG \u00b6 The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications Generalized Divisive Normalization \u00b6 This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Literature Review"},{"location":"projects/RBIG/Theory/literature/#literature-review","text":"Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization","title":"Literature Review"},{"location":"projects/RBIG/Theory/literature/#theory","text":"","title":"Theory"},{"location":"projects/RBIG/Theory/literature/#gaussianization","text":"The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Gaussianization"},{"location":"projects/RBIG/Theory/literature/#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks","title":"Journal Articles"},{"location":"projects/RBIG/Theory/literature/#rbig","text":"The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications","title":"RBIG"},{"location":"projects/RBIG/Theory/literature/#generalized-divisive-normalization","text":"This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Generalized Divisive Normalization"},{"location":"projects/RBIG/Theory/related/","text":"Related Methods \u00b6 Deep Density Destructors Main Idea Normalizing Flows Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations Deep Density Destructors \u00b6 Main Idea \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| Normalizing Flows \u00b6 Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} Loss Function \u00b6 We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless. Sampling \u00b6 If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} Choice of Transformations \u00b6 The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian. Prior Distribution \u00b6 This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple. Jacobian \u00b6 This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed. Resources \u00b6 Best Tutorials \u00b6 Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang Survey of Literature \u00b6 Neural Density Estimators \u00b6 Deep Density Destructors \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/ Tutorials \u00b6 RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019) Algorithms \u00b6 * RBIG Upgrades \u00b6 Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF Cutting Edge \u00b6 Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code Github Implementations \u00b6 Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Related Methods"},{"location":"projects/RBIG/Theory/related/#related-methods","text":"Deep Density Destructors Main Idea Normalizing Flows Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations","title":"Related Methods"},{"location":"projects/RBIG/Theory/related/#deep-density-destructors","text":"","title":"Deep Density Destructors"},{"location":"projects/RBIG/Theory/related/#main-idea","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Main Idea"},{"location":"projects/RBIG/Theory/related/#normalizing-flows","text":"Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned}","title":"Normalizing Flows"},{"location":"projects/RBIG/Theory/related/#loss-function","text":"We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless.","title":"Loss Function"},{"location":"projects/RBIG/Theory/related/#sampling","text":"If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned}","title":"Sampling"},{"location":"projects/RBIG/Theory/related/#choice-of-transformations","text":"The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian.","title":"Choice of Transformations"},{"location":"projects/RBIG/Theory/related/#prior-distribution","text":"This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple.","title":"Prior Distribution"},{"location":"projects/RBIG/Theory/related/#jacobian","text":"This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed.","title":"Jacobian"},{"location":"projects/RBIG/Theory/related/#resources","text":"","title":"Resources"},{"location":"projects/RBIG/Theory/related/#best-tutorials","text":"Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang","title":"Best Tutorials"},{"location":"projects/RBIG/Theory/related/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"projects/RBIG/Theory/related/#neural-density-estimators","text":"","title":"Neural Density Estimators"},{"location":"projects/RBIG/Theory/related/#deep-density-destructors_1","text":"","title":"Deep Density Destructors"},{"location":"projects/RBIG/Theory/related/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"projects/RBIG/Theory/related/#tutorials","text":"RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019)","title":"Tutorials"},{"location":"projects/RBIG/Theory/related/#algorithms","text":"*","title":"Algorithms"},{"location":"projects/RBIG/Theory/related/#rbig-upgrades","text":"Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF","title":"RBIG Upgrades"},{"location":"projects/RBIG/Theory/related/#cutting-edge","text":"Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code","title":"Cutting Edge"},{"location":"projects/RBIG/Theory/related/#github-implementations","text":"Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Github Implementations"},{"location":"projects/similarity/","text":"Similarity Measures \u00b6 Description I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed. Kernel Parameter Estimation \u00b6 In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links Main Project Page LaTeX Doc FFT Talk Information Measures for Climate Model Comparisons \u00b6 In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links Main Projecct Page LaTeX Doc FFT Talk Information Measures for Drought Factors \u00b6 Summary In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. In particular, I investigate the Important Links Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk Software: PySim \u00b6 Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links Github Repository","title":"Overview"},{"location":"projects/similarity/#similarity-measures","text":"Description I am very interested in the notion of similarity: what it means, how can we estimate similarity and how does it work in practice. Below are some of the main projects I have been working on which include an empirical study, some applications and some software that was developed.","title":"Similarity Measures"},{"location":"projects/similarity/#kernel-parameter-estimation","text":"In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. Unsupervised kernel methods can suffer if the parameters are not estimated correctly. So I go through and empirically look at different ways we can represent our data and different ways we can estimate the parameters for the unsupervised kernel method. I investigate the following questions: Will standardizing the data beforehand affect the results? How does the parameter estimator affect the results? Which variation of HSIC gives the best representation of the similarity (center the kernel, normalize the score)? How does this all compare to mutual information for known high-dimensional, multivariate distributions? Important Links Main Project Page LaTeX Doc FFT Talk","title":"Kernel Parameter Estimation"},{"location":"projects/similarity/#information-measures-for-climate-model-comparisons","text":"In this project, I used a Gaussianization model to look compare some CMIP5 models the spatial-temporal repre Important Links Main Projecct Page LaTeX Doc FFT Talk","title":"Information Measures for Climate Model Comparisons"},{"location":"projects/similarity/#information-measures-for-drought-factors","text":"Summary In this project, I look at how we one can estimate the parameters of the RBF kernel for various variations of the HSIC method; kernel alignment and centered kernel alignment. In particular, I investigate the Important Links Main Projecct Page LaTeX Doc Paper: Climate Informatics Poster: Climate Informatics Phi-Week Talk","title":"Information Measures for Drought Factors"},{"location":"projects/similarity/#software-pysim","text":"Some highlights include: Scikit-Learn Format to allow for pipeline, cross-validation and scoring The HSIC and all of it's variations including the randomized implementation Some basics for visualizations using the Taylor Diagram Some other methods for estimating similarity Important Links Github Repository","title":"Software: PySim"},{"location":"projects/similarity/kernel_alignment_params/","text":"Kernel Parameter Estimation \u00b6 Motivation \u00b6 What is Similarity? Why HSIC? The differences between HSIC The Problems with high-dimensional data Research Questions \u00b6 Demo Notebook See this notebook for a full break down of each research question and why it's important and possibly difficult. 1. Which Scorer should we use? \u00b6 We are looking at different \"HSIC scorers\" because they all vary in terms of whether they center the kernel matrix or if they normalize the score via the norm of the individual kernels. Different Scorers HSIC \\text{HSIC} = \\frac{1}{n(n-1)}\\langle K_xH,K_yH \\rangle_F \\text{HSIC} = \\frac{1}{n(n-1)}\\langle K_xH,K_yH \\rangle_F Notice : we have the centered kernels, K_xH K_xH and no normalization. Kernel Alignment \\text{KA} = \\frac{\\langle K_x,K_y \\rangle_F}{||K_x||_F||K_y||_F} \\text{KA} = \\frac{\\langle K_x,K_y \\rangle_F}{||K_x||_F||K_y||_F} Notice : We have the uncentered kernels and a normalization factor. Centered Kernel Alignment \\text{cKA} = \\frac{\\langle K_xH,K_yH \\rangle_F}{||K_xH||_F||K_yH||_F} \\text{cKA} = \\frac{\\langle K_xH,K_yH \\rangle_F}{||K_xH||_F||K_yH||_F} Notice : We have the centered kernels and a normalization factor. 2. Which Estimator should we use? \u00b6 Example Estimators Scott Notice : This method doesn't take into account the dimensionality Median Notice : We can scale this by a factor 3. Should we use different length scales or the same? \u00b6 4. Should we standardize our data? \u00b6 5. Summary of Parameters \u00b6 Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No Experiments \u00b6 Parameter Grid - 1D Data \u00b6 Notebook Parameter Grid - nD Data \u00b6 Todo Notebook doing some demos Mutual Information vs HSIC scores \u00b6 Todo Notebook doing some demos Results \u00b6 Take-Home Message I \u00b6 The median distance seems to be fairly robust in settings with different samples and dimensions. Scott and Silverman should probably avoided if you are not going to estimate the the parameter per feature. Take-Home Message II \u00b6 It appears that the centered kernel alignment (CKA) method is the most consistent when we compare the score versus the mutual information of known distributions. HSIC has some consistency but not entirely. The KA algorithm has no consistency whatsoever; avoid using this method for unsupervised problems.","title":"Overview"},{"location":"projects/similarity/kernel_alignment_params/#kernel-parameter-estimation","text":"","title":"Kernel Parameter Estimation"},{"location":"projects/similarity/kernel_alignment_params/#motivation","text":"What is Similarity? Why HSIC? The differences between HSIC The Problems with high-dimensional data","title":"Motivation"},{"location":"projects/similarity/kernel_alignment_params/#research-questions","text":"Demo Notebook See this notebook for a full break down of each research question and why it's important and possibly difficult.","title":"Research Questions"},{"location":"projects/similarity/kernel_alignment_params/#1-which-scorer-should-we-use","text":"We are looking at different \"HSIC scorers\" because they all vary in terms of whether they center the kernel matrix or if they normalize the score via the norm of the individual kernels. Different Scorers HSIC \\text{HSIC} = \\frac{1}{n(n-1)}\\langle K_xH,K_yH \\rangle_F \\text{HSIC} = \\frac{1}{n(n-1)}\\langle K_xH,K_yH \\rangle_F Notice : we have the centered kernels, K_xH K_xH and no normalization. Kernel Alignment \\text{KA} = \\frac{\\langle K_x,K_y \\rangle_F}{||K_x||_F||K_y||_F} \\text{KA} = \\frac{\\langle K_x,K_y \\rangle_F}{||K_x||_F||K_y||_F} Notice : We have the uncentered kernels and a normalization factor. Centered Kernel Alignment \\text{cKA} = \\frac{\\langle K_xH,K_yH \\rangle_F}{||K_xH||_F||K_yH||_F} \\text{cKA} = \\frac{\\langle K_xH,K_yH \\rangle_F}{||K_xH||_F||K_yH||_F} Notice : We have the centered kernels and a normalization factor.","title":"1. Which Scorer should we use?"},{"location":"projects/similarity/kernel_alignment_params/#2-which-estimator-should-we-use","text":"Example Estimators Scott Notice : This method doesn't take into account the dimensionality Median Notice : We can scale this by a factor","title":"2. Which Estimator should we use?"},{"location":"projects/similarity/kernel_alignment_params/#3-should-we-use-different-length-scales-or-the-same","text":"","title":"3. Should we use different length scales or the same?"},{"location":"projects/similarity/kernel_alignment_params/#4-should-we-standardize-our-data","text":"","title":"4. Should we standardize our data?"},{"location":"projects/similarity/kernel_alignment_params/#5-summary-of-parameters","text":"Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No","title":"5. Summary of Parameters"},{"location":"projects/similarity/kernel_alignment_params/#experiments","text":"","title":"Experiments"},{"location":"projects/similarity/kernel_alignment_params/#parameter-grid-1d-data","text":"Notebook","title":"Parameter Grid - 1D Data"},{"location":"projects/similarity/kernel_alignment_params/#parameter-grid-nd-data","text":"Todo Notebook doing some demos","title":"Parameter Grid - nD Data"},{"location":"projects/similarity/kernel_alignment_params/#mutual-information-vs-hsic-scores","text":"Todo Notebook doing some demos","title":"Mutual Information vs HSIC scores"},{"location":"projects/similarity/kernel_alignment_params/#results","text":"","title":"Results"},{"location":"projects/similarity/kernel_alignment_params/#take-home-message-i","text":"The median distance seems to be fairly robust in settings with different samples and dimensions. Scott and Silverman should probably avoided if you are not going to estimate the the parameter per feature.","title":"Take-Home Message I"},{"location":"projects/similarity/kernel_alignment_params/#take-home-message-ii","text":"It appears that the centered kernel alignment (CKA) method is the most consistent when we compare the score versus the mutual information of known distributions. HSIC has some consistency but not entirely. The KA algorithm has no consistency whatsoever; avoid using this method for unsupervised problems.","title":"Take-Home Message II"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/","text":"Motivation for this Study \u00b6 In this document, I will be looking at the motivation behind this study and why we would like to pursue this further. Code Blocks Imports import sys , os import warnings import tqdm import random import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt # Insert path to model directory,. cwd = os . getcwd () path = f \" { cwd } /../../src\" sys . path . insert ( 0 , path ) # Insert path to package,. pysim_path = f \"/home/emmanuel/code/pysim/\" sys . path . insert ( 0 , pysim_path ) # toy datasets from data.toy import generate_dependence_data from data.distribution import DataParams from dataclasses import dataclass # Kernel Dependency measure from sklearn.preprocessing import StandardScaler from sklearn.gaussian_process.kernels import RBF from models.dependence import HSICModel from pysim.kernel.utils import get_init_gammas , get_gamma_grid , estimate_sigma import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-talk' ]) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings FIG_PATH = \"/home/emmanuel/projects/2019_hsic_align/results/figures/1d_dataset/demo/\" % load_ext autoreload % autoreload 2 Useful Functions def standardize_data ( X , Y , standardize : bool = False ): X = StandardScaler () . fit_transform ( X ) Y = StandardScaler () . fit_transform ( Y ) return X , Y def get_sigma ( X , Y , method : str = 'silverman' , per_dimension : bool = False , separate_scales : bool = False ): # sigma parameters subsample = None percent = 0.20 random_state = 123 sigma_X = estimate_sigma ( X , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) sigma_Y = estimate_sigma ( Y , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) if separate_scales : sigma_X = np . mean ([ sigma_X , sigma_Y ]) sigma_Y = np . mean ([ sigma_X , sigma_Y ]) return sigma_X , sigma_Y def get_hsic ( X , Y , scorer : str , sigma_X = None , sigma_Y = None ): # init hsic model class hsic_model = HSICModel () # hsic model params if sigma_X is not None : hsic_model . kernel_X = RBF ( sigma_X ) hsic_model . kernel_Y = RBF ( sigma_Y ) # get hsic score hsic_val = hsic_model . get_score ( X , Y , scorer ) return hsic_val 1D Example \u00b6 Code Block # data params dataset = 'sine' num_points = 1000 seed = 123 noise = 0.1 # get dataset X , Y = generate_dependence_data ( dataset = dataset , num_points = num_points , seed = seed , noise_x = noise , noise_y = noise ) # plot fig , ax = plt . subplots () ax . scatter ( X [: 100 ,:], Y [: 100 ,:]) plt . tight_layout () fig . savefig ( FIG_PATH + f \"demo_ { dataset } .png\" ) plt . show () Fig I : An example 1D Sine Curve. Let's take a simple 1D distribution: a sine curve. It is clear that there is a nonlinear relationship between them that cannot be captured (well) by linear methods. We are interested in looking at the dependence between X X and Y Y . We have the HSIC family of methods: HSIC, kernel alignment and centered kernel alignment. They are all very similar but there are some subtle differences. We will highlight them as we go through the overview. Let's take a generic approach and use the default HSIC, KA and CKA methods to try and estimate the dependence between X,Y X,Y . If we run the algorithm, we get the following results. Question I - Which Algorithm? \u00b6 Code Block results_df = pd . DataFrame () method = 'scott' per_dimension = False separate_scales = False # sigma_X, sigma_y = get_sigma( # X, Y, # method=method, # per_dimension=per_dimension, # separate_scales=separate_scales # ) method = 'default' sigma_X , sigma_Y = None , None scorer = 'hsic' results_df = results_df . append ( pd . DataFrame ({ \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q1' ]),) print ( results_df . to_markdown ()) hsic ka cka Q1 0.0582356 0.688475 0.588434 Notice how all of the values are slightly difference. This is because of the composition of the methods. We can highlight the differences with a simple table. Method Centered Kernel Normalized HSIC Yes No Kernel Alignment No Yes Centered Kernel Alignment Yes No So each method has a slightly different formulation but they are mostly the same. So now the next question is: how do we estimate the parameters of the kernel used? Well the default is simply \\sigma=1.0 \\sigma=1.0 but we know that this won't do as the kernel depends on the parameters of the kernel. In this case we are using the most commonly used kernel: the Radial Basis Function (RBF). Since this is a 1D example, I will use some generic estimators called the \"Silverman Rule\" and \"Scott Rule\". These are very commonly found in packages like scipy.stats.gaussian_kde or statsmodels.nonparametric.bandwidth . They are mostly used for the Kernel Density Estimation (KDE) where we need a decent parameter to approximate the kernel to get a decent density estimate. So what happens with the methods and the results? Question II - Which Parameter Estimator? \u00b6 Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = True results_df = pd . DataFrame () for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = per_dimension , separate_scales = separate_scales ) results_df = results_df . append ( pd . DataFrame ({ # \"sigma_x\": [sigma_X], # \"sigma_y\": [sigma_Y], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q2' ]),) print ( results_df . to_markdown ()) Estimator hsic ka cka Q2 scott 0.0575482 0.660478 0.530685 Q2 silverman 0.0515751 0.6345 0.515583 Q2 median 0.066173 0.702005 0.556274 Question III - How do we estimate the length scale? \u00b6 Use the same length scale? Use different length scales? Use a length scale per dimension (D>1) Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] results_df = pd . DataFrame () for iscaler in separate_scales : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = per_dimension , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ # \"sigma_x\": [sigma_X], \"separate\" : [ iscaler ], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q3' ]),) print ( results_df . to_markdown ()) separate Estimator hsic ka cka Q3 True scott 0.0575482 0.660478 0.530685 Q3 True silverman 0.0515751 0.6345 0.515583 Q3 True median 0.066173 0.702005 0.556274 Q3 False scott 0.0601095 0.696988 0.596866 Q3 False silverman 0.0524045 0.66827 0.577468 Q3 False median 0.0728568 0.739607 0.620757 Question IV - Standardize Data? \u00b6 We could also standardize our data... This could actually change the size of each of the features which could eliminate the need to apply separate length scales. Code Block standardize = [ True , False ] methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] results_df = pd . DataFrame () for istandard in standardize : X_ , Y_ = standardize_data ( X , Y , istandard ) for iscaler in separate_scales : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X_ , Y_ , method = imethod , per_dimension = per_dimension , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"separate\" : [ iscaler ], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X_ , Y_ , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X_ , Y_ , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X_ , Y_ , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q4' ]),) print ( results_df . to_markdown ()) standardize separate Estimator hsic ka cka Q4 True True scott 0.0601095 0.696988 0.596866 Q4 True True silverman 0.0524045 0.66827 0.577468 Q4 True True median 0.0729923 0.74078 0.623443 Q4 True False scott 0.0601095 0.696988 0.596866 Q4 True False silverman 0.0524045 0.66827 0.577468 Q4 True False median 0.0728568 0.739607 0.620757 Q4 False True scott 0.0601095 0.696988 0.596866 Q4 False True silverman 0.0524045 0.66827 0.577468 Q4 False True median 0.0729923 0.74078 0.623443 Q4 False False scott 0.0601095 0.696988 0.596866 Q4 False False silverman 0.0524045 0.66827 0.577468 Q4 False False median 0.0728568 0.739607 0.620757 Now we see that the values you get are quite different for all methods. What happens if we use different sigmas? Todo Show a plot of the different parameters and how much they vary. No need to see the actual origins. Just need to highlight the variance in the estimates. Verdict \u00b6 Well, hard to say as it depends on the parameters. Every researcher I've met who dealt with kernel methods seems to have a suggestion that they swear by but I never know who to follow. My thoughts is that we should use dedicated sigma values per dataset however, that still leaves us with other methods that we may want to try. So we're going to repeat the same experiment but with a 2D dataset and we will see that the difficult will increase again. 2D Example \u00b6 For this experiment, we're going to take two 2D datasets each generated from a T-Student distribution. We will apply the same sequence as we did above and we will end the section by adding another option for picking the parameters. Code Block # initialize Data Params class dataset = 'tstudent' samples = 1_000 dimensions = 2 std = 5 nu = 8 trial = 1 standardize = False # initialize params example_params = DataParams ( dataset = dataset , samples = samples , dimensions = dimensions , std = std , nu = nu , trial = trial , standardize = standardize ) # generate some parameters inputs = example_params . generate_data () sns . jointplot ( x = inputs . X , y = inputs . Y ) plt . tight_layout () plt . savefig ( FIG_PATH + f \"demo_ { dataset } .png\" ) Fig II : An example 2D T-Student distribution. Question III (Revisited) - Different Length Scales? \u00b6 Now we can revisit this question because we actually could estimate a different length scale depending upon the dimensionality. One problem with scott or Silverman's method is that it takes into account the entire dataset instead of having one estimate per feature. Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] separate_dimensions = [ True , False ] results_df = pd . DataFrame () for iscaler in separate_scales : for idim in separate_dimensions : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = idim , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"Separate Dimensions\" : [ idim ], \"Separate Length Scales\" : [ iscaler ], 'Param Estimator' : [ imethod ], \"HSIC\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"KA\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"CKA\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q3' ]),) print ( results_df . to_markdown ()) standardize Separate Dimensions Separate Length Scales Param Estimator HSIC KA CKA Q3 False True True scott 0.0575482 0.660478 0.530685 Q3 False True True silverman 0.0515751 0.6345 0.515583 Q3 False True True median 0.066173 0.702005 0.556274 Q3 False False True scott 0.0575482 0.660478 0.530685 Q3 False False True silverman 0.0515751 0.6345 0.515583 Q3 False False True median 0.066173 0.702005 0.556274 Q3 False True False scott 0.0601095 0.696988 0.596866 Q3 False True False silverman 0.0524045 0.66827 0.577468 Q3 False True False median 0.0728568 0.739607 0.620757 Q3 False False False scott 0.0601095 0.696988 0.596866 Q3 False False False silverman 0.0524045 0.66827 0.577468 Q3 False False False median 0.0728568 0.739607 0.620757 Q1-Q4 \u00b6 So now, let's look at all questions for the 2D data distribution Code Block standardize = [ True , False ] methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] separate_dimensions = [ True , False ] results_df = pd . DataFrame () for istandard in standardize : X_ , Y_ = standardize_data ( X , Y , istandard ) for iscaler in separate_scales : for idim in separate_dimensions : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X_ , Y_ , method = imethod , per_dimension = idim , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"Separate Dimensions\" : [ idim ], \"Separate Length Scales\" : [ iscaler ], 'Param Estimator' : [ imethod ], \"HSIC\" : [ get_hsic ( X_ , Y_ , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"KA\" : [ get_hsic ( X_ , Y_ , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"CKA\" : [ get_hsic ( X_ , Y_ , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q4' ]),) print ( results_df . to_markdown ()) standardize Separate Dimensions Separate Length Scales Param Estimator HSIC KA CKA Q4 True True True scott 0.0601095 0.696988 0.596866 Q4 True True True silverman 0.0524045 0.66827 0.577468 Q4 True True True median 0.0729923 0.74078 0.623443 Q4 True False True scott 0.0601095 0.696988 0.596866 Q4 True False True silverman 0.0524045 0.66827 0.577468 Q4 True False True median 0.0729923 0.74078 0.623443 Q4 True True False scott 0.0601095 0.696988 0.596866 Q4 True True False silverman 0.0524045 0.66827 0.577468 Q4 True True False median 0.0728568 0.739607 0.620757 Q4 True False False scott 0.0601095 0.696988 0.596866 Q4 True False False silverman 0.0524045 0.66827 0.577468 Q4 True False False median 0.0728568 0.739607 0.620757 Q4 False True True scott 0.0601095 0.696988 0.596866 Q4 False True True silverman 0.0524045 0.66827 0.577468 Q4 False True True median 0.0729923 0.74078 0.623443 Q4 False False True scott 0.0601095 0.696988 0.596866 Q4 False False True silverman 0.0524045 0.66827 0.577468 Q4 False False True median 0.0729923 0.74078 0.623443 Q4 False True False scott 0.0601095 0.696988 0.596866 Q4 False True False silverman 0.0524045 0.66827 0.577468 Q4 False True False median 0.0728568 0.739607 0.620757 Q4 False False False scott 0.0601095 0.696988 0.596866 Q4 False False False silverman 0.0524045 0.66827 0.577468 Q4 False False False median 0.0728568 0.739607 0.620757 Verdict \u00b6 For the distributions, it seemed to be a little more consistent but with higher dimensions and more samples, these estimators start to fail. But then, we still don't have good alternative estimators. Todo Show a plot of the different parameters and how much they vary. No need to see the actual origins. Just need to highlight the variance in the estimates. What Now? \u00b6 I will be looking at the following: Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No","title":"Motivation"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#motivation-for-this-study","text":"In this document, I will be looking at the motivation behind this study and why we would like to pursue this further. Code Blocks Imports import sys , os import warnings import tqdm import random import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt # Insert path to model directory,. cwd = os . getcwd () path = f \" { cwd } /../../src\" sys . path . insert ( 0 , path ) # Insert path to package,. pysim_path = f \"/home/emmanuel/code/pysim/\" sys . path . insert ( 0 , pysim_path ) # toy datasets from data.toy import generate_dependence_data from data.distribution import DataParams from dataclasses import dataclass # Kernel Dependency measure from sklearn.preprocessing import StandardScaler from sklearn.gaussian_process.kernels import RBF from models.dependence import HSICModel from pysim.kernel.utils import get_init_gammas , get_gamma_grid , estimate_sigma import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-talk' ]) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings FIG_PATH = \"/home/emmanuel/projects/2019_hsic_align/results/figures/1d_dataset/demo/\" % load_ext autoreload % autoreload 2 Useful Functions def standardize_data ( X , Y , standardize : bool = False ): X = StandardScaler () . fit_transform ( X ) Y = StandardScaler () . fit_transform ( Y ) return X , Y def get_sigma ( X , Y , method : str = 'silverman' , per_dimension : bool = False , separate_scales : bool = False ): # sigma parameters subsample = None percent = 0.20 random_state = 123 sigma_X = estimate_sigma ( X , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) sigma_Y = estimate_sigma ( Y , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) if separate_scales : sigma_X = np . mean ([ sigma_X , sigma_Y ]) sigma_Y = np . mean ([ sigma_X , sigma_Y ]) return sigma_X , sigma_Y def get_hsic ( X , Y , scorer : str , sigma_X = None , sigma_Y = None ): # init hsic model class hsic_model = HSICModel () # hsic model params if sigma_X is not None : hsic_model . kernel_X = RBF ( sigma_X ) hsic_model . kernel_Y = RBF ( sigma_Y ) # get hsic score hsic_val = hsic_model . get_score ( X , Y , scorer ) return hsic_val","title":"Motivation for this Study"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#1d-example","text":"Code Block # data params dataset = 'sine' num_points = 1000 seed = 123 noise = 0.1 # get dataset X , Y = generate_dependence_data ( dataset = dataset , num_points = num_points , seed = seed , noise_x = noise , noise_y = noise ) # plot fig , ax = plt . subplots () ax . scatter ( X [: 100 ,:], Y [: 100 ,:]) plt . tight_layout () fig . savefig ( FIG_PATH + f \"demo_ { dataset } .png\" ) plt . show () Fig I : An example 1D Sine Curve. Let's take a simple 1D distribution: a sine curve. It is clear that there is a nonlinear relationship between them that cannot be captured (well) by linear methods. We are interested in looking at the dependence between X X and Y Y . We have the HSIC family of methods: HSIC, kernel alignment and centered kernel alignment. They are all very similar but there are some subtle differences. We will highlight them as we go through the overview. Let's take a generic approach and use the default HSIC, KA and CKA methods to try and estimate the dependence between X,Y X,Y . If we run the algorithm, we get the following results.","title":"1D Example"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#question-i-which-algorithm","text":"Code Block results_df = pd . DataFrame () method = 'scott' per_dimension = False separate_scales = False # sigma_X, sigma_y = get_sigma( # X, Y, # method=method, # per_dimension=per_dimension, # separate_scales=separate_scales # ) method = 'default' sigma_X , sigma_Y = None , None scorer = 'hsic' results_df = results_df . append ( pd . DataFrame ({ \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q1' ]),) print ( results_df . to_markdown ()) hsic ka cka Q1 0.0582356 0.688475 0.588434 Notice how all of the values are slightly difference. This is because of the composition of the methods. We can highlight the differences with a simple table. Method Centered Kernel Normalized HSIC Yes No Kernel Alignment No Yes Centered Kernel Alignment Yes No So each method has a slightly different formulation but they are mostly the same. So now the next question is: how do we estimate the parameters of the kernel used? Well the default is simply \\sigma=1.0 \\sigma=1.0 but we know that this won't do as the kernel depends on the parameters of the kernel. In this case we are using the most commonly used kernel: the Radial Basis Function (RBF). Since this is a 1D example, I will use some generic estimators called the \"Silverman Rule\" and \"Scott Rule\". These are very commonly found in packages like scipy.stats.gaussian_kde or statsmodels.nonparametric.bandwidth . They are mostly used for the Kernel Density Estimation (KDE) where we need a decent parameter to approximate the kernel to get a decent density estimate. So what happens with the methods and the results?","title":"Question I - Which Algorithm?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#question-ii-which-parameter-estimator","text":"Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = True results_df = pd . DataFrame () for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = per_dimension , separate_scales = separate_scales ) results_df = results_df . append ( pd . DataFrame ({ # \"sigma_x\": [sigma_X], # \"sigma_y\": [sigma_Y], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q2' ]),) print ( results_df . to_markdown ()) Estimator hsic ka cka Q2 scott 0.0575482 0.660478 0.530685 Q2 silverman 0.0515751 0.6345 0.515583 Q2 median 0.066173 0.702005 0.556274","title":"Question II - Which Parameter Estimator?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#question-iii-how-do-we-estimate-the-length-scale","text":"Use the same length scale? Use different length scales? Use a length scale per dimension (D>1) Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] results_df = pd . DataFrame () for iscaler in separate_scales : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = per_dimension , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ # \"sigma_x\": [sigma_X], \"separate\" : [ iscaler ], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q3' ]),) print ( results_df . to_markdown ()) separate Estimator hsic ka cka Q3 True scott 0.0575482 0.660478 0.530685 Q3 True silverman 0.0515751 0.6345 0.515583 Q3 True median 0.066173 0.702005 0.556274 Q3 False scott 0.0601095 0.696988 0.596866 Q3 False silverman 0.0524045 0.66827 0.577468 Q3 False median 0.0728568 0.739607 0.620757","title":"Question III - How do we estimate the length scale?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#question-iv-standardize-data","text":"We could also standardize our data... This could actually change the size of each of the features which could eliminate the need to apply separate length scales. Code Block standardize = [ True , False ] methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] results_df = pd . DataFrame () for istandard in standardize : X_ , Y_ = standardize_data ( X , Y , istandard ) for iscaler in separate_scales : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X_ , Y_ , method = imethod , per_dimension = per_dimension , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"separate\" : [ iscaler ], 'Estimator' : [ imethod ], \"hsic\" : [ get_hsic ( X_ , Y_ , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"ka\" : [ get_hsic ( X_ , Y_ , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"cka\" : [ get_hsic ( X_ , Y_ , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q4' ]),) print ( results_df . to_markdown ()) standardize separate Estimator hsic ka cka Q4 True True scott 0.0601095 0.696988 0.596866 Q4 True True silverman 0.0524045 0.66827 0.577468 Q4 True True median 0.0729923 0.74078 0.623443 Q4 True False scott 0.0601095 0.696988 0.596866 Q4 True False silverman 0.0524045 0.66827 0.577468 Q4 True False median 0.0728568 0.739607 0.620757 Q4 False True scott 0.0601095 0.696988 0.596866 Q4 False True silverman 0.0524045 0.66827 0.577468 Q4 False True median 0.0729923 0.74078 0.623443 Q4 False False scott 0.0601095 0.696988 0.596866 Q4 False False silverman 0.0524045 0.66827 0.577468 Q4 False False median 0.0728568 0.739607 0.620757 Now we see that the values you get are quite different for all methods. What happens if we use different sigmas? Todo Show a plot of the different parameters and how much they vary. No need to see the actual origins. Just need to highlight the variance in the estimates.","title":"Question IV - Standardize Data?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#verdict","text":"Well, hard to say as it depends on the parameters. Every researcher I've met who dealt with kernel methods seems to have a suggestion that they swear by but I never know who to follow. My thoughts is that we should use dedicated sigma values per dataset however, that still leaves us with other methods that we may want to try. So we're going to repeat the same experiment but with a 2D dataset and we will see that the difficult will increase again.","title":"Verdict"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#2d-example","text":"For this experiment, we're going to take two 2D datasets each generated from a T-Student distribution. We will apply the same sequence as we did above and we will end the section by adding another option for picking the parameters. Code Block # initialize Data Params class dataset = 'tstudent' samples = 1_000 dimensions = 2 std = 5 nu = 8 trial = 1 standardize = False # initialize params example_params = DataParams ( dataset = dataset , samples = samples , dimensions = dimensions , std = std , nu = nu , trial = trial , standardize = standardize ) # generate some parameters inputs = example_params . generate_data () sns . jointplot ( x = inputs . X , y = inputs . Y ) plt . tight_layout () plt . savefig ( FIG_PATH + f \"demo_ { dataset } .png\" ) Fig II : An example 2D T-Student distribution.","title":"2D Example"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#question-iii-revisited-different-length-scales","text":"Now we can revisit this question because we actually could estimate a different length scale depending upon the dimensionality. One problem with scott or Silverman's method is that it takes into account the entire dataset instead of having one estimate per feature. Code Block methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] separate_dimensions = [ True , False ] results_df = pd . DataFrame () for iscaler in separate_scales : for idim in separate_dimensions : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X , Y , method = imethod , per_dimension = idim , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"Separate Dimensions\" : [ idim ], \"Separate Length Scales\" : [ iscaler ], 'Param Estimator' : [ imethod ], \"HSIC\" : [ get_hsic ( X , Y , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"KA\" : [ get_hsic ( X , Y , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"CKA\" : [ get_hsic ( X , Y , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q3' ]),) print ( results_df . to_markdown ()) standardize Separate Dimensions Separate Length Scales Param Estimator HSIC KA CKA Q3 False True True scott 0.0575482 0.660478 0.530685 Q3 False True True silverman 0.0515751 0.6345 0.515583 Q3 False True True median 0.066173 0.702005 0.556274 Q3 False False True scott 0.0575482 0.660478 0.530685 Q3 False False True silverman 0.0515751 0.6345 0.515583 Q3 False False True median 0.066173 0.702005 0.556274 Q3 False True False scott 0.0601095 0.696988 0.596866 Q3 False True False silverman 0.0524045 0.66827 0.577468 Q3 False True False median 0.0728568 0.739607 0.620757 Q3 False False False scott 0.0601095 0.696988 0.596866 Q3 False False False silverman 0.0524045 0.66827 0.577468 Q3 False False False median 0.0728568 0.739607 0.620757","title":"Question III (Revisited) - Different Length Scales?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#q1-q4","text":"So now, let's look at all questions for the 2D data distribution Code Block standardize = [ True , False ] methods = [ 'scott' , 'silverman' , 'median' ] per_dimension = False separate_scales = [ True , False ] separate_dimensions = [ True , False ] results_df = pd . DataFrame () for istandard in standardize : X_ , Y_ = standardize_data ( X , Y , istandard ) for iscaler in separate_scales : for idim in separate_dimensions : for imethod in methods : sigma_X , sigma_Y = get_sigma ( X_ , Y_ , method = imethod , per_dimension = idim , separate_scales = iscaler ) results_df = results_df . append ( pd . DataFrame ({ \"standardize\" : [ istandard ], \"Separate Dimensions\" : [ idim ], \"Separate Length Scales\" : [ iscaler ], 'Param Estimator' : [ imethod ], \"HSIC\" : [ get_hsic ( X_ , Y_ , 'hsic' , sigma_X , sigma_Y )], # Estimate HSIC \"KA\" : [ get_hsic ( X_ , Y_ , 'ka' , sigma_X , sigma_Y )], # Estimate KA \"CKA\" : [ get_hsic ( X_ , Y_ , 'cka' , sigma_X , sigma_Y )], # Estimate CKA }, index = [ 'Q4' ]),) print ( results_df . to_markdown ()) standardize Separate Dimensions Separate Length Scales Param Estimator HSIC KA CKA Q4 True True True scott 0.0601095 0.696988 0.596866 Q4 True True True silverman 0.0524045 0.66827 0.577468 Q4 True True True median 0.0729923 0.74078 0.623443 Q4 True False True scott 0.0601095 0.696988 0.596866 Q4 True False True silverman 0.0524045 0.66827 0.577468 Q4 True False True median 0.0729923 0.74078 0.623443 Q4 True True False scott 0.0601095 0.696988 0.596866 Q4 True True False silverman 0.0524045 0.66827 0.577468 Q4 True True False median 0.0728568 0.739607 0.620757 Q4 True False False scott 0.0601095 0.696988 0.596866 Q4 True False False silverman 0.0524045 0.66827 0.577468 Q4 True False False median 0.0728568 0.739607 0.620757 Q4 False True True scott 0.0601095 0.696988 0.596866 Q4 False True True silverman 0.0524045 0.66827 0.577468 Q4 False True True median 0.0729923 0.74078 0.623443 Q4 False False True scott 0.0601095 0.696988 0.596866 Q4 False False True silverman 0.0524045 0.66827 0.577468 Q4 False False True median 0.0729923 0.74078 0.623443 Q4 False True False scott 0.0601095 0.696988 0.596866 Q4 False True False silverman 0.0524045 0.66827 0.577468 Q4 False True False median 0.0728568 0.739607 0.620757 Q4 False False False scott 0.0601095 0.696988 0.596866 Q4 False False False silverman 0.0524045 0.66827 0.577468 Q4 False False False median 0.0728568 0.739607 0.620757","title":"Q1-Q4"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#verdict_1","text":"For the distributions, it seemed to be a little more consistent but with higher dimensions and more samples, these estimators start to fail. But then, we still don't have good alternative estimators. Todo Show a plot of the different parameters and how much they vary. No need to see the actual origins. Just need to highlight the variance in the estimates.","title":"Verdict"},{"location":"projects/similarity/kernel_alignment_params/notebooks/1.0_motivation/#what-now","text":"I will be looking at the following: Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No","title":"What Now?"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/","text":"Parameter Space - 1D \u00b6 I've broken down the components so that it's easier to pass appropriate parameters. The main functions as as follows: Standardize Data yes or no Get Sigma Handles the ways to estimate the parameter per dimension per dataset (x and/or y) estimator (median, mean, silverman, scott, etc) Get HSIC scorer Handles the HSIC method (hsic, ka, cka) Code Blocks Imports import sys , os # Insert path to model directory,. cwd = os . getcwd () path = f \" { cwd } /../../src\" sys . path . insert ( 0 , path ) # Insert path to package,. pysim_path = f \"/home/emmanuel/code/pysim/\" sys . path . insert ( 0 , pysim_path ) import warnings from tqdm import tqdm import random import pandas as pd import numpy as np import argparse from sklearn.utils import check_random_state # toy datasets from data.toy import generate_dependence_data from data.distribution import DataParams from features.utils import dict_product # Kernel Dependency measure from sklearn.gaussian_process.kernels import RBF from pysim.kernel.hsic import HSIC from pysim.kernel.utils import estimate_sigma , get_sigma_grid #GammaParam, SigmaParam # RBIG IT measures from models.dependence import HSICModel # Plotting from visualization.distribution import plot_scorer # experiment helpers from tqdm import tqdm from experiments.utils import dict_product , run_parallel_step # Plotting Procedures import matplotlib import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-paper' ]) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 Standardize Data from typing import Optional , Tuple def standardize_data ( X : np . ndarray , Y : np . ndarray , standardize : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: X = StandardScaler () . fit_transform ( X ) Y = StandardScaler () . fit_transform ( Y ) return X , Y Estimate Sigma def get_sigma ( X : np . ndarray , Y : np . ndarray , method : str = 'silverman' , percent : Optional [ float ] = None , per_dimension : bool = False , separate_scales : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: # sigma parameters subsample = None random_state = 123 sigma_X = estimate_sigma ( X , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) sigma_Y = estimate_sigma ( Y , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) if separate_scales : sigma_X = np . mean ([ sigma_X , sigma_Y ]) sigma_Y = np . mean ([ sigma_X , sigma_Y ]) return sigma_X , sigma_Y HSIC def get_hsic ( X : np . ndarray , Y : np . ndarray , scorer : str , sigma_X : Optional [ float ] = None , sigma_Y : Optional [ float ] = None ) -> float : # init hsic model class hsic_model = HSICModel () # hsic model params if sigma_X is not None : hsic_model . kernel_X = RBF ( sigma_X ) hsic_model . kernel_Y = RBF ( sigma_Y ) # get hsic score hsic_val = hsic_model . get_score ( X , Y , scorer ) return hsic_val def plot_toy_data ( X , Y , subsample : Optional [ int ] = None ): # plot fig , ax = plt . subplots () ax . scatter ( X [: subsample ,:], Y [: subsample ,:]) return fig , ax Datasets \u00b6 For this experiment, we will be looking at 4 simple 1D datasets: Line Sine Circle Random Code Block datasets = [ 'line' , 'sine' , 'circle' , 'random' ] num_points = 1_000 seed = 123 noise = 0.1 for idataset in datasets : # get dataset X , Y = generate_dependence_data ( dataset = idataset , num_points = num_points , seed = seed , noise_x = noise , noise_y = noise ) fig , ax = plot_toy_data ( X , Y , 100 ) ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) plt . tight_layout () fig . savefig ( FIG_PATH + f \"demo_ { idataset } .png\" ) plt . show () Research Questions \u00b6 Which Algorithm? Which Parameter Estimator? Standardize or Not? Part I - The Sigma Space \u00b6 Experiment \u00b6 For this first part, we want to look a the entire \\sigma \\sigma space for the RBF kernel. We will vary the \\sigma \\sigma parameter and use 20 grid poinnts for both X X and Y Y . Since we're dealing with 1D data, we will not have to worry about per dimension estimates. Free Parameters Dataset (sine, line, circle, random) Scorer (hsic, cka, ka) Sigma X,Y (grid space) We fix all other parameters as they are not necessary for this first step. Code Blocks Sigma Grid def sigma_grid ( sigma_X , factor = 2 , n_grid_points = 20 ): return np . logspace ( np . log10 ( sigma_X * 10 ** ( - factor )), np . log10 ( sigma_X * 10 ** ( factor )), n_grid_points ) Experimental Params This made a list of dictionary values with every possible combination of the parameters we listed. Now if we call the first element of this list, we can pass these parameters into our HSIC function to calculate the score. This allows us to do the calculations in parallel instead of looping through every single combination. # experimental parameters n_grid_points = 40 # sigma_grid = np.logspace(-3, 3, n_grid_points) # initialize sigma (use the median) sigma_X , sigma_Y = get_sigma ( X , Y , method = 'mean' ) print ( sigma_X , sigma_Y ) # create a grid sigma_X_grid = sigma_grid ( sigma_X , factor = 3 , n_grid_points = n_grid_points ) sigma_Y_grid = sigma_grid ( sigma_Y , factor = 3 , n_grid_points = n_grid_points ) # create a parameter grid parameters = { \"dataset\" : [ 'sine' , 'line' , 'random' , 'circle' ], \"scorer\" : [ 'hsic' , 'ka' , 'cka' ], \"sigma_X\" : np . copy ( sigma_X_grid ), \"sigma_Y\" : np . copy ( sigma_Y_grid ), } # Get a list of all parameters parameters = list ( dict_product ( parameters )) # check # of parameters n_params = len ( parameters ) print ( f \"Number of params: { n_params } \" ) print ( f \"First set of params: \\n { parameters [ 0 ] } \" ) 1 2 3 4 0.32917944341805494 0.3439255265652333 Number of params: 19200 First set of params: {'dataset': 'sine', 'scorer': 'hsic', 'sigma_X': 0.00032917944341805485, 'sigma_Y': 0.00034392552656523323} Experimental Step Now, we need to make an experimental step function. This function will be the HSIC function that called within the parallel loop. I want it to also return a pd.DataFrame with the columns holding the parameters. This will make things easier for us to keep track of things as well as plot our results. from typing import Dict def step ( params : Dict , X : np . ndarray , Y : np . ndarray ) -> pd . DataFrame : # get dataset X , Y = generate_dependence_data ( dataset = params [ 'dataset' ], num_points = 1_000 , seed = 123 , noise_x = 0.1 , noise_y = 0.1 ) # calculate the hsic value score = get_hsic ( X , Y , params [ 'scorer' ], params [ 'sigma_X' ], params [ 'sigma_Y' ]) # create a dataframe with the results and params results_df = pd . DataFrame ({ 'dataset' : [ params [ 'dataset' ]], 'scorer' : [ params [ 'scorer' ]], 'sigma_X' : [ params [ 'sigma_X' ]], 'sigma_Y' : [ params [ 'sigma_Y' ]], 'score' : score , },) return results_df Example Now we can loop through and calculate the hsic value for each of the \\sigma \\sigma -parameters that we have enlisted. And we will do it in parallel to save time. I'm on a server with 28 cores free so best believe I will be using all of them... # test the result res_test = step ( parameters [ 0 ], X , Y ) # quick test res_keys = [ 'dataset' , 'scorer' , 'sigma_X' , 'sigma_Y' , 'score' ] assert res_keys == res_test . columns . tolist () # print out results res_test . head () . to_markdown () dataset scorer sigma_X sigma_Y score 0 sine hsic 0.000329179 0.000343926 0.000997738 Experiment verbose = 1 n_jobs = - 1 results = run_parallel_step ( exp_step = step , parameters = parameters , n_jobs = n_jobs , verbose = verbose , X = X , Y = Y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [Parallel(n_jobs=-1)]: Using backend LokyBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 232 tasks | elapsed: 2.0s [Parallel(n_jobs=-1)]: Done 732 tasks | elapsed: 5.6s [Parallel(n_jobs=-1)]: Done 1432 tasks | elapsed: 10.5s [Parallel(n_jobs=-1)]: Done 2332 tasks | elapsed: 15.1s [Parallel(n_jobs=-1)]: Done 3432 tasks | elapsed: 20.7s [Parallel(n_jobs=-1)]: Done 4732 tasks | elapsed: 29.9s [Parallel(n_jobs=-1)]: Done 6232 tasks | elapsed: 40.6s [Parallel(n_jobs=-1)]: Done 7932 tasks | elapsed: 48.7s [Parallel(n_jobs=-1)]: Done 9832 tasks | elapsed: 1.0min [Parallel(n_jobs=-1)]: Done 11932 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 14232 tasks | elapsed: 1.5min [Parallel(n_jobs=-1)]: Done 16732 tasks | elapsed: 1.8min [Parallel(n_jobs=-1)]: Done 19145 out of 19200 | elapsed: 2.0min remaining: 0.3s [Parallel(n_jobs=-1)]: Done 19200 out of 19200 | elapsed: 2.0min finished # test (number of results = n parameters) assert n_params == len ( results ) results_df = pd . concat ( results , ignore_index = True ) results_df . head () . to_markdown () dataset scorer sigma_X sigma_Y score 0 sine hsic 0.000329179 0.000343926 0.000997738 1 sine hsic 0.000329179 0.000490129 0.000997927 2 sine hsic 0.000329179 0.000698484 0.000998668 3 sine hsic 0.000329179 0.000995412 0.000999668 4 sine hsic 0.000329179 0.00141856 0.00100054 Part II - Specific Methods \u00b6 In the above section, we showed the full parameter space. But what happens if we just look at specific ways to estimate the sigma? For th Free Parameters : Dataset (sine, line, circle, random) Scorer (hsic, cka, ka) Sigma Estimator (mean, median, silverman, scott) Experiment \u00b6 Code Block # initialize sigma (use the median) sigma_X , sigma_Y = get_sigma ( X , Y , method = 'mean' ) # create a parameter grid parameters = { \"dataset\" : [ 'sine' , 'line' , 'random' , 'circle' ], \"scorer\" : [ 'hsic' , 'ka' , 'cka' ], \"estimator\" : [ 'mean' , 'median' , 'mean' ] } # Get a list of all parameters parameters = list ( dict_product ( parameters )) # check # of parameters n_params = len ( parameters ) print ( f \"Number of params: { n_params } \" ) print ( f \"First set of params: \\n { parameters [ 0 ] } \" ) 1 2 3 Number of params: 36 First set of params: {'dataset': 'sine', 'scorer': 'hsic', 'estimator': 'mean'} from typing import Dict def step ( params : Dict , X : np . ndarray , Y : np . ndarray ) -> pd . DataFrame : # get dataset X , Y = generate_dependence_data ( dataset = params [ 'dataset' ], num_points = 1_000 , seed = 123 , noise_x = 0.1 , noise_y = 0.1 ) # estimate sigma sigma_X , sigma_Y = get_sigma ( X , Y , method = params [ 'estimator' ]) # calculate the hsic value score = get_hsic ( X , Y , params [ 'scorer' ], sigma_X , sigma_Y ) # create a dataframe with the results and params results_df = pd . DataFrame ({ 'dataset' : [ params [ 'dataset' ]], 'scorer' : [ params [ 'scorer' ]], 'estimator' : [ params [ 'estimator' ]], 'sigma_X' : [ sigma_X ], 'sigma_Y' : [ sigma_Y ], 'score' : score , },) return results_df # test the result res_test = step ( parameters [ 0 ], X , Y ) # quick test res_keys = [ 'dataset' , 'scorer' , 'estimator' , 'sigma_X' , 'sigma_Y' , 'score' ] assert res_keys == res_test . columns . tolist (), f 'Not true: { res_test . columns . tolist () } ' # print out results res_test . head () . to_markdown () dataset scorer estimator sigma_X sigma_Y score 0 sine hsic mean 0.329179 0.820625 0.0661783 verbose = 1 n_jobs = 1 results = run_parallel_step ( exp_step = step , parameters = parameters , n_jobs = n_jobs , verbose = verbose , X = X , Y = Y ) 1 2 [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 36 out of 36 | elapsed: 4.3s finished # test (number of results = n parameters) assert n_params == len ( results ) results_est_df = pd . concat ( results , ignore_index = True ) results_est_df . head () . to_markdown () dataset scorer estimator sigma_X sigma_Y score 0 sine hsic mean 0.329179 0.820625 0.0661783 1 sine hsic median 0.289021 0.72562 0.0770415 2 sine hsic mean 0.329179 0.820625 0.0661783 3 sine ka mean 0.329179 0.820625 0.91219 4 sine ka median 0.289021 0.72562 0.896967 Visualization - HeatMaps \u00b6 Code Block def plot_all_params ( grid_df : pd . DataFrame , params_df : Optional [ pd . DataFrame ] = None , scorer : str = 'hsc' , dataset : str = 'sine' , ): fig , ax = plt . subplots ( figsize = ( 6 , 5 )) # =========================================== # Plot Gridded DataFrame # =========================================== # subset hsic method grid_df_ = grid_df [ grid_df [ 'scorer' ] == scorer ] . drop ( 'scorer' , axis = 1 ) # subset dataset grid_df_ = grid_df_ [ grid_df_ [ 'dataset' ] == dataset ] . drop ( 'dataset' , axis = 1 ) # create a heatmap grid_df_ = pd . pivot_table ( grid_df_ , values = 'score' , index = [ 'sigma_Y' ], columns = 'sigma_X' ) # print(grid_df_) # min max if scorer == 'hsic' : vmax = 0.11 vmin = grid_df_ . values . min () else : vmax = 1.0 vmin = grid_df_ . values . min () # print(vmin) # heatmap_data.columns = np.sqrt(1 / 2 * heatmap_data.columns.values) # heatmap_data.index = np.sqrt(1 / 2 * heatmap_data.index.values) X , Y = np . meshgrid ( grid_df_ . index . values , grid_df_ . columns . values , ) pts = ax . pcolormesh ( X , Y , grid_df_ . values , #vmin=0, vmax=vmax, cmap = 'Reds' , vmin = 0 , vmax = vmax , # norm=colors.LogNorm(vmin=vmin, vmax=vmax) ) # colorbar fig . colorbar ( pts , ax = ax ) # ax = sns.heatmap( # data=grid_df_, # xticklabels=grid_df_.columns.values.round(decimals=2), # yticklabels=grid_df_.index.values.round(decimals=2), # vmin=0, vmax=vmax # ) # =========================================== # Plot Params # =========================================== if params_df is not None : params_df_ = params_df [ params_df [ 'dataset' ] == dataset ] # subset hsic method params_df_ = params_df_ [ params_df_ [ 'scorer' ] == scorer ] # plot X estimators = [ ( 'median' , 'black' ), ( 'mean' , 'green' ), ( 'silverman' , 'blue' ), ( 'scott' , 'red' ), ] for iest , icolor in estimators : # Plot X # print( # params_df_[params_df_['estimator'] == iest].sigma_X, # params_df_[params_df_['estimator'] == iest].sigma_Y # ) ax . scatter ( params_df_ [ params_df_ [ 'estimator' ] == iest ] . sigma_X , params_df_ [ params_df_ [ 'estimator' ] == iest ] . sigma_Y , s = 500 , c = icolor , label = f \" { iest . capitalize () } X\" , zorder = 3 , marker = '.' ) # Plot Y # # ax.scatter( # # , # # params_df[params_df['estimator'] == iest].score, # # s=300, c=icolor, label=f\"{iest.capitalize()} Y\", zorder=3, marker='.') ax . legend () return fig , ax Line Dataset \u00b6 Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'line' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment Sine Dataset \u00b6 Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'sine' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment 1 ![png](pics/param_grid/output_47_1.png) Centered Kernel Alignment Circle Dataset \u00b6 Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'circle' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment Random Dataset \u00b6 Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'random' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment","title":"1D Parameter Space"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#parameter-space-1d","text":"I've broken down the components so that it's easier to pass appropriate parameters. The main functions as as follows: Standardize Data yes or no Get Sigma Handles the ways to estimate the parameter per dimension per dataset (x and/or y) estimator (median, mean, silverman, scott, etc) Get HSIC scorer Handles the HSIC method (hsic, ka, cka) Code Blocks Imports import sys , os # Insert path to model directory,. cwd = os . getcwd () path = f \" { cwd } /../../src\" sys . path . insert ( 0 , path ) # Insert path to package,. pysim_path = f \"/home/emmanuel/code/pysim/\" sys . path . insert ( 0 , pysim_path ) import warnings from tqdm import tqdm import random import pandas as pd import numpy as np import argparse from sklearn.utils import check_random_state # toy datasets from data.toy import generate_dependence_data from data.distribution import DataParams from features.utils import dict_product # Kernel Dependency measure from sklearn.gaussian_process.kernels import RBF from pysim.kernel.hsic import HSIC from pysim.kernel.utils import estimate_sigma , get_sigma_grid #GammaParam, SigmaParam # RBIG IT measures from models.dependence import HSICModel # Plotting from visualization.distribution import plot_scorer # experiment helpers from tqdm import tqdm from experiments.utils import dict_product , run_parallel_step # Plotting Procedures import matplotlib import matplotlib.pyplot as plt import seaborn as sns plt . style . use ([ 'seaborn-paper' ]) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 Standardize Data from typing import Optional , Tuple def standardize_data ( X : np . ndarray , Y : np . ndarray , standardize : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: X = StandardScaler () . fit_transform ( X ) Y = StandardScaler () . fit_transform ( Y ) return X , Y Estimate Sigma def get_sigma ( X : np . ndarray , Y : np . ndarray , method : str = 'silverman' , percent : Optional [ float ] = None , per_dimension : bool = False , separate_scales : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: # sigma parameters subsample = None random_state = 123 sigma_X = estimate_sigma ( X , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) sigma_Y = estimate_sigma ( Y , subsample = subsample , method = method , percent = percent , random_state = random_state , per_dimension = per_dimension ) if separate_scales : sigma_X = np . mean ([ sigma_X , sigma_Y ]) sigma_Y = np . mean ([ sigma_X , sigma_Y ]) return sigma_X , sigma_Y HSIC def get_hsic ( X : np . ndarray , Y : np . ndarray , scorer : str , sigma_X : Optional [ float ] = None , sigma_Y : Optional [ float ] = None ) -> float : # init hsic model class hsic_model = HSICModel () # hsic model params if sigma_X is not None : hsic_model . kernel_X = RBF ( sigma_X ) hsic_model . kernel_Y = RBF ( sigma_Y ) # get hsic score hsic_val = hsic_model . get_score ( X , Y , scorer ) return hsic_val def plot_toy_data ( X , Y , subsample : Optional [ int ] = None ): # plot fig , ax = plt . subplots () ax . scatter ( X [: subsample ,:], Y [: subsample ,:]) return fig , ax","title":"Parameter Space - 1D"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#datasets","text":"For this experiment, we will be looking at 4 simple 1D datasets: Line Sine Circle Random Code Block datasets = [ 'line' , 'sine' , 'circle' , 'random' ] num_points = 1_000 seed = 123 noise = 0.1 for idataset in datasets : # get dataset X , Y = generate_dependence_data ( dataset = idataset , num_points = num_points , seed = seed , noise_x = noise , noise_y = noise ) fig , ax = plot_toy_data ( X , Y , 100 ) ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) plt . tight_layout () fig . savefig ( FIG_PATH + f \"demo_ { idataset } .png\" ) plt . show ()","title":"Datasets"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#research-questions","text":"Which Algorithm? Which Parameter Estimator? Standardize or Not?","title":"Research Questions"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#part-i-the-sigma-space","text":"","title":"Part I - The Sigma Space"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#experiment","text":"For this first part, we want to look a the entire \\sigma \\sigma space for the RBF kernel. We will vary the \\sigma \\sigma parameter and use 20 grid poinnts for both X X and Y Y . Since we're dealing with 1D data, we will not have to worry about per dimension estimates. Free Parameters Dataset (sine, line, circle, random) Scorer (hsic, cka, ka) Sigma X,Y (grid space) We fix all other parameters as they are not necessary for this first step. Code Blocks Sigma Grid def sigma_grid ( sigma_X , factor = 2 , n_grid_points = 20 ): return np . logspace ( np . log10 ( sigma_X * 10 ** ( - factor )), np . log10 ( sigma_X * 10 ** ( factor )), n_grid_points ) Experimental Params This made a list of dictionary values with every possible combination of the parameters we listed. Now if we call the first element of this list, we can pass these parameters into our HSIC function to calculate the score. This allows us to do the calculations in parallel instead of looping through every single combination. # experimental parameters n_grid_points = 40 # sigma_grid = np.logspace(-3, 3, n_grid_points) # initialize sigma (use the median) sigma_X , sigma_Y = get_sigma ( X , Y , method = 'mean' ) print ( sigma_X , sigma_Y ) # create a grid sigma_X_grid = sigma_grid ( sigma_X , factor = 3 , n_grid_points = n_grid_points ) sigma_Y_grid = sigma_grid ( sigma_Y , factor = 3 , n_grid_points = n_grid_points ) # create a parameter grid parameters = { \"dataset\" : [ 'sine' , 'line' , 'random' , 'circle' ], \"scorer\" : [ 'hsic' , 'ka' , 'cka' ], \"sigma_X\" : np . copy ( sigma_X_grid ), \"sigma_Y\" : np . copy ( sigma_Y_grid ), } # Get a list of all parameters parameters = list ( dict_product ( parameters )) # check # of parameters n_params = len ( parameters ) print ( f \"Number of params: { n_params } \" ) print ( f \"First set of params: \\n { parameters [ 0 ] } \" ) 1 2 3 4 0.32917944341805494 0.3439255265652333 Number of params: 19200 First set of params: {'dataset': 'sine', 'scorer': 'hsic', 'sigma_X': 0.00032917944341805485, 'sigma_Y': 0.00034392552656523323} Experimental Step Now, we need to make an experimental step function. This function will be the HSIC function that called within the parallel loop. I want it to also return a pd.DataFrame with the columns holding the parameters. This will make things easier for us to keep track of things as well as plot our results. from typing import Dict def step ( params : Dict , X : np . ndarray , Y : np . ndarray ) -> pd . DataFrame : # get dataset X , Y = generate_dependence_data ( dataset = params [ 'dataset' ], num_points = 1_000 , seed = 123 , noise_x = 0.1 , noise_y = 0.1 ) # calculate the hsic value score = get_hsic ( X , Y , params [ 'scorer' ], params [ 'sigma_X' ], params [ 'sigma_Y' ]) # create a dataframe with the results and params results_df = pd . DataFrame ({ 'dataset' : [ params [ 'dataset' ]], 'scorer' : [ params [ 'scorer' ]], 'sigma_X' : [ params [ 'sigma_X' ]], 'sigma_Y' : [ params [ 'sigma_Y' ]], 'score' : score , },) return results_df Example Now we can loop through and calculate the hsic value for each of the \\sigma \\sigma -parameters that we have enlisted. And we will do it in parallel to save time. I'm on a server with 28 cores free so best believe I will be using all of them... # test the result res_test = step ( parameters [ 0 ], X , Y ) # quick test res_keys = [ 'dataset' , 'scorer' , 'sigma_X' , 'sigma_Y' , 'score' ] assert res_keys == res_test . columns . tolist () # print out results res_test . head () . to_markdown () dataset scorer sigma_X sigma_Y score 0 sine hsic 0.000329179 0.000343926 0.000997738 Experiment verbose = 1 n_jobs = - 1 results = run_parallel_step ( exp_step = step , parameters = parameters , n_jobs = n_jobs , verbose = verbose , X = X , Y = Y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [Parallel(n_jobs=-1)]: Using backend LokyBackend with 28 concurrent workers. [Parallel(n_jobs=-1)]: Done 232 tasks | elapsed: 2.0s [Parallel(n_jobs=-1)]: Done 732 tasks | elapsed: 5.6s [Parallel(n_jobs=-1)]: Done 1432 tasks | elapsed: 10.5s [Parallel(n_jobs=-1)]: Done 2332 tasks | elapsed: 15.1s [Parallel(n_jobs=-1)]: Done 3432 tasks | elapsed: 20.7s [Parallel(n_jobs=-1)]: Done 4732 tasks | elapsed: 29.9s [Parallel(n_jobs=-1)]: Done 6232 tasks | elapsed: 40.6s [Parallel(n_jobs=-1)]: Done 7932 tasks | elapsed: 48.7s [Parallel(n_jobs=-1)]: Done 9832 tasks | elapsed: 1.0min [Parallel(n_jobs=-1)]: Done 11932 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 14232 tasks | elapsed: 1.5min [Parallel(n_jobs=-1)]: Done 16732 tasks | elapsed: 1.8min [Parallel(n_jobs=-1)]: Done 19145 out of 19200 | elapsed: 2.0min remaining: 0.3s [Parallel(n_jobs=-1)]: Done 19200 out of 19200 | elapsed: 2.0min finished # test (number of results = n parameters) assert n_params == len ( results ) results_df = pd . concat ( results , ignore_index = True ) results_df . head () . to_markdown () dataset scorer sigma_X sigma_Y score 0 sine hsic 0.000329179 0.000343926 0.000997738 1 sine hsic 0.000329179 0.000490129 0.000997927 2 sine hsic 0.000329179 0.000698484 0.000998668 3 sine hsic 0.000329179 0.000995412 0.000999668 4 sine hsic 0.000329179 0.00141856 0.00100054","title":"Experiment"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#part-ii-specific-methods","text":"In the above section, we showed the full parameter space. But what happens if we just look at specific ways to estimate the sigma? For th Free Parameters : Dataset (sine, line, circle, random) Scorer (hsic, cka, ka) Sigma Estimator (mean, median, silverman, scott)","title":"Part II - Specific Methods"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#experiment_1","text":"Code Block # initialize sigma (use the median) sigma_X , sigma_Y = get_sigma ( X , Y , method = 'mean' ) # create a parameter grid parameters = { \"dataset\" : [ 'sine' , 'line' , 'random' , 'circle' ], \"scorer\" : [ 'hsic' , 'ka' , 'cka' ], \"estimator\" : [ 'mean' , 'median' , 'mean' ] } # Get a list of all parameters parameters = list ( dict_product ( parameters )) # check # of parameters n_params = len ( parameters ) print ( f \"Number of params: { n_params } \" ) print ( f \"First set of params: \\n { parameters [ 0 ] } \" ) 1 2 3 Number of params: 36 First set of params: {'dataset': 'sine', 'scorer': 'hsic', 'estimator': 'mean'} from typing import Dict def step ( params : Dict , X : np . ndarray , Y : np . ndarray ) -> pd . DataFrame : # get dataset X , Y = generate_dependence_data ( dataset = params [ 'dataset' ], num_points = 1_000 , seed = 123 , noise_x = 0.1 , noise_y = 0.1 ) # estimate sigma sigma_X , sigma_Y = get_sigma ( X , Y , method = params [ 'estimator' ]) # calculate the hsic value score = get_hsic ( X , Y , params [ 'scorer' ], sigma_X , sigma_Y ) # create a dataframe with the results and params results_df = pd . DataFrame ({ 'dataset' : [ params [ 'dataset' ]], 'scorer' : [ params [ 'scorer' ]], 'estimator' : [ params [ 'estimator' ]], 'sigma_X' : [ sigma_X ], 'sigma_Y' : [ sigma_Y ], 'score' : score , },) return results_df # test the result res_test = step ( parameters [ 0 ], X , Y ) # quick test res_keys = [ 'dataset' , 'scorer' , 'estimator' , 'sigma_X' , 'sigma_Y' , 'score' ] assert res_keys == res_test . columns . tolist (), f 'Not true: { res_test . columns . tolist () } ' # print out results res_test . head () . to_markdown () dataset scorer estimator sigma_X sigma_Y score 0 sine hsic mean 0.329179 0.820625 0.0661783 verbose = 1 n_jobs = 1 results = run_parallel_step ( exp_step = step , parameters = parameters , n_jobs = n_jobs , verbose = verbose , X = X , Y = Y ) 1 2 [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 36 out of 36 | elapsed: 4.3s finished # test (number of results = n parameters) assert n_params == len ( results ) results_est_df = pd . concat ( results , ignore_index = True ) results_est_df . head () . to_markdown () dataset scorer estimator sigma_X sigma_Y score 0 sine hsic mean 0.329179 0.820625 0.0661783 1 sine hsic median 0.289021 0.72562 0.0770415 2 sine hsic mean 0.329179 0.820625 0.0661783 3 sine ka mean 0.329179 0.820625 0.91219 4 sine ka median 0.289021 0.72562 0.896967","title":"Experiment"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#visualization-heatmaps","text":"Code Block def plot_all_params ( grid_df : pd . DataFrame , params_df : Optional [ pd . DataFrame ] = None , scorer : str = 'hsc' , dataset : str = 'sine' , ): fig , ax = plt . subplots ( figsize = ( 6 , 5 )) # =========================================== # Plot Gridded DataFrame # =========================================== # subset hsic method grid_df_ = grid_df [ grid_df [ 'scorer' ] == scorer ] . drop ( 'scorer' , axis = 1 ) # subset dataset grid_df_ = grid_df_ [ grid_df_ [ 'dataset' ] == dataset ] . drop ( 'dataset' , axis = 1 ) # create a heatmap grid_df_ = pd . pivot_table ( grid_df_ , values = 'score' , index = [ 'sigma_Y' ], columns = 'sigma_X' ) # print(grid_df_) # min max if scorer == 'hsic' : vmax = 0.11 vmin = grid_df_ . values . min () else : vmax = 1.0 vmin = grid_df_ . values . min () # print(vmin) # heatmap_data.columns = np.sqrt(1 / 2 * heatmap_data.columns.values) # heatmap_data.index = np.sqrt(1 / 2 * heatmap_data.index.values) X , Y = np . meshgrid ( grid_df_ . index . values , grid_df_ . columns . values , ) pts = ax . pcolormesh ( X , Y , grid_df_ . values , #vmin=0, vmax=vmax, cmap = 'Reds' , vmin = 0 , vmax = vmax , # norm=colors.LogNorm(vmin=vmin, vmax=vmax) ) # colorbar fig . colorbar ( pts , ax = ax ) # ax = sns.heatmap( # data=grid_df_, # xticklabels=grid_df_.columns.values.round(decimals=2), # yticklabels=grid_df_.index.values.round(decimals=2), # vmin=0, vmax=vmax # ) # =========================================== # Plot Params # =========================================== if params_df is not None : params_df_ = params_df [ params_df [ 'dataset' ] == dataset ] # subset hsic method params_df_ = params_df_ [ params_df_ [ 'scorer' ] == scorer ] # plot X estimators = [ ( 'median' , 'black' ), ( 'mean' , 'green' ), ( 'silverman' , 'blue' ), ( 'scott' , 'red' ), ] for iest , icolor in estimators : # Plot X # print( # params_df_[params_df_['estimator'] == iest].sigma_X, # params_df_[params_df_['estimator'] == iest].sigma_Y # ) ax . scatter ( params_df_ [ params_df_ [ 'estimator' ] == iest ] . sigma_X , params_df_ [ params_df_ [ 'estimator' ] == iest ] . sigma_Y , s = 500 , c = icolor , label = f \" { iest . capitalize () } X\" , zorder = 3 , marker = '.' ) # Plot Y # # ax.scatter( # # , # # params_df[params_df['estimator'] == iest].score, # # s=300, c=icolor, label=f\"{iest.capitalize()} Y\", zorder=3, marker='.') ax . legend () return fig , ax","title":"Visualization - HeatMaps"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#line-dataset","text":"Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'line' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment","title":"Line Dataset"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#sine-dataset","text":"Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'sine' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment 1 ![png](pics/param_grid/output_47_1.png) Centered Kernel Alignment","title":"Sine Dataset"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#circle-dataset","text":"Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'circle' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment","title":"Circle Dataset"},{"location":"projects/similarity/kernel_alignment_params/notebooks/2.0_preliminary_exp/#random-dataset","text":"Code Block scorers = results_df [ 'scorer' ] . unique () . tolist () idataset = 'random' for iscorer in scorers : fig , ax = plot_all_params ( results_df , results_est_df , scorer = iscorer , dataset = idataset ) # ax.legend(ncol=1, fontsize=15) plt . xscale ( 'log' , basex = 10 ) plt . yscale ( 'log' , basey = 10 ) ax . set_xlabel ( r '$\\sigma_X$' , fontsize = 20 ) ax . set_ylabel ( r '$\\sigma_Y$' , fontsize = 20 ) ax . set_title ( f ' { iscorer . upper () } Score, { idataset . capitalize () } ' , fontsize = 20 ) plt . tight_layout () plt . show () HSIC Kernel Alignment Centered Kernel Alignment","title":"Random Dataset"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/","text":"Distributions vs Mutual Information \u00b6 Idea \u00b6 In this notebook, we will be exploring how we can estimate the HSIC parmaeter for different distributions and look at how it compares to MI measures. Normally the procedure for calculating HSIC is as follows: Calculate kernel matrices for X and Y Center both kernel matrices Find the Frobenius norm between the kernel matrices This works well but there is no certain way to estimate the parameter for each of the kernel matrices. There is another paper that is called the Kernel Tangent Alignment (KTA). This method is different as it is calculated like so: Calculate the kernel matrices for X and Y Find the Frobenius norm between the kernel matrices Normalize the value by the Frobenius norm of X and Y individually This works in a similar way to the HSIC method. The difference is that you do the normalization procedure. The final algorithm is the Centered Kernel Tangent Alignment (cKTA) method which is a combination of the previous two methods. The algorithm is as follows: Calculate the kernel matrices for X and Y Center both kernel matrices Find the Frobenius norm between the kernel matrices Normalize the value by the Frobenius norm of X and Y individually As you can see, it would appear that this method is the most complete in the sense that it incorporates all steps to ensure that our data is sufficiently scaled to a way that is directly comparable. This notebook will attempt to see which of these methods provides a good estimate for a sigma value for the kernel matrices and how do their output values compare to mutual information measures. Experiments \u00b6 Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No Results \u00b6 Not Standardized \u00b6 Same Length Scale \u00b6 Difference Length Scales \u00b6 Median Distances Fig I : Median Distances | Not Standardized - For different HSIC Score methods Silverman n Scott Fig I : Standard Methods (Silverman, Scott) | Not Standardized - For different HSIC Score methods Different Length Scales per Dimension \u00b6 Standardized \u00b6 Same Length Scale \u00b6 Difference Length Scaless \u00b6 Different Length Scales per Dimension \u00b6","title":"Mutual Info vs Scores"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#distributions-vs-mutual-information","text":"","title":"Distributions vs Mutual Information"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#idea","text":"In this notebook, we will be exploring how we can estimate the HSIC parmaeter for different distributions and look at how it compares to MI measures. Normally the procedure for calculating HSIC is as follows: Calculate kernel matrices for X and Y Center both kernel matrices Find the Frobenius norm between the kernel matrices This works well but there is no certain way to estimate the parameter for each of the kernel matrices. There is another paper that is called the Kernel Tangent Alignment (KTA). This method is different as it is calculated like so: Calculate the kernel matrices for X and Y Find the Frobenius norm between the kernel matrices Normalize the value by the Frobenius norm of X and Y individually This works in a similar way to the HSIC method. The difference is that you do the normalization procedure. The final algorithm is the Centered Kernel Tangent Alignment (cKTA) method which is a combination of the previous two methods. The algorithm is as follows: Calculate the kernel matrices for X and Y Center both kernel matrices Find the Frobenius norm between the kernel matrices Normalize the value by the Frobenius norm of X and Y individually As you can see, it would appear that this method is the most complete in the sense that it incorporates all steps to ensure that our data is sufficiently scaled to a way that is directly comparable. This notebook will attempt to see which of these methods provides a good estimate for a sigma value for the kernel matrices and how do their output values compare to mutual information measures.","title":"Idea"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#experiments","text":"Options Standardize Yes / No Parameter Estimator Mean, Median, Silverman, etc Center Kernel Yes / No Normalized Score Yes / No","title":"Experiments"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#results","text":"","title":"Results"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#not-standardized","text":"","title":"Not Standardized"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#same-length-scale","text":"","title":"Same Length Scale"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#difference-length-scales","text":"Median Distances Fig I : Median Distances | Not Standardized - For different HSIC Score methods Silverman n Scott Fig I : Standard Methods (Silverman, Scott) | Not Standardized - For different HSIC Score methods","title":"Difference Length Scales"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#different-length-scales-per-dimension","text":"","title":"Different Length Scales per Dimension"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#standardized","text":"","title":"Standardized"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#same-length-scale_1","text":"","title":"Same Length Scale"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#difference-length-scaless","text":"","title":"Difference Length Scaless"},{"location":"projects/similarity/kernel_alignment_params/notebooks/4.0_dist_vs_mi/#different-length-scales-per-dimension_1","text":"","title":"Different Length Scales per Dimension"},{"location":"resources/","text":"My Resources \u00b6 Python \u00b6 I use python as my primary programming language and I have tried to really make use of what the community has to offer to help me with my tasks. I do things from scratch in the beginning but then I heavily refactor and prefer to use popular, up-to-date libraries with well-tested code. I mostly work with machine learning Standard Stack Integrated Development Environments Earth Science Stack Deep Learning Software Good Programming Practices Gaussian Processes \u00b6 I have worked extensively with Gaussian processes (GPs) and I have also done quite a lot of research on the literature and software side of most modern GPs.I have a detailed webpage where I outline some of the most modern Gaussian processs. Webpage: jejjohnson.github.io/gp_model_zoo I also work a lot with Gaussian process regression and how they deal with uncertainty. In this setting, I have looked specifically at what happens when we want to propagate uncertainty from the inputs through the GP function through training and/or predictions. I have had some success with this but the literature was very scattered when I first started. So I decided to Webpage: jejjohnson.github.io/uncertain_gps","title":"My Resources"},{"location":"resources/#my-resources","text":"","title":"My Resources"},{"location":"resources/#python","text":"I use python as my primary programming language and I have tried to really make use of what the community has to offer to help me with my tasks. I do things from scratch in the beginning but then I heavily refactor and prefer to use popular, up-to-date libraries with well-tested code. I mostly work with machine learning Standard Stack Integrated Development Environments Earth Science Stack Deep Learning Software Good Programming Practices","title":"Python"},{"location":"resources/#gaussian-processes","text":"I have worked extensively with Gaussian processes (GPs) and I have also done quite a lot of research on the literature and software side of most modern GPs.I have a detailed webpage where I outline some of the most modern Gaussian processs. Webpage: jejjohnson.github.io/gp_model_zoo I also work a lot with Gaussian process regression and how they deal with uncertainty. In this setting, I have looked specifically at what happens when we want to propagate uncertainty from the inputs through the GP function through training and/or predictions. I have had some success with this but the literature was very scattered when I first started. So I decided to Webpage: jejjohnson.github.io/uncertain_gps","title":"Gaussian Processes"},{"location":"resources/dl_overview/","text":"Software \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? Anatomy of good DL software Convergence of the Libraries So what to choose? List of Software Core Packages TensorFlow (TF) PyTorch Other Packages What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So what to choose? \u00b6 There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health. List of Software \u00b6 There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning. Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Software"},{"location":"resources/dl_overview/#software","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Last Updated: 18-Jan-2020 What is Deep Learning? Anatomy of good DL software Convergence of the Libraries So what to choose? List of Software Core Packages TensorFlow (TF) PyTorch Other Packages","title":"Software"},{"location":"resources/dl_overview/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"resources/dl_overview/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"resources/dl_overview/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer here : Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.","title":"Convergence of the Libraries"},{"location":"resources/dl_overview/#so-what-to-choose","text":"There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. !> Warning : The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"So what to choose?"},{"location":"resources/dl_overview/#list-of-software","text":"There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning.","title":"List of Software"},{"location":"resources/dl_overview/#core-packages","text":"","title":"Core Packages"},{"location":"resources/dl_overview/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"resources/dl_overview/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"resources/dl_overview/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"resources/mkdocs/","text":"MKDocs Tips n Tricks \u00b6 Extensions \u00b6 Admonition \u00b6 This is great, it allows me to add little blocks of things I find interesting. Note with title !!! note \"Title\" without title !!! note \"\" Collapsible !!! - open by default ??? - collapsible Defaults \u00b6 Note Button Can use note or seealso Abstract Button Can use abstract,summary,tldr Info Button Can use info,todo Fire Button tip,hint,important Checkmark Button success, check, done Question Mark question,help,faq Warning Sign warning,caution,attention X Mark failure,fail,missing Danger sign danger, error Bug sign bug Numbered List bug Quotation Marks quote, cite What Now? \u00b6 Question How does this work if I want to design my own symbol? Todo Design my own symbol and add it to the research notebook! CodeHilite \u00b6 Very simple, we have codeblocks with highlights. Using Python: import numpy as np and using Bash: echo \"Times are changing\" Colocate Different bits of Code \u00b6 I can also collapse them! TensorFlow import tensorflow as tf PyTorch import torch Line Numbers \u00b6 We can also enable line numbers with the linenums: true extension. 1 2 import numpy as np import matplotlib.pyplot as plt Highlight Line Numbers \u00b6 1 2 3 import numpy as np import matplotlib.pyplot as plt from scipy import * # Don't do this!! Meta Data \u00b6 --- title: PyTest Tricks description: The deep learning python stack authors: - J. Emmanuel Johnson path: docs/snippets/testing source: pytest.md ---","title":"MKDocs Tips"},{"location":"resources/mkdocs/#mkdocs-tips-n-tricks","text":"","title":"MKDocs Tips n Tricks"},{"location":"resources/mkdocs/#extensions","text":"","title":"Extensions"},{"location":"resources/mkdocs/#admonition","text":"This is great, it allows me to add little blocks of things I find interesting. Note with title !!! note \"Title\" without title !!! note \"\" Collapsible !!! - open by default ??? - collapsible","title":"Admonition"},{"location":"resources/mkdocs/#defaults","text":"Note Button Can use note or seealso Abstract Button Can use abstract,summary,tldr Info Button Can use info,todo Fire Button tip,hint,important Checkmark Button success, check, done Question Mark question,help,faq Warning Sign warning,caution,attention X Mark failure,fail,missing Danger sign danger, error Bug sign bug Numbered List bug Quotation Marks quote, cite","title":"Defaults"},{"location":"resources/mkdocs/#what-now","text":"Question How does this work if I want to design my own symbol? Todo Design my own symbol and add it to the research notebook!","title":"What Now?"},{"location":"resources/mkdocs/#codehilite","text":"Very simple, we have codeblocks with highlights. Using Python: import numpy as np and using Bash: echo \"Times are changing\"","title":"CodeHilite"},{"location":"resources/mkdocs/#colocate-different-bits-of-code","text":"I can also collapse them! TensorFlow import tensorflow as tf PyTorch import torch","title":"Colocate Different bits of Code"},{"location":"resources/mkdocs/#line-numbers","text":"We can also enable line numbers with the linenums: true extension. 1 2 import numpy as np import matplotlib.pyplot as plt","title":"Line Numbers"},{"location":"resources/mkdocs/#highlight-line-numbers","text":"1 2 3 import numpy as np import matplotlib.pyplot as plt from scipy import * # Don't do this!!","title":"Highlight Line Numbers"},{"location":"resources/mkdocs/#meta-data","text":"--- title: PyTest Tricks description: The deep learning python stack authors: - J. Emmanuel Johnson path: docs/snippets/testing source: pytest.md ---","title":"Meta Data"},{"location":"resources/my_workflow/","text":"My WorkFlow \u00b6 0. Ideas - Markdown \u00b6 1. Prototyping - JupyterLab \u00b6 2. Good - VSCode \u00b6 3. Theory & Empirical Documentation - Docsify \u00b6","title":"My WorkFlow"},{"location":"resources/my_workflow/#my-workflow","text":"","title":"My WorkFlow"},{"location":"resources/my_workflow/#0-ideas-markdown","text":"","title":"0. Ideas - Markdown"},{"location":"resources/my_workflow/#1-prototyping-jupyterlab","text":"","title":"1. Prototyping - JupyterLab"},{"location":"resources/my_workflow/#2-good-vscode","text":"","title":"2. Good - VSCode"},{"location":"resources/my_workflow/#3-theory-empirical-documentation-docsify","text":"","title":"3. Theory &amp; Empirical Documentation - Docsify"},{"location":"resources/reproducibility/","text":"Reproducibility \u00b6 My Philosophy \u00b6 Documentation \u00b6 Packages \u00b6 PyTorch Lightning \u00b6 Kedro (Quantum Black Labs) \u00b6","title":"Reproducibility"},{"location":"resources/reproducibility/#reproducibility","text":"","title":"Reproducibility"},{"location":"resources/reproducibility/#my-philosophy","text":"","title":"My Philosophy"},{"location":"resources/reproducibility/#documentation","text":"","title":"Documentation"},{"location":"resources/reproducibility/#packages","text":"","title":"Packages"},{"location":"resources/reproducibility/#pytorch-lightning","text":"","title":"PyTorch Lightning"},{"location":"resources/reproducibility/#kedro-quantum-black-labs","text":"","title":"Kedro (Quantum Black Labs)"},{"location":"resources/machine_learning/","text":"Machine Learning \u00b6 Made With ML - Newsletter","title":"Machine Learning"},{"location":"resources/machine_learning/#machine-learning","text":"Made With ML - Newsletter","title":"Machine Learning"},{"location":"resources/machine_learning/books/","text":"Recommended Books \u00b6 Deep Learning \u00b6 Bayesian \u00b6 A Student's Guide to Bayesian Statitics - Lambert | Solutions Bayesian Data Analysis - Gelman Statistical Rethinking - McElreath Bayes' Rule: A Tutorial Introduction to Bayes Analysis - Stone Bayesian Methods for Hackers - Davidson-Pilon Bayesian Statistics for Beginners: Step-by-Step Approach - Donovan","title":"Recommended Books"},{"location":"resources/machine_learning/books/#recommended-books","text":"","title":"Recommended Books"},{"location":"resources/machine_learning/books/#deep-learning","text":"","title":"Deep Learning"},{"location":"resources/machine_learning/books/#bayesian","text":"A Student's Guide to Bayesian Statitics - Lambert | Solutions Bayesian Data Analysis - Gelman Statistical Rethinking - McElreath Bayes' Rule: A Tutorial Introduction to Bayes Analysis - Stone Bayesian Methods for Hackers - Davidson-Pilon Bayesian Statistics for Beginners: Step-by-Step Approach - Donovan","title":"Bayesian"},{"location":"resources/machine_learning/deep_learning/","text":"Deep Learning \u00b6 Batch Normalization \u00b6 Why Batch Norm Causes Exploding Gradients - Kyle Luther Books \u00b6 Dive Into Deep Learning","title":"Deep Learning"},{"location":"resources/machine_learning/deep_learning/#deep-learning","text":"","title":"Deep Learning"},{"location":"resources/machine_learning/deep_learning/#batch-normalization","text":"Why Batch Norm Causes Exploding Gradients - Kyle Luther","title":"Batch Normalization"},{"location":"resources/machine_learning/deep_learning/#books","text":"Dive Into Deep Learning","title":"Books"},{"location":"resources/machine_learning/interpretability/","text":"Interpretable Machine Learning \u00b6 Packages \u00b6 interpret","title":"Interpretable Machine Learning"},{"location":"resources/machine_learning/interpretability/#interpretable-machine-learning","text":"","title":"Interpretable Machine Learning"},{"location":"resources/machine_learning/interpretability/#packages","text":"interpret","title":"Packages"},{"location":"resources/machine_learning/physics/","text":"Physic-Informed Machine Learning \u00b6 Deep Learning \u00b6 General Overview","title":"Physic-Informed Machine Learning"},{"location":"resources/machine_learning/physics/#physic-informed-machine-learning","text":"","title":"Physic-Informed Machine Learning"},{"location":"resources/machine_learning/physics/#deep-learning","text":"General Overview","title":"Deep Learning"},{"location":"resources/python/ides/","text":"Integraded Development Environment (IDE) \u00b6 So this is something that MATLAB and R users don't have to deal with: which IDE should I use ? I'm sorry but there is not correct answer, there are many and it all depends on what is your objective. I will list a few suggestions below and then you can decide for yourself which one you need to choose. Coding Interactively ( Recommended ) Full Featured Python Editors 1. Jack of All Trades ( Recommended ) 2. Python Extremist 3. The Hacker 4. Former MATLAB users Coding Interactively ( Recommended ) \u00b6 So first and foremost, I am going to suggest that you use JupyterLab . It is an interactive environment to allow you to program step-by-step. You can visualize tables of data, run code and also visualize figures. The part I mentioned is mainly the 'Jupyter Notebook'. The lab features much more. It has a terminal, a text editor and you can also visualize the directories without leaving the notebook environment. It's the ultimate program-as-you-go type of environment. Most people use it as default and I personally always start coding in this environment right away when I want to work on something. So hop onboard! Note : A lot of just stay with Jupyter Notebooks only for whatever reason they prefer. JupyterLab includes jupyter notebook. So you can always switch to the traditional notebook environment whenever you wish. You just need to switch to tree view by changing the URL in your browser from http://localhost:8888/lab to http://localhost:8888/tree . Tutorials Blog - JupyterLab: Evolution of the Jupyter Notebook > A nice walkthrough of the lab interface. Blog - Working Efficiently with JupyterLab Notebooks > I highly recommend you look through this as it will advise you how to use best programming practices. A lot of people have spent hours on their code with an 'apparent bug' when in reality they just ran the notebooks out of order or something like that. Working with SLURM Blog - Running Jupyter Notebooks Remotely with Slurm Blog - Running Jupyter Lab Remotely JupyterLab Extension Jumping Feet First Into Code If you just want to try something that's relatively not so complicated, then I highly recommend you just using a google colaboratory notebook . It's a Jupyter Notebook(-like) interface, most python packages that you will need are already installed, you have access to CPUs, RAM, GPUs and even TPUs. I use it all the time when I want to just test something out or try something new. Full Featured Python Editors \u00b6 The editors above are special cases. But if you want a full featured Python editor then I would highly suggest the ones I list below. You may not think you need one, especially those dedicated users of JupyterLab. But I can assure you that if you want to make good reproducible code. 1. Jack of All Trades ( Recommended ) \u00b6 The most popular IDE as of now would be Visual Studio Code (VSCode). I really like it because it is hackable like Sublime or Atom but it also just works . I rarely run into IDE-breaking changes. It's also fairly lightweight and supports remote computing via ssh out of the box! Backed by Microsoft, it is definitely my most recommended IDE for almost any open-source programming language. It may support all languages but the Python specifics are great. They even have some support Jupyter Notebooks and it comes shipped with the Anaconda distribution . 2. Python Extremist \u00b6 Probably the best IDE out there that is Python specific is PyCharm . It's backed by JetBrains and they have been around for a while. Source : 9 Reasons You Should Be Using PyCharm (2015) - Michael Kennedy 3. The Hacker \u00b6 I would suggest that you learn VIM and hack your way into creating an IDE for yourself. I'm not a proficient VIM user but I can definitely see the benefits if you would like to be \"one with your terminal\". In edition, you will be able to edit anywhere as long as you have access to a terminal. I often say Python programmers (that don't use the out-of-the-box Anaconda GUI) have more abilities with the terminal that say MATLAB or SPSS users simply because python users typically spend quite a large amount of time trying to configure things. Especially when we want to use stuff remotely. But VIM users...are a different breed altogether. So needless to say the learning curve is huge. But...the rewards will be Jordi level heights. Source : Vim as a Python IDE, or Python IDE as Vim (2013) - Dmitry Filippov Tutorials Blog: Real Python - VIM and Python: A Match Made in Heaven 4. Former MATLAB users \u00b6 If you're coming from MATLAB and want to minimize the amount of changes you do during the transition, then I recommend that you start with Spyder . It is the most complete environment available to date for people who like working with everything at their fingertips in front of them; i.e. script, terminal, variable explorer, etc. It was put on hiatus for a while but it is now sponsored by QuantSight and NumFOCUS which means it has a proper team of devs. Source : Spyder Blog Remote Development So there are ways to utilize remote kernels (i.e. python environments in other locations). I personally haven't tested any way to do it but I'll like some resources below. Tutorials Spyder Docs - Connect to an external kernel Blog - Connecting Spyder IDE to a remote IPython kernel (2019) Blog - How to connect your Spyder IDE to an external ipython kernel with SSH PuTTY tunnel (2019)","title":"Python IDEs"},{"location":"resources/python/ides/#integraded-development-environment-ide","text":"So this is something that MATLAB and R users don't have to deal with: which IDE should I use ? I'm sorry but there is not correct answer, there are many and it all depends on what is your objective. I will list a few suggestions below and then you can decide for yourself which one you need to choose. Coding Interactively ( Recommended ) Full Featured Python Editors 1. Jack of All Trades ( Recommended ) 2. Python Extremist 3. The Hacker 4. Former MATLAB users","title":"Integraded Development Environment (IDE)"},{"location":"resources/python/ides/#coding-interactively-recommended","text":"So first and foremost, I am going to suggest that you use JupyterLab . It is an interactive environment to allow you to program step-by-step. You can visualize tables of data, run code and also visualize figures. The part I mentioned is mainly the 'Jupyter Notebook'. The lab features much more. It has a terminal, a text editor and you can also visualize the directories without leaving the notebook environment. It's the ultimate program-as-you-go type of environment. Most people use it as default and I personally always start coding in this environment right away when I want to work on something. So hop onboard! Note : A lot of just stay with Jupyter Notebooks only for whatever reason they prefer. JupyterLab includes jupyter notebook. So you can always switch to the traditional notebook environment whenever you wish. You just need to switch to tree view by changing the URL in your browser from http://localhost:8888/lab to http://localhost:8888/tree . Tutorials Blog - JupyterLab: Evolution of the Jupyter Notebook > A nice walkthrough of the lab interface. Blog - Working Efficiently with JupyterLab Notebooks > I highly recommend you look through this as it will advise you how to use best programming practices. A lot of people have spent hours on their code with an 'apparent bug' when in reality they just ran the notebooks out of order or something like that. Working with SLURM Blog - Running Jupyter Notebooks Remotely with Slurm Blog - Running Jupyter Lab Remotely JupyterLab Extension Jumping Feet First Into Code If you just want to try something that's relatively not so complicated, then I highly recommend you just using a google colaboratory notebook . It's a Jupyter Notebook(-like) interface, most python packages that you will need are already installed, you have access to CPUs, RAM, GPUs and even TPUs. I use it all the time when I want to just test something out or try something new.","title":"Coding Interactively (Recommended)"},{"location":"resources/python/ides/#full-featured-python-editors","text":"The editors above are special cases. But if you want a full featured Python editor then I would highly suggest the ones I list below. You may not think you need one, especially those dedicated users of JupyterLab. But I can assure you that if you want to make good reproducible code.","title":"Full Featured Python Editors"},{"location":"resources/python/ides/#1-jack-of-all-trades-recommended","text":"The most popular IDE as of now would be Visual Studio Code (VSCode). I really like it because it is hackable like Sublime or Atom but it also just works . I rarely run into IDE-breaking changes. It's also fairly lightweight and supports remote computing via ssh out of the box! Backed by Microsoft, it is definitely my most recommended IDE for almost any open-source programming language. It may support all languages but the Python specifics are great. They even have some support Jupyter Notebooks and it comes shipped with the Anaconda distribution .","title":"1. Jack of All Trades (Recommended)"},{"location":"resources/python/ides/#2-python-extremist","text":"Probably the best IDE out there that is Python specific is PyCharm . It's backed by JetBrains and they have been around for a while. Source : 9 Reasons You Should Be Using PyCharm (2015) - Michael Kennedy","title":"2. Python Extremist"},{"location":"resources/python/ides/#3-the-hacker","text":"I would suggest that you learn VIM and hack your way into creating an IDE for yourself. I'm not a proficient VIM user but I can definitely see the benefits if you would like to be \"one with your terminal\". In edition, you will be able to edit anywhere as long as you have access to a terminal. I often say Python programmers (that don't use the out-of-the-box Anaconda GUI) have more abilities with the terminal that say MATLAB or SPSS users simply because python users typically spend quite a large amount of time trying to configure things. Especially when we want to use stuff remotely. But VIM users...are a different breed altogether. So needless to say the learning curve is huge. But...the rewards will be Jordi level heights. Source : Vim as a Python IDE, or Python IDE as Vim (2013) - Dmitry Filippov Tutorials Blog: Real Python - VIM and Python: A Match Made in Heaven","title":"3. The Hacker"},{"location":"resources/python/ides/#4-former-matlab-users","text":"If you're coming from MATLAB and want to minimize the amount of changes you do during the transition, then I recommend that you start with Spyder . It is the most complete environment available to date for people who like working with everything at their fingertips in front of them; i.e. script, terminal, variable explorer, etc. It was put on hiatus for a while but it is now sponsored by QuantSight and NumFOCUS which means it has a proper team of devs. Source : Spyder Blog Remote Development So there are ways to utilize remote kernels (i.e. python environments in other locations). I personally haven't tested any way to do it but I'll like some resources below. Tutorials Spyder Docs - Connect to an external kernel Blog - Connecting Spyder IDE to a remote IPython kernel (2019) Blog - How to connect your Spyder IDE to an external ipython kernel with SSH PuTTY tunnel (2019)","title":"4. Former MATLAB users"},{"location":"resources/python/tidbits/","text":"Interesting Tidbits \u00b6 Einsum Notation \u00b6 Understanding PyTorch Einsum Einsum is all you need Einstein Summation in Numpy Software Engineering \u00b6 Awesome SWE 4 ML Testing \u00b6 Annotating Code Tests and Selectively Running Tests - Blog XArray \u00b6 XBatcher Saving and Loading Models \u00b6 sklearn \u00b6 Sklearn Docs ONNX sklearn-onnx Deploying an Sklearn model with onnx and fast-api - blog Deployment","title":"Interesting Tidbits"},{"location":"resources/python/tidbits/#interesting-tidbits","text":"","title":"Interesting Tidbits"},{"location":"resources/python/tidbits/#einsum-notation","text":"Understanding PyTorch Einsum Einsum is all you need Einstein Summation in Numpy","title":"Einsum Notation"},{"location":"resources/python/tidbits/#software-engineering","text":"Awesome SWE 4 ML","title":"Software Engineering"},{"location":"resources/python/tidbits/#testing","text":"Annotating Code Tests and Selectively Running Tests - Blog","title":"Testing"},{"location":"resources/python/tidbits/#xarray","text":"XBatcher","title":"XArray"},{"location":"resources/python/tidbits/#saving-and-loading-models","text":"","title":"Saving and Loading Models"},{"location":"resources/python/tidbits/#sklearn","text":"Sklearn Docs ONNX sklearn-onnx Deploying an Sklearn model with onnx and fast-api - blog Deployment","title":"sklearn"},{"location":"resources/python/code/einsum/","text":"Einsum \u00b6 Really Fascinating Tutorials \u00b6 Einsum Extensions: A Wish List for Readable Matrix Ops - Madison May","title":"Einsum"},{"location":"resources/python/code/einsum/#einsum","text":"Really Fascinating","title":"Einsum"},{"location":"resources/python/code/einsum/#tutorials","text":"Einsum Extensions: A Wish List for Readable Matrix Ops - Madison May","title":"Tutorials"},{"location":"resources/python/code/large_scale/","text":"Large Scale \u00b6 Packages \u00b6 Vaex \u00b6 This is an out-of-core dataframes library for Python, ML and visualization. This is sort of like pandas with some built-in functions. It also plays well with scikit-learn. Train 1 Billion Samples with Vaex and Scikit-Learn","title":"Large Scale"},{"location":"resources/python/code/large_scale/#large-scale","text":"","title":"Large Scale"},{"location":"resources/python/code/large_scale/#packages","text":"","title":"Packages"},{"location":"resources/python/code/large_scale/#vaex","text":"This is an out-of-core dataframes library for Python, ML and visualization. This is sort of like pandas with some built-in functions. It also plays well with scikit-learn. Train 1 Billion Samples with Vaex and Scikit-Learn","title":"Vaex"},{"location":"resources/python/good_code/","text":"Tutorials \u00b6 Overviews \u00b6 Python 102 for scientific computing and data analysis Good overview. Includes things like organization, testing, documentation, logging, cli, and optimization HyperModern Python Part I - Setup Part II - Testing Part III - Linting Part IV - Typing Part V - Documentation Part VI - CI/CD Python for HPC: Community Materials Creeds \u00b6 Better Scientifc Software (BBSW)","title":"Tutorials"},{"location":"resources/python/good_code/#tutorials","text":"","title":"Tutorials"},{"location":"resources/python/good_code/#overviews","text":"Python 102 for scientific computing and data analysis Good overview. Includes things like organization, testing, documentation, logging, cli, and optimization HyperModern Python Part I - Setup Part II - Testing Part III - Linting Part IV - Typing Part V - Documentation Part VI - CI/CD Python for HPC: Community Materials","title":"Overviews"},{"location":"resources/python/good_code/#creeds","text":"Better Scientifc Software (BBSW)","title":"Creeds"},{"location":"resources/python/good_code/documentation/","text":"Documentation \u00b6 Tutorials \u00b6 Documentation with Sphinx - Betterscientificsoftware Templates \u00b6 scikit-learn Function Documentation Template sklearn-template | documentation Sphinx Gallery Template","title":"Documentation"},{"location":"resources/python/good_code/documentation/#documentation","text":"","title":"Documentation"},{"location":"resources/python/good_code/documentation/#tutorials","text":"Documentation with Sphinx - Betterscientificsoftware","title":"Tutorials"},{"location":"resources/python/good_code/documentation/#templates","text":"scikit-learn Function Documentation Template sklearn-template | documentation Sphinx Gallery Template","title":"Templates"},{"location":"resources/python/good_code/experiments/","text":"Experiments \u00b6 Releasing Research Code Hydra \u00b6 Minimal Example - github Comet \u00b6 PyTorch Lightning \u00b6 Using Optuna to Optimize PyTorch Lightning HyperParameters","title":"Experiments"},{"location":"resources/python/good_code/experiments/#experiments","text":"Releasing Research Code","title":"Experiments"},{"location":"resources/python/good_code/experiments/#hydra","text":"Minimal Example - github","title":"Hydra"},{"location":"resources/python/good_code/experiments/#comet","text":"","title":"Comet"},{"location":"resources/python/good_code/experiments/#pytorch-lightning","text":"Using Optuna to Optimize PyTorch Lightning HyperParameters","title":"PyTorch Lightning"},{"location":"resources/python/good_code/git/","text":"Git \u00b6 Git Workflow \u00b6 A Git workflow with branches - Karink Nudson The simplest and clearest git workflow guide you'll find. Start here. A Successful Git Branching Model - Vincent Driessen","title":"Git"},{"location":"resources/python/good_code/git/#git","text":"","title":"Git"},{"location":"resources/python/good_code/git/#git-workflow","text":"A Git workflow with branches - Karink Nudson The simplest and clearest git workflow guide you'll find. Start here. A Successful Git Branching Model - Vincent Driessen","title":"Git Workflow"},{"location":"resources/python/good_code/good_code/","text":"Good Code \u00b6 Logging \u00b6 This is something I often use whenever I am in the process of building software and I think there are some key things that need to be documented. It is often much better than print statements. I often do this when I'm not really sure if what I did is correct in the process. It's really important to log. Especially when you're doing server computing and you need a history of what was going on. My Style * INFO - General stuff of where I am at in the program so I can follow the control of flow * DEBUG - Typically more about sizes/shapes of my matrices or possibly in the checks * WARNING - Where things could go wrong but I have ignored this part due to some reason. Tutorials * Python Logging: A Stroll Through the Source Code - [RealPython](https://realpython.com/python-logging-source-code/) * Python Logging Cheat Sheet - [gist](https://gist.github.com/jonepl/dd5dc90a5bc1b86b2fc2b3a244af7fc6) * The Hitchhikers Guide to Python: Logging - [blog](https://docs.python-guide.org/writing/logging/) * Good Logging Practice in Python - [blog](https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/) * Logging CookBook - [Python Docs](https://docs.python.org/3/howto/logging-cookbook.html) * Corey Schafer * Logging Basics - Logging to Files, Setting Levels, and Formating - [Youtube](https://www.youtube.com/watch?v=-ARI4Cz-awo&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=2&t=0s) * Logging Advanced: Loggers, Handlers, and Formatters - [Youtube](https://www.youtube.com/watch?v=jxmzY9soFXg&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=4&t=0s) Testing \u00b6 Something that we all should do but don't always do. It's important for the long run but it seems annoying for the short game. But overall, you cannot go wrong with tests; you just can't. My Style * Package - PyTest * IDE - VSCode Tutorials * Testing Python Applications with PyTest - [Blog](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest) * Getting Started with Testing in Python - [RealPython](https://realpython.com/python-testing/) * Testing Your Python Code with PyTest - [SciPy 2019](https://www.youtube.com/watch?v=LX2ksGYXJ80) * Learn PyTest in 60 Minutes: Python Unit Testing Framework - [Youtube](https://www.youtube.com/watch?v=bbp_849-RZ4) * Python Testing in VSCode - [VSCode Docs](https://code.visualstudio.com/docs/python/testing) * Eric Ma * Testing Data Science Code - [YouTube](https://www.youtube.com/watch?v=fmVbtHMHEZc) * Best Testing Practices - [PyCon 2017](https://www.youtube.com/watch?v=yACtdj1_IxE) Repository Organization \u00b6 Packaging \u00b6 Source : * PyPA * How to Package Your Python Code Type Checking \u00b6 Continuous Integration \u00b6 WorkFlow \u00b6 JupyterLab - Prototyping, Remote Computing VSCode - Package Management, Remote Computing Remote Computing - SSH (JLab, VSCode)","title":"Good Code"},{"location":"resources/python/good_code/good_code/#good-code","text":"","title":"Good Code"},{"location":"resources/python/good_code/good_code/#logging","text":"This is something I often use whenever I am in the process of building software and I think there are some key things that need to be documented. It is often much better than print statements. I often do this when I'm not really sure if what I did is correct in the process. It's really important to log. Especially when you're doing server computing and you need a history of what was going on. My Style * INFO - General stuff of where I am at in the program so I can follow the control of flow * DEBUG - Typically more about sizes/shapes of my matrices or possibly in the checks * WARNING - Where things could go wrong but I have ignored this part due to some reason. Tutorials * Python Logging: A Stroll Through the Source Code - [RealPython](https://realpython.com/python-logging-source-code/) * Python Logging Cheat Sheet - [gist](https://gist.github.com/jonepl/dd5dc90a5bc1b86b2fc2b3a244af7fc6) * The Hitchhikers Guide to Python: Logging - [blog](https://docs.python-guide.org/writing/logging/) * Good Logging Practice in Python - [blog](https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/) * Logging CookBook - [Python Docs](https://docs.python.org/3/howto/logging-cookbook.html) * Corey Schafer * Logging Basics - Logging to Files, Setting Levels, and Formating - [Youtube](https://www.youtube.com/watch?v=-ARI4Cz-awo&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=2&t=0s) * Logging Advanced: Loggers, Handlers, and Formatters - [Youtube](https://www.youtube.com/watch?v=jxmzY9soFXg&list=PLMdgUBu5wWKxObYWmWbwxDhlBXqUObLNY&index=4&t=0s)","title":"Logging"},{"location":"resources/python/good_code/good_code/#testing","text":"Something that we all should do but don't always do. It's important for the long run but it seems annoying for the short game. But overall, you cannot go wrong with tests; you just can't. My Style * Package - PyTest * IDE - VSCode Tutorials * Testing Python Applications with PyTest - [Blog](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest) * Getting Started with Testing in Python - [RealPython](https://realpython.com/python-testing/) * Testing Your Python Code with PyTest - [SciPy 2019](https://www.youtube.com/watch?v=LX2ksGYXJ80) * Learn PyTest in 60 Minutes: Python Unit Testing Framework - [Youtube](https://www.youtube.com/watch?v=bbp_849-RZ4) * Python Testing in VSCode - [VSCode Docs](https://code.visualstudio.com/docs/python/testing) * Eric Ma * Testing Data Science Code - [YouTube](https://www.youtube.com/watch?v=fmVbtHMHEZc) * Best Testing Practices - [PyCon 2017](https://www.youtube.com/watch?v=yACtdj1_IxE)","title":"Testing"},{"location":"resources/python/good_code/good_code/#repository-organization","text":"","title":"Repository Organization"},{"location":"resources/python/good_code/good_code/#packaging","text":"Source : * PyPA * How to Package Your Python Code","title":"Packaging"},{"location":"resources/python/good_code/good_code/#type-checking","text":"","title":"Type Checking"},{"location":"resources/python/good_code/good_code/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"resources/python/good_code/good_code/#workflow","text":"JupyterLab - Prototyping, Remote Computing VSCode - Package Management, Remote Computing Remote Computing - SSH (JLab, VSCode)","title":"WorkFlow"},{"location":"resources/python/good_code/pip/","text":"Pip \u00b6 Personal Pip Package \u00b6 Make Your Project Pippable - Maria Antoniak Probably the clearest and simplest introduction you'll find. Start with this if you don't know what to do. Python: Create a pip installable package More details.","title":"Pip"},{"location":"resources/python/good_code/pip/#pip","text":"","title":"Pip"},{"location":"resources/python/good_code/pip/#personal-pip-package","text":"Make Your Project Pippable - Maria Antoniak Probably the clearest and simplest introduction you'll find. Start with this if you don't know what to do. Python: Create a pip installable package More details.","title":"Personal Pip Package"},{"location":"resources/python/good_code/typing/","text":"","title":"Typing"},{"location":"resources/python/packages/geospatial/","text":"Earth-Sci Stuff \u00b6 Compilation \u00b6 Essential Packages for GeoSpatial Analysis in Python - James Brennan Visualization \u00b6 Visualizing 2 Billion Pixel Rasters w. DataShader and XArray","title":"Earth-Sci Stuff"},{"location":"resources/python/packages/geospatial/#earth-sci-stuff","text":"","title":"Earth-Sci Stuff"},{"location":"resources/python/packages/geospatial/#compilation","text":"Essential Packages for GeoSpatial Analysis in Python - James Brennan","title":"Compilation"},{"location":"resources/python/packages/geospatial/#visualization","text":"Visualizing 2 Billion Pixel Rasters w. DataShader and XArray","title":"Visualization"},{"location":"resources/python/packages/jax/","text":"Jax \u00b6 Tutorials \u00b6 A First Look at Jax - Madison May Getting Started with Jax (MLPs, CNNs & RNNs) - Robert Tjarko Lange Flax * Flax: Google's First Open Source Approach to Flexibility in Machine Learning - Hackernoon Haiku * Finetuning Transformers with Jax + Haiku - Madison May Interesting Algorithms \u00b6 Kernel Least Squares Gaussian Processes","title":"Jax"},{"location":"resources/python/packages/jax/#jax","text":"","title":"Jax"},{"location":"resources/python/packages/jax/#tutorials","text":"A First Look at Jax - Madison May Getting Started with Jax (MLPs, CNNs & RNNs) - Robert Tjarko Lange Flax * Flax: Google's First Open Source Approach to Flexibility in Machine Learning - Hackernoon Haiku * Finetuning Transformers with Jax + Haiku - Madison May","title":"Tutorials"},{"location":"resources/python/packages/jax/#interesting-algorithms","text":"Kernel Least Squares Gaussian Processes","title":"Interesting Algorithms"},{"location":"resources/python/packages/logging/","text":"","title":"Logging"},{"location":"resources/python/packages/pandas/","text":"Pandas \u00b6 Resources \u00b6 Pandas Vault - A Collection of useful pandas scripts.","title":"Pandas"},{"location":"resources/python/packages/pandas/#pandas","text":"","title":"Pandas"},{"location":"resources/python/packages/pandas/#resources","text":"Pandas Vault - A Collection of useful pandas scripts.","title":"Resources"},{"location":"resources/python/packages/parallel/","text":"Parallel Programming \u00b6 Blogs \u00b6 Every Python Programmer Should Know the Not-So-Secret ThreadPool Gives a good overview of threading, asyncio and multiprocessing. Makes a case for mp . Uses the pool.map function.","title":"Parallel Programming"},{"location":"resources/python/packages/parallel/#parallel-programming","text":"","title":"Parallel Programming"},{"location":"resources/python/packages/parallel/#blogs","text":"Every Python Programmer Should Know the Not-So-Secret ThreadPool Gives a good overview of threading, asyncio and multiprocessing. Makes a case for mp . Uses the pool.map function.","title":"Blogs"},{"location":"resources/python/packages/pytorch/","text":"PyTorch \u00b6 Basics \u00b6 Effective PyTorch Add-Ons \u00b6 Skorch","title":"PyTorch"},{"location":"resources/python/packages/pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"resources/python/packages/pytorch/#basics","text":"Effective PyTorch","title":"Basics"},{"location":"resources/python/packages/pytorch/#add-ons","text":"Skorch","title":"Add-Ons"},{"location":"resources/python/packages/sql/","text":"SQL \u00b6 I'm not good at SQL but I would like to learn. SQLite \u00b6 Fast subsets of large datasets with Pandas and SQLite - blog Teaches you a minimal way to replace large csv files within a SQL database","title":"SQL"},{"location":"resources/python/packages/sql/#sql","text":"I'm not good at SQL but I would like to learn.","title":"SQL"},{"location":"resources/python/packages/sql/#sqlite","text":"Fast subsets of large datasets with Pandas and SQLite - blog Teaches you a minimal way to replace large csv files within a SQL database","title":"SQLite"},{"location":"resources/python/remote_computing/","text":"Overview \u00b6","title":"Overview"},{"location":"resources/python/remote_computing/#overview","text":"","title":"Overview"},{"location":"resources/python/remote_computing/jupyter_slurm/","text":"JupyterLab + Slurm \u00b6 Resources : Yale Center For Research and Computing Guide Running Jupyter Notebooks Interactively with SLURM - Alexander Lab @WHOI --port-retries = 0 --ip = '*' --NotebookApp.shutdown_no_activity_timeout = 600","title":"JupyterLab + Slurm"},{"location":"resources/python/remote_computing/jupyter_slurm/#jupyterlab-slurm","text":"Resources : Yale Center For Research and Computing Guide Running Jupyter Notebooks Interactively with SLURM - Alexander Lab @WHOI --port-retries = 0 --ip = '*' --NotebookApp.shutdown_no_activity_timeout = 600","title":"JupyterLab + Slurm"},{"location":"resources/python/software_stacks/dl_software/","text":"Deep Learning Software for Python \u00b6 Core Packages \u00b6 TensorFlow (TF) \u00b6 This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd. PyTorch \u00b6 This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular . Other Packages \u00b6 Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Deep Learning"},{"location":"resources/python/software_stacks/dl_software/#deep-learning-software-for-python","text":"","title":"Deep Learning Software for Python"},{"location":"resources/python/software_stacks/dl_software/#core-packages","text":"","title":"Core Packages"},{"location":"resources/python/software_stacks/dl_software/#tensorflow-tf","text":"This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding. TensorFlow Probability (TFP) As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same layers as the tf.keras library with some edward2 integration. Edward2 (Ed2) While there is some integration of edward2 into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced layers such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP. GPFlow (GPF) This is a special library for SOTA GPs that's built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.","title":"TensorFlow (TF)"},{"location":"resources/python/software_stacks/dl_software/#pytorch","text":"This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you're starting out, most people will recommend you start with PyTorch. Pyro This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you'll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don't find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I'm sure this will change. GPyTorch This is the most scalable GP library currently that's built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library. fastai This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active forum . I think it's a good first pass if you're looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good courses that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive. JAX This is the successor for the popular autograd package that is now backed by Google. It is basically numpy on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a grad function; recursively if you want high-order gradients. It's still fairly early in development but it's gaining popularity very fast and has shown to be very competitive . It's fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become popular .","title":"PyTorch"},{"location":"resources/python/software_stacks/dl_software/#other-packages","text":"Although these are in the 'other' category, it doesn't mean that they are lower tier by any means. I just put them here because I'm not too familiar with them outside of the media. Chainer Preferred Networks ( Japanese Company ) MXNet Amazon Theano maintained by the PyMC3 developers. PyMC3 & PyMC4 CNTK Microsoft","title":"Other Packages"},{"location":"resources/python/software_stacks/earthsci/","text":"Earth Science Tools \u00b6 These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing. DataStructures \u00b6 Xarray \u00b6 Source : Xarray Data Structure documentation This image says a lot: is the default package for handling spatial-temporal-variable datasets. This alone has helped me contain data where I care about the meta-data as well. Numpy Arrays are great but they are limited in their retention of meta-data. In addition, it has many features that allow you to work with it from a numpy array perspective and even better from a pandas perspective. It makes the whole ordeal a lot easier. GeoPandas \u00b6 If you don't work with arrays and you prefer to use shapefiles, then I suggest using GeoPandas to store this data. In the end, it is exactly like Pandas but it has some special routines that cater to working with Earth Sci data. I am no expert and I have really only used the plotting routines and the shape extract routines. But in the end, as a data structure, this would be an easy go to with a relatively low learning curve if you're already familiar with Pandas. Manipulating ShapeFiles \u00b6 RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase. Visualization \u00b6 Built-In \u00b6 These are packages where the routine is built-in as a side option and not a fully-fledged packaged dedicated to this. Not saying that the built-in functionality isn't extensive, but typically it might use another framework to do some simple routines. xarray The xarray docs have a few examples for how one can plot. geopandas This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy A package that handles all of the projections needed for better visualizations of the globe. Works well with matplotlib, geopandas, and xarray. Tutorials Maps in Scientific Python > A great tutorial by Rabernat Dedicated Plotting Libraries \u00b6 folium This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . geoviews kepler.gl pyviz Other Useful Utilities \u00b6 Regridding: xESMF \u00b6 Ever had one dataset in xarray that had one lat,lon resolution and then you have another dataset with a different lat,lon resolution? Well, you can use this package to easily move from one coordinate grid reference to another. It removes a lot of the difficulty and it is relatively fast for small-medium sized datasets. Geometries: rioxarray \u00b6 A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else. Rasterio \u00b6","title":"Earth Science"},{"location":"resources/python/software_stacks/earthsci/#earth-science-tools","text":"These are a few simple tools that can be helpful with dealing spatial-temporal aware datasets in particular from the xarray package. These xr.Datasets are in the format (lat x lon x time x variable) and many times we just need X and y. There are a few useful functions in here that will help getting coverting that data into useful arrays for processing.","title":"Earth Science Tools"},{"location":"resources/python/software_stacks/earthsci/#datastructures","text":"","title":"DataStructures"},{"location":"resources/python/software_stacks/earthsci/#xarray","text":"Source : Xarray Data Structure documentation This image says a lot: is the default package for handling spatial-temporal-variable datasets. This alone has helped me contain data where I care about the meta-data as well. Numpy Arrays are great but they are limited in their retention of meta-data. In addition, it has many features that allow you to work with it from a numpy array perspective and even better from a pandas perspective. It makes the whole ordeal a lot easier.","title":"Xarray"},{"location":"resources/python/software_stacks/earthsci/#geopandas","text":"If you don't work with arrays and you prefer to use shapefiles, then I suggest using GeoPandas to store this data. In the end, it is exactly like Pandas but it has some special routines that cater to working with Earth Sci data. I am no expert and I have really only used the plotting routines and the shape extract routines. But in the end, as a data structure, this would be an easy go to with a relatively low learning curve if you're already familiar with Pandas.","title":"GeoPandas"},{"location":"resources/python/software_stacks/earthsci/#manipulating-shapefiles","text":"RegionMask > Some additional functionality for having specialized regions. Shapely > The original library which allows one parse shapefiles in an efficient way. Affine > The package used to do the tranformation of the polygons to the lat,lon coordinates. Rasterio > Very powerful python package that is really good at transforming the coordinates of your datasets. See Gonzalo's tutorial for a more specific usecase.","title":"Manipulating ShapeFiles"},{"location":"resources/python/software_stacks/earthsci/#visualization","text":"","title":"Visualization"},{"location":"resources/python/software_stacks/earthsci/#built-in","text":"These are packages where the routine is built-in as a side option and not a fully-fledged packaged dedicated to this. Not saying that the built-in functionality isn't extensive, but typically it might use another framework to do some simple routines. xarray The xarray docs have a few examples for how one can plot. geopandas This package handles polygons and I have found that it's really good for plotting polygons out-of-the-box. cartopy A package that handles all of the projections needed for better visualizations of the globe. Works well with matplotlib, geopandas, and xarray. Tutorials Maps in Scientific Python > A great tutorial by Rabernat","title":"Built-In"},{"location":"resources/python/software_stacks/earthsci/#dedicated-plotting-libraries","text":"folium This is the first of the packages on this list that starts to utilize javascript under the hood. This one is particularly nice for things to do with maps and polygons. hvplot A nice package that offers a higher level API to the Bokeh library. It allows you do do quite a lot of interactive plots. They have some tutorials for geographic data whether it be polygons or gridded data. holoviews This has been recommended for large scale datasets with millions of points ! They have some tutorials for polygons and xarray grids . geoviews kepler.gl pyviz","title":"Dedicated Plotting Libraries"},{"location":"resources/python/software_stacks/earthsci/#other-useful-utilities","text":"","title":"Other Useful Utilities"},{"location":"resources/python/software_stacks/earthsci/#regridding-xesmf","text":"Ever had one dataset in xarray that had one lat,lon resolution and then you have another dataset with a different lat,lon resolution? Well, you can use this package to easily move from one coordinate grid reference to another. It removes a lot of the difficulty and it is relatively fast for small-medium sized datasets.","title":"Regridding: xESMF"},{"location":"resources/python/software_stacks/earthsci/#geometries-rioxarray","text":"A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Geometries: rioxarray"},{"location":"resources/python/software_stacks/earthsci/#rasterio","text":"","title":"Rasterio"},{"location":"resources/python/software_stacks/python_stack/","text":"Python Packages \u00b6 Specialized Stack \u00b6 scikit-image Statsmodels xarray Automatic Differentiation Stack \u00b6 These programs are mainly focused on automatic differentiation (a.k.a. AutoGrad). Each package is backed by some big company (e.g. Google, Facebook, Microsoft or Amazon).There are many packages nowadays, each with their pros and cons, but I will recommend the most popular. In the beginning, static graphs (define first then run after) was the standard but nowadays the dynamic method (define/run as you go) is more standard due to its popularity amongst the research community. So the differences between many of the libraries are starting to converge. Please go to this webpage for a more detailed overview of the SOTA deep learning packages in python. Kernel Methods \u00b6 So I haven't found any designated kernel methods library ( open market?! ) but there are packages that have kernel methods within them. There are many packages that have GPs. scikit-learn This library has some of the standard kernel functions and kernel methods such as KPCA , SVMs , KRR and some kernel approximation schemes such as the nystrom method and RFF. Note : they do not have the kernel approximation schemes actually integrated into the KRR algorithm ( open market?! ). For that, you can see my implementation . Visualization Stack \u00b6 Geospatial Processing Stack \u00b6 Geopandas This would be the easiest package to use when we need to deal with shape files. It also follows the pandas syntax closes with added plotting capabilities. Rasterio Shapely rioxarray A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Python Ecosystem"},{"location":"resources/python/software_stacks/python_stack/#python-packages","text":"","title":"Python Packages"},{"location":"resources/python/software_stacks/python_stack/#specialized-stack","text":"scikit-image Statsmodels xarray","title":"Specialized Stack"},{"location":"resources/python/software_stacks/python_stack/#automatic-differentiation-stack","text":"These programs are mainly focused on automatic differentiation (a.k.a. AutoGrad). Each package is backed by some big company (e.g. Google, Facebook, Microsoft or Amazon).There are many packages nowadays, each with their pros and cons, but I will recommend the most popular. In the beginning, static graphs (define first then run after) was the standard but nowadays the dynamic method (define/run as you go) is more standard due to its popularity amongst the research community. So the differences between many of the libraries are starting to converge. Please go to this webpage for a more detailed overview of the SOTA deep learning packages in python.","title":"Automatic Differentiation Stack"},{"location":"resources/python/software_stacks/python_stack/#kernel-methods","text":"So I haven't found any designated kernel methods library ( open market?! ) but there are packages that have kernel methods within them. There are many packages that have GPs. scikit-learn This library has some of the standard kernel functions and kernel methods such as KPCA , SVMs , KRR and some kernel approximation schemes such as the nystrom method and RFF. Note : they do not have the kernel approximation schemes actually integrated into the KRR algorithm ( open market?! ). For that, you can see my implementation .","title":"Kernel Methods"},{"location":"resources/python/software_stacks/python_stack/#visualization-stack","text":"","title":"Visualization Stack"},{"location":"resources/python/software_stacks/python_stack/#geospatial-processing-stack","text":"Geopandas This would be the easiest package to use when we need to deal with shape files. It also follows the pandas syntax closes with added plotting capabilities. Rasterio Shapely rioxarray A useful package that allows you to couple geometries with xarray. You can mask or reproject data. I like this package because it's simple and it focuses on what it is good at and nothing else.","title":"Geospatial Processing Stack"},{"location":"resources/python/software_stacks/standard_stack/","text":"Standard Python Stack \u00b6 For the most part, you'll see this at the top of everyone's scientific computing notebook/script: import numpy as np import pandas as pd from scipy.stats import norm from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt This is the impact of these libraries on the python community. By far, the most mature, the most robust and have the most documentation. In principle, you should be able to do almost any kind of scientific computing from start to finish with these libraries. Below I list them and I give the most useful resources that I have use (and still use). Just remember, when in doubt, stackoverflow is your best friend. Numpy \u00b6 The standard library for utilizing the most fundamental data structure for scientific computing: the array. It also has many linear algebra routines that have been optimized in C/C++ behind the scences. Often times doing things in raw python can get a massive speed up by doing things with numpy. The must have package for everyones python stack. It also has some of the best documentation for python packages. Tutorials Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 2 - Intro to Numpy Intro to Numpy A Visual Guide to Numpy From Python to Numpy 100 Exercises in Numpy Broadcasting Tutorial Einsum - I | II Scipy \u00b6 The other defacto library for doing scientific computing. This package is quite heavily linked with numpy but it does have it's own suite of routines. This library also has linear algebra routines including sparse matrices. But it also has signal processing, probability and statistics and optimization functions. Another library with some of the best documentation. Resources Documentation Scipy Lecture Notes Scikit-Learn \u00b6 This is the de facto library for machine learning. It will have all of the standard algorithms for machine learning. They also have a lot of utilities that can be useful when preprocessing or training your algorithms. The API (the famous .fit() , .predict() , and .transform() ) is great and has been adopted in many other machine learning packages. There is also no other tutorial that comes close to being helpful as the documentation. Resources Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 5 - Machine Learning Creating your own estimator in scikit-learn - Daniel Hnyk (2015) Pandas \u00b6 The also most fundamental package that's often used in the preprocessing chain is the fundamental datastructure known as a pandas table. Taking inspiration from R, this features data with meaningful meta data attached to it as columns or as rows. It also allows entries other than floats and ints. The best part is that is has reallt fast routines to do data manipulation and advanced calculations. This is the hardest package to get accustomed to compared to the previous packages but with a little time and effort, you can easily become one of the most effective data scientists around. The documentation is excellent but I don't find it as friendly as the other packages. Because it has a bit of a learning curve and there are likely more than 5 ways to do what you want to do, you'll find a lot of tutorials online. Resources Documentation Chris Albon Snippets Python Data Science Handbook - Jake Vanderplas - Chapter 3 - Intro to Pandas Greg Reda 3 part Tutorial I - Intro to Pandas Structures II - Working with DataFrames III - Using Pandas with the MovieLens Dataset Tom Augspurger - 7-Part Tutorial - Modern Pandas Chris Fonnesneck Class - Advanced Statistical Computing Class Matplotlib \u00b6 This is the default plotting library for python. Most people have a love-hate relationship with this library. I find it very easy to plot things but it gets difficult when I need to modify something down to the T. Like pandas, I think this library has a bit of a learning curve if you really want to make kick-ass plots. The documentation is good but the problem is I don't understand the data structures or language for creating plots; creating plotting libraries is quite difficult for this very reason. This is a great library but prepared to be a bit frustrated at times down the road. Resources Matplotlib Gallery Anatomy of Matplotlib An Inquiry Into Matplotlib's Figures Python Plotting with Matplotlib - Real Python Python Data Science Handbook - Jake Vanderplas - Chapter 4 - Visualization with Matplotlib Creating Publication-Quality Figures with Matplotlib Matplotlib Cheatsheet","title":"Standard"},{"location":"resources/python/software_stacks/standard_stack/#standard-python-stack","text":"For the most part, you'll see this at the top of everyone's scientific computing notebook/script: import numpy as np import pandas as pd from scipy.stats import norm from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt This is the impact of these libraries on the python community. By far, the most mature, the most robust and have the most documentation. In principle, you should be able to do almost any kind of scientific computing from start to finish with these libraries. Below I list them and I give the most useful resources that I have use (and still use). Just remember, when in doubt, stackoverflow is your best friend.","title":"Standard Python Stack"},{"location":"resources/python/software_stacks/standard_stack/#numpy","text":"The standard library for utilizing the most fundamental data structure for scientific computing: the array. It also has many linear algebra routines that have been optimized in C/C++ behind the scences. Often times doing things in raw python can get a massive speed up by doing things with numpy. The must have package for everyones python stack. It also has some of the best documentation for python packages. Tutorials Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 2 - Intro to Numpy Intro to Numpy A Visual Guide to Numpy From Python to Numpy 100 Exercises in Numpy Broadcasting Tutorial Einsum - I | II","title":"Numpy"},{"location":"resources/python/software_stacks/standard_stack/#scipy","text":"The other defacto library for doing scientific computing. This package is quite heavily linked with numpy but it does have it's own suite of routines. This library also has linear algebra routines including sparse matrices. But it also has signal processing, probability and statistics and optimization functions. Another library with some of the best documentation. Resources Documentation Scipy Lecture Notes","title":"Scipy"},{"location":"resources/python/software_stacks/standard_stack/#scikit-learn","text":"This is the de facto library for machine learning. It will have all of the standard algorithms for machine learning. They also have a lot of utilities that can be useful when preprocessing or training your algorithms. The API (the famous .fit() , .predict() , and .transform() ) is great and has been adopted in many other machine learning packages. There is also no other tutorial that comes close to being helpful as the documentation. Resources Documentation Python Data Science Handbook - Jake Vanderplas - Chapter 5 - Machine Learning Creating your own estimator in scikit-learn - Daniel Hnyk (2015)","title":"Scikit-Learn"},{"location":"resources/python/software_stacks/standard_stack/#pandas","text":"The also most fundamental package that's often used in the preprocessing chain is the fundamental datastructure known as a pandas table. Taking inspiration from R, this features data with meaningful meta data attached to it as columns or as rows. It also allows entries other than floats and ints. The best part is that is has reallt fast routines to do data manipulation and advanced calculations. This is the hardest package to get accustomed to compared to the previous packages but with a little time and effort, you can easily become one of the most effective data scientists around. The documentation is excellent but I don't find it as friendly as the other packages. Because it has a bit of a learning curve and there are likely more than 5 ways to do what you want to do, you'll find a lot of tutorials online. Resources Documentation Chris Albon Snippets Python Data Science Handbook - Jake Vanderplas - Chapter 3 - Intro to Pandas Greg Reda 3 part Tutorial I - Intro to Pandas Structures II - Working with DataFrames III - Using Pandas with the MovieLens Dataset Tom Augspurger - 7-Part Tutorial - Modern Pandas Chris Fonnesneck Class - Advanced Statistical Computing Class","title":"Pandas"},{"location":"resources/python/software_stacks/standard_stack/#matplotlib","text":"This is the default plotting library for python. Most people have a love-hate relationship with this library. I find it very easy to plot things but it gets difficult when I need to modify something down to the T. Like pandas, I think this library has a bit of a learning curve if you really want to make kick-ass plots. The documentation is good but the problem is I don't understand the data structures or language for creating plots; creating plotting libraries is quite difficult for this very reason. This is a great library but prepared to be a bit frustrated at times down the road. Resources Matplotlib Gallery Anatomy of Matplotlib An Inquiry Into Matplotlib's Figures Python Plotting with Matplotlib - Real Python Python Data Science Handbook - Jake Vanderplas - Chapter 4 - Visualization with Matplotlib Creating Publication-Quality Figures with Matplotlib Matplotlib Cheatsheet","title":"Matplotlib"},{"location":"resources/tips/","text":"Cheat Sheets \u00b6","title":"Cheat Sheets"},{"location":"resources/tips/#cheat-sheets","text":"","title":"Cheat Sheets"},{"location":"resources/tips/conda/","text":"Conda \u00b6","title":"Conda"},{"location":"resources/tips/conda/#conda","text":"","title":"Conda"},{"location":"resources/tips/github/","text":"DOI for a Repository - GitHub","title":"Github"},{"location":"resources/tips/sklearn/","text":"Scikit-Learn \u00b6","title":"Scikit-Learn"},{"location":"resources/tips/sklearn/#scikit-learn","text":"","title":"Scikit-Learn"},{"location":"snippets/xarray/","text":"XArray \u00b6 Seasonal Grouping \u00b6 Extract Time Series (from Location) \u00b6 time_series = data . isel ( x = 1000 , y = 1000 ) . to_pandas () . dropna () Mean Across Multiple Dimensions \u00b6 data . mean ( dim = [ 'lat' , 'lon' ])","title":"XArray"},{"location":"snippets/xarray/#xarray","text":"","title":"XArray"},{"location":"snippets/xarray/#seasonal-grouping","text":"","title":"Seasonal Grouping"},{"location":"snippets/xarray/#extract-time-series-from-location","text":"time_series = data . isel ( x = 1000 , y = 1000 ) . to_pandas () . dropna ()","title":"Extract Time Series (from Location)"},{"location":"snippets/xarray/#mean-across-multiple-dimensions","text":"data . mean ( dim = [ 'lat' , 'lon' ])","title":"Mean Across Multiple Dimensions"},{"location":"snippets/bash/args/","text":"Arguments in Scripts \u00b6 #!/bin/bash ### Print total arguments and their values echo \"Total Arguments:\" $# echo \"All Arguments values:\" $@ ### Command arguments can be accessed as echo \"First->\" $1 echo \"Second->\" $2 # You can also access all arguments in an array and use them in a script. args =( \" $@ \" ) echo \"First->\" ${ args [0] } echo \"Second->\" ${ args [1] }","title":"Arguments in Scripts"},{"location":"snippets/bash/args/#arguments-in-scripts","text":"#!/bin/bash ### Print total arguments and their values echo \"Total Arguments:\" $# echo \"All Arguments values:\" $@ ### Command arguments can be accessed as echo \"First->\" $1 echo \"Second->\" $2 # You can also access all arguments in an array and use them in a script. args =( \" $@ \" ) echo \"First->\" ${ args [0] } echo \"Second->\" ${ args [1] }","title":"Arguments in Scripts"},{"location":"snippets/bash/loops/","text":"Loops \u00b6 For Loops \u00b6 #!/usr/bin/env bash for i in 1 2 3 4 5 do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10 } do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10..2 } do echo \"Doing $i now\" done Source","title":"Loops"},{"location":"snippets/bash/loops/#loops","text":"","title":"Loops"},{"location":"snippets/bash/loops/#for-loops","text":"#!/usr/bin/env bash for i in 1 2 3 4 5 do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10 } do echo \"Doing $i now\" done #!/usr/bin/env bash for i in { 1 ..10..2 } do echo \"Doing $i now\" done Source","title":"For Loops"},{"location":"snippets/bash/run_scripts/","text":"Running Subsequent Scripts \u00b6 Case I: Run Script 1, Wait, Run Script 2 \u00b6 For this, we want to wait for [script1.py](http://script1.py) to finish successfully, then we run script2.py . #!/usr/bin/env bash python script1.py & python script2.py Case II: Run Script 1, Wait, Run Script 2 IFF Script 1 has failed \u00b6 This is the case where we want to run [script2.py](http://script2.py) but IFF [script1.py](http://script1.py) has finished #!/usr/bin/env bash python script1.py || python script2.py Case III: Run script1 AND script 2 \u00b6 We want to run both scripts concurrently as background processes #!/usr/bin/env bash python script1.py & python script2.py","title":"Running Subsequent Scripts"},{"location":"snippets/bash/run_scripts/#running-subsequent-scripts","text":"","title":"Running Subsequent Scripts"},{"location":"snippets/bash/run_scripts/#case-i-run-script-1-wait-run-script-2","text":"For this, we want to wait for [script1.py](http://script1.py) to finish successfully, then we run script2.py . #!/usr/bin/env bash python script1.py & python script2.py","title":"Case I: Run Script 1, Wait, Run Script 2"},{"location":"snippets/bash/run_scripts/#case-ii-run-script-1-wait-run-script-2-iff-script-1-has-failed","text":"This is the case where we want to run [script2.py](http://script2.py) but IFF [script1.py](http://script1.py) has finished #!/usr/bin/env bash python script1.py || python script2.py","title":"Case II: Run Script 1, Wait, Run Script 2 IFF Script 1 has failed"},{"location":"snippets/bash/run_scripts/#case-iii-run-script1-and-script-2","text":"We want to run both scripts concurrently as background processes #!/usr/bin/env bash python script1.py & python script2.py","title":"Case III: Run script1 AND script 2"},{"location":"snippets/bayesian/sklearn_egp/","text":"Error in GPs in Sklearn \u00b6 Example in AstroML = Example | Example II","title":"Error in GPs in Sklearn"},{"location":"snippets/bayesian/sklearn_egp/#error-in-gps-in-sklearn","text":"Example in AstroML = Example | Example II","title":"Error in GPs in Sklearn"},{"location":"snippets/earth/cart_geo/","text":"Cartesian Coordinates 2 Geocoordinates \u00b6 Geocoordinates 2 Cartesian Coordinates \u00b6 import pandas as pd import numpy as np def geo_2_cartesian ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transforms geo coordinates (lat, lon) to cartesian coordinates (x, y, z). Parameters ---------- df : pd.DataFrame A dataframe with the geo coordinates values. The columns need to have the following ['lat', 'lon] Returns ------- df : pd.DataFrame A dataframe with the converted values. Example ------- >> df = geo_2_cartesian(df) \"\"\" cols = df . columns . tolist () if \"lat\" not in cols or \"lon\" not in cols : print ( \"lat,lon columns not present in df.\" ) return df # approximate earth radius earth_radius = 6371 # transform from degrees to radians # df = df.apply(lambda x: np.deg2rad(x) if x.name in ['lat', 'lat'] else x) df [ \"lat\" ] = np . deg2rad ( df [ \"lat\" ]) df [ \"lon\" ] = np . deg2rad ( df [ \"lon\" ]) # From Geo coords to cartesian coords df [ \"x\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . cos ( df [ \"lon\" ]) df [ \"y\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . sin ( df [ \"lon\" ]) df [ \"z\" ] = earth_radius * np . sin ( df [ \"lat\" ]) # drop original lat,lon columns df = df . drop ([ \"lat\" , \"lon\" ], axis = 1 ) return df Cartesian Coordinates 2 Geocoordinates \u00b6 def cartesian_2_geo ( x : np . ndarray , y : np . ndarray , z : np . ndarray ): R = 6371 # radius of the earth lat = np . degrees ( np . arcsin ( z / R )) lon = np . degrees ( np . arctan2 ( y , x )) return lat , lon Source : Blog | Stackoverflow","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/earth/cart_geo/#cartesian-coordinates-2-geocoordinates","text":"","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/earth/cart_geo/#geocoordinates-2-cartesian-coordinates","text":"import pandas as pd import numpy as np def geo_2_cartesian ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transforms geo coordinates (lat, lon) to cartesian coordinates (x, y, z). Parameters ---------- df : pd.DataFrame A dataframe with the geo coordinates values. The columns need to have the following ['lat', 'lon] Returns ------- df : pd.DataFrame A dataframe with the converted values. Example ------- >> df = geo_2_cartesian(df) \"\"\" cols = df . columns . tolist () if \"lat\" not in cols or \"lon\" not in cols : print ( \"lat,lon columns not present in df.\" ) return df # approximate earth radius earth_radius = 6371 # transform from degrees to radians # df = df.apply(lambda x: np.deg2rad(x) if x.name in ['lat', 'lat'] else x) df [ \"lat\" ] = np . deg2rad ( df [ \"lat\" ]) df [ \"lon\" ] = np . deg2rad ( df [ \"lon\" ]) # From Geo coords to cartesian coords df [ \"x\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . cos ( df [ \"lon\" ]) df [ \"y\" ] = earth_radius * np . cos ( df [ \"lat\" ]) * np . sin ( df [ \"lon\" ]) df [ \"z\" ] = earth_radius * np . sin ( df [ \"lat\" ]) # drop original lat,lon columns df = df . drop ([ \"lat\" , \"lon\" ], axis = 1 ) return df","title":"Geocoordinates 2 Cartesian Coordinates"},{"location":"snippets/earth/cart_geo/#cartesian-coordinates-2-geocoordinates_1","text":"def cartesian_2_geo ( x : np . ndarray , y : np . ndarray , z : np . ndarray ): R = 6371 # radius of the earth lat = np . degrees ( np . arcsin ( z / R )) lon = np . degrees ( np . arctan2 ( y , x )) return lat , lon Source : Blog | Stackoverflow","title":"Cartesian Coordinates 2 Geocoordinates"},{"location":"snippets/ml/anomaly/","text":"Anomaly Detection \u00b6 # define threshold instances = 4 # 4% # calculate densityes densities = model . score_samples ( X ) # calculate density threshold density_threshold = np . percentile ( densities , instances ) # reproduce the anomalies anomalies = X [ densities < densities_threshold ] For more examples, see the pyOD documentation. In particular: predict_proba - predict the probability of a sample being an outlier. predict - predict if a sample is an outlier or not.","title":"Anomaly Detection"},{"location":"snippets/ml/anomaly/#anomaly-detection","text":"# define threshold instances = 4 # 4% # calculate densityes densities = model . score_samples ( X ) # calculate density threshold density_threshold = np . percentile ( densities , instances ) # reproduce the anomalies anomalies = X [ densities < densities_threshold ] For more examples, see the pyOD documentation. In particular: predict_proba - predict the probability of a sample being an outlier. predict - predict if a sample is an outlier or not.","title":"Anomaly Detection"},{"location":"snippets/numpy/bisection/","text":"Bisection search \u00b6","title":"Bisection search"},{"location":"snippets/numpy/bisection/#bisection-search","text":"","title":"Bisection search"},{"location":"snippets/numpy/euclidean/","text":"Efficient Euclidean Distance Calculation - Numpy Einsum \u00b6 This script uses numpy's einsum function to calculate the euclidean distance. Resources: import numpy as np def euclidean_distance_einsum ( X , Y ): \"\"\"Efficiently calculates the euclidean distance between two vectors using Numpys einsum function. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) Returns ------- D : array, (n_samples, n_samples) \"\"\" XX = np . einsum ( 'ij,ij->i' , X , X )[:, np . newaxis ] YY = np . einsum ( 'ij,ij->i' , Y , Y ) # XY = 2 * np.einsum('ij,kj->ik', X, Y) XY = 2 * np . dot ( X , Y . T ) return XX + YY - XY An alternative way per stackoverflow would be to do it in one shot. Sources How to calculate euclidean distance between pair of rows of a numpy array Calculate Distance between numpy arrays einsum and distance calculations How can the Euclidean distance be calculated with NumPy? Using Python numpy einsum to obtain dot product between 2 Matrices High-Performance computation in Python | NumPy","title":"Efficient Euclidean Distance Calculation - Numpy Einsum"},{"location":"snippets/numpy/euclidean/#efficient-euclidean-distance-calculation-numpy-einsum","text":"This script uses numpy's einsum function to calculate the euclidean distance. Resources: import numpy as np def euclidean_distance_einsum ( X , Y ): \"\"\"Efficiently calculates the euclidean distance between two vectors using Numpys einsum function. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) Returns ------- D : array, (n_samples, n_samples) \"\"\" XX = np . einsum ( 'ij,ij->i' , X , X )[:, np . newaxis ] YY = np . einsum ( 'ij,ij->i' , Y , Y ) # XY = 2 * np.einsum('ij,kj->ik', X, Y) XY = 2 * np . dot ( X , Y . T ) return XX + YY - XY An alternative way per stackoverflow would be to do it in one shot. Sources How to calculate euclidean distance between pair of rows of a numpy array Calculate Distance between numpy arrays einsum and distance calculations How can the Euclidean distance be calculated with NumPy? Using Python numpy einsum to obtain dot product between 2 Matrices High-Performance computation in Python | NumPy","title":"Efficient Euclidean Distance Calculation - Numpy Einsum"},{"location":"snippets/numpy/group_sum/","text":"Add every n values in array \u00b6 This is the case where you want to add every 3. So for example: a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] We want the following if we add every 3 values: b = [ 1 + 2 + 3 , 4 + 5 + 6 , 7 + 8 + 9 ] b = [ 6 , 14 , 24 ] We can do this by reshaping the array Make sure it's at minimum, a 2D array. a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) . reshape ( - 1 , 1 ) print ( a . shape ) (9,1) Reshape via a chunksize chunk_size = 3 a = a . reshape ( - 1 , chunk_size , a . shape [ 1 ]) print ( a . shape ) (3, 3, 1) Sum the middle column. a = a . sum ( 1 ) print ( a . shape ) print ( a . squeeze ()) (3, 1) [ 6 15 24] Note : the size of a has to be divisible by the chunk-size. So in our case 9/3=3 so we're good. But this wouldn't work for 10/3 because we have some remainder. Source : StackOverFlow In One Shot \u00b6 We can do this in one shot using the numpy built-in function: import numpy as np chunk_size = 3 # 1d arrays a = np . ones (( 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size )) # n-d arrays a = np . ones (( 9 , 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size ), axis = 0 )","title":"Add every n values in array"},{"location":"snippets/numpy/group_sum/#add-every-n-values-in-array","text":"This is the case where you want to add every 3. So for example: a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] We want the following if we add every 3 values: b = [ 1 + 2 + 3 , 4 + 5 + 6 , 7 + 8 + 9 ] b = [ 6 , 14 , 24 ] We can do this by reshaping the array Make sure it's at minimum, a 2D array. a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) . reshape ( - 1 , 1 ) print ( a . shape ) (9,1) Reshape via a chunksize chunk_size = 3 a = a . reshape ( - 1 , chunk_size , a . shape [ 1 ]) print ( a . shape ) (3, 3, 1) Sum the middle column. a = a . sum ( 1 ) print ( a . shape ) print ( a . squeeze ()) (3, 1) [ 6 15 24] Note : the size of a has to be divisible by the chunk-size. So in our case 9/3=3 so we're good. But this wouldn't work for 10/3 because we have some remainder. Source : StackOverFlow","title":"Add every n values in array"},{"location":"snippets/numpy/group_sum/#in-one-shot","text":"We can do this in one shot using the numpy built-in function: import numpy as np chunk_size = 3 # 1d arrays a = np . ones (( 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size )) # n-d arrays a = np . ones (( 9 , 9 )) a = np . add . reduceat ( a , np . arange ( 0 , len ( a ), chunk_size ), axis = 0 )","title":"In One Shot"},{"location":"snippets/numpy/rbf_kernel/","text":"Optimized RBF kernel using numexpr \u00b6 A fast implementation of the RBF Kernel using numexpr. Using the fact that: ||x-y|| 2 = ||x|| 2 + ||y|| 2 - 2 * x T * y Resources * Fast Implementation - StackOverFlow import numpy as np import numexpr as ne from sklearn.metrics import euclidean_distances from sklearn.check def rbf_kernel_ne ( X , Y = None , length_scale = 1.0 , signal_variance = 1.0 ): \"\"\"This function calculates the RBF kernel. It has been optimized using some advice found online. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) length_scale : float, default: 1.0 signal_variance : float, default: 1.0 Returns ------- K : array, (n_samples x d_dimensions) Resources --------- StackOverFlow: https://goo.gl/FXbgkj \"\"\" X_norm = np . einsum ( 'ij,ij->i' , X , X ) if Y is not None : Y_norm = np . einsum ( 'ij,ij->i' , Y , Y ) else : Y = X Y_norm = X_norm K = ne . evaluate ( 'v * exp(-g * (A + B - 2 * C))' , { 'A' : X_norm [:, None ], 'B' : Y_norm [ None , :], 'C' : np . dot ( X , Y . T ), 'g' : 1 / ( 2 * length_scale ** 2 ), 'v' : signal_variance }) return K def rbf_kernel ( X , Y = None , signal_variance = 1.0 , length_scale = 1.0 ): \"\"\" Compute the rbf (gaussian) kernel between X and Y:: K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the :ref:`User Guide <rbf_kernel>`. Parameters ---------- X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features Returns ------- kernel_matrix : array of shape (n_samples_X, n_samples_Y) \"\"\" X , Y = check_pairwise_arrays ( X , Y ) X /= length_scale Y /= length_scale K = - 0.5 * euclidean_distances ( X , Y , squared = True ) np . exp ( K , K ) # exponentiate K in-place K *= signal_variance # multiply by signal_variance return K","title":"Optimized RBF kernel using `numexpr`"},{"location":"snippets/numpy/rbf_kernel/#optimized-rbf-kernel-using-numexpr","text":"A fast implementation of the RBF Kernel using numexpr. Using the fact that: ||x-y|| 2 = ||x|| 2 + ||y|| 2 - 2 * x T * y Resources * Fast Implementation - StackOverFlow import numpy as np import numexpr as ne from sklearn.metrics import euclidean_distances from sklearn.check def rbf_kernel_ne ( X , Y = None , length_scale = 1.0 , signal_variance = 1.0 ): \"\"\"This function calculates the RBF kernel. It has been optimized using some advice found online. Parameters ---------- X : array, (n_samples x d_dimensions) Y : array, (n_samples x d_dimensions) length_scale : float, default: 1.0 signal_variance : float, default: 1.0 Returns ------- K : array, (n_samples x d_dimensions) Resources --------- StackOverFlow: https://goo.gl/FXbgkj \"\"\" X_norm = np . einsum ( 'ij,ij->i' , X , X ) if Y is not None : Y_norm = np . einsum ( 'ij,ij->i' , Y , Y ) else : Y = X Y_norm = X_norm K = ne . evaluate ( 'v * exp(-g * (A + B - 2 * C))' , { 'A' : X_norm [:, None ], 'B' : Y_norm [ None , :], 'C' : np . dot ( X , Y . T ), 'g' : 1 / ( 2 * length_scale ** 2 ), 'v' : signal_variance }) return K def rbf_kernel ( X , Y = None , signal_variance = 1.0 , length_scale = 1.0 ): \"\"\" Compute the rbf (gaussian) kernel between X and Y:: K(x, y) = exp(-gamma ||x-y||^2) for each pair of rows x in X and y in Y. Read more in the :ref:`User Guide <rbf_kernel>`. Parameters ---------- X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features Returns ------- kernel_matrix : array of shape (n_samples_X, n_samples_Y) \"\"\" X , Y = check_pairwise_arrays ( X , Y ) X /= length_scale Y /= length_scale K = - 0.5 * euclidean_distances ( X , Y , squared = True ) np . exp ( K , K ) # exponentiate K in-place K *= signal_variance # multiply by signal_variance return K","title":"Optimized RBF kernel using numexpr"},{"location":"snippets/python/conda/","text":"Conda \u00b6","title":"Conda"},{"location":"snippets/python/conda/#conda","text":"","title":"Conda"},{"location":"snippets/python/conda_envs/","text":"My Typical Conda Environments \u00b6","title":"My Typical Conda Environments"},{"location":"snippets/python/conda_envs/#my-typical-conda-environments","text":"","title":"My Typical Conda Environments"},{"location":"snippets/python/dict_2_list/","text":"Dictionaries to Lists \u00b6 Full Function \"\"\" NOTE! Need the mypy (typing) package >>> !pip install mypy \"\"\" import itertools from typing import Dict , List def dict_product ( dicts : Dict ) -> List [ Dict ]: \"\"\"Returns the product of a dictionary with lists Parameters ---------- dicts : Dict, a dictionary where each key has a list of inputs Returns ------- prod : List[Dict] the list of dictionary products Example ------- >>> parameters = { \"samples\": [100, 1_000, 10_000], \"dimensions\": [2, 3, 10, 100, 1_000] } >>> parameters = list(dict_product(parameters)) >>> parameters [{'samples': 100, 'dimensions': 2}, {'samples': 100, 'dimensions': 3}, {'samples': 1000, 'dimensions': 2}, {'samples': 1000, 'dimensions': 3}, {'samples': 10000, 'dimensions': 2}, {'samples': 10000, 'dimensions': 3}] \"\"\" return ( dict ( zip ( dicts . keys (), x )) for x in itertools . product ( * dicts . values ())) Resources : Using itertools.product with dictionaries - Stephan Tulkens Using itertools.product instead of nested for loops - Stephan Tulkens","title":"Dictionaries to Lists"},{"location":"snippets/python/dict_2_list/#dictionaries-to-lists","text":"Full Function \"\"\" NOTE! Need the mypy (typing) package >>> !pip install mypy \"\"\" import itertools from typing import Dict , List def dict_product ( dicts : Dict ) -> List [ Dict ]: \"\"\"Returns the product of a dictionary with lists Parameters ---------- dicts : Dict, a dictionary where each key has a list of inputs Returns ------- prod : List[Dict] the list of dictionary products Example ------- >>> parameters = { \"samples\": [100, 1_000, 10_000], \"dimensions\": [2, 3, 10, 100, 1_000] } >>> parameters = list(dict_product(parameters)) >>> parameters [{'samples': 100, 'dimensions': 2}, {'samples': 100, 'dimensions': 3}, {'samples': 1000, 'dimensions': 2}, {'samples': 1000, 'dimensions': 3}, {'samples': 10000, 'dimensions': 2}, {'samples': 10000, 'dimensions': 3}] \"\"\" return ( dict ( zip ( dicts . keys (), x )) for x in itertools . product ( * dicts . values ())) Resources : Using itertools.product with dictionaries - Stephan Tulkens Using itertools.product instead of nested for loops - Stephan Tulkens","title":"Dictionaries to Lists"},{"location":"snippets/python/lists_tricks/","text":"Tricks with Lists \u00b6 Unpacking a List of tuples \u00b6 I've found this useful in situations where I need to do parallel processing and my output is a tuple of 2 elements. So for example, let's have the following. list_of_tuples = ( '1' , 'a' ), ( '2' , 'b' ), ( '3' , 'c' ), ( '4' , 'd' ) Now I would like to unpack this into 2 lists. I can do this like so: list1 , list2 = zip ( * list_of_tuples ) So if I print the lists, I will get the following: list1 , list2 ('1', '2', '3', '4'), ('a', 'b', 'c', 'd')","title":"Tricks with Lists"},{"location":"snippets/python/lists_tricks/#tricks-with-lists","text":"","title":"Tricks with Lists"},{"location":"snippets/python/lists_tricks/#unpacking-a-list-of-tuples","text":"I've found this useful in situations where I need to do parallel processing and my output is a tuple of 2 elements. So for example, let's have the following. list_of_tuples = ( '1' , 'a' ), ( '2' , 'b' ), ( '3' , 'c' ), ( '4' , 'd' ) Now I would like to unpack this into 2 lists. I can do this like so: list1 , list2 = zip ( * list_of_tuples ) So if I print the lists, I will get the following: list1 , list2 ('1', '2', '3', '4'), ('a', 'b', 'c', 'd')","title":"Unpacking a List of tuples"},{"location":"snippets/python/parallel/","text":"Parallel Processing \u00b6 Multiprocessing Module \u00b6 === info \"Full Function\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ```python from joblib import Parallel, delayed from typing import Callable, Iterable def run_parallel_step( exp_step: Callable, parameters: Iterable, n_jobs: int = 2, verbose: int = 1, **kwargs ) -> List: \"\"\"Helper function to run experimental loops in parallel Parameters ---------- exp_step : Callable a callable function which does each experimental step parameters : Iterable, an iterable (List, Dict, etc) of parameters to be looped through n_jobs : int, default=2 the number of cores to use verbose : int, default=1 the amount of information to display in the Returns ------- results : List list of the results from the function Examples -------- Example 1 - No keyword arguments >>> parameters = [1, 10, 100] >>> def step(x): return x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1 ) >>> results [1, 100, 10000] Example II: Keyword arguments >>> parameters = [1, 10, 100] >>> def step(x, a=1.0): return a * x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1, a=10 ) >>> results [100, 10000, 1000000] \"\"\" # loop through parameters results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) return results ```","title":"Parallel Processing"},{"location":"snippets/python/parallel/#parallel-processing","text":"","title":"Parallel Processing"},{"location":"snippets/python/parallel/#multiprocessing-module","text":"=== info \"Full Function\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ```python from joblib import Parallel, delayed from typing import Callable, Iterable def run_parallel_step( exp_step: Callable, parameters: Iterable, n_jobs: int = 2, verbose: int = 1, **kwargs ) -> List: \"\"\"Helper function to run experimental loops in parallel Parameters ---------- exp_step : Callable a callable function which does each experimental step parameters : Iterable, an iterable (List, Dict, etc) of parameters to be looped through n_jobs : int, default=2 the number of cores to use verbose : int, default=1 the amount of information to display in the Returns ------- results : List list of the results from the function Examples -------- Example 1 - No keyword arguments >>> parameters = [1, 10, 100] >>> def step(x): return x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1 ) >>> results [1, 100, 10000] Example II: Keyword arguments >>> parameters = [1, 10, 100] >>> def step(x, a=1.0): return a * x ** 2 >>> results = run_parallel_step( exp_step=step, parameters=parameters, n_jobs=1, verbose=1, a=10 ) >>> results [100, 10000, 1000000] \"\"\" # loop through parameters results = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(exp_step)(iparam, **kwargs) for iparam in parameters ) return results ```","title":"Multiprocessing Module"},{"location":"snippets/python/setup/","text":"My Setup File \u00b6 #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pipenv install twine --dev import io import os import sys from shutil import rmtree from setuptools import Command , find_packages , setup # Package meta-data. NAME = \"rbig\" DESCRIPTION = \"Gaussianization Flows in Python.\" URL = \"https://github.com/jejjohnson/rbig\" EMAIL = \"jemanjohnson34@gmail.com\" AUTHOR = \"J. Emmanuel Johnson\" REQUIRES_PYTHON = \">=3.8.0\" VERSION = \"1.1.0\" # Keywords KEYWORDS = [ \"machine learning python scikit-learn gaussianization\" ] # What packages are required for this module to be executed? REQUIRED = [ \"numpy\" , \"scipy\" , \"scikit-learn\" ,] # What packages are optional? EXTRAS = { \"dev\" : [ \"flake8\" , \"pylint\" , # checkers \"black\" , \"isort\" , # formatters \"mypy\" , # Type checking ], \"notebooks\" : [ \"ipykernel\" # Notebooks ], \"examples\" : [ \"matplotlib\" , \"seaborn\" # almost always have plots ], \"tests\" : [ \"pytest\" ], # test library } # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os . path . abspath ( os . path . dirname ( __file__ )) # Import the README and use it as the long-description. # Note: this will only work if 'README.md' is present in your MANIFEST.in file! try : with io . open ( os . path . join ( here , \"README.md\" ), encoding = \"utf-8\" ) as f : long_description = \" \\n \" + f . read () except FileNotFoundError : long_description = DESCRIPTION # Load the package's __version__.py module as a dictionary. about = {} if not VERSION : project_slug = NAME . lower () . replace ( \"-\" , \"_\" ) . replace ( \" \" , \"_\" ) with open ( os . path . join ( here , project_slug , \"__version__.py\" )) as f : exec ( f . read (), about ) else : about [ \"__version__\" ] = VERSION class UploadCommand ( Command ): \"\"\"Support setup.py upload.\"\"\" description = \"Build and publish the package.\" user_options = [] @staticmethod def status ( s ): \"\"\"Prints things in bold.\"\"\" print ( \" \\033 [1m {0} \\033 [0m\" . format ( s )) def initialize_options ( self ): pass def finalize_options ( self ): pass def run ( self ): try : self . status ( \"Removing previous builds\u2026\" ) rmtree ( os . path . join ( here , \"dist\" )) except OSError : pass self . status ( \"Building Source and Wheel (universal) distribution\u2026\" ) os . system ( \" {0} setup.py sdist bdist_wheel --universal\" . format ( sys . executable )) self . status ( \"Uploading the package to PyPI via Twine\u2026\" ) os . system ( \"twine upload dist/*\" ) self . status ( \"Pushing git tags\u2026\" ) os . system ( \"git tag v {0} \" . format ( about [ \"__version__\" ])) os . system ( \"git push --tags\" ) sys . exit () # Where the magic happens: setup ( name = NAME , version = about [ \"__version__\" ], description = DESCRIPTION , long_description = long_description , long_description_content_type = \"text/markdown\" , author = AUTHOR , author_email = EMAIL , python_requires = REQUIRES_PYTHON , url = URL , packages = find_packages ( exclude = [ \"tests\" , \"*.tests\" , \"*.tests.*\" , \"tests.*\" ]), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, setup_requires = [ \"setuptools-yaml\" ], metadata_yaml = \"environment.yml\" , install_requires = REQUIRED , extras_require = EXTRAS , include_package_data = True , license = \"MIT\" , keywords = KEYWORDS , classifiers = [ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers \"Development Status :: 3 - Alpha\" , \"License :: OSI Approved :: MIT License\" , \"Intended Audience :: Science/Research\" , \"Programming Language :: Python\" , \"Programming Language :: Python :: 3.8\" , ], # $ setup.py publish support. cmdclass = { \"upload\" : UploadCommand }, )","title":"My Setup File"},{"location":"snippets/python/setup/#my-setup-file","text":"#!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pipenv install twine --dev import io import os import sys from shutil import rmtree from setuptools import Command , find_packages , setup # Package meta-data. NAME = \"rbig\" DESCRIPTION = \"Gaussianization Flows in Python.\" URL = \"https://github.com/jejjohnson/rbig\" EMAIL = \"jemanjohnson34@gmail.com\" AUTHOR = \"J. Emmanuel Johnson\" REQUIRES_PYTHON = \">=3.8.0\" VERSION = \"1.1.0\" # Keywords KEYWORDS = [ \"machine learning python scikit-learn gaussianization\" ] # What packages are required for this module to be executed? REQUIRED = [ \"numpy\" , \"scipy\" , \"scikit-learn\" ,] # What packages are optional? EXTRAS = { \"dev\" : [ \"flake8\" , \"pylint\" , # checkers \"black\" , \"isort\" , # formatters \"mypy\" , # Type checking ], \"notebooks\" : [ \"ipykernel\" # Notebooks ], \"examples\" : [ \"matplotlib\" , \"seaborn\" # almost always have plots ], \"tests\" : [ \"pytest\" ], # test library } # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os . path . abspath ( os . path . dirname ( __file__ )) # Import the README and use it as the long-description. # Note: this will only work if 'README.md' is present in your MANIFEST.in file! try : with io . open ( os . path . join ( here , \"README.md\" ), encoding = \"utf-8\" ) as f : long_description = \" \\n \" + f . read () except FileNotFoundError : long_description = DESCRIPTION # Load the package's __version__.py module as a dictionary. about = {} if not VERSION : project_slug = NAME . lower () . replace ( \"-\" , \"_\" ) . replace ( \" \" , \"_\" ) with open ( os . path . join ( here , project_slug , \"__version__.py\" )) as f : exec ( f . read (), about ) else : about [ \"__version__\" ] = VERSION class UploadCommand ( Command ): \"\"\"Support setup.py upload.\"\"\" description = \"Build and publish the package.\" user_options = [] @staticmethod def status ( s ): \"\"\"Prints things in bold.\"\"\" print ( \" \\033 [1m {0} \\033 [0m\" . format ( s )) def initialize_options ( self ): pass def finalize_options ( self ): pass def run ( self ): try : self . status ( \"Removing previous builds\u2026\" ) rmtree ( os . path . join ( here , \"dist\" )) except OSError : pass self . status ( \"Building Source and Wheel (universal) distribution\u2026\" ) os . system ( \" {0} setup.py sdist bdist_wheel --universal\" . format ( sys . executable )) self . status ( \"Uploading the package to PyPI via Twine\u2026\" ) os . system ( \"twine upload dist/*\" ) self . status ( \"Pushing git tags\u2026\" ) os . system ( \"git tag v {0} \" . format ( about [ \"__version__\" ])) os . system ( \"git push --tags\" ) sys . exit () # Where the magic happens: setup ( name = NAME , version = about [ \"__version__\" ], description = DESCRIPTION , long_description = long_description , long_description_content_type = \"text/markdown\" , author = AUTHOR , author_email = EMAIL , python_requires = REQUIRES_PYTHON , url = URL , packages = find_packages ( exclude = [ \"tests\" , \"*.tests\" , \"*.tests.*\" , \"tests.*\" ]), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, setup_requires = [ \"setuptools-yaml\" ], metadata_yaml = \"environment.yml\" , install_requires = REQUIRED , extras_require = EXTRAS , include_package_data = True , license = \"MIT\" , keywords = KEYWORDS , classifiers = [ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers \"Development Status :: 3 - Alpha\" , \"License :: OSI Approved :: MIT License\" , \"Intended Audience :: Science/Research\" , \"Programming Language :: Python\" , \"Programming Language :: Python :: 3.8\" , ], # $ setup.py publish support. cmdclass = { \"upload\" : UploadCommand }, )","title":"My Setup File"},{"location":"snippets/pytorch/device/","text":"Device Agnostic \u00b6 import argparse import torch parser = argparse . ArgumentParser ( description = 'PyTorch Example' ) parser . add_argument ( '--disable-cuda' , action = 'store_true' , help = 'Disable CUDA' ) args = parser . parse_args () args . device = None if not args . disable_cuda and torch . cuda . is_available (): args . device = torch . device ( 'cuda' ) else : args . device = torch . device ( 'cpu' )","title":"Device Agnostic"},{"location":"snippets/pytorch/device/#device-agnostic","text":"import argparse import torch parser = argparse . ArgumentParser ( description = 'PyTorch Example' ) parser . add_argument ( '--disable-cuda' , action = 'store_true' , help = 'Disable CUDA' ) args = parser . parse_args () args . device = None if not args . disable_cuda and torch . cuda . is_available (): args . device = torch . device ( 'cuda' ) else : args . device = torch . device ( 'cpu' )","title":"Device Agnostic"},{"location":"snippets/pytorch/histograms/","text":"Histograms in PyTorch \u00b6 It's not very well known but there is a histogram function in PyTorch. Function # histogram parameters bins = 4 bounds = ( 4 , 0 ) # calculate the histogram hist = torch . histc ( torch . tensor ([ 1. , 2. , 1. ]), bins = bins , min = bounds [ 0 ], max = bounds [ 1 ]) # normalize histogram to sum to 1 hist = hist . div ( hist . sum ()) Calculating Bin Edges Unfortunately, we have to do this manually as the pytorch function doesn't spit out the entire function. # calculate the bin edges bin_edges = torch . linspace ( bounds [ 0 ], bounds [ 1 ], steps = bins )","title":"Histograms in PyTorch"},{"location":"snippets/pytorch/histograms/#histograms-in-pytorch","text":"It's not very well known but there is a histogram function in PyTorch. Function # histogram parameters bins = 4 bounds = ( 4 , 0 ) # calculate the histogram hist = torch . histc ( torch . tensor ([ 1. , 2. , 1. ]), bins = bins , min = bounds [ 0 ], max = bounds [ 1 ]) # normalize histogram to sum to 1 hist = hist . div ( hist . sum ()) Calculating Bin Edges Unfortunately, we have to do this manually as the pytorch function doesn't spit out the entire function. # calculate the bin edges bin_edges = torch . linspace ( bounds [ 0 ], bounds [ 1 ], steps = bins )","title":"Histograms in PyTorch"},{"location":"snippets/pytorch/interp/","text":"Interpolating in PyTorch \u00b6 Officially, there is not interp function in PyTorch. However, we do have the searchsorted function. This function performs a bisection Bisection Search \u00b6 Numpy Implementation \u00b6 np . searchsorted ([ 1 , 2 , 3 , 4 , 5 ]) Example # interpolation method new_y = np . interp ( new_x , old_x , old_y ) # bisection method new_y_ss = old_y [ np . searchsorted ( old_x , new_x , side = 'right' )] PyTorch Implementation \u00b6 def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 Source : Pyro Library | Neural Spline Flows","title":"Interpolating in PyTorch"},{"location":"snippets/pytorch/interp/#interpolating-in-pytorch","text":"Officially, there is not interp function in PyTorch. However, we do have the searchsorted function. This function performs a bisection","title":"Interpolating in PyTorch"},{"location":"snippets/pytorch/interp/#bisection-search","text":"","title":"Bisection Search"},{"location":"snippets/pytorch/interp/#numpy-implementation","text":"np . searchsorted ([ 1 , 2 , 3 , 4 , 5 ]) Example # interpolation method new_y = np . interp ( new_x , old_x , old_y ) # bisection method new_y_ss = old_y [ np . searchsorted ( old_x , new_x , side = 'right' )]","title":"Numpy Implementation"},{"location":"snippets/pytorch/interp/#pytorch-implementation","text":"def search_sorted ( bin_locations , inputs , eps = 1e-6 ): \"\"\" Searches for which bin an input belongs to (in a way that is parallelizable and amenable to autodiff) \"\"\" bin_locations [ ... , - 1 ] += eps return torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 Source : Pyro Library | Neural Spline Flows","title":"PyTorch Implementation"},{"location":"snippets/pytorch/keops/","text":"KeOps - Gaussian Kernel \u00b6 import torch from pykeops.torch import Kernel , kernel_product # Geneerate the data as pytorch tensors x = torch . randn ( 1000 , 3 , requires_grad = True ) y = torch . randn ( 2000 , 3 , requires_grad = True ) b = torch . randn ( 2000 , 2 , requires_grad = True ) # ARD length_scale length_scale = torch . tensor ([ . 5 ], requires_grad = True ) params = { \"id\" : Kernel ( \"gaussian(x,y)\" ), \"gamma\" : 1. / length_scale ** 2 , } # Differentiable wrt x, y, b, length_scale a = kernel_product ( params , x , y , b )","title":"KeOps - Gaussian Kernel"},{"location":"snippets/pytorch/keops/#keops-gaussian-kernel","text":"import torch from pykeops.torch import Kernel , kernel_product # Geneerate the data as pytorch tensors x = torch . randn ( 1000 , 3 , requires_grad = True ) y = torch . randn ( 2000 , 3 , requires_grad = True ) b = torch . randn ( 2000 , 2 , requires_grad = True ) # ARD length_scale length_scale = torch . tensor ([ . 5 ], requires_grad = True ) params = { \"id\" : Kernel ( \"gaussian(x,y)\" ), \"gamma\" : 1. / length_scale ** 2 , } # Differentiable wrt x, y, b, length_scale a = kernel_product ( params , x , y , b )","title":"KeOps - Gaussian Kernel"},{"location":"snippets/pytorch/loops_tqdm/","text":"Loops with TQDM \u00b6 A simple way to use a nice progress bar instead of polluting your screen with print statements. import tqdm def train ( xtrain , ytrain , model , criterion , optimizer , n_epochs = 1_000 ): with tqdm . trange ( n_epochs ) as bar : for epoch in bar : # loop over the dataset multiple times # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = model ( xtrain ) loss = criterion ( outputs , ytrain ) loss . backward () optimizer . step () # print statistics postfix = dict ( Loss = f \" { loss . item () : .3f } \" ) bar . set_postfix ( postfix ) Source : DeepBayes.ru 2019 Notebook","title":"Loops with TQDM"},{"location":"snippets/pytorch/loops_tqdm/#loops-with-tqdm","text":"A simple way to use a nice progress bar instead of polluting your screen with print statements. import tqdm def train ( xtrain , ytrain , model , criterion , optimizer , n_epochs = 1_000 ): with tqdm . trange ( n_epochs ) as bar : for epoch in bar : # loop over the dataset multiple times # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = model ( xtrain ) loss = criterion ( outputs , ytrain ) loss . backward () optimizer . step () # print statistics postfix = dict ( Loss = f \" { loss . item () : .3f } \" ) bar . set_postfix ( postfix ) Source : DeepBayes.ru 2019 Notebook","title":"Loops with TQDM"},{"location":"snippets/pytorch/multi_kernel/","text":"def kernel_product ( x , y , mode = \"gaussian\" , s = 1. ): x_i = x . unsqueeze ( 1 ) y_j = y . unsqueeze ( 0 ) xmy = (( x_i - y_j ) ** 2 ) . sum ( 2 ) if mode == \"gaussian\" : K = torch . exp ( - xmy / s ** 2 ) ) elif mode == \"laplace\" : K = torch . exp ( - torch . sqrt ( xmy + ( s ** 2 ))) elif mode == \"energy\" : K = torch . pow ( xmy + ( s ** 2 ), -. 25 ) return torch . t ( K )","title":"Multi kernel"},{"location":"snippets/pytorch/numpy/","text":"PyTorch Tensors 2 Numpy Adaptors \u00b6 import torch class TensorNumpyAdapter : \"\"\" Class for adapter interface between numpy array type and Tensor objects in PyTorch. \"\"\" def to_tensor ( self , x ): return torch . from_numpy ( x ) . float () def to_numpy ( self , x ): return x . numpy () Source : PyGlow Example","title":"PyTorch Tensors 2 Numpy Adaptors"},{"location":"snippets/pytorch/numpy/#pytorch-tensors-2-numpy-adaptors","text":"import torch class TensorNumpyAdapter : \"\"\" Class for adapter interface between numpy array type and Tensor objects in PyTorch. \"\"\" def to_tensor ( self , x ): return torch . from_numpy ( x ) . float () def to_numpy ( self , x ): return x . numpy () Source : PyGlow Example","title":"PyTorch Tensors 2 Numpy Adaptors"},{"location":"snippets/pytorch/rbf_kernel/","text":"This snippet showcases using PyTorch and calculating a kernel function. Below I have a sample script to do an RBF function along with the gradients in PyTorch. from typing import Union import numpy as np import torch # GPU + autodiff library from torch.autograd import grad class RBF : def __init__ ( self , length_scale : float = 1.0 , signal_variance : float = 1.0 , device : Union [ Bool , str ] = None ) -> None : # initialize parameters self . length_scale = torch . tensor ( length_scale , dtype = torch . float32 , device = self . device , requires_grad = True ) self . signal_variance = torch . tensor ( signal_variance , dtype = torch . float32 , device = self . device , requires_grad = True ) if device is None : self . device = torch . device ( \"cpu\" ) else : self . device = device def __call__ ( self , X : np . ndarray , Y : Union [ Bool , np . ndarray ] = None ) -> np . ndarray : # convert inputs to pytorch tensors X = torch . tensor ( X , dtype = torch . float32 , device = self . device ) if Y is None : Y = X else : Y = torch . tensor ( Y , dtype = torch . float32 , device = self . device ) # Divide by length scale X = torch . div ( X , self . length_scale ) Y = torch . div ( Y , self . length_scale ) # Re-indexing X_i = X [:, None , :] # shape (N, D) -> (N, 1, D) Y_j = Y [ None , :, :] # shape (N, D) -> (1, N, D) # Actual Computations sqd = torch . sum ( ( X_i - Y_j ) ** 2 , 2 ) # |X_i - Y_j|^2 K_qq = torch . exp ( - 0.5 * sqd ) # Gaussian Kernel K_qq = torch . mul ( self . signal_variance , K_qq ) # Signal Variance return K_qq . detach () . to_numpy () def gradient_X ( self , X ): return None def gradient_X2 ( self , X ): return None def gradient_XX ( self , X : np . ndarray , Y : Union [ Bool , np . ndarray ] = None ) -> np . ndarray : # Convert to tensor that requires Grad X = torch . tensor ( length_scale , dtype = torch . float32 , device = self . device , requires_grad = True ) if Y is None : Y = X else : Y = torch . tensor ( Y , dtype = torch . float32 , device = self . device , requires_grad = True ) # compute the gradient kernel w.r.t. to the two inputs J = grad ( self . __call__ ( X , Y )) return J def gradient_XX2 ( self , X , Y = None ): return None Below we can see how one would actually do that in practice. # With PyTorch, using the GPU is simple use_gpy = torch . cuda . is_available () dtype = torch . cuda . FloatTensor if use_gpu else torch . FloatTensor N = 5000 # cloud of 5,000 points D = 3 # 3D q = np . random . rand ( N , D ) p = np . random . rand ( N , D ) s = 1. # Store arbitrary arrays on the CPU or GPU: q = torch . from_numpy ( q ) . type ( dtype ) p = torch . from_numpy ( p ) . type ( dtype ) s = torch . Tensor ([ 1. ]) . type ( dtype ) # Tell PyTorch to track the variabls \"q\" and \"p\" q . requires_grad = True p . requires_grad = True # Rescale with length_scale q = torch . div ( q , s ) # Re-indexing q_i = q [:, None , :] # shape (N, D) -> (N, 1, D) q_j = q [ None , :, :] # shape (N, D) -> (1, N, D) # Actual Computations sqd = torch . sum ( ( q_i - q_j ) ** 2 , 2 ) # |q_i - q_j|^2 K_qq = torch . exp ( - sqd / s ** 2 ) # Gaussian Kernel v = K_qq @ p # matrix mult. (N,N) @ (N,D) = (N,D) # Automatic Differentiation [ dq , dp ] = grad ( H , [ q , p ] ) # Hamiltonian H(q,p): .5*<p,v> H = . 5 * torch . dot ( p . view ( - 1 ), v . view ( - 1 ) ) Source : Presentation for Autograd and mathematics.","title":"Rbf kernel"},{"location":"snippets/scipy/cholesky/","text":"The Cholesky Decomposition \u00b6 I use this quite often whenever I'm dealing with Gaussian processes and kernel methods. Instead of doing the solver, we can simply use the cho_factor and cho_solve that's built into the scipy library. Direct Solver \u00b6 # use the direct solver weights = scipy . linalg . solve ( K + alpha * identity , y ) Cholesky Factor \u00b6 # cholesky factor L , lower = scipy . linalg . cho_factor ( K + alpha * identity ) # cholesky solver weights = scipy . linalg . cho_solve (( L , lower ), y )","title":"The Cholesky Decomposition"},{"location":"snippets/scipy/cholesky/#the-cholesky-decomposition","text":"I use this quite often whenever I'm dealing with Gaussian processes and kernel methods. Instead of doing the solver, we can simply use the cho_factor and cho_solve that's built into the scipy library.","title":"The Cholesky Decomposition"},{"location":"snippets/scipy/cholesky/#direct-solver","text":"# use the direct solver weights = scipy . linalg . solve ( K + alpha * identity , y )","title":"Direct Solver"},{"location":"snippets/scipy/cholesky/#cholesky-factor","text":"# cholesky factor L , lower = scipy . linalg . cho_factor ( K + alpha * identity ) # cholesky solver weights = scipy . linalg . cho_solve (( L , lower ), y )","title":"Cholesky Factor"},{"location":"snippets/testing/pytest/","text":"Annotating Tests \u00b6 import pytest @pytest . mark . slow def test_that_runs_slowly (): ... @pytest . mark . data def test_that_goes_on_data (): ... @pytest . mark . slow @pytest . mark . data def test_that_goes_on_data_slowly (): ... > py.test -m \"slow\" > py.test -m \"data\" > py.test -m \"not data\" Source : Eric Ma - Blog","title":"PyTest Tricks"},{"location":"snippets/testing/pytest/#annotating-tests","text":"import pytest @pytest . mark . slow def test_that_runs_slowly (): ... @pytest . mark . data def test_that_goes_on_data (): ... @pytest . mark . slow @pytest . mark . data def test_that_goes_on_data_slowly (): ... > py.test -m \"slow\" > py.test -m \"data\" > py.test -m \"not data\" Source : Eric Ma - Blog","title":"Annotating Tests"},{"location":"snippets/testing/tips/","text":"","title":"Tips"},{"location":"snippets/visualization/matplotlib/colorbars/","text":"Scaling \u00b6 cbar = plt . colorbar ( pts , fraction = 0.046 ,) Normalizing \u00b6 import matplotlib.colors as colors boundaries = ( 0.0 , 1.0 ) # plot data pts = ax . pcolor ( X , Y , Z , norm = colors . Normalize ( vmin = boundaries [ 0 ], vmax = boundaries [ 1 ]) cmap = 'grays' ) # plot colorbar cbar = plt . colorbar ( pts , ax = ax , extend = 'both' ) Normalizations Log Scale Symmetric Log Scale Resources Matplotlib Tutorial - ColorMap Norms Jake Vanderplas - Customizing Colorbars","title":"Colorbars"},{"location":"snippets/visualization/matplotlib/colorbars/#scaling","text":"cbar = plt . colorbar ( pts , fraction = 0.046 ,)","title":"Scaling"},{"location":"snippets/visualization/matplotlib/colorbars/#normalizing","text":"import matplotlib.colors as colors boundaries = ( 0.0 , 1.0 ) # plot data pts = ax . pcolor ( X , Y , Z , norm = colors . Normalize ( vmin = boundaries [ 0 ], vmax = boundaries [ 1 ]) cmap = 'grays' ) # plot colorbar cbar = plt . colorbar ( pts , ax = ax , extend = 'both' ) Normalizations Log Scale Symmetric Log Scale Resources Matplotlib Tutorial - ColorMap Norms Jake Vanderplas - Customizing Colorbars","title":"Normalizing"},{"location":"talks/","text":"My Talks \u00b6 Fast Friday Talks (FFTs) \u00b6 (2020-01-31) - Kernel Alignment and the Kernel Parameter Analysis Conferences \u00b6 Phi-Week (2019) Sensitivity Analysis for Gaussian Processes Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors AGU (2019) Climate Model Intercomparison with Multivariate Information Theoretic Measures Groups \u00b6 KERMES (2018) Kernel Methods for Earth Observation KERMES (2020) Input Uncertainty Propagation in Gaussian Process Regression Models","title":"My Talks"},{"location":"talks/#my-talks","text":"","title":"My Talks"},{"location":"talks/#fast-friday-talks-ffts","text":"(2020-01-31) - Kernel Alignment and the Kernel Parameter Analysis","title":"Fast Friday Talks (FFTs)"},{"location":"talks/#conferences","text":"Phi-Week (2019) Sensitivity Analysis for Gaussian Processes Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors AGU (2019) Climate Model Intercomparison with Multivariate Information Theoretic Measures","title":"Conferences"},{"location":"talks/#groups","text":"KERMES (2018) Kernel Methods for Earth Observation KERMES (2020) Input Uncertainty Propagation in Gaussian Process Regression Models","title":"Groups"},{"location":"talks/2018_kermes_egp/","text":"KERMES Meetup 2018 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Kernel Methods for Earth Observation: Gaussian Processes and Derivatives \u00b6 Date: 2018 Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/IPL-UV/gp_error_propagation Slides \u00b6","title":"KERMES Meetup 2018"},{"location":"talks/2018_kermes_egp/#kermes-meetup-2018","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"KERMES Meetup 2018"},{"location":"talks/2018_kermes_egp/#kernel-methods-for-earth-observation-gaussian-processes-and-derivatives","text":"Date: 2018","title":"Kernel Methods for Earth Observation: Gaussian Processes and Derivatives"},{"location":"talks/2018_kermes_egp/#resources","text":"Code You can find the code for my experiments on this repository: github.com/IPL-UV/gp_error_propagation","title":"Resources"},{"location":"talks/2018_kermes_egp/#slides","text":"","title":"Slides"},{"location":"talks/2019_agu_rbigclima/","text":"AGU 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Climate Model Intercomparison with Multivariate Information Theoretic Measures \u00b6 Date: 2019 - 12 - 05 Day: Thursday Abstract \u00b6 Earth system models, or climate models, are fundamental tools to understand climate change. The Coupled Model Intercomparison Project (CMIP) provides outputs from many coupled atmosphere-ocean general circulation models for a number of different climate forcing scenarios. CMIP is essential to assess model performance and representativity during the historical period and quantifications of the causes of the spread in future projections. In some aspects, model accuracy has improved significantly over the different CMIP phases, but biases and uncertainties in their projections still remain, and notable differences between models exist. In this work we adopt a novel information theory (IT) perspective for evaluating and comparing climate models. Information content is model independent, and allows us to compare climate model simulations and study their differences in information units. IT measures, such as entropy, total correlation, divergences and mutual information, allow us to potentially encapsulate some interactions and phenomena that each climate model may exhibit. We introduce the rotation-based iterative Gaussianization (RBIG) method to address the inherent problem of high-dimensionality in probability density function estimation and in turn IT measures estimation. The RBIG method is a generative model that is robust to noise and dimensionality and is computationally efficient. We will show intercomparison scenarios between CMIP5 model simulations at a monthly resolution and for key models and variables. We will show assessment of the relative information content, and divergences among models, across time and space. These results provide a better unbiased evaluation of models in addition to being a valid comparative measure of shared information. Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs Slides \u00b6","title":"AGU 2019"},{"location":"talks/2019_agu_rbigclima/#agu-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"AGU 2019"},{"location":"talks/2019_agu_rbigclima/#climate-model-intercomparison-with-multivariate-information-theoretic-measures","text":"Date: 2019 - 12 - 05 Day: Thursday","title":"Climate Model Intercomparison with Multivariate Information Theoretic Measures"},{"location":"talks/2019_agu_rbigclima/#abstract","text":"Earth system models, or climate models, are fundamental tools to understand climate change. The Coupled Model Intercomparison Project (CMIP) provides outputs from many coupled atmosphere-ocean general circulation models for a number of different climate forcing scenarios. CMIP is essential to assess model performance and representativity during the historical period and quantifications of the causes of the spread in future projections. In some aspects, model accuracy has improved significantly over the different CMIP phases, but biases and uncertainties in their projections still remain, and notable differences between models exist. In this work we adopt a novel information theory (IT) perspective for evaluating and comparing climate models. Information content is model independent, and allows us to compare climate model simulations and study their differences in information units. IT measures, such as entropy, total correlation, divergences and mutual information, allow us to potentially encapsulate some interactions and phenomena that each climate model may exhibit. We introduce the rotation-based iterative Gaussianization (RBIG) method to address the inherent problem of high-dimensionality in probability density function estimation and in turn IT measures estimation. The RBIG method is a generative model that is robust to noise and dimensionality and is computationally efficient. We will show intercomparison scenarios between CMIP5 model simulations at a monthly resolution and for key models and variables. We will show assessment of the relative information content, and divergences among models, across time and space. These results provide a better unbiased evaluation of models in addition to being a valid comparative measure of shared information.","title":"Abstract"},{"location":"talks/2019_agu_rbigclima/#resources","text":"Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs","title":"Resources"},{"location":"talks/2019_agu_rbigclima/#slides","text":"","title":"Slides"},{"location":"talks/2019_phiweek_gpsens/","text":"Phi Week 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Sensitivity Analysis for Gaussian Processes: Gradient-Based Methods for Emulation \u00b6 Date: 2019 Resources \u00b6 Code You can find the code for my experiments on my labs repository: github.com/IPL-UV/gp_sens Slides \u00b6","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_gpsens/#phi-week-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_gpsens/#sensitivity-analysis-for-gaussian-processes-gradient-based-methods-for-emulation","text":"Date: 2019","title":"Sensitivity Analysis for Gaussian Processes: Gradient-Based Methods for Emulation"},{"location":"talks/2019_phiweek_gpsens/#resources","text":"Code You can find the code for my experiments on my labs repository: github.com/IPL-UV/gp_sens","title":"Resources"},{"location":"talks/2019_phiweek_gpsens/#slides","text":"","title":"Slides"},{"location":"talks/2019_phiweek_rbigad/","text":"Phi Week 2019 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors \u00b6 Date: 2019 Resources \u00b6 Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs Slides \u00b6","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_rbigad/#phi-week-2019","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Phi Week 2019"},{"location":"talks/2019_phiweek_rbigad/#unsupervised-machine-learning-exploring-spatial-temporal-relationships-and-drought-factors","text":"Date: 2019","title":"Unsupervised Machine Learning: Exploring Spatial Temporal Relationships and Drought Factors"},{"location":"talks/2019_phiweek_rbigad/#resources","text":"Code You can find the code for my experiments on this repository: github.com/jejjohnson/2019_rbig_rs","title":"Resources"},{"location":"talks/2019_phiweek_rbigad/#slides","text":"","title":"Slides"},{"location":"talks/2020_fft_01_31_hsic_align/","text":"Fast Friday Talk \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com An empirical investigation of Kernel Alignment parameters \u00b6 Date: 2020 - 01 - 31 Day: Friday Time: 1200 Abstract \u00b6 When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring. Slides \u00b6","title":"Fast Friday Talk"},{"location":"talks/2020_fft_01_31_hsic_align/#fast-friday-talk","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Fast Friday Talk"},{"location":"talks/2020_fft_01_31_hsic_align/#an-empirical-investigation-of-kernel-alignment-parameters","text":"Date: 2020 - 01 - 31 Day: Friday Time: 1200","title":"An empirical investigation of Kernel Alignment parameters"},{"location":"talks/2020_fft_01_31_hsic_align/#abstract","text":"When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring.","title":"Abstract"},{"location":"talks/2020_fft_01_31_hsic_align/#slides","text":"","title":"Slides"},{"location":"talks/2020_kermes_egp/","text":"KERMES Meetup 2020 \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020 Input Uncertainty Propagation in Gaussian Process Regression Models \u00b6 Resources \u00b6 Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website . Slides \u00b6","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes_egp/#kermes-meetup-2020","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Date: 2020","title":"KERMES Meetup 2020"},{"location":"talks/2020_kermes_egp/#input-uncertainty-propagation-in-gaussian-process-regression-models","text":"","title":"Input Uncertainty Propagation in Gaussian Process Regression Models"},{"location":"talks/2020_kermes_egp/#resources","text":"Demo Colab You can see a colab notebook with some of the plots for the presentation. Code You can find the code for my experiments on this repository: github.com/jejjohnson/uncertain_gps . There you can also find the project website .","title":"Resources"},{"location":"talks/2020_kermes_egp/#slides","text":"","title":"Slides"},{"location":"talks/fft_2020_01_31_hsic_align/","text":"Fast Friday Talk \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com An empirical investigation of Kernel Alignment parameters \u00b6 Date: 2020 - 01 - 31 Day: Friday Time: 1200 Abstract \u00b6 When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring. Slides \u00b6","title":"Fast Friday Talk"},{"location":"talks/fft_2020_01_31_hsic_align/#fast-friday-talk","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com","title":"Fast Friday Talk"},{"location":"talks/fft_2020_01_31_hsic_align/#an-empirical-investigation-of-kernel-alignment-parameters","text":"Date: 2020 - 01 - 31 Day: Friday Time: 1200","title":"An empirical investigation of Kernel Alignment parameters"},{"location":"talks/fft_2020_01_31_hsic_align/#abstract","text":"When we have lots of data and want to make comparisons, we need methods to give us an overall summary. A very popular method is to measure the covariance and/or correlation which tackles this problem from a variability perspective. It's good but there are limitations to this method: 1) It can only handle linear relationships and 2) It's not clear how it applies to multivariate/multi-dimensional data. We can use a nonlinear kernel function on this covariance matrix which addresses some of the limitations that the covariance/correlation measures exhibit. However, there is a problem that all kernel methods have: the parameters of the kernel function. For a supervised learning problem such as regression or classification, you have an objective that you want to minimize so it's clear which parameters you should use. However in unsupervised settings, there is no objective so it's not clear how we pick the parameters for the kernel function. We do a detailed empirical analysis using different kernel parameter initialization schemes, for different toy datasets, and with different sample and dimension sizes. The obvious question we want to answer is: which kernel parameter should I use? And the answers that we found was as expected: it depends... But the nevertheless, the results are reassuring.","title":"Abstract"},{"location":"talks/fft_2020_01_31_hsic_align/#slides","text":"","title":"Slides"},{"location":"thesis/","text":"My Thesis \u00b6 Chapter 2 - Key Ideas \u00b6 1. Sensitivity \u00b6 2. Uncertainty \u00b6 3. Similarity \u00b6 Chapter 3 - Data Representation \u00b6 1. Kernels \u00b6 2. Random Functions \u00b6 3. Neural Networks \u00b6 Chapter 4 - Information \u00b6 Density Estimation \u00b6 Classical Gaussianization RBIG Information Theory \u00b6 Information RBIG Chapter 5 - Applications \u00b6 1. Sensitivity \u00b6 2. Uncertainty \u00b6 3. Similarity \u00b6","title":"My Thesis"},{"location":"thesis/#my-thesis","text":"","title":"My Thesis"},{"location":"thesis/#chapter-2-key-ideas","text":"","title":"Chapter 2 - Key Ideas"},{"location":"thesis/#1-sensitivity","text":"","title":"1. Sensitivity"},{"location":"thesis/#2-uncertainty","text":"","title":"2. Uncertainty"},{"location":"thesis/#3-similarity","text":"","title":"3. Similarity"},{"location":"thesis/#chapter-3-data-representation","text":"","title":"Chapter 3 - Data Representation"},{"location":"thesis/#1-kernels","text":"","title":"1. Kernels"},{"location":"thesis/#2-random-functions","text":"","title":"2. Random Functions"},{"location":"thesis/#3-neural-networks","text":"","title":"3. Neural Networks"},{"location":"thesis/#chapter-4-information","text":"","title":"Chapter 4 - Information"},{"location":"thesis/#density-estimation","text":"Classical Gaussianization RBIG","title":"Density Estimation"},{"location":"thesis/#information-theory","text":"Information RBIG","title":"Information Theory"},{"location":"thesis/#chapter-5-applications","text":"","title":"Chapter 5 - Applications"},{"location":"thesis/#1-sensitivity_1","text":"","title":"1. Sensitivity"},{"location":"thesis/#2-uncertainty_1","text":"","title":"2. Uncertainty"},{"location":"thesis/#3-similarity_1","text":"","title":"3. Similarity"},{"location":"thesis/contribution/","text":"Related Publications \u00b6 Philosophy \u00b6 Overall, I don't want to waste anything. I find it a shame that we have so many resources separated. For example, the documentation for code (especially dense packages) often have no math associated with this. Another example is the thesis of a graduate student often has no code snippets or lab examples of some of the stuff that they have implemented. Why aren't these fused together? I'm not saying that we need to have super long documents with code snippets, but it costs you nothing to add your relevant thesis snippets to your code documentation. So, I will try to be doing all things at once: Thesis Blog Posts Software Journal Articles Journal Articles \u00b6 SAKAME RBIG 4 RS Letters \u00b6 EGP HSIC Alignment EGP 2.0 Workshops \u00b6 ICML 2019 - RBIG4NF CI - RBIG Colaborations \u00b6 IT 4 DNN ML 4 OCN Drought GSA w. GPR","title":"Related Publications"},{"location":"thesis/contribution/#related-publications","text":"","title":"Related Publications"},{"location":"thesis/contribution/#philosophy","text":"Overall, I don't want to waste anything. I find it a shame that we have so many resources separated. For example, the documentation for code (especially dense packages) often have no math associated with this. Another example is the thesis of a graduate student often has no code snippets or lab examples of some of the stuff that they have implemented. Why aren't these fused together? I'm not saying that we need to have super long documents with code snippets, but it costs you nothing to add your relevant thesis snippets to your code documentation. So, I will try to be doing all things at once: Thesis Blog Posts Software Journal Articles","title":"Philosophy"},{"location":"thesis/contribution/#journal-articles","text":"SAKAME RBIG 4 RS","title":"Journal Articles"},{"location":"thesis/contribution/#letters","text":"EGP HSIC Alignment EGP 2.0","title":"Letters"},{"location":"thesis/contribution/#workshops","text":"ICML 2019 - RBIG4NF CI - RBIG","title":"Workshops"},{"location":"thesis/contribution/#colaborations","text":"IT 4 DNN ML 4 OCN Drought GSA w. GPR","title":"Colaborations"},{"location":"thesis/logistics/","text":"Logistics \u00b6 Inspiration Thesis \u00b6 Neural Density Estimation and Likelihood-Free Inference - Papamakarios, 2019 Broadening the Scope of Gaussian Processes for Large-Scale Learning - Cutajar, 2019 Uncertainty in Deep Learning - Gal (2016) Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Model-Based Understanding of facial expressions - Sauer (2013) From Dependence to Causation - Lopez-Paz (2016) Understanding Random Forests: From Theory to Practice - Louppe (2014) Inspiring Talks \u00b6 Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS 2018/2019 - Part I | Part 2 | Part 3 | Slides Story Principle \u00b6 There is typically a 3-step rule for telling a story especially when you want people to learn something from it. We're stupid, we need repitition. The repeat rule of 3 is what I'll use to structure the thesis. Tell them what you're going to tell them Key Ideas Tell them Data Representation Information Tell them what you told them Discussion Conclusion Writing Principles (SEED) \u00b6 I will try to follow this principle for writing. brief and succinct. I want my statements to really explore and develop one single idea before moving on to the next idea. So in order to keep things in control, I will follow the SEED principle. Statement Explanation Evidence Development Resources * Writing Science: How to write papers that get cited and proposals that get funded - Joshua Schimel (2011) Philosophy \u00b6 Explanations If you can't explain it simply, then you don't understand it enough - Einstein Code Snippets Talk is cheap. Show me the code - Linus Torvald Lab Notebooks OK. But how does it work in practice? AutoDiff all the things! Sleeper Theorems I'll leave it up to the user as an exercise | It's easy to show that | as seen in [1] Parting Words \u00b6 I like to leave parting words. I see this in a blog by Matthew Rocklin all of the time What I did I do What I did I not do What I could have done better What will I do in the future? Another way to look at it is to have strengths and limitations in my discussions and conclusions. In my highschool, I typically had to do 5 of each; sometimes 10 of each if we behaved badly in class. It was tough but it made us think critically and reflect upon our work. Details \u00b6 Often times, we have many details that I think is important to know but not necessarily important to tell the story. Theorems, derivations, side notes, are very important but sometimes I think we can omit them from the main body of the text. I will create a few tabs that will hide some of these details I deem irrelevant for the story. Details These are simply details that I feel are side notes. Code Code examples that maybe tell you how to algorithmically do something. They might also feature snippets of some practical modifcations that may occur in the field. Proof I am not a fan of proofs being in the main body of the text (unless the text is about proofs). In ML, often this is not necessary except for a theoretical paper. In applied settings, we only need main equations and the rest of the details can go in the appendix. Resources Extra links where a better explanation can be given. Reproducibility \u00b6 Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Resources **Source**: Association for Computing Machinery (2016)","title":"Logistics"},{"location":"thesis/logistics/#logistics","text":"","title":"Logistics"},{"location":"thesis/logistics/#inspiration-thesis","text":"Neural Density Estimation and Likelihood-Free Inference - Papamakarios, 2019 Broadening the Scope of Gaussian Processes for Large-Scale Learning - Cutajar, 2019 Uncertainty in Deep Learning - Gal (2016) Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Model-Based Understanding of facial expressions - Sauer (2013) From Dependence to Causation - Lopez-Paz (2016) Understanding Random Forests: From Theory to Practice - Louppe (2014)","title":"Inspiration Thesis"},{"location":"thesis/logistics/#inspiring-talks","text":"Planting the Seeds of Probabilistic Thinking - Shakir Mohammed, MLSS 2018/2019 - Part I | Part 2 | Part 3 | Slides","title":"Inspiring Talks"},{"location":"thesis/logistics/#story-principle","text":"There is typically a 3-step rule for telling a story especially when you want people to learn something from it. We're stupid, we need repitition. The repeat rule of 3 is what I'll use to structure the thesis. Tell them what you're going to tell them Key Ideas Tell them Data Representation Information Tell them what you told them Discussion Conclusion","title":"Story Principle"},{"location":"thesis/logistics/#writing-principles-seed","text":"I will try to follow this principle for writing. brief and succinct. I want my statements to really explore and develop one single idea before moving on to the next idea. So in order to keep things in control, I will follow the SEED principle. Statement Explanation Evidence Development Resources * Writing Science: How to write papers that get cited and proposals that get funded - Joshua Schimel (2011)","title":"Writing Principles (SEED)"},{"location":"thesis/logistics/#philosophy","text":"Explanations If you can't explain it simply, then you don't understand it enough - Einstein Code Snippets Talk is cheap. Show me the code - Linus Torvald Lab Notebooks OK. But how does it work in practice? AutoDiff all the things! Sleeper Theorems I'll leave it up to the user as an exercise | It's easy to show that | as seen in [1]","title":"Philosophy"},{"location":"thesis/logistics/#parting-words","text":"I like to leave parting words. I see this in a blog by Matthew Rocklin all of the time What I did I do What I did I not do What I could have done better What will I do in the future? Another way to look at it is to have strengths and limitations in my discussions and conclusions. In my highschool, I typically had to do 5 of each; sometimes 10 of each if we behaved badly in class. It was tough but it made us think critically and reflect upon our work.","title":"Parting Words"},{"location":"thesis/logistics/#details","text":"Often times, we have many details that I think is important to know but not necessarily important to tell the story. Theorems, derivations, side notes, are very important but sometimes I think we can omit them from the main body of the text. I will create a few tabs that will hide some of these details I deem irrelevant for the story. Details These are simply details that I feel are side notes. Code Code examples that maybe tell you how to algorithmically do something. They might also feature snippets of some practical modifcations that may occur in the field. Proof I am not a fan of proofs being in the main body of the text (unless the text is about proofs). In ML, often this is not necessary except for a theoretical paper. In applied settings, we only need main equations and the rest of the details can go in the appendix. Resources Extra links where a better explanation can be given.","title":"Details"},{"location":"thesis/logistics/#reproducibility","text":"Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Resources **Source**: Association for Computing Machinery (2016)","title":"Reproducibility"},{"location":"thesis/outline/","text":"Thesis Outline \u00b6 --- \u00b6 Chapter I - Introduction \u00b6 1.1 Earth Science in the Wild \u00b6 1.1.1 Problems \u00b6 Earth Observation Climate Extreme Events - Drought Ocean 1.1.2 Data \u00b6 Remote Sensing Physical Models Generalized Models Emulation 1.1.3 Drawbacks \u00b6 What/Which? - best model? How? - how does the model work? Why? - get knowledge 1.2 - Fundamental ML Problems \u00b6 1.2.1 - Representations \u00b6 Discriminative Generative Information 1.2.2 - Interpretation \u00b6 Affect (Sensitivity) Noise Uncertainty 1.2.3 - Understanding \u00b6 Correlation Dependence Causation 1.3 - This Thesis \u00b6 1.3.1 - Investigation \u00b6 Research Question: Can we push forward the notion of uncertainty in EO applications? Discriminative Approach - GPs Sensitivity Uncertainty Generative Approach - RBIG IT measures Intermmediate Steps: Investigations that needed to happen Uncertain GPs HSIC Parameter Estimation Applications: Earth Science Data Cubes Ocean Data Drought Indices Climate Models 1.3.2 - OutReach \u00b6 Reproducibility Blog Posts PyCon (?) 1.3.3 - My Contributions \u00b6 Code I try to include links to model zoos whenever possible with tutorials of how to do certain things from scratch. Toy Examples I will do many toy examples to highlight important concepts. Blogs This highlights things that I think the ML community should know Sleeper Theorems There are many things that you should know to read the thesis. Just like many papers. And sometimes it's not possible to put them in the appendix. So I will put it in boxes. Supplementary Material I am a strong advocate of telling the story without the need to go through unnecessary mathematics. So I dump all necessary derivations in the appendix. Including Notation and supplementary material 1.3.4 - Organization \u00b6 Part I - Data Representation Approach Part II - Discriminative Modeling Approach Part III - Generative Modeling Approach --- \u00b6 Chapter II - Data Representation \u00b6 2.1 Kernel Methods \u00b6 Theory Regression (Classification), (KRR, GPR, SVM) Dimensionality Reduction (KPCA, KECA, DRR) Density (KDE, Distribution Regression) Information Theory (Renyi Stuff) Similarity Measures - Covariance v.s. Correlation (HSIC, KA) Sleeper Theorems Mercer's Theorem HSIC \\equiv \\equiv MMD 2.2 Density Estimation \u00b6 Parametric Gaussian Mixture of Gaussians Classical Binning (Histogram) - Kernel - Smooth kNN - Adaptive Neural Density Estimation Normalizing Flows Density Destructor Gaussianization Conditional Density Estimation 2.3 Neural Networks \u00b6 Discriminative Neural Networks Probabilistic Neural Networks Fully Bayesian Neural Networks 2.4 Other \u00b6 Generalized Linear Models Ensemble Methods 2.4 All Connected \u00b6 Kernel Entropy Components Conditional 2.3 Modeling Approaches \u00b6 2.3.1 Discriminative Models \u00b6 2.3.2 Generative Models \u00b6 2.2.3 Information Theory \u00b6 Signal ... Approximate Measures (I, H, MI, TC) Change of Variables \u00b6 --- \u00b6 Chapter III - Discriminative Model \u00b6 1.1 Regression \u00b6 1.1.1 Kernel Ridge Regression \u00b6 1.1.2 Gaussian Process Regression \u00b6 1.3 Sensitivity \u00b6 1.3.1 Concept \u00b6 1.3.2 Derivative \u00b6 1.2 Uncertainty \u00b6 1.4 Applications \u00b6 ESDC Sampling Principal Curves ESDC - HSIC Sensitivity --- \u00b6 Chapter IV - Generative Model \u00b6 4.1 Outline \u00b6 4.2 Probability \u00b6 Concepts Bayesian Formulation Change of Variables Sleeper Theorems Variational Inference Jensen's Inequality Change of Variables 4.3 Generative Models \u00b6 Density Destructors Normalizing Flows Sleeper Theorems MSE vs LL vs KLD 4.4 Information Theory \u00b6 4.4.1 IT Measures \u00b6 Signal ... Approximate Measures (I, H, MI, TC) 4.4.2 Estimators \u00b6 Gaussian KNN/KDP RBIG HSIC/KA 4.5 - Applications \u00b6 4.5.1 Spatial-Temporal Analysis \u00b6 EGU18 ( C ) CI19 ( W ) RBIG 4 RS ( J ) 4.5.1 Droughts \u00b6 CI19 ( W ) RBIG 4 RS ( J ) 4.5.3 Climate Models \u00b6 AGU ( C ) RBIG 4 RS ( J ) 4.6 - Lab Notebooks \u00b6 Change of Variables Proof of Concept RBIG - Step-by-Step Kernel Derivatives (Regression) \u00b6 Regression: KRR, GPR, Classification: SVM Feature Selection: O/KECA Dependence: HSIC, rHSIC Kernel Model Zoo Application Regression: Sampling Regression: Sensitivity Analysis Input Uncertainty Appendix Kernel Methods Theorem Sensitivity Input Uncertainty \u00b6 Extended Literature Review Connection with Kalman Filters GP Model Zoo (GPy, TF, GPyTorch) Context Uncertainty in the Literature Application IASI Ocean bbP Alvaro Data Appendix Taylor Series Moment Matching Variational Inference IT Measures \u00b6 Application Earth Science Data Cubes (spatial-temporal) Drought Indices Climate Models Dependence \u00b6 Covariance vs Correlation HSIC and Kernel Alignment Mutual Information Appendices \u00b6 Mathematical Preliminaries Linear Algebra Probability Theory \u00b6","title":"Thesis Outline"},{"location":"thesis/outline/#thesis-outline","text":"","title":"Thesis Outline"},{"location":"thesis/outline/#-","text":"","title":"---"},{"location":"thesis/outline/#chapter-i-introduction","text":"","title":"Chapter I - Introduction"},{"location":"thesis/outline/#11-earth-science-in-the-wild","text":"","title":"1.1 Earth Science in the Wild"},{"location":"thesis/outline/#111-problems","text":"Earth Observation Climate Extreme Events - Drought Ocean","title":"1.1.1 Problems"},{"location":"thesis/outline/#112-data","text":"Remote Sensing Physical Models Generalized Models Emulation","title":"1.1.2 Data"},{"location":"thesis/outline/#113-drawbacks","text":"What/Which? - best model? How? - how does the model work? Why? - get knowledge","title":"1.1.3 Drawbacks"},{"location":"thesis/outline/#12-fundamental-ml-problems","text":"","title":"1.2 - Fundamental ML Problems"},{"location":"thesis/outline/#121-representations","text":"Discriminative Generative Information","title":"1.2.1 - Representations"},{"location":"thesis/outline/#122-interpretation","text":"Affect (Sensitivity) Noise Uncertainty","title":"1.2.2 - Interpretation"},{"location":"thesis/outline/#123-understanding","text":"Correlation Dependence Causation","title":"1.2.3 - Understanding"},{"location":"thesis/outline/#13-this-thesis","text":"","title":"1.3 - This Thesis"},{"location":"thesis/outline/#131-investigation","text":"Research Question: Can we push forward the notion of uncertainty in EO applications? Discriminative Approach - GPs Sensitivity Uncertainty Generative Approach - RBIG IT measures Intermmediate Steps: Investigations that needed to happen Uncertain GPs HSIC Parameter Estimation Applications: Earth Science Data Cubes Ocean Data Drought Indices Climate Models","title":"1.3.1 - Investigation"},{"location":"thesis/outline/#132-outreach","text":"Reproducibility Blog Posts PyCon (?)","title":"1.3.2 - OutReach"},{"location":"thesis/outline/#133-my-contributions","text":"Code I try to include links to model zoos whenever possible with tutorials of how to do certain things from scratch. Toy Examples I will do many toy examples to highlight important concepts. Blogs This highlights things that I think the ML community should know Sleeper Theorems There are many things that you should know to read the thesis. Just like many papers. And sometimes it's not possible to put them in the appendix. So I will put it in boxes. Supplementary Material I am a strong advocate of telling the story without the need to go through unnecessary mathematics. So I dump all necessary derivations in the appendix. Including Notation and supplementary material","title":"1.3.3 - My Contributions"},{"location":"thesis/outline/#134-organization","text":"Part I - Data Representation Approach Part II - Discriminative Modeling Approach Part III - Generative Modeling Approach","title":"1.3.4 - Organization"},{"location":"thesis/outline/#-_1","text":"","title":"---"},{"location":"thesis/outline/#chapter-ii-data-representation","text":"","title":"Chapter II - Data Representation"},{"location":"thesis/outline/#21-kernel-methods","text":"Theory Regression (Classification), (KRR, GPR, SVM) Dimensionality Reduction (KPCA, KECA, DRR) Density (KDE, Distribution Regression) Information Theory (Renyi Stuff) Similarity Measures - Covariance v.s. Correlation (HSIC, KA) Sleeper Theorems Mercer's Theorem HSIC \\equiv \\equiv MMD","title":"2.1 Kernel Methods"},{"location":"thesis/outline/#22-density-estimation","text":"Parametric Gaussian Mixture of Gaussians Classical Binning (Histogram) - Kernel - Smooth kNN - Adaptive Neural Density Estimation Normalizing Flows Density Destructor Gaussianization Conditional Density Estimation","title":"2.2 Density Estimation"},{"location":"thesis/outline/#23-neural-networks","text":"Discriminative Neural Networks Probabilistic Neural Networks Fully Bayesian Neural Networks","title":"2.3 Neural Networks"},{"location":"thesis/outline/#24-other","text":"Generalized Linear Models Ensemble Methods","title":"2.4 Other"},{"location":"thesis/outline/#24-all-connected","text":"Kernel Entropy Components Conditional","title":"2.4 All Connected"},{"location":"thesis/outline/#23-modeling-approaches","text":"","title":"2.3 Modeling Approaches"},{"location":"thesis/outline/#231-discriminative-models","text":"","title":"2.3.1 Discriminative Models"},{"location":"thesis/outline/#232-generative-models","text":"","title":"2.3.2 Generative Models"},{"location":"thesis/outline/#223-information-theory","text":"Signal ... Approximate Measures (I, H, MI, TC)","title":"2.2.3 Information Theory"},{"location":"thesis/outline/#change-of-variables","text":"","title":"Change of Variables"},{"location":"thesis/outline/#-_2","text":"","title":"---"},{"location":"thesis/outline/#chapter-iii-discriminative-model","text":"","title":"Chapter III - Discriminative Model"},{"location":"thesis/outline/#11-regression","text":"","title":"1.1 Regression"},{"location":"thesis/outline/#111-kernel-ridge-regression","text":"","title":"1.1.1 Kernel Ridge Regression"},{"location":"thesis/outline/#112-gaussian-process-regression","text":"","title":"1.1.2 Gaussian Process Regression"},{"location":"thesis/outline/#13-sensitivity","text":"","title":"1.3 Sensitivity"},{"location":"thesis/outline/#131-concept","text":"","title":"1.3.1 Concept"},{"location":"thesis/outline/#132-derivative","text":"","title":"1.3.2 Derivative"},{"location":"thesis/outline/#12-uncertainty","text":"","title":"1.2 Uncertainty"},{"location":"thesis/outline/#14-applications","text":"ESDC Sampling Principal Curves ESDC - HSIC Sensitivity","title":"1.4 Applications"},{"location":"thesis/outline/#-_3","text":"","title":"---"},{"location":"thesis/outline/#chapter-iv-generative-model","text":"","title":"Chapter IV - Generative Model"},{"location":"thesis/outline/#41-outline","text":"","title":"4.1 Outline"},{"location":"thesis/outline/#42-probability","text":"Concepts Bayesian Formulation Change of Variables Sleeper Theorems Variational Inference Jensen's Inequality Change of Variables","title":"4.2 Probability"},{"location":"thesis/outline/#43-generative-models","text":"Density Destructors Normalizing Flows Sleeper Theorems MSE vs LL vs KLD","title":"4.3 Generative Models"},{"location":"thesis/outline/#44-information-theory","text":"","title":"4.4 Information Theory"},{"location":"thesis/outline/#441-it-measures","text":"Signal ... Approximate Measures (I, H, MI, TC)","title":"4.4.1 IT Measures"},{"location":"thesis/outline/#442-estimators","text":"Gaussian KNN/KDP RBIG HSIC/KA","title":"4.4.2 Estimators"},{"location":"thesis/outline/#45-applications","text":"","title":"4.5 - Applications"},{"location":"thesis/outline/#451-spatial-temporal-analysis","text":"EGU18 ( C ) CI19 ( W ) RBIG 4 RS ( J )","title":"4.5.1 Spatial-Temporal Analysis"},{"location":"thesis/outline/#451-droughts","text":"CI19 ( W ) RBIG 4 RS ( J )","title":"4.5.1 Droughts"},{"location":"thesis/outline/#453-climate-models","text":"AGU ( C ) RBIG 4 RS ( J )","title":"4.5.3 Climate Models"},{"location":"thesis/outline/#46-lab-notebooks","text":"Change of Variables Proof of Concept RBIG - Step-by-Step","title":"4.6 - Lab Notebooks"},{"location":"thesis/outline/#kernel-derivatives-regression","text":"Regression: KRR, GPR, Classification: SVM Feature Selection: O/KECA Dependence: HSIC, rHSIC Kernel Model Zoo Application Regression: Sampling Regression: Sensitivity Analysis Input Uncertainty Appendix Kernel Methods Theorem Sensitivity","title":"Kernel Derivatives (Regression)"},{"location":"thesis/outline/#input-uncertainty","text":"Extended Literature Review Connection with Kalman Filters GP Model Zoo (GPy, TF, GPyTorch) Context Uncertainty in the Literature Application IASI Ocean bbP Alvaro Data Appendix Taylor Series Moment Matching Variational Inference","title":"Input Uncertainty"},{"location":"thesis/outline/#it-measures","text":"Application Earth Science Data Cubes (spatial-temporal) Drought Indices Climate Models","title":"IT Measures"},{"location":"thesis/outline/#dependence","text":"Covariance vs Correlation HSIC and Kernel Alignment Mutual Information","title":"Dependence"},{"location":"thesis/outline/#appendices","text":"Mathematical Preliminaries Linear Algebra Probability Theory","title":"Appendices"},{"location":"thesis/outline/#_1","text":"","title":""},{"location":"thesis/quotes/","text":"","title":"Quotes"},{"location":"thesis/reproducibility/","text":"Reproducibility \u00b6 Concepts \u00b6 Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Source : Association for Computing Machinery (2016) Blog Posts \u00b6 Personal Code \u00b6 Kernel Model Zoo \u00b6 Regression Kernel Ridge Regression Derivatives RBF Kernel ARD Kernel Linear Polynomial ArcTangent Kernels Random Fourier Features randomized Nystrom Approximation Dependence Estimation HSIC, Kernel Alignment randomized HSIC Gaussian Process Model Zoo \u00b6 Exact GP Sparse GP Uncertain Inputs Linearized Extended Unscented Moment-Matching GP Variational Inference Sensitivity Analysis RBIG 2.0 \u00b6 pyRBIG RBIG Flows ITE Tools ESDC Tools \u00b6 Xarray Tutorial Dask Tutorial Package Contribution \u00b6 EmuKit","title":"Reproducibility"},{"location":"thesis/reproducibility/#reproducibility","text":"","title":"Reproducibility"},{"location":"thesis/reproducibility/#concepts","text":"Repeatability : Same team, same experimental setup Replicability : different team, same experimental setup Reproducibility : different team, different experimental set up Source : Association for Computing Machinery (2016)","title":"Concepts"},{"location":"thesis/reproducibility/#blog-posts","text":"","title":"Blog Posts"},{"location":"thesis/reproducibility/#personal-code","text":"","title":"Personal Code"},{"location":"thesis/reproducibility/#kernel-model-zoo","text":"Regression Kernel Ridge Regression Derivatives RBF Kernel ARD Kernel Linear Polynomial ArcTangent Kernels Random Fourier Features randomized Nystrom Approximation Dependence Estimation HSIC, Kernel Alignment randomized HSIC","title":"Kernel Model Zoo"},{"location":"thesis/reproducibility/#gaussian-process-model-zoo","text":"Exact GP Sparse GP Uncertain Inputs Linearized Extended Unscented Moment-Matching GP Variational Inference Sensitivity Analysis","title":"Gaussian Process Model Zoo"},{"location":"thesis/reproducibility/#rbig-20","text":"pyRBIG RBIG Flows ITE Tools","title":"RBIG 2.0"},{"location":"thesis/reproducibility/#esdc-tools","text":"Xarray Tutorial Dask Tutorial","title":"ESDC Tools"},{"location":"thesis/reproducibility/#package-contribution","text":"EmuKit","title":"Package Contribution"},{"location":"thesis/scratch/","text":"My Thesis: A Overview \u00b6 These are some scratch notes for organizing my thesis. IDEAS to Add \u00b6 Two Views on Regression with PyMC3 and sklearn - prezi Main Idea \u00b6 Spatial-Temporal, High Dimensional, Complex Data - What do we do with it? Data Inherent Features - Point Estimates, Spatial Meaning, Temporal Meaning Understaing - Similarities \u2192 Correlations \u2192 Dependencies \u2192 Causation ML Emulator Attributes - Sensitivity, Uncertainty, Scale, Error Propagation ML Problems \u00b6 Representations? Sensitivity Uncertainty Estimates Noise Characterization Background \u00b6 Representations (Kernels, Random Features, Deep Networks) Uncertainty Noise Analysis Key Concepts \u00b6 Representations - Kernels, Random Features, NNs|PNN|BNNs Similarity Measures - Naive, HSIC, IT Uncertainty - Epistemic, Aleatoric, Out-of-Distribution Methods - Discriminative (Model), Density Destructors (Density Estimation) Model-Based \u00b6 Representations Analysis - Derivative, Sensitivity Uncertainty Characterization - Output-Variance (eGP, eSGP), Input-Training (eVGP, BGPLVM) Applications Emulation + Sensitivity Multi-Output + Sensitivity Information-Based \u00b6 IT Measures Classic Methods - single-dimension/multivariate, mean, std dev, pt est. stats generative modeling - VAE, GAN, NF, DDD GAUSSIANIZATION Neural Networks, Deep GPs Noise Characterization Require Densities - Gaussian, Mixture, Histogram, KDE, Neural, RBIG Applications climate model + noise + MI sampling Applications \u00b6 Climate Models Spatial Representations Noise Characterization Information Theory Estimates IASI Error Propagation Uncertainty Estimates Multi-Output Spatial-Temporal ARGP - BBP Data Multi-Output Spatial-Temporal Sensitivity Uncertainty Drought Variables Temporal Emulation Sensitivity Uncertainty","title":"My Thesis: A Overview"},{"location":"thesis/scratch/#my-thesis-a-overview","text":"These are some scratch notes for organizing my thesis.","title":"My Thesis: A Overview"},{"location":"thesis/scratch/#ideas-to-add","text":"Two Views on Regression with PyMC3 and sklearn - prezi","title":"IDEAS to Add"},{"location":"thesis/scratch/#main-idea","text":"Spatial-Temporal, High Dimensional, Complex Data - What do we do with it? Data Inherent Features - Point Estimates, Spatial Meaning, Temporal Meaning Understaing - Similarities \u2192 Correlations \u2192 Dependencies \u2192 Causation ML Emulator Attributes - Sensitivity, Uncertainty, Scale, Error Propagation","title":"Main Idea"},{"location":"thesis/scratch/#ml-problems","text":"Representations? Sensitivity Uncertainty Estimates Noise Characterization","title":"ML Problems"},{"location":"thesis/scratch/#background","text":"Representations (Kernels, Random Features, Deep Networks) Uncertainty Noise Analysis","title":"Background"},{"location":"thesis/scratch/#key-concepts","text":"Representations - Kernels, Random Features, NNs|PNN|BNNs Similarity Measures - Naive, HSIC, IT Uncertainty - Epistemic, Aleatoric, Out-of-Distribution Methods - Discriminative (Model), Density Destructors (Density Estimation)","title":"Key Concepts"},{"location":"thesis/scratch/#model-based","text":"Representations Analysis - Derivative, Sensitivity Uncertainty Characterization - Output-Variance (eGP, eSGP), Input-Training (eVGP, BGPLVM) Applications Emulation + Sensitivity Multi-Output + Sensitivity","title":"Model-Based"},{"location":"thesis/scratch/#information-based","text":"IT Measures Classic Methods - single-dimension/multivariate, mean, std dev, pt est. stats generative modeling - VAE, GAN, NF, DDD GAUSSIANIZATION Neural Networks, Deep GPs Noise Characterization Require Densities - Gaussian, Mixture, Histogram, KDE, Neural, RBIG Applications climate model + noise + MI sampling","title":"Information-Based"},{"location":"thesis/scratch/#applications","text":"Climate Models Spatial Representations Noise Characterization Information Theory Estimates IASI Error Propagation Uncertainty Estimates Multi-Output Spatial-Temporal ARGP - BBP Data Multi-Output Spatial-Temporal Sensitivity Uncertainty Drought Variables Temporal Emulation Sensitivity Uncertainty","title":"Applications"},{"location":"thesis/toc/","text":"Data Representations \u00b6 Features Neural Networks Probabilistic Neural Networks","title":"Toc"},{"location":"thesis/toc/#data-representations","text":"Features Neural Networks Probabilistic Neural Networks","title":"Data Representations"},{"location":"thesis/chapters/1_introduction/1.1_earth_sci/","text":"","title":"1.1 earth sci"},{"location":"thesis/chapters/1_introduction/1.2_ml_problems/","text":"","title":"1.2 ml problems"},{"location":"thesis/chapters/1_introduction/1.3_thesis/","text":"Motivation \u00b6 So I have been looking into ways that we can use information theory measures (ITMs) to do summary statistics for large data structures. The data structures can be thought of as cubes where we care about the latitude, longitude and the time for different variables. Concretely, we are looking at a 4D dataset in the form of $$ \\mathbf{X} = X(\\phi,\\lambda, t, z) $$ where \\phi \\phi is the latitude, \\lambda \\lambda is the longitude, t t is the time, z z is the variable. So we have a dataset where we have spatial, temporal and variable considerations.","title":"1.3 thesis"},{"location":"thesis/chapters/1_introduction/1.3_thesis/#motivation","text":"So I have been looking into ways that we can use information theory measures (ITMs) to do summary statistics for large data structures. The data structures can be thought of as cubes where we care about the latitude, longitude and the time for different variables. Concretely, we are looking at a 4D dataset in the form of $$ \\mathbf{X} = X(\\phi,\\lambda, t, z) $$ where \\phi \\phi is the latitude, \\lambda \\lambda is the longitude, t t is the time, z z is the variable. So we have a dataset where we have spatial, temporal and variable considerations.","title":"Motivation"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/","text":"Uncertainty \u00b6 Uncertainty Overview of Overview What is Uncertainty? Aleatoric Uncertainty, \\sigma^2 \\sigma^2 Intuition: 1D Regression Real Function Intuition: Confidence Intervals Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 Uncertainty in the Error Generalization Uncertainty Over Functions Distribution Shift Overview of Overview \u00b6 Models w. Uncertainty Epistemic - use Bayes approaches Aleatoric - use Bayes Output - Homeoscedastic, Heteroscedastic Input? - augment Out-of-Dist: outside the scope Measure it Directly Expected Uncertainty PDF Estimation What is Uncertainty? \u00b6 Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Out-of-Distribution, Distribution Shift when the distribution we learn from is different from the testing data. Expected Uncertainty Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input. Aleatoric Uncertainty, \\sigma^2 \\sigma^2 \u00b6 This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day... Intuition: 1D Regression \u00b6 Sources Intution Examples - Colab Notebook Real Function \u00b6 Intuition: Confidence Intervals \u00b6 Figure : Intuition of for the Taylor expansion for a model: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\epsilon_y \\epsilon_y - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x . Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 \u00b6 The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find Uncertainty in the Error Generalization \u00b6 First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model. Uncertainty Over Functions \u00b6 In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 . Distribution Shift \u00b6 In ML, we typically assume that our data is stationary; meaning that it will always come from the same distribution. This is not always the case as sometimes we just only observed a portion of the data, e.g. space and time. Need Example : Earth science, in time and space? This is also essential in the causality realm where such a bad assumption could lead to incorrect causal graphs.","title":"Uncertainty"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#uncertainty","text":"Uncertainty Overview of Overview What is Uncertainty? Aleatoric Uncertainty, \\sigma^2 \\sigma^2 Intuition: 1D Regression Real Function Intuition: Confidence Intervals Epistemic Uncertainty, \\nu_{**}^2 \\nu_{**}^2 Uncertainty in the Error Generalization Uncertainty Over Functions Distribution Shift","title":"Uncertainty"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#overview-of-overview","text":"Models w. Uncertainty Epistemic - use Bayes approaches Aleatoric - use Bayes Output - Homeoscedastic, Heteroscedastic Input? - augment Out-of-Dist: outside the scope Measure it Directly Expected Uncertainty PDF Estimation","title":"Overview of Overview"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#what-is-uncertainty","text":"Before we talk about the types of neural networks that handle uncertainty, we first need to define some terms about uncertainty. There are three main types of uncertainty but they each Aleatoric (Data) irreducible uncertainty when the output is inherently random - IWSDGP Epistemic (Model) model/reducible uncertainty when the output depends determininstically on the input, but there is uncertainty due to lack of observations - IWSDGP Out-of-Distribution, Distribution Shift when the distribution we learn from is different from the testing data. Expected Uncertainty Aleatoric uncertainty is the uncertainty we have in our data. We can break down the uncertainty for the Data into further categories: the inputs X X versus the outputs Y Y . We can further break down the types into homoscedastic, where we have continuous noise for the inputs and heteroscedastic, where we have uncertain elements per input.","title":"What is Uncertainty?"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#aleatoric-uncertainty-sigma2sigma2","text":"This corresponds to there being uncertainty on the data itself. We assume that the measurements, y y we have some amount of uncertainty that is irreducible due to measurement error, e.g. observation/sensor noise or some additive noise component. A really good example of this is when you think of the dice player and the mean value and variance value of the rolls. No matter how many times you roll the dice, you won't ever reduce the uncertainty. If we can assume some model over this noise, e.g. Normally distributed, then we use maximum likelihood estimation (MLE) to find the parameter of this distribution. I want to point out that this term is often only assumed to be connected with y y , the measurement error. They often assume that the X X 's are clean and have no error. However, in many cases, I know especially in my field of Earth sciences, we have uncertainty in the X X 's as well. This is important for error propagation which will lead to more credible uncertainty measurements. One way to handle this is to assume that the likelihood term \\sigma^2 \\sigma^2 is not a constant but instead a function of X X , \\sigma^2(x) \\sigma^2(x) . This is one way to ensure that this variance estimate changes depending upon the value of X. Alternatively, we can also assume that X X is not really variable but instead a latent variable. In this formulation we assume that we only have access to some noisy observations x_\\mu x_\\mu and there is an additive noise component \\Sigma_x \\Sigma_x (which can be known or unknown depending on the application). In this instance, we need to propogate this uncertainty through each of the values within the dataset on top of the uncertain parameters. In the latent variable model community, they do look at this but I haven't seen too much work on this in the applied uncertainty community (i.e. people who have known uncertainties they would like to account for). I hope to change that one day...","title":"Aleatoric Uncertainty, \\sigma^2\\sigma^2"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#intuition-1d-regression","text":"Sources Intution Examples - Colab Notebook","title":"Intuition: 1D Regression"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#real-function","text":"","title":"Real Function"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#intuition-confidence-intervals","text":"Figure : Intuition of for the Taylor expansion for a model: a) y=f(\\mathbf x) + \\epsilon_y y=f(\\mathbf x) + \\epsilon_y b) y=f(\\mathbf x + \\epsilon_x) y=f(\\mathbf x + \\epsilon_x) c) y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y y=f(\\mathbf x + \\epsilon_x) + \\epsilon_y The key idea to think about is what contributes to how far away the error bars are from the approximated mean function. The above graphs will help facilitate the argument given below. There are two main components: Output noise \\epsilon_y \\epsilon_y - the further away the output points are from the approximated function will contribute to the confidence intervals. However, this will affect the vertical components where it is flat and not so much when there is a large slope. Input noise \\epsilon_x \\epsilon_x - the influence of the input noise depends on the slope of the function. i.e. if the function is fully flat, then the input noise doesn't affect the vertical distance between our measurement point and the approximated function; contrast this with a function fully sloped then we have a high contribution to the confidence interval. So there are two components of competing forces: \\sigma_y^2 \\sigma_y^2 and \\epsilon_x \\epsilon_x and the \\epsilon_x \\epsilon_x is dependent upon the slope of our function \\partial f(\\cdot) \\partial f(\\cdot) w.r.t. \\mathbf x \\mathbf x .","title":"Intuition: Confidence Intervals"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#epistemic-uncertainty-nu_2nu_2","text":"The second term is the uncertainty over the function values before the noise corruption \\sigma^2 \\sigma^2 . In this instance, we find","title":"Epistemic Uncertainty, \\nu_{**}^2\\nu_{**}^2"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#uncertainty-in-the-error-generalization","text":"First we would like to define all of the sources of uncertainty more concretely. Let's say we have a model y=f(x)+e y=f(x)+e . For starters, we can decompose the generalization error term: \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathbb{E}\\left[ l(f(x) + e, \\hat{f}(x)) \\right] \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\mathcal{E}(f) + \\left( \\mathcal{E}(\\hat{f}) - \\mathcal{E}(f^*) \\right) + \\left( \\mathcal{E}(f^*) - \\mathcal{E}(f) \\right) \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} \\mathcal{E}(\\hat{f}) = \\underset{\\text{Bayes Rate}}{\\mathcal{E}_{y}} + \\underset{\\text{Estimation}}{\\mathcal{E}_{x}} + \\underset{\\text{Approx. Error}}{\\mathcal{E}_{f}} where \\mathcal{E}_{y} \\mathcal{E}_{y} is the best possible prediction we can achieve do to the noise e e thus it cannot be avoided; \\mathcal{E}_{x} \\mathcal{E}_{x} is due to the finite-sample problem; and \\mathcal{E}_{f} \\mathcal{E}_{f} is the model 'wrongness' (the fact that all models are wrong but some are useful). \\textbf{Note:} as the number of samples decrease, then the model wrongness will increase. More samples will also allow us to decrease the estimation error. However, many times we are still certain of our uncertainty and we would like to propagate this knowledge through our ML model.","title":"Uncertainty in the Error Generalization"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#uncertainty-over-functions","text":"In this section, we will look at the Bayesian treatment of uncertainty and will continue to define the terms aleatoric and epistemic uncertainty in the Bayesian language. Below we briefly outline the Bayesian model functionality in terms of Neural networks. Prior : p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) p(w_{h,d}) = \\mathcal{N}(w_{h,d} | 0, s^2) where W \\in \\mathbb{R}^{H \\times D} W \\in \\mathbb{R}^{H \\times D} . Likelihood p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) p(Y|X, W) = \\prod_H \\mathcal{N}(y_h | f^W(x_h), \\sigma^2) where f^W(x) = W^T\\phi(x) f^W(x) = W^T\\phi(x) , \\phi(x) \\phi(x) is a N dimensional vector. Posterior P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) P(W|X,Y) = \\mathcal{N}(W| \\mu, \\Sigma) where: \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\mu = \\Sigma \\sigma^{-2}\\Phi(X)^TY \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} \\Sigma = (\\sigma^{-2} \\Phi(X)^\\top\\Phi(X) + s^2\\mathbf{I}_D)^{-1} Predictive p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) p(y^*|x^*, X, Y) = \\mathcal{N}(y^*| \\mu_*, \\nu_{**}^2) where: \\mu_* = \\mu^T\\phi(X^*) \\mu_* = \\mu^T\\phi(X^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) \\nu_{**}^2 = \\sigma^2 + \\phi(x^*)^\\top\\Sigma \\phi(x^*) Strictly speaking from the predictive uncertainty formulation above, uncertainty has two components: the variance from the likelihood term \\sigma^2 \\sigma^2 and the variance from the posterior term \\nu_{**}^2 \\nu_{**}^2 .","title":"Uncertainty Over Functions"},{"location":"thesis/chapters/2_key_ideas/1_uncertainty/#distribution-shift","text":"In ML, we typically assume that our data is stationary; meaning that it will always come from the same distribution. This is not always the case as sometimes we just only observed a portion of the data, e.g. space and time. Need Example : Earth science, in time and space? This is also essential in the causality realm where such a bad assumption could lead to incorrect causal graphs.","title":"Distribution Shift"},{"location":"thesis/chapters/2_key_ideas/2_similarity/","text":"Similarity \u00b6 What is it? \u00b6 When making comparisons between objects, the simplest question we can ask ourselves is how 'similar' one object is to another. It's a simple question but it's very difficult to answer. Simalirity in everyday life is somewhat easy to grasp intuitively but it's not easy to convey specific instructions to a computer. A 1 . For example, the saying, \"it's like comparing apples to oranges\" is usually said when you try to tell someone that something is not comparable. But actually, we can compare apples and oranges. We can compare the shape, color, composition and even how delicious we personally think they are. So let's consider the datacube structure that was mentioned above. How would we compare two variables z_1 z_1 and z_2 z_2 . Trends \u00b6 In this representation, we are essentially doing one type or processing and then A parallel coordinate visualization is practical byt only certain pairwise comparisons are possible. If we look at just the temporal component, then we could just plot the time series at different points along the globe. Trends often do not expose similarity in an intuitive way. Constraints \u00b6 Orthogonal Transformations \u00b6 Linear Transformations \u00b6 Isotropic Scaling \u00b6 Multivariate \u00b6 Curse of Dimensionality \u00b6 Classic Methods \u00b6 Summary Statistics \u00b6 Boxplots allow us to sumarize these statistics Taylor Diagram \u00b6 A Taylor Diagram is a concise statistical summary of how well patterns match each other in terms of their correlation, their root-mean-square difference and the ratio of their variances. With this diagram, we can simultaneously plot each of the summary statistics, e.g. standard deviation, root mean squared error (RMSE) and the R correlation coefficient. The original reference can be found here 2 . $$ \\text{RMS}^2 = \\sigma_x^2+\\sigma_y^2-2\\sigma_x\\sigma_yR_{xy} \\ c^2 = a^2 + b^2 - 2ab \\cos \\theta $$ This is a well understood diagram in the Earth science and climate community. It is also easy to compute. Correlation \u00b6 A scatterplot matrix can be impractical for many outputs. Example with Anscombes Quartet why IT measures might be useful for correlation plots. HSIC \u00b6 Mutual Information \u00b6 Information Theory \u00b6 Mutual Information is the counterpart to using information theory methods. It requires an estimation step which may introduce additional uncertainties Extends nicely to different types of data (e.g. discrete, categorical, multivariate, multidimentional) Exposes non-linearities which may be difficult to see via (linear) correlations Kernel Approximations: Although there are some differences for different estimators, relative distances are consistent A Primer \u00b6 Entropy - measure of information uncertainty of X X Joint Entropy - uncertinaty of X,Y X,Y Conditional Entropy - uncertainty of X X given that I know Y Y Mutual Information - how much knowning X X reduces the uncertainty of Y Y I(X,Y)= I(X,Y)= Normalized Mutual Information \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} Variation of Information \u00b6 A measure of distance in information theory space. VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) where: VI(X,Y)=0 VI(X,Y)=0 Iff X X and Y Y are the same H(X,Y)=H(X)=H(Y)=I(X,Y) H(X,Y)=H(X)=H(Y)=I(X,Y) VI(X,Y) < H(X,Y) VI(X,Y) < H(X,Y) If X X and Y Y are different but dependent H(X,Y)<H(X) + H(Y) H(X,Y)<H(X) + H(Y) VI(X,Y)=H(X,Y) VI(X,Y)=H(X,Y) if X X and Y Y are independent H(X,Y)=H(X) + H(Y) H(X,Y)=H(X) + H(Y) I(X,Y)=0 I(X,Y)=0 Questions \u00b6 Are there correlations across seasons or latitudes Are there large descrepancies in the different outputs? Classes of Methods \u00b6 Resources \u00b6 Websites \u00b6 Taylor Diagrams Papers \u00b6","title":"Similarity"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#similarity","text":"","title":"Similarity"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#what-is-it","text":"When making comparisons between objects, the simplest question we can ask ourselves is how 'similar' one object is to another. It's a simple question but it's very difficult to answer. Simalirity in everyday life is somewhat easy to grasp intuitively but it's not easy to convey specific instructions to a computer. A 1 . For example, the saying, \"it's like comparing apples to oranges\" is usually said when you try to tell someone that something is not comparable. But actually, we can compare apples and oranges. We can compare the shape, color, composition and even how delicious we personally think they are. So let's consider the datacube structure that was mentioned above. How would we compare two variables z_1 z_1 and z_2 z_2 .","title":"What is it?"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#trends","text":"In this representation, we are essentially doing one type or processing and then A parallel coordinate visualization is practical byt only certain pairwise comparisons are possible. If we look at just the temporal component, then we could just plot the time series at different points along the globe. Trends often do not expose similarity in an intuitive way.","title":"Trends"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#constraints","text":"","title":"Constraints"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#orthogonal-transformations","text":"","title":"Orthogonal Transformations"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#linear-transformations","text":"","title":"Linear Transformations"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#isotropic-scaling","text":"","title":"Isotropic Scaling"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#multivariate","text":"","title":"Multivariate"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#curse-of-dimensionality","text":"","title":"Curse of Dimensionality"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#classic-methods","text":"","title":"Classic Methods"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#summary-statistics","text":"Boxplots allow us to sumarize these statistics","title":"Summary Statistics"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#taylor-diagram","text":"A Taylor Diagram is a concise statistical summary of how well patterns match each other in terms of their correlation, their root-mean-square difference and the ratio of their variances. With this diagram, we can simultaneously plot each of the summary statistics, e.g. standard deviation, root mean squared error (RMSE) and the R correlation coefficient. The original reference can be found here 2 . $$ \\text{RMS}^2 = \\sigma_x^2+\\sigma_y^2-2\\sigma_x\\sigma_yR_{xy} \\ c^2 = a^2 + b^2 - 2ab \\cos \\theta $$ This is a well understood diagram in the Earth science and climate community. It is also easy to compute.","title":"Taylor Diagram"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#correlation","text":"A scatterplot matrix can be impractical for many outputs. Example with Anscombes Quartet why IT measures might be useful for correlation plots.","title":"Correlation"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#hsic","text":"","title":"HSIC"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#mutual-information","text":"","title":"Mutual Information"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#information-theory","text":"Mutual Information is the counterpart to using information theory methods. It requires an estimation step which may introduce additional uncertainties Extends nicely to different types of data (e.g. discrete, categorical, multivariate, multidimentional) Exposes non-linearities which may be difficult to see via (linear) correlations Kernel Approximations: Although there are some differences for different estimators, relative distances are consistent","title":"Information Theory"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#a-primer","text":"Entropy - measure of information uncertainty of X X Joint Entropy - uncertinaty of X,Y X,Y Conditional Entropy - uncertainty of X X given that I know Y Y Mutual Information - how much knowning X X reduces the uncertainty of Y Y I(X,Y)= I(X,Y)= Normalized Mutual Information \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\tilde{I}(X,Y) = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}}","title":"A Primer"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#variation-of-information","text":"A measure of distance in information theory space. VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) VI(X,Y) = H(X|Y) + H(Y|X) \\\\ VI(X,Y) = H(X) + H(Y) -2I(X,Y) where: VI(X,Y)=0 VI(X,Y)=0 Iff X X and Y Y are the same H(X,Y)=H(X)=H(Y)=I(X,Y) H(X,Y)=H(X)=H(Y)=I(X,Y) VI(X,Y) < H(X,Y) VI(X,Y) < H(X,Y) If X X and Y Y are different but dependent H(X,Y)<H(X) + H(Y) H(X,Y)<H(X) + H(Y) VI(X,Y)=H(X,Y) VI(X,Y)=H(X,Y) if X X and Y Y are independent H(X,Y)=H(X) + H(Y) H(X,Y)=H(X) + H(Y) I(X,Y)=0 I(X,Y)=0","title":"Variation of Information"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#questions","text":"Are there correlations across seasons or latitudes Are there large descrepancies in the different outputs?","title":"Questions"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#classes-of-methods","text":"","title":"Classes of Methods"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#resources","text":"","title":"Resources"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#websites","text":"Taylor Diagrams","title":"Websites"},{"location":"thesis/chapters/2_key_ideas/2_similarity/#papers","text":"","title":"Papers"},{"location":"thesis/chapters/2_key_ideas/3_sensitivity/","text":"","title":"3 sensitivity"},{"location":"thesis/chapters/3_data_representation/hybrid/","text":"Hybrid Approaches \u00b6 NN + GP - Prob Neural Net NN + GP - DGP NN + Random - RFE","title":"Hybrid Approaches"},{"location":"thesis/chapters/3_data_representation/hybrid/#hybrid-approaches","text":"NN + GP - Prob Neural Net NN + GP - DGP NN + Random - RFE","title":"Hybrid Approaches"},{"location":"thesis/chapters/3_data_representation/kernels/","text":"Kernel Methods \u00b6 Kernel Methods What is a Kernel function? Interpreting Kernel Functions (Parameters) Derivatives + Sensivity Literature Gaussian Processes Deep Gaussian Processes 1 Kernel Methods \u00b6 1.1 What is a Kernel Function? \u00b6 Similarity Measure + NonLinear Function higher order features Examples * Regression (KRR, GPR) * Classification (SVM) * Feature Selection (KPOLS) * Dimensionality Reduction () * Output Normalized Methods (Laplacian Eigenmaps) * Density Estimation (KDE) * Dependence Estimation (HSIC) * Distance (MMD) * Scaling (Nystrom, rNystrom, RKS) * [Blog](http://evelinag.com/Ariadne/covarianceFunctions.html) * GPML Book - [Chapter 4 - Covariance Functions](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) * Gustau - [Kernel Methods in Machine Learning](http://isp.uv.es/courses.html) **Intuition** * [From Dependence to Causation](https://arxiv.org/pdf/1607.03300.pdf) > Nice introduction with a nice plot. Also discusses the nystrom method randomized kernels. I like like to 'different representations'. Can use example where we can do dot products between features to use a nice linear method. * [Kernel Methods for Pattern Analysis]() 1.2 Interpreting Kernel Parameters \u00b6 Example RBF Kernel \u00b6 This is by far the most popular kernel you will come across when you are working with kernel methods. It's the most generalizable and it's fairly easy to train and interpret. K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} where: \\nu^2 > 0 \\nu^2 > 0 is the signal variance \\lambda > 0 \\lambda > 0 is the length scale \\sigma_{\\text{noise}}^2 \\sigma_{\\text{noise}}^2 is the noise variance Distance Metric - d(x,y)_{E} d(x,y)_{E} This is the distance metric used to describe the kernel function. For the RBF kernel, this distance metric is the Euclidean distance ( ||x-y||_2^2 ||x-y||_2^2 ). But we can switch this measure for another, e.g. the L_1 L_1 -norm produces the Laplacian kernel. In most 'stationary' kernels, this measure is the foundation for this family of kernels. It acts as a smoother. Signal Variance - \\nu^2>0 \\nu^2>0 This is simply a scaling factor. It determines the variation of function values from their mean. Small values of \\nu^2 \\nu^2 characterize functions that stay close to their mean value, larger values allow for more variation. If the signal variance is too large, the modelled function will be more susceptible to outliers. Note : Almost every kernel has this parameter in front as it is just a scale factor. Length Scale - \\lambda > 0 \\lambda > 0 This parameter describes how smooth a function is. Small \\lambda \\lambda means the function values change quickly, large \\lambda \\lambda characterizes functions that change slowly. This parameter determines how far we can reliably extrapolate from the training data. Note: Put \\lambda \\lambda is constant, automatic relevance determine, diagonal, etc. Noise Variance - \\sigma_{\\text{noise}}^2>0 \\sigma_{\\text{noise}}^2>0 This parameter describes how much noise we expect to be present in the data. Other Kernels \u00b6 Issues : * Limited Expressiveness * Neural Networks - Infinitely Wide Other Kernels * Spectral Mixture Kernel * ArcCosine - Neural Networks * Neural Tangent Kernels * Deep Kernel learning * Convolutional Kernels **Kernel Resources** * Learning with Spectral Kernels - [Prezi](https://www.hiit.fi/wp-content/uploads/2018/04/Spectral-Kernels-S12.pdf) * Learning Scalable Deep Kernels with Recurrent Structure - Al Shedivat (2017) * Deep Kernel Learning - Wilson (2015) * Deep convolutional Gaussian processes - Blomqvist (2018) * Stochastic Variational Deep Kernel Learning - Wilson (2016) * Non-Stationary Spectral Kernels - Remes (2017) **Resources** * Kernel Cookbook * A Visual Comparison of Gaussian Regression Kernels - [towardsdatascience](https://towardsdatascience.com/a-visual-comparison-of-gaussian-process-regression-kernels-8d47f2c9f63c) * [Kernel Functions](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/) * [PyKernels](https://github.com/gmum/pykernels) * [Sklearn kernels](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/gaussian_process/kernels.py#L1146) **Key Figures** * [Markus Heinonen](https://users.aalto.fi/~heinom10/) **Cutting Edge** * [Bayesian Approaches to Distribution Regression](http://www.gatsby.ucl.ac.uk/~dougals/slides/bdr-nips/#/) * HSIC * [HSIC, A Measure of Independence?](http://www.cmap.polytechnique.fr/~zoltan.szabo/talks/invited_talk/Zoltan_Szabo_invited_talk_EPFL_LIONS_28_02_2018_slides.pdf) * [Measuring Dependence and Conditional Dependence with Kernels](http://people.tuebingen.mpg.de/causal-learning/slides/CaMaL_Fukumizu.pdf) 3 Derivatives & Sensitivity \u00b6 **Resources** * Computing gradients via GPR - [stack](https://stats.stackexchange.com/questions/373446/computing-gradients-via-gaussian-process-regression) > Nice overfew of formula in a very easy way. * [Derivatives of GP](https://mathoverflow.net/questions/289424/derivatives-of-gaussian-processes) > Simple way to take a derivative of a kernel * [Differentiating GPs](http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf) * [GPyTorch Implementation](https://gpytorch.readthedocs.io/en/latest/examples/10_GP_Regression_Derivative_Information/index.html) **Literature** * [Direct Density Derivative and its Applications to KLD](http://proceedings.mlr.press/v38/sasaki15.pdf) * [Kernel Derivative Approximation with RFF](https://arxiv.org/abs/1810.05207) * An SVD and Derivative Kernel Approach to Learning from Geometric Data - Wong * Scaling Gaussian Process Regression with Derivatives - Eriksson 2018 * MultiDimensional SVM to Include the Samples of the Derivatives in the Reconstruction Function - Perez-Cruz () * Linearized Gaussian Processes for Fast Data-driven Model Predictive Control - Nghiem et. al. (2019) Bayesian Treatment: Gaussian Processes \u00b6 Composition: Deep Gaussian Processes \u00b6 Connections \u00b6","title":"Kernel Methods"},{"location":"thesis/chapters/3_data_representation/kernels/#kernel-methods","text":"Kernel Methods What is a Kernel function? Interpreting Kernel Functions (Parameters) Derivatives + Sensivity Literature Gaussian Processes Deep Gaussian Processes","title":"Kernel Methods"},{"location":"thesis/chapters/3_data_representation/kernels/#1-kernel-methods","text":"","title":"1 Kernel Methods"},{"location":"thesis/chapters/3_data_representation/kernels/#11-what-is-a-kernel-function","text":"Similarity Measure + NonLinear Function higher order features Examples * Regression (KRR, GPR) * Classification (SVM) * Feature Selection (KPOLS) * Dimensionality Reduction () * Output Normalized Methods (Laplacian Eigenmaps) * Density Estimation (KDE) * Dependence Estimation (HSIC) * Distance (MMD) * Scaling (Nystrom, rNystrom, RKS) * [Blog](http://evelinag.com/Ariadne/covarianceFunctions.html) * GPML Book - [Chapter 4 - Covariance Functions](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) * Gustau - [Kernel Methods in Machine Learning](http://isp.uv.es/courses.html) **Intuition** * [From Dependence to Causation](https://arxiv.org/pdf/1607.03300.pdf) > Nice introduction with a nice plot. Also discusses the nystrom method randomized kernels. I like like to 'different representations'. Can use example where we can do dot products between features to use a nice linear method. * [Kernel Methods for Pattern Analysis]()","title":"1.1 What is a Kernel Function?"},{"location":"thesis/chapters/3_data_representation/kernels/#12-interpreting-kernel-parameters","text":"","title":"1.2 Interpreting Kernel Parameters"},{"location":"thesis/chapters/3_data_representation/kernels/#example-rbf-kernel","text":"This is by far the most popular kernel you will come across when you are working with kernel methods. It's the most generalizable and it's fairly easy to train and interpret. K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} K(x,y)=\\nu^2 \\cdot \\text{exp}\\left( - \\frac{d(x,y)}{2\\lambda^2} \\right) + \\sigma_{\\text{noise}}^2 \\cdot \\delta_{ij} where: \\nu^2 > 0 \\nu^2 > 0 is the signal variance \\lambda > 0 \\lambda > 0 is the length scale \\sigma_{\\text{noise}}^2 \\sigma_{\\text{noise}}^2 is the noise variance Distance Metric - d(x,y)_{E} d(x,y)_{E} This is the distance metric used to describe the kernel function. For the RBF kernel, this distance metric is the Euclidean distance ( ||x-y||_2^2 ||x-y||_2^2 ). But we can switch this measure for another, e.g. the L_1 L_1 -norm produces the Laplacian kernel. In most 'stationary' kernels, this measure is the foundation for this family of kernels. It acts as a smoother. Signal Variance - \\nu^2>0 \\nu^2>0 This is simply a scaling factor. It determines the variation of function values from their mean. Small values of \\nu^2 \\nu^2 characterize functions that stay close to their mean value, larger values allow for more variation. If the signal variance is too large, the modelled function will be more susceptible to outliers. Note : Almost every kernel has this parameter in front as it is just a scale factor. Length Scale - \\lambda > 0 \\lambda > 0 This parameter describes how smooth a function is. Small \\lambda \\lambda means the function values change quickly, large \\lambda \\lambda characterizes functions that change slowly. This parameter determines how far we can reliably extrapolate from the training data. Note: Put \\lambda \\lambda is constant, automatic relevance determine, diagonal, etc. Noise Variance - \\sigma_{\\text{noise}}^2>0 \\sigma_{\\text{noise}}^2>0 This parameter describes how much noise we expect to be present in the data.","title":"Example RBF Kernel"},{"location":"thesis/chapters/3_data_representation/kernels/#other-kernels","text":"Issues : * Limited Expressiveness * Neural Networks - Infinitely Wide Other Kernels * Spectral Mixture Kernel * ArcCosine - Neural Networks * Neural Tangent Kernels * Deep Kernel learning * Convolutional Kernels **Kernel Resources** * Learning with Spectral Kernels - [Prezi](https://www.hiit.fi/wp-content/uploads/2018/04/Spectral-Kernels-S12.pdf) * Learning Scalable Deep Kernels with Recurrent Structure - Al Shedivat (2017) * Deep Kernel Learning - Wilson (2015) * Deep convolutional Gaussian processes - Blomqvist (2018) * Stochastic Variational Deep Kernel Learning - Wilson (2016) * Non-Stationary Spectral Kernels - Remes (2017) **Resources** * Kernel Cookbook * A Visual Comparison of Gaussian Regression Kernels - [towardsdatascience](https://towardsdatascience.com/a-visual-comparison-of-gaussian-process-regression-kernels-8d47f2c9f63c) * [Kernel Functions](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/) * [PyKernels](https://github.com/gmum/pykernels) * [Sklearn kernels](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/gaussian_process/kernels.py#L1146) **Key Figures** * [Markus Heinonen](https://users.aalto.fi/~heinom10/) **Cutting Edge** * [Bayesian Approaches to Distribution Regression](http://www.gatsby.ucl.ac.uk/~dougals/slides/bdr-nips/#/) * HSIC * [HSIC, A Measure of Independence?](http://www.cmap.polytechnique.fr/~zoltan.szabo/talks/invited_talk/Zoltan_Szabo_invited_talk_EPFL_LIONS_28_02_2018_slides.pdf) * [Measuring Dependence and Conditional Dependence with Kernels](http://people.tuebingen.mpg.de/causal-learning/slides/CaMaL_Fukumizu.pdf)","title":"Other Kernels"},{"location":"thesis/chapters/3_data_representation/kernels/#3-derivatives-sensitivity","text":"**Resources** * Computing gradients via GPR - [stack](https://stats.stackexchange.com/questions/373446/computing-gradients-via-gaussian-process-regression) > Nice overfew of formula in a very easy way. * [Derivatives of GP](https://mathoverflow.net/questions/289424/derivatives-of-gaussian-processes) > Simple way to take a derivative of a kernel * [Differentiating GPs](http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf) * [GPyTorch Implementation](https://gpytorch.readthedocs.io/en/latest/examples/10_GP_Regression_Derivative_Information/index.html) **Literature** * [Direct Density Derivative and its Applications to KLD](http://proceedings.mlr.press/v38/sasaki15.pdf) * [Kernel Derivative Approximation with RFF](https://arxiv.org/abs/1810.05207) * An SVD and Derivative Kernel Approach to Learning from Geometric Data - Wong * Scaling Gaussian Process Regression with Derivatives - Eriksson 2018 * MultiDimensional SVM to Include the Samples of the Derivatives in the Reconstruction Function - Perez-Cruz () * Linearized Gaussian Processes for Fast Data-driven Model Predictive Control - Nghiem et. al. (2019)","title":"3 Derivatives &amp; Sensitivity"},{"location":"thesis/chapters/3_data_representation/kernels/#bayesian-treatment-gaussian-processes","text":"","title":"Bayesian Treatment: Gaussian Processes"},{"location":"thesis/chapters/3_data_representation/kernels/#composition-deep-gaussian-processes","text":"","title":"Composition: Deep Gaussian Processes"},{"location":"thesis/chapters/3_data_representation/kernels/#connections","text":"","title":"Connections"},{"location":"thesis/chapters/3_data_representation/neural_nets/","text":"Neural Networks \u00b6 So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well Resources * Jee - Prob CV * Masterclass in regression - Aboleth - docs Weight Space \u00b6 This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence Bayesian Treatment \u00b6 Probabilistic Neural Network \u00b6 This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions. Learning: Maximum Likelihood \u00b6 Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} Final Layer \u00b6 This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric) Bayesian Neural Network \u00b6 So there are a few Benchmark datasets we can look at to determine Current top: * MC Dropout * Mean-Field Variational Inference * Deep Ensembles * Ensemble MC Dropout Benchmark Repos: * [OATML](https://github.com/OATML/bdl-benchmarks) * [Hugh Salimbeni](https://github.com/hughsalimbeni/bayesian_benchmarks) **Resources** * Neural Network Diagrams - [stack](https://softwarerecs.stackexchange.com/questions/47841/drawing-neural-networks#targetText=Drawing%20neural%20networks&targetText=Similar%20to%20the%20figures%20in,multilayer%20perceptron%20(neural%20network).) * MLSS 2019, Moscow - Yarin Gal - [Prezi I](http://bdl101.ml/MLSS_2019_BDL_1.pdf) | [Prezi II](http://bdl101.ml/MLSS_2019_BDL_2.pdf) * Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - [Blog](https://medium.com/lean-in-women-in-tech-india/fast-and-scalable-estimation-of-uncertainty-using-bayesian-deep-learning-e312571042bb) * Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - [Blog](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) * How Bayesian Methods Embody Occam's razor - [blog](https://medium.com/neuralspace/how-bayesian-methods-embody-occams-razor-43f3d0253137) * DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - [blog](https://medium.com/@ahmdtaha/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15) * Uncertainty Estimation in Supervised Learning - [Video](https://www.youtube.com/watch?v=P4WUl7TDdLo&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW&index=29&t=0s) | [Slides](https://github.com/bayesgroup/deepbayes-2019/tree/master/lectures/day6) **Blogs** * [Regression with Probabilistic Layers in TensorFlow Probability](https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf) * [Variational Inference for Bayesian Neural Networks](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/) (2019) | TensorFlow * Brenden Hasz * [Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability](https://brendanhasz.github.io/2018/12/03/tfp-regression.html) * [Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference](https://brendanhasz.github.io/2019/06/12/tfp-gmm.html) * [Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0](https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html) * Yarin Gal * [What My Deep Model Doesn't Know...](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) * High Level Series of Posts * [Probabilistic Deep Learning: Bayes by Backprop](https://medium.com/neuralspace/probabilistic-deep-learning-bayes-by-backprop-c4a3de0d9743) * [When machine learning meets complexity: why Bayesian deep learning is unavoidable](https://medium.com/neuralspace/when-machine-learning-meets-complexity-why-bayesian-deep-learning-is-unavoidable-55c97aa2a9cc) * [Bayesian Convolutional Neural Networks with Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) * [Reflections on Bayesian Inference in Probabilistic Deep Learning](https://medium.com/@laumannfelix/reflections-on-bayesian-inference-in-probabilistic-deep-learning-416376e42dc0) **Software** * [TensorFlow Probability]() * [Edward2]() * [PyTorch]() * [Pyro]() **Papers** * DropOut as Bayesian Approximation - [Paper](https://arxiv.org/pdf/1506.02142.pdf) | [Code]() | [Tutorial](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/) * Uncertainty Decomposition in BNNs with Latent Variables - [arxiv](https://arxiv.org/abs/1706.08495) * Practical Deep Learning with Bayesian Principles - [arxiv](https://arxiv.org/abs/1906.02506) * Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - [Paper]() * Probabilistic Numerics and Uncertainty in Computations - [Paper](https://arxiv.org/pdf/1506.01326.pdf) * Bayesian Inference of Log Determinants - [Paper](https://arxiv.org/pdf/1704.01445.pdf) **Code** * [A Regression Master Class with Aboleth](https://aboleth.readthedocs.io/en/stable/tutorials/some_regressors.html) * BNN Implementations - [Github](https://github.com/JavierAntoran/Bayesian-Neural-Networks) * A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - [Github](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)","title":"Neural Networks"},{"location":"thesis/chapters/3_data_representation/neural_nets/#neural-networks","text":"So in terms of neural networks and uncertainty, I would say there are about 3 classes of neural networks ranging from no uncertainty to full uncertainty. They include: * generic neural networks (NNs) which have no uncertainty * Probabilistic Neural Networks (PNNs) which have uncertainty in the predictions * Bayesian Neural Networks (BNNs) which have uncertainty on the weights as well Resources * Jee - Prob CV * Masterclass in regression - Aboleth - docs","title":"Neural Networks"},{"location":"thesis/chapters/3_data_representation/neural_nets/#weight-space","text":"This neural network is vanilla with no uncertainty at all. We have no probability assumptions about our data. We merely want some function f f that maps our data from X \\rightarrow Y X \\rightarrow Y . Summary Very expressive architectures Training yields the best model parameters Cannot quantify the output uncertainty or confidence","title":"Weight Space"},{"location":"thesis/chapters/3_data_representation/neural_nets/#bayesian-treatment","text":"","title":"Bayesian Treatment"},{"location":"thesis/chapters/3_data_representation/neural_nets/#probabilistic-neural-network","text":"This class of neural networks are very cheap to produce. They basically attach a probability distribution on the final layer of the network. They don't have any probability distributions on and of the weights of the network. Another way to think of it is as a feature extractor that maps all of the data to a . Another big difference is the training procedure. Typically one would have to use a log-likelihood estimation but this isn't always necessary for PNN with simpler assumptions.","title":"Probabilistic Neural Network"},{"location":"thesis/chapters/3_data_representation/neural_nets/#learning-maximum-likelihood","text":"Given some training data D=(X_i, Y_i), i=1,2,\\ldots,N D=(X_i, Y_i), i=1,2,\\ldots,N , we want to learn the parameters W W by maximizing the log likelihood i.e. \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned} \\begin{aligned} W^*&= \\underset{W}{\\text{argmax}} \\log p(D|W) \\\\ \\log p(D|W) &= \\sum_{i=1}^N \\log p(Y_i|X_i,W) \\end{aligned}","title":"Learning: Maximum Likelihood"},{"location":"thesis/chapters/3_data_representation/neural_nets/#final-layer","text":"This network is the simplest to implement: add a distribution as the final layer. Typically we assume a Gaussian distribution because it is the easiest to understand and the training procedure is also quite simple. \\mathcal{N}(\\mu(x), \\sigma) \\mathcal{N}(\\mu(x), \\sigma) where \\mu \\mu is our point estimate and \\sigma \\sigma is a constant noise parameter. Notice how the only parameter in our distribution is the mean function \\mu \\mu . We assume that the \\sigma \\sigma parameter is constant across every \\mu(x_i) \\mu(x_i) . This is known as a homoscedastic model. In this blog post they classified this as a known knowns . When training, we can use the negative log-likelihood or the mean absolute squared loss function. In terms of applicability, this model won't be so difficult to train but it isn't the most robust model because in the real world, we won't find such a simple noise assumption. But it's a cheap way to add some uncertainty estimates to your predictions. Regression p( p( Summary Extends deterministic DNN to produce an output distribution, p(y|x) p(y|x) Uses maximum likelihood estimation to obtain the best model It is still time consuming training and can still overfit It can only quantify data uncertainty (aleatoric)","title":"Final Layer"},{"location":"thesis/chapters/3_data_representation/neural_nets/#bayesian-neural-network","text":"So there are a few Benchmark datasets we can look at to determine Current top: * MC Dropout * Mean-Field Variational Inference * Deep Ensembles * Ensemble MC Dropout Benchmark Repos: * [OATML](https://github.com/OATML/bdl-benchmarks) * [Hugh Salimbeni](https://github.com/hughsalimbeni/bayesian_benchmarks) **Resources** * Neural Network Diagrams - [stack](https://softwarerecs.stackexchange.com/questions/47841/drawing-neural-networks#targetText=Drawing%20neural%20networks&targetText=Similar%20to%20the%20figures%20in,multilayer%20perceptron%20(neural%20network).) * MLSS 2019, Moscow - Yarin Gal - [Prezi I](http://bdl101.ml/MLSS_2019_BDL_1.pdf) | [Prezi II](http://bdl101.ml/MLSS_2019_BDL_2.pdf) * Fast and Scalable Estimation of Uncertainty using Bayesian Deep Learning - [Blog](https://medium.com/lean-in-women-in-tech-india/fast-and-scalable-estimation-of-uncertainty-using-bayesian-deep-learning-e312571042bb) * Making Your Neural Network Say \"I Don't Know\" - Bayesian NNs using Pyro and PyTorch - [Blog](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) * How Bayesian Methods Embody Occam's razor - [blog](https://medium.com/neuralspace/how-bayesian-methods-embody-occams-razor-43f3d0253137) * DropOUt as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - [blog](https://medium.com/@ahmdtaha/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15) * Uncertainty Estimation in Supervised Learning - [Video](https://www.youtube.com/watch?v=P4WUl7TDdLo&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW&index=29&t=0s) | [Slides](https://github.com/bayesgroup/deepbayes-2019/tree/master/lectures/day6) **Blogs** * [Regression with Probabilistic Layers in TensorFlow Probability](https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf) * [Variational Inference for Bayesian Neural Networks](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/) (2019) | TensorFlow * Brenden Hasz * [Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability](https://brendanhasz.github.io/2018/12/03/tfp-regression.html) * [Bayesian Gaussian Mixture Modeling with Stochastic Variational Inference](https://brendanhasz.github.io/2019/06/12/tfp-gmm.html) * [Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0](https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html) * Yarin Gal * [What My Deep Model Doesn't Know...](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) * High Level Series of Posts * [Probabilistic Deep Learning: Bayes by Backprop](https://medium.com/neuralspace/probabilistic-deep-learning-bayes-by-backprop-c4a3de0d9743) * [When machine learning meets complexity: why Bayesian deep learning is unavoidable](https://medium.com/neuralspace/when-machine-learning-meets-complexity-why-bayesian-deep-learning-is-unavoidable-55c97aa2a9cc) * [Bayesian Convolutional Neural Networks with Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) * [Reflections on Bayesian Inference in Probabilistic Deep Learning](https://medium.com/@laumannfelix/reflections-on-bayesian-inference-in-probabilistic-deep-learning-416376e42dc0) **Software** * [TensorFlow Probability]() * [Edward2]() * [PyTorch]() * [Pyro]() **Papers** * DropOut as Bayesian Approximation - [Paper](https://arxiv.org/pdf/1506.02142.pdf) | [Code]() | [Tutorial](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/) * Uncertainty Decomposition in BNNs with Latent Variables - [arxiv](https://arxiv.org/abs/1706.08495) * Practical Deep Learning with Bayesian Principles - [arxiv](https://arxiv.org/abs/1906.02506) * Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks - Foong et. al. (2019) - [Paper]() * Probabilistic Numerics and Uncertainty in Computations - [Paper](https://arxiv.org/pdf/1506.01326.pdf) * Bayesian Inference of Log Determinants - [Paper](https://arxiv.org/pdf/1704.01445.pdf) **Code** * [A Regression Master Class with Aboleth](https://aboleth.readthedocs.io/en/stable/tutorials/some_regressors.html) * BNN Implementations - [Github](https://github.com/JavierAntoran/Bayesian-Neural-Networks) * A Comprehensive Guide to Bayesian CNN with Variational Inference - Shridhar et al. (2019) - [Github](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)","title":"Bayesian Neural Network"},{"location":"thesis/chapters/3_data_representation/random_kernels/","text":"Randomized \u00b6 Nystrom Approximation \u00b6 Random Fourier Features \u00b6 Bayesian Treatment \u00b6 Sparse Spectrum Gaussian Processes \u00b6 Composition \u00b6 Random Feature Expansions \u00b6","title":"Randomized"},{"location":"thesis/chapters/3_data_representation/random_kernels/#randomized","text":"","title":"Randomized"},{"location":"thesis/chapters/3_data_representation/random_kernels/#nystrom-approximation","text":"","title":"Nystrom Approximation"},{"location":"thesis/chapters/3_data_representation/random_kernels/#random-fourier-features","text":"","title":"Random Fourier Features"},{"location":"thesis/chapters/3_data_representation/random_kernels/#bayesian-treatment","text":"","title":"Bayesian Treatment"},{"location":"thesis/chapters/3_data_representation/random_kernels/#sparse-spectrum-gaussian-processes","text":"","title":"Sparse Spectrum Gaussian Processes"},{"location":"thesis/chapters/3_data_representation/random_kernels/#composition","text":"","title":"Composition"},{"location":"thesis/chapters/3_data_representation/random_kernels/#random-feature-expansions","text":"","title":"Random Feature Expansions"},{"location":"thesis/chapters/4_information/density/0_overview/","text":"Overview \u00b6 blog * VAE Non-Invertible (VAEs) Invertible: Inverse Sampling Theorem (Uniform and PDFs) Change of Variables Core Topics Parametric Gaussianization Deep Density Destructors Information Theory Measures Generalized Divisive Normalization Supplementary Inverse Sampling Theorem Change of Variables Formula Entropy NegEntropy Classical * Parametric * Histogram * KDE * kNN Neural Density Deep Density Destructor Normalizing Flows Parametric Gaussianization Original: Projection Pursuit, Gaussianization Modern: Rotation-Based Iterative Gaussianization (RBIG) Approx: Generalized Divisive Normalization (GDN)","title":"Overview"},{"location":"thesis/chapters/4_information/density/0_overview/#overview","text":"blog * VAE Non-Invertible (VAEs) Invertible: Inverse Sampling Theorem (Uniform and PDFs) Change of Variables Core Topics Parametric Gaussianization Deep Density Destructors Information Theory Measures Generalized Divisive Normalization Supplementary Inverse Sampling Theorem Change of Variables Formula Entropy NegEntropy Classical * Parametric * Histogram * KDE * kNN Neural Density Deep Density Destructor Normalizing Flows Parametric Gaussianization Original: Projection Pursuit, Gaussianization Modern: Rotation-Based Iterative Gaussianization (RBIG) Approx: Generalized Divisive Normalization (GDN)","title":"Overview"},{"location":"thesis/chapters/4_information/density/classical/","text":"Density Estimation \u00b6 Classical (Binning, Kernel, Adaptive | KNN) Change of Variables (NF, Density Destructor, Gaussianization) Conditional","title":"Density Estimation"},{"location":"thesis/chapters/4_information/density/classical/#density-estimation","text":"Classical (Binning, Kernel, Adaptive | KNN) Change of Variables (NF, Density Destructor, Gaussianization) Conditional","title":"Density Estimation"},{"location":"thesis/chapters/4_information/density/gaussianization/","text":"Parametric Gaussianization \u00b6 Key Papers: * Gaussianization - Chen and Gopinath (2001) * Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2010) Why Gaussianization? \u00b6 Gaussianization : Transform multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . History \u00b6 Cost Function \u00b6 So, we have essentially described a model that transforms the data from the original data distribution \\mathcal{D} \\mathcal{D} to the normal distribution \\mathcal{N} \\mathcal{N} so now the question is: how well did we approximate the base distribution \\mathcal{N} \\mathcal{N} . We can use something called negentropy which is how far the transformed distribution is from the normal distribution. More concretely, it is the KLD between the transformed distribution, P_y P_y and the standard normal distribution, \\mathcal{N}\\sim(0, \\mathbf{I}) \\mathcal{N}\\sim(0, \\mathbf{I}) . We can write down the standard definition of entropy like so D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx However, it might make a bit more sense intuitively to rewrite this equation in terms of expectations. \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] This basically says want the expected value between the probabilities of our approximate base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) and the real base distribution \\mathcal{N}(z) \\mathcal{N}(z) . We have the equation of \\mathcal{P}_x(x) \\mathcal{P}_x(x) in terms of the probability of the base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) , so we can plug that into our negentropy \\mathcal{J}(\\mathcal{P}_z) \\mathcal{J}(\\mathcal{P}_z) formulation \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] We can unravel the log probabilities to something much simpler: \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, it's difficult to compute the expectations in terms of the base distribution z z . Instead let's make it factor of our data. We can do this by unravelling the \\mathbb{E}_z \\mathbb{E}_z \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Again, we utilize the fact that we've done a change of variables which means we can rewrite the expectation in terms of the Data distribution: \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] which means we can simplify this to be the expectation w.r.t. to the data distribution: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, to be more concrete about where our variables are coming from, we can substitute the z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) into our negentropy formulation: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] So now when it comes to minimizing the loss function, we just need to take the derivative w.r.t. to the parameters \\theta \\theta . All of our terms in this equation are dependent on the parameter \\theta \\theta . \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] The derivative of an expectation of something is the same as the expectation of a derivative ( \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] ) using the dominated convergence theorem ( stackoverflow ). So we can just take the derivative w.r.t. \\theta \\theta inside of the expectation $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x)) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} {\\theta}(x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ Let's take it term by term. First of all, we can see that the \\log \\mathcal{P}_x(x) \\log \\mathcal{P}_x(x) has no parameters dependent upon \\theta \\theta so we can immediately cancel that term. $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\cancel{\\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x))} - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} {\\theta}(x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ The second term ??? third term Practically speaking, this is a bit difficult to calculate. Instead we can do a procedure that measures how much more Gaussian the approximate base distribution has become as a result of the transformation \\mathcal{G}_{\\theta}(x) \\mathcal{G}_{\\theta}(x) . -- In the Wild \u00b6 Gaussianization for Fast and Accurate Inference from Cosmological Data Nice formula for how to calculate the likelihood.","title":"Parametric Gaussianization"},{"location":"thesis/chapters/4_information/density/gaussianization/#parametric-gaussianization","text":"Key Papers: * Gaussianization - Chen and Gopinath (2001) * Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2010)","title":"Parametric Gaussianization"},{"location":"thesis/chapters/4_information/density/gaussianization/#why-gaussianization","text":"Gaussianization : Transform multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"thesis/chapters/4_information/density/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| \\mathcal{P}_x(x)= \\mathcal{P}_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"thesis/chapters/4_information/density/gaussianization/#history","text":"","title":"History"},{"location":"thesis/chapters/4_information/density/gaussianization/#cost-function","text":"So, we have essentially described a model that transforms the data from the original data distribution \\mathcal{D} \\mathcal{D} to the normal distribution \\mathcal{N} \\mathcal{N} so now the question is: how well did we approximate the base distribution \\mathcal{N} \\mathcal{N} . We can use something called negentropy which is how far the transformed distribution is from the normal distribution. More concretely, it is the KLD between the transformed distribution, P_y P_y and the standard normal distribution, \\mathcal{N}\\sim(0, \\mathbf{I}) \\mathcal{N}\\sim(0, \\mathbf{I}) . We can write down the standard definition of entropy like so D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx D_{KLD}(P_z||\\mathcal{N}(0, \\mathbf{I}))=\\int_{-\\infty}^{\\infty}\\mathcal{P}_z(z) \\log \\frac{\\mathcal{P}_z(z)}{\\mathcal{N}(0, \\mathbf{I})}dx However, it might make a bit more sense intuitively to rewrite this equation in terms of expectations. \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_z(z) - \\log \\mathcal{N}(z)\\right] This basically says want the expected value between the probabilities of our approximate base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) and the real base distribution \\mathcal{N}(z) \\mathcal{N}(z) . We have the equation of \\mathcal{P}_x(x) \\mathcal{P}_x(x) in terms of the probability of the base distribution \\mathcal{P}_z(z) \\mathcal{P}_z(z) , so we can plug that into our negentropy \\mathcal{J}(\\mathcal{P}_z) \\mathcal{J}(\\mathcal{P}_z) formulation \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\left( \\mathcal{P}_x(x)\\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\right) - \\log \\mathcal{N}(z)\\right] We can unravel the log probabilities to something much simpler: \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\mathbb{E}_z\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, it's difficult to compute the expectations in terms of the base distribution z z . Instead let's make it factor of our data. We can do this by unravelling the \\mathbb{E}_z \\mathbb{E}_z \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_z(z)\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Again, we utilize the fact that we've done a change of variables which means we can rewrite the expectation in terms of the Data distribution: \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)=\\sum_{-\\infty}^{\\infty}\\mathcal{P}_{x}\\left( x \\right) \\left| \\frac{\\partial z}{\\partial x} \\right|^{-1}\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] which means we can simplify this to be the expectation w.r.t. to the data distribution: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial z}{\\partial x} \\right| - \\log \\mathcal{N}(z)\\right] Now, to be more concrete about where our variables are coming from, we can substitute the z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) into our negentropy formulation: \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\mathcal{J}(\\mathcal{P}_z)= \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] So now when it comes to minimizing the loss function, we just need to take the derivative w.r.t. to the parameters \\theta \\theta . All of our terms in this equation are dependent on the parameter \\theta \\theta . \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] \\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_x\\left[ \\log \\mathcal{P}_x(x) - \\log \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| - \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x))\\right] The derivative of an expectation of something is the same as the expectation of a derivative ( \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] \\frac{\\partial}{\\partial \\theta}(\\mathbb{E}_x[\\cdot]=\\mathbb{E}_x[\\frac{\\partial}{\\partial \\theta}(\\cdot)] ) using the dominated convergence theorem ( stackoverflow ). So we can just take the derivative w.r.t. \\theta \\theta inside of the expectation $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x)) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} {\\theta}(x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ Let's take it term by term. First of all, we can see that the \\log \\mathcal{P}_x(x) \\log \\mathcal{P}_x(x) has no parameters dependent upon \\theta \\theta so we can immediately cancel that term. $$\\frac{\\partial \\mathcal{J}(\\mathcal{P}_z)}{\\partial \\theta}= \\mathbb{E} x\\left[ \\cancel{\\frac{\\partial}{\\partial \\theta}(\\log \\mathcal{P}_x(x))} - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\left| \\frac{\\partial \\mathcal{G} {\\theta}(x)}{\\partial x} \\right|\\right) - \\frac{\\partial}{\\partial \\theta} \\left( \\log \\mathcal{N}(\\mathcal{G}_{\\theta}(x)) \\right) \\right]$$ The second term ??? third term Practically speaking, this is a bit difficult to calculate. Instead we can do a procedure that measures how much more Gaussian the approximate base distribution has become as a result of the transformation \\mathcal{G}_{\\theta}(x) \\mathcal{G}_{\\theta}(x) . --","title":"Cost Function"},{"location":"thesis/chapters/4_information/density/gaussianization/#in-the-wild","text":"Gaussianization for Fast and Accurate Inference from Cosmological Data Nice formula for how to calculate the likelihood.","title":"In the Wild"},{"location":"thesis/chapters/4_information/density/nfs/","text":"Normalizing Flows \u00b6 Density Destructor \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| Survey of Literature \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Normalizing Flows"},{"location":"thesis/chapters/4_information/density/nfs/#normalizing-flows","text":"","title":"Normalizing Flows"},{"location":"thesis/chapters/4_information/density/nfs/#density-destructor","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) z \\sim \\mathcal{P}_\\mathcal{Z}$$ $$\\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Density Destructor"},{"location":"thesis/chapters/4_information/density/nfs/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"thesis/chapters/4_information/density/nfs/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"thesis/chapters/4_information/density/parametric/","text":"","title":"Parametric"},{"location":"thesis/chapters/4_information/density/rbig/","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 Motivation \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Algorithm \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"thesis/chapters/4_information/density/rbig/#rotation-based-iterative-gaussianization-rbig","text":"","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"thesis/chapters/4_information/density/rbig/#motivation","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Motivation"},{"location":"thesis/chapters/4_information/density/rbig/#algorithm","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Algorithm"},{"location":"thesis/chapters/4_information/density/rbig/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"thesis/chapters/4_information/density/rbig/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"thesis/chapters/4_information/density/rbig/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"thesis/chapters/4_information/density/rbig/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"thesis/chapters/4_information/information/1_information/","text":"Information Theory \u00b6 In this report, I will be outlining what exactly Information theory is and what it means in the machine learning context. Many times when people ask me what I do, I say that I look at Information theory (IT) measures in the context of a generative network. But sometimes I have a difficult time convincing myself that these measures are actually useful. I think this is partially because I don't fully understand the magnitude of IT measures and what they can do. So this post is designed to help me (and others) really dig deep into the space of information measures and I hope this will help someone else who is also interested in understanding IT without any formal classes. This post will not be formula heavy and will instead focus on concepts. See the end of the post for some additional references where you can explore each of these ideas even further. Definition: What is Information? \u00b6 Firstly, let's start with the definition of information . This is a difficult definition and I've seen many different explanations which are outlined below. The amount of surprise, i.e. I'm surprised when something that doesn't usually happen, happens. - Jesus Malo, 2019 This is my first definition which really resonates with me till this day. I think the concept of surprise is quite easy to grasp because it is intuitive. The first example I have is when I google the answer to my question. Things that are informative Information is what remains after every iota of natural redundancy has been squeezed out of a message, and after every aimless syllable of noise has been removed. It is the unfettered essence that passes from computer to computer, from satellite to Earth, from eye to brain, and (over many generations of natural selection) from the natural world to the collective gene pool of every species. - James V Stone, 2015 This is a very good general definition of information from a broad perspective. It is very involved so it stresses how information can be thought of from many different perspectives including remote sensing, neuroscience and biology. It also tries to hint at the distinction between useful information and noise: signal (useful) and noise (useless). This definition comes from the first chapter of the book written by James Stone (2015). I highly recommend you read it as it gives the best introduction to information theory that I have ever seen. Information is the resolution of uncertainty. - Shannon C, 1948 This is the classic definition of uncertainty given by the creator himself. ... In information theory, one variable provides information about another variable when knowledge of the first, on average, reduces uncertainty in the second. - Cover & Thomas, 2006 This is technically the definition of mutual information (MI) which is the extension of entropy to 2 or more variables. However, I think this definition actually captures almost all aspects of the information theory measures Concrete Definitions \u00b6 So we have expressed how these methods work in words but I am Deterministic Perspective \u00b6 The number of data points that one needs to represent a signal without an error. - Gabor, 1946 \\text{Information} \\propto \\Delta x \\cdot \\Delta f \\text{Information} \\propto \\Delta x \\cdot \\Delta f Units: Wavelets (Log ones) Probabilistic Perspective \u00b6 The reduction in uncertainty that was caused by the knowledge of a signal. - Shannon, 1948 \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx Units: Bits In this definition, the uncertainty is related to the volume of the PDF (more volume means more uncertainty). Information theory makes statements about the shapes of probability distributions. Interpretable Quantities: Bits \u00b6 Key Idea : One bit is the amount of information required to choose between two equally probable alternatives Key Idea : If you have n n bits of information then you can choose from m=2^n m=2^n equally probable alternatives. Equivalently, if you have to choose between m m equally probable alternatives, then you need n=\\log_2 m n=\\log_2 m bits of information. Key Idea : A bit is the amount of information required to choose between two equally probable alternatives (e.g. left/right), whereas a binary digit is the value of a binary variable, which can adapt one of two possible values (i.e. 0/1). Motivation: Why use IT measures? \u00b6 I often wonder why should one use information theory measures in the first place. What makes IT a useful tool in the different science fields. I found this introduction from [1] to have the best characterization of why one should use IT measures as they outline a few reasons why one can use IT measures. Information theory is model independent This statement is very powerful but they basically mean that we do not need to make any assumptions about the structure or the interactions between the data. We input the data and we get out a quantitative measure that tells us the relationship within our data. That allows us to be flexible as we should be able to capture different types of phenomena without the need to make assumptions and assumed models that could result in limiting results. No need to make a pre-defined model. It sounds nice as a first step analysis. Warning: Often, we do need to make assumptions about our data, e.g. independence between observations, constant output, and also the bin size that is needed when capturing the probability distribution. These parameters can definitely have an impact on the final numbers that we obtain. Information theory is data independent This statement means that we can apply IT measures on any type of data. We can also do certain transformations of said data and then apply the IT measures on the resulting transformation. This allows us to monitor how the information changes through transformations which can be useful in applications ranging from biological such as the brain to machine learning such as deep neural networks. Information theory can detect linear and nonlinear interactions This speaks for itself. As we get more and more data and get better at measuring with better sensors, we can no longer describe the word with linear approximations. The relationships between variables are often non-linear and we need models that are able to capture these nonlinear interactions. IT measures can capture both and this can be useful in practice. Information theory is naturally multivariate In principle, IT measures are well-calibrated enough to handle large multivariate (and multi-dimensional) systems. The easiest number of variables to understand include 2-3 variables but the techniques can be applied to a higher number of variables. Warning: In practice, you may find that many techniques are not equipped to handle high-dimensional and high-multivariate simply because the method of PDF estimation becomes super difficult with big data. This is essential to calculate any kind of IT measure so this can end up being the bottleneck in many algorithms. Information theory produces results in general units of bits (or nats) The units from IT measures are universal. No matter what the inputs are of the units, we will get the same amount of information in the output. This allows for easy comparisons between different variables under different transformations. Warning: The units are universal and absolute but it does not mean they convey meaning or understanding. I think it's important to look at them from a relative perspective, e.g. IT measure 1 has 2 bits which is higher than IT measure 2 which has only 1 bit. Overall, we can see that IT measures can handle multi-dimension and multivariate data, are model-free and have universal units of comparison. They are dependent on accurate PDF estimates and this. They have a strong argument for preliminary analysis of variables and variable interactions as well as complex systems. [1]: Timme & Lapish, A Tutorial for Information Theory in Neuroscience , eNeuro , 2018 What can IT measures tell us? \u00b6 The authors from [1] also pose a good question for us to answer about why one should use IT measures. I agree with the authors that it is important to know what types of questions we can answer given some model or measurement. In the context of IT measures, they highlight a few things we can gain: IT measures can quantify uncertainty of one or more variables. It is possible to quantify how much a variable expected to vary as well as the expected noise that we use in the system. 2. IT measures can be used to restrict the space of possible models. I have personal experience with this as we used this to determine how many spatial-spectral-temporal features were necessary for inputs to a model. We also looked at the information content of different variables and could determine how well this did. We also can highlight things we cannot do: We cannot produce models that describe how a system functions. A good example is Mutual information. It can tell you how many bits of information is shared between data but not anything about how the system of variables are related; just that the variables are related to an extent. 2. The output units are universal so they cannot be used to produce outputs in terms of the original input variables. In other words, I cannot use the outputs of the IT measures as outputs to a model as they make little sense in the real world. Again, the authors highlight (and I would like to highlight as well): IT measures can be a good way to help build your model, e.g. it can limit the amount of variables you would like to use based on expected uncertainty or mutual information content. It is a quantified measure in absolute relative units which can help the user make decisions in what variables to include or what transformations to use. Information (Revisited) \u00b6 Now we will start to do a deep dive into IT measures and define them in a more concrete sense. I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} The intuitive definition is important because it really showcases how the heuristic works in the end. I'm actually not entirely sure if there is a mathematical way to formalate this without some sort of axiom that we mentioned before about surprise and uncertainty . We use logs because... Example Pt I: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve Entropy \u00b6 This is an upper bound on the amount of information you can convey without any loss ( source ). More entropy means more randomness or uncertainty H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx We use logs so that wee get sums of entropies. It implies independence but the log also forces sums. H(Y|X) = H(X,Y)-H(X) H(Y|X) = H(X,Y)-H(X) H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy Examples \u00b6 Example Pt II: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve Under Transformations \u00b6 In my line of work, we work with generative models that utilize the change of variable formulation in order to estimate some distribution with H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] Under rotation: Entropy is invariant Under scale: Entropy is ...??? Computational Cost...? Relative Entropy (Kullback Leibler Divergence) \u00b6 This is the measure of the distance between two distributions. I like the term relative entropy because it offers a different perspective in relation to information theory measures. D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 If you've studied machine learning then you are fully aware that it is not a distance as this measure is not symmetric i.e. D_{KL}(p||q) \\neq D_{KL}(q||p) D_{KL}(p||q) \\neq D_{KL}(q||p) . Under Transformations \u00b6 The KLD is invariance under invertible affine transformations, e.g. b = \\mu + Ga, \\nabla F = G b = \\mu + Ga, \\nabla F = G Mutual Information \u00b6 This is the reduction of uncertainty of one random variable due to the knowledge of another (like the definition above). It is the amount of information one r.v. contains about another r.v.. Total Correlation \u00b6 This isn't really talked about outside of the ML community but I think this is a useful measure to have; especially when dealing with multi-dimensional and multi-variate datesets. PDF Estimation \u00b6 For almost all of these measures to work, we need to have a really good PDF estimation of our dataset, \\mathcal{X} \\mathcal{X} . This is a hard problem and should not be taken lightly. There is an entire field of methods that can be used, e.g. autoregressive models, generative networks, and Gaussianization. One of the simplest techniques (and often fairly effective) is just histogram transformation. I work specifically with Gaussianization methods and we have found that a simple histogram transformation works really well. It also led to some properties which allow one to estimate some IT measures in unison with PDF estimation. Another way of estimating PDFs would be to look at kernel methods (Parezen Windows). A collaborator works with this methodology and has found success in utitlize kernel methods and have also been able to provide good IT measures through these techniques. References \u00b6 Supplementary Material \u00b6 GPs and IT \u00b6 References \u00b6 Gaussian Processes and Information Theory \u00b6 Information Theory \u00b6 Information Theory Tutorial: The Manifold Things Information Measures - YouTube On Measures of Entropy and Information - Understanding Interdependency Through Complex Information Sharing - Rosas et. al. (2016) The Information Bottleneck of Deep Learning - Youtube Maximum Entropy Distributions - blog Articles A New Outlook on Shannon's Information Measures - Yeung (1991) - pdf A Brief Introduction to Shannon's Information Theory - Chen (2018) - arxiv Information Theory for Intelligent People - DeDeo (2018) - pdf Blogs Visual Information Theory - Colah (2015) - blog Better Intuition for Information Theory - Kirsch (2019) - blog A Brief History of Information Theory - Vasiloudis (2019) - blog Information Theory of Deep Learning - Sharma (2019) - blog Books Information theory Software \u00b6 Python Packages \u00b6 Discrete Information Theory ( dit ) - Github | Docs Python Information Theory Measures (****) - Github | Docs Parallelized Mutual Information Measures - blog | Github Implementations \u00b6 Mutual Information Calculation with Numpy - stackoverflow Unexplored Stuff \u00b6 These are my extra notes from resources I have found.","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#information-theory","text":"In this report, I will be outlining what exactly Information theory is and what it means in the machine learning context. Many times when people ask me what I do, I say that I look at Information theory (IT) measures in the context of a generative network. But sometimes I have a difficult time convincing myself that these measures are actually useful. I think this is partially because I don't fully understand the magnitude of IT measures and what they can do. So this post is designed to help me (and others) really dig deep into the space of information measures and I hope this will help someone else who is also interested in understanding IT without any formal classes. This post will not be formula heavy and will instead focus on concepts. See the end of the post for some additional references where you can explore each of these ideas even further.","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#definition-what-is-information","text":"Firstly, let's start with the definition of information . This is a difficult definition and I've seen many different explanations which are outlined below. The amount of surprise, i.e. I'm surprised when something that doesn't usually happen, happens. - Jesus Malo, 2019 This is my first definition which really resonates with me till this day. I think the concept of surprise is quite easy to grasp because it is intuitive. The first example I have is when I google the answer to my question. Things that are informative Information is what remains after every iota of natural redundancy has been squeezed out of a message, and after every aimless syllable of noise has been removed. It is the unfettered essence that passes from computer to computer, from satellite to Earth, from eye to brain, and (over many generations of natural selection) from the natural world to the collective gene pool of every species. - James V Stone, 2015 This is a very good general definition of information from a broad perspective. It is very involved so it stresses how information can be thought of from many different perspectives including remote sensing, neuroscience and biology. It also tries to hint at the distinction between useful information and noise: signal (useful) and noise (useless). This definition comes from the first chapter of the book written by James Stone (2015). I highly recommend you read it as it gives the best introduction to information theory that I have ever seen. Information is the resolution of uncertainty. - Shannon C, 1948 This is the classic definition of uncertainty given by the creator himself. ... In information theory, one variable provides information about another variable when knowledge of the first, on average, reduces uncertainty in the second. - Cover & Thomas, 2006 This is technically the definition of mutual information (MI) which is the extension of entropy to 2 or more variables. However, I think this definition actually captures almost all aspects of the information theory measures","title":"Definition: What is Information?"},{"location":"thesis/chapters/4_information/information/1_information/#concrete-definitions","text":"So we have expressed how these methods work in words but I am","title":"Concrete Definitions"},{"location":"thesis/chapters/4_information/information/1_information/#deterministic-perspective","text":"The number of data points that one needs to represent a signal without an error. - Gabor, 1946 \\text{Information} \\propto \\Delta x \\cdot \\Delta f \\text{Information} \\propto \\Delta x \\cdot \\Delta f Units: Wavelets (Log ones)","title":"Deterministic Perspective"},{"location":"thesis/chapters/4_information/information/1_information/#probabilistic-perspective","text":"The reduction in uncertainty that was caused by the knowledge of a signal. - Shannon, 1948 \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx \\text{Information} \\propto \\int_{\\mathcal{X}} p(x) \\cdot \\log_2 \\frac{1}{p(x)} \\cdot dx Units: Bits In this definition, the uncertainty is related to the volume of the PDF (more volume means more uncertainty). Information theory makes statements about the shapes of probability distributions.","title":"Probabilistic Perspective"},{"location":"thesis/chapters/4_information/information/1_information/#interpretable-quantities-bits","text":"Key Idea : One bit is the amount of information required to choose between two equally probable alternatives Key Idea : If you have n n bits of information then you can choose from m=2^n m=2^n equally probable alternatives. Equivalently, if you have to choose between m m equally probable alternatives, then you need n=\\log_2 m n=\\log_2 m bits of information. Key Idea : A bit is the amount of information required to choose between two equally probable alternatives (e.g. left/right), whereas a binary digit is the value of a binary variable, which can adapt one of two possible values (i.e. 0/1).","title":"Interpretable Quantities: Bits"},{"location":"thesis/chapters/4_information/information/1_information/#motivation-why-use-it-measures","text":"I often wonder why should one use information theory measures in the first place. What makes IT a useful tool in the different science fields. I found this introduction from [1] to have the best characterization of why one should use IT measures as they outline a few reasons why one can use IT measures. Information theory is model independent This statement is very powerful but they basically mean that we do not need to make any assumptions about the structure or the interactions between the data. We input the data and we get out a quantitative measure that tells us the relationship within our data. That allows us to be flexible as we should be able to capture different types of phenomena without the need to make assumptions and assumed models that could result in limiting results. No need to make a pre-defined model. It sounds nice as a first step analysis. Warning: Often, we do need to make assumptions about our data, e.g. independence between observations, constant output, and also the bin size that is needed when capturing the probability distribution. These parameters can definitely have an impact on the final numbers that we obtain. Information theory is data independent This statement means that we can apply IT measures on any type of data. We can also do certain transformations of said data and then apply the IT measures on the resulting transformation. This allows us to monitor how the information changes through transformations which can be useful in applications ranging from biological such as the brain to machine learning such as deep neural networks. Information theory can detect linear and nonlinear interactions This speaks for itself. As we get more and more data and get better at measuring with better sensors, we can no longer describe the word with linear approximations. The relationships between variables are often non-linear and we need models that are able to capture these nonlinear interactions. IT measures can capture both and this can be useful in practice. Information theory is naturally multivariate In principle, IT measures are well-calibrated enough to handle large multivariate (and multi-dimensional) systems. The easiest number of variables to understand include 2-3 variables but the techniques can be applied to a higher number of variables. Warning: In practice, you may find that many techniques are not equipped to handle high-dimensional and high-multivariate simply because the method of PDF estimation becomes super difficult with big data. This is essential to calculate any kind of IT measure so this can end up being the bottleneck in many algorithms. Information theory produces results in general units of bits (or nats) The units from IT measures are universal. No matter what the inputs are of the units, we will get the same amount of information in the output. This allows for easy comparisons between different variables under different transformations. Warning: The units are universal and absolute but it does not mean they convey meaning or understanding. I think it's important to look at them from a relative perspective, e.g. IT measure 1 has 2 bits which is higher than IT measure 2 which has only 1 bit. Overall, we can see that IT measures can handle multi-dimension and multivariate data, are model-free and have universal units of comparison. They are dependent on accurate PDF estimates and this. They have a strong argument for preliminary analysis of variables and variable interactions as well as complex systems. [1]: Timme & Lapish, A Tutorial for Information Theory in Neuroscience , eNeuro , 2018","title":"Motivation: Why use IT measures?"},{"location":"thesis/chapters/4_information/information/1_information/#what-can-it-measures-tell-us","text":"The authors from [1] also pose a good question for us to answer about why one should use IT measures. I agree with the authors that it is important to know what types of questions we can answer given some model or measurement. In the context of IT measures, they highlight a few things we can gain: IT measures can quantify uncertainty of one or more variables. It is possible to quantify how much a variable expected to vary as well as the expected noise that we use in the system. 2. IT measures can be used to restrict the space of possible models. I have personal experience with this as we used this to determine how many spatial-spectral-temporal features were necessary for inputs to a model. We also looked at the information content of different variables and could determine how well this did. We also can highlight things we cannot do: We cannot produce models that describe how a system functions. A good example is Mutual information. It can tell you how many bits of information is shared between data but not anything about how the system of variables are related; just that the variables are related to an extent. 2. The output units are universal so they cannot be used to produce outputs in terms of the original input variables. In other words, I cannot use the outputs of the IT measures as outputs to a model as they make little sense in the real world. Again, the authors highlight (and I would like to highlight as well): IT measures can be a good way to help build your model, e.g. it can limit the amount of variables you would like to use based on expected uncertainty or mutual information content. It is a quantified measure in absolute relative units which can help the user make decisions in what variables to include or what transformations to use.","title":"What can IT measures tell us?"},{"location":"thesis/chapters/4_information/information/1_information/#information-revisited","text":"Now we will start to do a deep dive into IT measures and define them in a more concrete sense. I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} I(x)= \\underset{Intuition}{\\log \\frac{1}{p(x)}} = \\underset{Simplified}{- \\log p(x)} The intuitive definition is important because it really showcases how the heuristic works in the end. I'm actually not entirely sure if there is a mathematical way to formalate this without some sort of axiom that we mentioned before about surprise and uncertainty . We use logs because... Example Pt I: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve","title":"Information (Revisited)"},{"location":"thesis/chapters/4_information/information/1_information/#entropy","text":"This is an upper bound on the amount of information you can convey without any loss ( source ). More entropy means more randomness or uncertainty H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx H(X)=\\int_{\\mathcal{X}}p(x)\\cdot \\log p(x) \\cdot dx We use logs so that wee get sums of entropies. It implies independence but the log also forces sums. H(Y|X) = H(X,Y)-H(X) H(Y|X) = H(X,Y)-H(X) H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy H(Y|X) = \\int_{\\mathcal{X}, \\mathcal{Y}}p(x,y) \\log \\frac{p(x,y)}{p(x)}dxdy","title":"Entropy"},{"location":"thesis/chapters/4_information/information/1_information/#examples","text":"Example Pt II: Delta Function, Uniform Function, Binomial Curve, Gaussian Curve","title":"Examples"},{"location":"thesis/chapters/4_information/information/1_information/#under-transformations","text":"In my line of work, we work with generative models that utilize the change of variable formulation in order to estimate some distribution with H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] H(Y) = H(X) + \\mathbb{E}\\left[ \\log |\\nabla f(X)|\\right] Under rotation: Entropy is invariant Under scale: Entropy is ...??? Computational Cost...?","title":"Under Transformations"},{"location":"thesis/chapters/4_information/information/1_information/#relative-entropy-kullback-leibler-divergence","text":"This is the measure of the distance between two distributions. I like the term relative entropy because it offers a different perspective in relation to information theory measures. D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 D_{KL}(p||q) = \\int_{\\mathcal{X}}p(x) \\cdot \\log \\frac{p(x)}{q(x)} \\cdot dx \\geq 0 If you've studied machine learning then you are fully aware that it is not a distance as this measure is not symmetric i.e. D_{KL}(p||q) \\neq D_{KL}(q||p) D_{KL}(p||q) \\neq D_{KL}(q||p) .","title":"Relative Entropy (Kullback Leibler Divergence)"},{"location":"thesis/chapters/4_information/information/1_information/#under-transformations_1","text":"The KLD is invariance under invertible affine transformations, e.g. b = \\mu + Ga, \\nabla F = G b = \\mu + Ga, \\nabla F = G","title":"Under Transformations"},{"location":"thesis/chapters/4_information/information/1_information/#mutual-information","text":"This is the reduction of uncertainty of one random variable due to the knowledge of another (like the definition above). It is the amount of information one r.v. contains about another r.v..","title":"Mutual Information"},{"location":"thesis/chapters/4_information/information/1_information/#total-correlation","text":"This isn't really talked about outside of the ML community but I think this is a useful measure to have; especially when dealing with multi-dimensional and multi-variate datesets.","title":"Total Correlation"},{"location":"thesis/chapters/4_information/information/1_information/#pdf-estimation","text":"For almost all of these measures to work, we need to have a really good PDF estimation of our dataset, \\mathcal{X} \\mathcal{X} . This is a hard problem and should not be taken lightly. There is an entire field of methods that can be used, e.g. autoregressive models, generative networks, and Gaussianization. One of the simplest techniques (and often fairly effective) is just histogram transformation. I work specifically with Gaussianization methods and we have found that a simple histogram transformation works really well. It also led to some properties which allow one to estimate some IT measures in unison with PDF estimation. Another way of estimating PDFs would be to look at kernel methods (Parezen Windows). A collaborator works with this methodology and has found success in utitlize kernel methods and have also been able to provide good IT measures through these techniques.","title":"PDF Estimation"},{"location":"thesis/chapters/4_information/information/1_information/#references","text":"","title":"References"},{"location":"thesis/chapters/4_information/information/1_information/#supplementary-material","text":"","title":"Supplementary Material"},{"location":"thesis/chapters/4_information/information/1_information/#gps-and-it","text":"","title":"GPs and IT"},{"location":"thesis/chapters/4_information/information/1_information/#references_1","text":"","title":"References"},{"location":"thesis/chapters/4_information/information/1_information/#gaussian-processes-and-information-theory","text":"","title":"Gaussian Processes and Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#information-theory_1","text":"Information Theory Tutorial: The Manifold Things Information Measures - YouTube On Measures of Entropy and Information - Understanding Interdependency Through Complex Information Sharing - Rosas et. al. (2016) The Information Bottleneck of Deep Learning - Youtube Maximum Entropy Distributions - blog Articles A New Outlook on Shannon's Information Measures - Yeung (1991) - pdf A Brief Introduction to Shannon's Information Theory - Chen (2018) - arxiv Information Theory for Intelligent People - DeDeo (2018) - pdf Blogs Visual Information Theory - Colah (2015) - blog Better Intuition for Information Theory - Kirsch (2019) - blog A Brief History of Information Theory - Vasiloudis (2019) - blog Information Theory of Deep Learning - Sharma (2019) - blog Books Information theory","title":"Information Theory"},{"location":"thesis/chapters/4_information/information/1_information/#software","text":"","title":"Software"},{"location":"thesis/chapters/4_information/information/1_information/#python-packages","text":"Discrete Information Theory ( dit ) - Github | Docs Python Information Theory Measures (****) - Github | Docs Parallelized Mutual Information Measures - blog | Github","title":"Python Packages"},{"location":"thesis/chapters/4_information/information/1_information/#implementations","text":"Mutual Information Calculation with Numpy - stackoverflow","title":"Implementations"},{"location":"thesis/chapters/4_information/information/1_information/#unexplored-stuff","text":"These are my extra notes from resources I have found.","title":"Unexplored Stuff"},{"location":"thesis/chapters/4_information/information/2_rbig/","text":"TODO \u00b6","title":"TODO"},{"location":"thesis/chapters/4_information/information/2_rbig/#todo","text":"","title":"TODO"},{"location":"thesis/chapters/5_applications/sensitivity/","text":"Sensitivity Applications \u00b6 Emulators (GPGSA) \u00b6 SAKAME \u00b6 MultiOuput Ocean \u00b6 MOGPs","title":"Sensitivity Applications"},{"location":"thesis/chapters/5_applications/sensitivity/#sensitivity-applications","text":"","title":"Sensitivity Applications"},{"location":"thesis/chapters/5_applications/sensitivity/#emulators-gpgsa","text":"","title":"Emulators (GPGSA)"},{"location":"thesis/chapters/5_applications/sensitivity/#sakame","text":"","title":"SAKAME"},{"location":"thesis/chapters/5_applications/sensitivity/#multiouput-ocean","text":"MOGPs","title":"MultiOuput Ocean"},{"location":"thesis/chapters/5_applications/similarity/","text":"Similarity Applications \u00b6 Spatial-Temporal (RBIG4EO) \u00b6 Drought Factors (RBIG4EO) \u00b6 Climate Models (RBIG4EO) \u00b6","title":"Similarity Applications"},{"location":"thesis/chapters/5_applications/similarity/#similarity-applications","text":"","title":"Similarity Applications"},{"location":"thesis/chapters/5_applications/similarity/#spatial-temporal-rbig4eo","text":"","title":"Spatial-Temporal (RBIG4EO)"},{"location":"thesis/chapters/5_applications/similarity/#drought-factors-rbig4eo","text":"","title":"Drought Factors (RBIG4EO)"},{"location":"thesis/chapters/5_applications/similarity/#climate-models-rbig4eo","text":"","title":"Climate Models (RBIG4EO)"},{"location":"thesis/chapters/5_applications/uncertainty/","text":"Uncertainty Applications \u00b6 Input Uncertainty (EGP, EGP2.0) \u00b6 EGP 1.0 \u00b6 EGP 2.0 \u00b6 Modeling Uncertainty (BNN4...) \u00b6","title":"Uncertainty Applications"},{"location":"thesis/chapters/5_applications/uncertainty/#uncertainty-applications","text":"","title":"Uncertainty Applications"},{"location":"thesis/chapters/5_applications/uncertainty/#input-uncertainty-egp-egp20","text":"","title":"Input Uncertainty (EGP, EGP2.0)"},{"location":"thesis/chapters/5_applications/uncertainty/#egp-10","text":"","title":"EGP 1.0"},{"location":"thesis/chapters/5_applications/uncertainty/#egp-20","text":"","title":"EGP 2.0"},{"location":"thesis/chapters/5_applications/uncertainty/#modeling-uncertainty-bnn4","text":"","title":"Modeling Uncertainty (BNN4...)"},{"location":"tutorials/","text":"Tutorials \u00b6","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/gaussian_processes/_ideas/","text":"From Scratch \u00b6","title":" ideas"},{"location":"tutorials/gaussian_processes/_ideas/#from-scratch","text":"","title":"From Scratch"},{"location":"tutorials/jax/","text":"r","title":"Index"},{"location":"tutorials/jax/egps/","text":"Error GPs using Jax \u00b6 Standard GP \u00b6 Linearized G \u00b6","title":"Error GPs using Jax"},{"location":"tutorials/jax/egps/#error-gps-using-jax","text":"","title":"Error GPs using Jax"},{"location":"tutorials/jax/egps/#standard-gp","text":"","title":"Standard GP"},{"location":"tutorials/jax/egps/#linearized-g","text":"","title":"Linearized G"},{"location":"tutorials/jax/ideas/","text":"Jax Tutorial Ideas \u00b6 Main Ideas \u00b6 Introduction Linear Regression, Regularization, vmap, jit, grad, jacobian, hessian Gaussian Processes Exact GP, Kernel, MLE, Comparing Methods \u00b6 Exact GP Linearized GP (Taylor Expansion, 1st Order, 2nd Order) Moment-Matching GP Scratch - Loops, numba Jax - vmap, jit MCMC GP Scratch - Collin numpyro - NUTS/HMC Variational GP scratch Idea - Pyro numpyro - SVI Data \u00b6 1D Examples 2D Examples IASI Example Ocean Data Future Stuff \u00b6 Sparse Models Deep Models DKL Models 1. Using Jax \u00b6 Basics: * grad * jit * vmap * jacobian * hessian * params * optimizers 2. Regression Master Class \u00b6 3. Kernel Methods \u00b6 Kernel Least Squares Gaussian Processes Krasserm Support Vector Machines 4. Special Algorithms \u00b6 Optimized Kernel Ridge Regression Gonzalo's Notebook Optimized Kernel Entropy Components Analysis 5. Input Uncertainty \u00b6 Taylor Expansion Moment Matching Variational MCMC (NUTS/HMC) Heteroscedastic Likelihood","title":"Jax Tutorial Ideas"},{"location":"tutorials/jax/ideas/#jax-tutorial-ideas","text":"","title":"Jax Tutorial Ideas"},{"location":"tutorials/jax/ideas/#main-ideas","text":"Introduction Linear Regression, Regularization, vmap, jit, grad, jacobian, hessian Gaussian Processes Exact GP, Kernel, MLE,","title":"Main Ideas"},{"location":"tutorials/jax/ideas/#comparing-methods","text":"Exact GP Linearized GP (Taylor Expansion, 1st Order, 2nd Order) Moment-Matching GP Scratch - Loops, numba Jax - vmap, jit MCMC GP Scratch - Collin numpyro - NUTS/HMC Variational GP scratch Idea - Pyro numpyro - SVI","title":"Comparing Methods"},{"location":"tutorials/jax/ideas/#data","text":"1D Examples 2D Examples IASI Example Ocean Data","title":"Data"},{"location":"tutorials/jax/ideas/#future-stuff","text":"Sparse Models Deep Models DKL Models","title":"Future Stuff"},{"location":"tutorials/jax/ideas/#1-using-jax","text":"Basics: * grad * jit * vmap * jacobian * hessian * params * optimizers","title":"1. Using Jax"},{"location":"tutorials/jax/ideas/#2-regression-master-class","text":"","title":"2. Regression Master Class"},{"location":"tutorials/jax/ideas/#3-kernel-methods","text":"Kernel Least Squares Gaussian Processes Krasserm Support Vector Machines","title":"3. Kernel Methods"},{"location":"tutorials/jax/ideas/#4-special-algorithms","text":"Optimized Kernel Ridge Regression Gonzalo's Notebook Optimized Kernel Entropy Components Analysis","title":"4. Special Algorithms"},{"location":"tutorials/jax/ideas/#5-input-uncertainty","text":"Taylor Expansion Moment Matching Variational MCMC (NUTS/HMC) Heteroscedastic Likelihood","title":"5. Input Uncertainty"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/","text":"Using Jupyter Notebooks for VSCode Remote Computing \u00b6 In this tutorial, I will quickly be going over how one can open up a Jupyter Notebook in VSCode from one that has been activated on a slurm server through an interactive node. 1. Connect to the server via VSCode \u00b6 2. Connect to an interactive node \u00b6 Try to use something explicit like the following command: srun --nodes = 1 --ntasks-per-node = 1 --cpus-per-task = 28 --time 100 :00:00 --exclude = nodo17 --job-name bash-jupyter --pty bash -i 3. Start a Jupyter Notebook \u00b6 conda activate jupyterlab jupyter notebook --ip localhost --port 3001 --no-browser 4. Open Jupyter Notebook in VSCode \u00b6 At this point, something should pop up asking you if you would like to enter a token or your password for your notebook.","title":"Using Jupyter Notebooks for VSCode Remote Computing"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#using-jupyter-notebooks-for-vscode-remote-computing","text":"In this tutorial, I will quickly be going over how one can open up a Jupyter Notebook in VSCode from one that has been activated on a slurm server through an interactive node.","title":"Using Jupyter Notebooks for VSCode Remote Computing"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#1-connect-to-the-server-via-vscode","text":"","title":"1. Connect to the server via VSCode"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#2-connect-to-an-interactive-node","text":"Try to use something explicit like the following command: srun --nodes = 1 --ntasks-per-node = 1 --cpus-per-task = 28 --time 100 :00:00 --exclude = nodo17 --job-name bash-jupyter --pty bash -i","title":"2. Connect to an interactive node"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#3-start-a-jupyter-notebook","text":"conda activate jupyterlab jupyter notebook --ip localhost --port 3001 --no-browser","title":"3. Start a Jupyter Notebook"},{"location":"tutorials/jupyter_vscode/vscode_jupyter/#4-open-jupyter-notebook-in-vscode","text":"At this point, something should pop up asking you if you would like to enter a token or your password for your notebook.","title":"4. Open Jupyter Notebook in VSCode"},{"location":"tutorials/manifold_learning/3_ssma/","text":"Semisupervised Manifold Alignment \u00b6 Main References \u00b6 Semisupervised Manifold Alignment of Multimodal Remote Sensing Images - Tuia et al. Classification of Urban Multi-Angular Image Sequences by Aligning their Manifolds - Trolliet et al. Multisensor Alignment of Image Manifolds - Tuia et al. Domain Adaption using Manifold Alignment - Trolliet Outline \u00b6 In this chapter, I introduce the semisupervsied manifold alignment (SSMA) method that was presented in [ linktopaper ]. I give a brief overview of the algorithm in section 1.1 followed by some notation declarations in 1.2. I proceed to talk about the cost function in section 1.3 followed by explicit construction of some cost function components in sections 1.4, 1.5 and 1.6. I give some insight into the projection functions in section 1.7 followed by some advantages of this method with other alignment methods in section 1.8. I conclude this chapter with some insight into the computational complexity of the SSMA method in section 1.9. 1.1 Overview \u00b6 The main idea of the semisupervised manifold alignment (SSMA) method is to align individual manifolds by projecting them into a joint latent space \\mathcal{F} \\mathcal{F} . With respect to remote sensing, the authors of [ linktopaper ] have provided evidence to support the notion that the local geometry of each image dataset is preserved and the regions with similar classes are brought together whilst the regions with dissimilar classes are pushed apart in the SSMA embedding. By using graph Laplacians for each of the similarity and dissimilarity terms, a Rayleigh Quotient function is minimized to extract projection functions. These projection functions are used to project the individual manifolds into a joint latent space. Explicit Algorithm ( in words ) \u00b6 1.2 Notation \u00b6 Let's have a series of M M images with each data matrix X^{m} X^{m} , where m=1,2,\\ldots, M m=1,2,\\ldots, M . Let each matrix X^{m} X^{m} be split into labeled samples \\{x_{i}^{m} \\}^{u_{m}}_{i=1} \\{x_{i}^{m} \\}^{u_{m}}_{i=1} and unlabeled samples \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} . Typically, there are many more labeled samples than unlabeled samples so let's assume l_{m} << u_{m} l_{m} << u_{m} . Let our data matrix be of d_{m}\\text{x }n_{m} d_{m}\\text{x }n_{m} dimensions which says that d d dimensional data by n_{m} n_{m} labeled and unlabeled samples. Succinctly, we have X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} , where n_{m}=l_{m}+u_{m} n_{m}=l_{m}+u_{m} . We can explicitly write each data matrix into a block diagonal matrix X X where X=diag(X_{1}, \\ldots, X_{M}) X=diag(X_{1}, \\ldots, X_{M}) and X \\in \\mathbb{R}^{d\\text{x}N} X \\in \\mathbb{R}^{d\\text{x}N} . X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} N N is the total number of labeled samples for all of the images and d d is the overall dimension of all of the images, i.e. N=\\sum_{m}^{M}n_{m} N=\\sum_{m}^{M}n_{m} and d=\\sum_{m}^{M}d_{m} d=\\sum_{m}^{M}d_{m} . Note: These images do not necessarily need to be exact replicas of the the same spectral density, the same spatial dimension or even the same sensor. Practically, this means that we do not need to have the same m m for each matrix X^{m} X^{m} . 1.3 Semisupervised Loss Function \u00b6 In [ linktopaper ], they construct a semisupervised loss function with the idea of aligning M M images to a common representation. Ultimately, they created M M projection functions \\mathcal{F} \\mathcal{F} that mapped image X^{m} X^{m} from X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} to f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} where m=1,2, \\ldots, M m=1,2, \\ldots, M . They ensure that the projection function f f brings samples from the same class closer together and pushes samples from different classes apart whilst still preserving the geometry of each individual manifold. Abstractly, the aim is to maximize the distance between the dissimilarities between the data sets and minimize the similarities between the datasets. This can be expressed via the following equation: \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} As you can see, if the denominator gets very large then this cost function will get very small. The Rayleigh Quotient is used because the problem has terms that need to be minimized and terms that need to be maximized. F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F F is a d d x d d projection matrix and the row blocks of F F correspond to the domain specific projection functions f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} that project the data matrix X^{m} X^{m} into the joint latent space. The matrix A A corresponds to the term that needs to be minimized and the matrix B B corresponds to the term that needs to be maximized. The authors break the matrix A A into two components, a geometric component G G and a class similarity component, S S . They want to preserve the manifold of each data matrix X^{m} X^{m} and bring same classes closer together in the joint latent subspace manifold. They combine the geometric component, G G and the similarity component, S S to create the discriminant component A A , i.e. A=S+\\mu G A=S+\\mu G where \\mu \\mu is a free parameter to monitor the tradeoff between the local manifold and the class similarity. The authors choose to use three graph Laplacian matrices to construct the terms in the Rayleigh quotient, G G , S S , and B B . 1.4 Geometric Spectral Similarity Term \u00b6 The G G term preserves the manifold of each individual data set X^{m} X^{m} throughout the transformation as no inter-domain relationships are considered. Affinity matrices are constructed for each X^{m} X^{m} and then put into a block diagonal matrix W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} . There are many ways to construct affinity matrices including k k nearest neighbour graphs ( k k -NN), \\epsilon \\epsilon -neighbourhood graphs ( \\epsilon \\epsilon -N) and Gaussian graphs. The graphs are assumed to be undirected so the affinity matrix should be symmetric, i.e. if x_{i} x_{i} is connected to x_{j} x_{j} then x_{j} x_{j} is connected to x_{i} x_{i} . An example of the final matrix W_g W_g is like so: W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} Entries in the matrices W_g^m W_g^m are W_{g}^{m}(i,j)=1 W_{g}^{m}(i,j)=1 if x_{i} x_{i} and x_{j} x_{j} are connected and 0 otherwise. Using the k k -NN weighted graph, this term ensures that the samples in the original m m domain remains in the same proximity in the new latent projected space. A graph laplacian, L_{g}^{m} L_{g}^{m} is constructed from each of the affinity matrices and put into a block diagonal matrix L_g L_g , where L_g^m=D_{g}^{m}-W_{g}^{m} L_g^m=D_{g}^{m}-W_{g}^{m} and D_{g}^{m} D_{g}^{m} is the degree matrix defined as D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) . The laplacian matrix L L is positive semi-definite and symmetric. An example of the final matrix D_g D_g and L_g L_g are below: D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} which just simplifies to L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} The term that needs to be minimized is now: G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) ( Put a note about the graph Laplacian. Maybe an appendix section? ) 1.5 Class Label Similarity Term \u00b6 The S S term ensures that the inter-domain classes are brought together in the joint latent space by pulling labeled samples together. The author use a matrix of class similarities to act as an affinity matrix. Entries in the matrices W_s^{m,m'}(i,j)=1 W_s^{m,m'}(i,j)=1 if the samples between class m m and m' m' have the same label and 0 otherwise where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_s L_s is constructed in the same way as the geometric term is. The term that needs to be minimized is now: S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) 1.6 Class Label Dissimilarity Term \u00b6 The B B term ensures that the inter-domain classes are pushed apart in the joint latent space by pushing different labeled samples apart. The author use a matrix of class dissimilarities to act as an affinity matrix. Entries in the matrices W_d^{m,m'}(i,j)=1 W_d^{m,m'}(i,j)=1 if the samples between class m m and m' m' have different labels and 0 otherwise, where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix is only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_d L_d is constructed in the same way as the geometric term is. The term that needs to be minimized is now: B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} which again simplifies to $$ B = tr(F^T X L_d X^T F) $$ 1.7 Projection Functions \u00b6 We can aggregate all of the graph Laplacians ( L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} ) and then use the Rayleigh Quoitient formulation to get our cost function that needs to be minimized: F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} (I want to talk more about the constraints...) We can find a solution to this minimization problem by looking for the smallest eigenvalues \\gamma_i \\gamma_i of the generalized eigenvalue problem: X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma The optimal solution to the generalized eigenvalue problem contains the projection functions necessary to project each X^m X^m into the joint latent space. If we look at F_{opt} F_{opt} , we can see the matrix will be built as follows: F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} To put it succinctly, we have a projection of X^m X^m from domain m m to the joint latent space F F of dimension, d d : \\mathcal{P}_{f}(X^m)=f^{mT}X^m \\mathcal{P}_{f}(X^m)=f^{mT}X^m Furthermore, any data matrix X^m X^m can be projected into the latent space of data matrix X^{m'} X^{m'} . Let X^p=X^mf^m X^p=X^mf^m be the projection of data matrix X^m X^m into the joint latent space. This can be translated into the latent space of X^{m'} X^{m'} via the translation X^{m1} =X^mf^m(f^{m'})^\\dagger X^{m1} =X^mf^m(f^{m'})^\\dagger where (f^{m'})^\\dagger (f^{m'})^\\dagger is the pseudoinverse of the eigenvectors of domain m' m' . 1.8 Properties and Comparison to other methods \u00b6 The authors outline some significant properties that set asside the SSMA method from other methods that have been used for domain adaption. Linearity \u00b6 This method constructs and defines explicit projection functions that can project data matrices X^m X^m into the joint latent space. That being said, this means that the Multisensor \u00b6 This method only considers the geometry of each individual X^m X^m exclusively and so there is no restriction on the number of hyperspectral image bands nor on the spectral band properties or amount. Multidomain \u00b6 This method can align domains to a joint latent space so there is no limitation on the number of domains. There is no requirement for there to be a leading domain where all other domains are similar to; although it would be wise to do so. PDF-based \u00b6 This method aligns the underlying manifold for each domain and so there is no need for there to be an co-registration of source domains nor does there need to be images with the same spatial or spectral resolution. Invertibility \u00b6 This method creates explicit projection functions that map from one target domain to a joint latent space. Likewise, it is also possible to project data from one target domain to another target domain. 1.9 Computational Complexity \u00b6 The SSMA method is a typical graph-based computational problem. The bulk of the computational effort comes with constructing the graph Laplacians, specifically the geometric term. A k-NN k-NN method is used to create the affinity matrix which can be on order ( order? ). The similarity and dissimilary terms are constructed by convolving vectors with the class labels which results in very sparse matrices. Storing matrices can be memory intense but there exists methods to combat this limitation. (Need to go into methods which can reduce the storage for large matrices.) There can be a significant cost to compute the eigenvalues of the generalized eigenvalue problem which can be of order ( order? ). We can combat the cost by either reducing the size of the problem or improving the algebraic computational process. Some iterative methods can be used to reduce the computational complexity from \\mathcal{O}(d^3) \\mathcal{O}(d^3) to \\mathcal{O}log(d) \\mathcal{O}log(d) . The cost to compute the eigenvalue decomposition can be avoided using faster methods such as approximate random projection singular value decomposition (ARSVD), Jacobi-Davidson QR (JDQR) factorization method, or multigrid methods.","title":"Semisupervised Manifold Alignment"},{"location":"tutorials/manifold_learning/3_ssma/#semisupervised-manifold-alignment","text":"","title":"Semisupervised Manifold Alignment"},{"location":"tutorials/manifold_learning/3_ssma/#main-references","text":"Semisupervised Manifold Alignment of Multimodal Remote Sensing Images - Tuia et al. Classification of Urban Multi-Angular Image Sequences by Aligning their Manifolds - Trolliet et al. Multisensor Alignment of Image Manifolds - Tuia et al. Domain Adaption using Manifold Alignment - Trolliet","title":"Main References"},{"location":"tutorials/manifold_learning/3_ssma/#outline","text":"In this chapter, I introduce the semisupervsied manifold alignment (SSMA) method that was presented in [ linktopaper ]. I give a brief overview of the algorithm in section 1.1 followed by some notation declarations in 1.2. I proceed to talk about the cost function in section 1.3 followed by explicit construction of some cost function components in sections 1.4, 1.5 and 1.6. I give some insight into the projection functions in section 1.7 followed by some advantages of this method with other alignment methods in section 1.8. I conclude this chapter with some insight into the computational complexity of the SSMA method in section 1.9.","title":"Outline"},{"location":"tutorials/manifold_learning/3_ssma/#11-overview","text":"The main idea of the semisupervised manifold alignment (SSMA) method is to align individual manifolds by projecting them into a joint latent space \\mathcal{F} \\mathcal{F} . With respect to remote sensing, the authors of [ linktopaper ] have provided evidence to support the notion that the local geometry of each image dataset is preserved and the regions with similar classes are brought together whilst the regions with dissimilar classes are pushed apart in the SSMA embedding. By using graph Laplacians for each of the similarity and dissimilarity terms, a Rayleigh Quotient function is minimized to extract projection functions. These projection functions are used to project the individual manifolds into a joint latent space.","title":"1.1 Overview"},{"location":"tutorials/manifold_learning/3_ssma/#explicit-algorithm-in-words","text":"","title":"Explicit Algorithm (in words)"},{"location":"tutorials/manifold_learning/3_ssma/#12-notation","text":"Let's have a series of M M images with each data matrix X^{m} X^{m} , where m=1,2,\\ldots, M m=1,2,\\ldots, M . Let each matrix X^{m} X^{m} be split into labeled samples \\{x_{i}^{m} \\}^{u_{m}}_{i=1} \\{x_{i}^{m} \\}^{u_{m}}_{i=1} and unlabeled samples \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} \\{x_{j}^{m}, y_{j}^{m} \\}^{l_{m}}_{j=1} . Typically, there are many more labeled samples than unlabeled samples so let's assume l_{m} << u_{m} l_{m} << u_{m} . Let our data matrix be of d_{m}\\text{x }n_{m} d_{m}\\text{x }n_{m} dimensions which says that d d dimensional data by n_{m} n_{m} labeled and unlabeled samples. Succinctly, we have X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} , where n_{m}=l_{m}+u_{m} n_{m}=l_{m}+u_{m} . We can explicitly write each data matrix into a block diagonal matrix X X where X=diag(X_{1}, \\ldots, X_{M}) X=diag(X_{1}, \\ldots, X_{M}) and X \\in \\mathbb{R}^{d\\text{x}N} X \\in \\mathbb{R}^{d\\text{x}N} . X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} X = \\begin{bmatrix} X^1 & 0 & 0 & 0 \\\\ 0 & X^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & X^M \\end{bmatrix}_{} N N is the total number of labeled samples for all of the images and d d is the overall dimension of all of the images, i.e. N=\\sum_{m}^{M}n_{m} N=\\sum_{m}^{M}n_{m} and d=\\sum_{m}^{M}d_{m} d=\\sum_{m}^{M}d_{m} . Note: These images do not necessarily need to be exact replicas of the the same spectral density, the same spatial dimension or even the same sensor. Practically, this means that we do not need to have the same m m for each matrix X^{m} X^{m} .","title":"1.2 Notation"},{"location":"tutorials/manifold_learning/3_ssma/#13-semisupervised-loss-function","text":"In [ linktopaper ], they construct a semisupervised loss function with the idea of aligning M M images to a common representation. Ultimately, they created M M projection functions \\mathcal{F} \\mathcal{F} that mapped image X^{m} X^{m} from X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} X^{m} \\in \\mathbb{R}^{d_{m}\\text{x } n_{m}} to f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m} \\text{x }d} where m=1,2, \\ldots, M m=1,2, \\ldots, M . They ensure that the projection function f f brings samples from the same class closer together and pushes samples from different classes apart whilst still preserving the geometry of each individual manifold. Abstractly, the aim is to maximize the distance between the dissimilarities between the data sets and minimize the similarities between the datasets. This can be expressed via the following equation: \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} \\text{Cost Function} = \\frac{\\text{Similarity} + \\mu \\text{Geometric}}{\\text{Dissimilarity}} As you can see, if the denominator gets very large then this cost function will get very small. The Rayleigh Quotient is used because the problem has terms that need to be minimized and terms that need to be maximized. F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin} \\frac{F^TAF}{F^TBF} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}BF)^{-1}F^{T}AF \\right)\\right\\} F F is a d d x d d projection matrix and the row blocks of F F correspond to the domain specific projection functions f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} f^{m} \\in \\mathbb{R}^{d_{m}\\text{x }d} that project the data matrix X^{m} X^{m} into the joint latent space. The matrix A A corresponds to the term that needs to be minimized and the matrix B B corresponds to the term that needs to be maximized. The authors break the matrix A A into two components, a geometric component G G and a class similarity component, S S . They want to preserve the manifold of each data matrix X^{m} X^{m} and bring same classes closer together in the joint latent subspace manifold. They combine the geometric component, G G and the similarity component, S S to create the discriminant component A A , i.e. A=S+\\mu G A=S+\\mu G where \\mu \\mu is a free parameter to monitor the tradeoff between the local manifold and the class similarity. The authors choose to use three graph Laplacian matrices to construct the terms in the Rayleigh quotient, G G , S S , and B B .","title":"1.3 Semisupervised Loss Function"},{"location":"tutorials/manifold_learning/3_ssma/#14-geometric-spectral-similarity-term","text":"The G G term preserves the manifold of each individual data set X^{m} X^{m} throughout the transformation as no inter-domain relationships are considered. Affinity matrices are constructed for each X^{m} X^{m} and then put into a block diagonal matrix W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} W^{m}_{g} \\in \\mathbb{R}^{n_{m}\\text{x }n_{m}} . There are many ways to construct affinity matrices including k k nearest neighbour graphs ( k k -NN), \\epsilon \\epsilon -neighbourhood graphs ( \\epsilon \\epsilon -N) and Gaussian graphs. The graphs are assumed to be undirected so the affinity matrix should be symmetric, i.e. if x_{i} x_{i} is connected to x_{j} x_{j} then x_{j} x_{j} is connected to x_{i} x_{i} . An example of the final matrix W_g W_g is like so: W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} W_g = \\begin{bmatrix} W_g^1 & 0 & 0 & 0 \\\\ 0 & W_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & W_g^M \\end{bmatrix} Entries in the matrices W_g^m W_g^m are W_{g}^{m}(i,j)=1 W_{g}^{m}(i,j)=1 if x_{i} x_{i} and x_{j} x_{j} are connected and 0 otherwise. Using the k k -NN weighted graph, this term ensures that the samples in the original m m domain remains in the same proximity in the new latent projected space. A graph laplacian, L_{g}^{m} L_{g}^{m} is constructed from each of the affinity matrices and put into a block diagonal matrix L_g L_g , where L_g^m=D_{g}^{m}-W_{g}^{m} L_g^m=D_{g}^{m}-W_{g}^{m} and D_{g}^{m} D_{g}^{m} is the degree matrix defined as D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) D_g^m(i,i)=\\sum_{j}^{n_{m}}W_g^m(i,j) . The laplacian matrix L L is positive semi-definite and symmetric. An example of the final matrix D_g D_g and L_g L_g are below: D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} D_g = \\begin{bmatrix} D_g^1 & 0 & 0 & 0 \\\\ 0 & D_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & D_g^M \\end{bmatrix} which just simplifies to L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} L_g = D_g - W_g = \\begin{bmatrix} L_g^1 & 0 & 0 & 0 \\\\ 0 & L_g^2 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\ldots & \\ldots & L_g^M \\end{bmatrix} The term that needs to be minimized is now: G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) G= \\sum_{m=1}^{M}\\sum_{i,j=1}^{n_{m}}W_{g}^{m}(i,j)||f^{mT}x_{i}^{m}-f^{mT}x_{j}^{m}||^{2} $$ $$ G = tr(F^T X L_g X^T F) ( Put a note about the graph Laplacian. Maybe an appendix section? )","title":"1.4 Geometric Spectral Similarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#15-class-label-similarity-term","text":"The S S term ensures that the inter-domain classes are brought together in the joint latent space by pulling labeled samples together. The author use a matrix of class similarities to act as an affinity matrix. Entries in the matrices W_s^{m,m'}(i,j)=1 W_s^{m,m'}(i,j)=1 if the samples between class m m and m' m' have the same label and 0 otherwise where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_s L_s is constructed in the same way as the geometric term is. The term that needs to be minimized is now: S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F) S= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_s^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} $$ $$ S = tr(F^T X L_s X^T F)","title":"1.5 Class Label Similarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#16-class-label-dissimilarity-term","text":"The B B term ensures that the inter-domain classes are pushed apart in the joint latent space by pushing different labeled samples apart. The author use a matrix of class dissimilarities to act as an affinity matrix. Entries in the matrices W_d^{m,m'}(i,j)=1 W_d^{m,m'}(i,j)=1 if the samples between class m m and m' m' have different labels and 0 otherwise, where m, m'=1,2,\\ldots, M m, m'=1,2,\\ldots, M . Because this affinity matrix is only comparing the labeled samples, we can be sure that this matrix will be very sparse. A graph laplacian, L_d L_d is constructed in the same way as the geometric term is. The term that needs to be minimized is now: B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} B= \\sum_{m,m'=1}^{M}\\sum_{i,j=1}^{l_m,l_m'}W_d^{m,m'}(i,j)||f^{mT}x_i^m-f^{mT}x_j^m||^{2} which again simplifies to $$ B = tr(F^T X L_d X^T F) $$","title":"1.6 Class Label Dissimilarity Term"},{"location":"tutorials/manifold_learning/3_ssma/#17-projection-functions","text":"We can aggregate all of the graph Laplacians ( L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} L_g, L_s, L_d \\in \\mathbb{R}^{n\\text{x}n} ) and then use the Rayleigh Quoitient formulation to get our cost function that needs to be minimized: F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} F_{opt}=\\underset{F}{argmin}\\left\\{ tr\\left( (F^{T}XL_dX^TF)^{-1} F^{T}X(\\mu L_g+L_s)X^TF \\right)\\right\\} (I want to talk more about the constraints...) We can find a solution to this minimization problem by looking for the smallest eigenvalues \\gamma_i \\gamma_i of the generalized eigenvalue problem: X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma X(\\mu L_g + L_s)X^T \\Gamma = \\lambda XL_dX^T\\Gamma The optimal solution to the generalized eigenvalue problem contains the projection functions necessary to project each X^m X^m into the joint latent space. If we look at F_{opt} F_{opt} , we can see the matrix will be built as follows: F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} F_{opt}= \\left[ \\sqrt{\\lambda_1}\\gamma_{1}|\\ldots|\\sqrt{\\lambda_M}\\gamma_{M}\\right]$$ $$F_{opt}= \\begin{bmatrix} f_1^1 & \\ldots & f_d^1 \\\\ f_1^2 & \\ldots & f_d^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ f_1^M & \\ldots & f_d^M \\end{bmatrix} To put it succinctly, we have a projection of X^m X^m from domain m m to the joint latent space F F of dimension, d d : \\mathcal{P}_{f}(X^m)=f^{mT}X^m \\mathcal{P}_{f}(X^m)=f^{mT}X^m Furthermore, any data matrix X^m X^m can be projected into the latent space of data matrix X^{m'} X^{m'} . Let X^p=X^mf^m X^p=X^mf^m be the projection of data matrix X^m X^m into the joint latent space. This can be translated into the latent space of X^{m'} X^{m'} via the translation X^{m1} =X^mf^m(f^{m'})^\\dagger X^{m1} =X^mf^m(f^{m'})^\\dagger where (f^{m'})^\\dagger (f^{m'})^\\dagger is the pseudoinverse of the eigenvectors of domain m' m' .","title":"1.7 Projection Functions"},{"location":"tutorials/manifold_learning/3_ssma/#18-properties-and-comparison-to-other-methods","text":"The authors outline some significant properties that set asside the SSMA method from other methods that have been used for domain adaption.","title":"1.8 Properties and Comparison to other methods"},{"location":"tutorials/manifold_learning/3_ssma/#linearity","text":"This method constructs and defines explicit projection functions that can project data matrices X^m X^m into the joint latent space. That being said, this means that the","title":"Linearity"},{"location":"tutorials/manifold_learning/3_ssma/#multisensor","text":"This method only considers the geometry of each individual X^m X^m exclusively and so there is no restriction on the number of hyperspectral image bands nor on the spectral band properties or amount.","title":"Multisensor"},{"location":"tutorials/manifold_learning/3_ssma/#multidomain","text":"This method can align domains to a joint latent space so there is no limitation on the number of domains. There is no requirement for there to be a leading domain where all other domains are similar to; although it would be wise to do so.","title":"Multidomain"},{"location":"tutorials/manifold_learning/3_ssma/#pdf-based","text":"This method aligns the underlying manifold for each domain and so there is no need for there to be an co-registration of source domains nor does there need to be images with the same spatial or spectral resolution.","title":"PDF-based"},{"location":"tutorials/manifold_learning/3_ssma/#invertibility","text":"This method creates explicit projection functions that map from one target domain to a joint latent space. Likewise, it is also possible to project data from one target domain to another target domain.","title":"Invertibility"},{"location":"tutorials/manifold_learning/3_ssma/#19-computational-complexity","text":"The SSMA method is a typical graph-based computational problem. The bulk of the computational effort comes with constructing the graph Laplacians, specifically the geometric term. A k-NN k-NN method is used to create the affinity matrix which can be on order ( order? ). The similarity and dissimilary terms are constructed by convolving vectors with the class labels which results in very sparse matrices. Storing matrices can be memory intense but there exists methods to combat this limitation. (Need to go into methods which can reduce the storage for large matrices.) There can be a significant cost to compute the eigenvalues of the generalized eigenvalue problem which can be of order ( order? ). We can combat the cost by either reducing the size of the problem or improving the algebraic computational process. Some iterative methods can be used to reduce the computational complexity from \\mathcal{O}(d^3) \\mathcal{O}(d^3) to \\mathcal{O}log(d) \\mathcal{O}log(d) . The cost to compute the eigenvalue decomposition can be avoided using faster methods such as approximate random projection singular value decomposition (ARSVD), Jacobi-Davidson QR (JDQR) factorization method, or multigrid methods.","title":"1.9 Computational Complexity"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/","text":"Laplacian Eigenmaps \u00b6 Given m m points \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N , we assume the m m points belong to an n n dimensional manifold where n n is much smaller or equal to N N . Our objective is to find a lower dimensional representation \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n where n<<N n<<N . Step 1. Construct the Adjacency Matrix \u00b6 We build a graph G G whose nodes i i and j j are connected if x_i x_i is among k k -NN or \\epsilon \\epsilon -ball graph. The distances between the data points are measured in the euclidean distance metric. This adjacency matrix A A represents the connectivity of the original data where k k , the number of neighbours, is the free parameter. Step 2. Use the heat kernel as weights for the Adjacency Matrix \u00b6 We want weighted edges on the graph to represent the proximity of the vertices to their adjacent neighbours. One common kernel to use is the diffusion weight matrix, W W . We can define W W like so: W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. Step 3. Solve the eigenvalue problem \u00b6 Let D D be a diagonal matrix D_{ii}= \\sum_{j}W_ij D_{ii}= \\sum_{j}W_ij . We can denote the lower dimensional representation by an m m x n n matrix y=(y_1, y_2, \\ldots, y_m)^T y=(y_1, y_2, \\ldots, y_m)^T where each row vector y_i \\in \\mathcal{R}^n y_i \\in \\mathcal{R}^n . Now, we want to minimize the following cost function \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} which is equivalent to minimizing the following \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) where L=D-W L=D-W is an m m x m m laplacian operator and I I is the identity matrix. The constraint y^TDy=I y^TDy=I denotes... The solution to this minimization problem is given by finding the first n n eigenvalue solutions to the generalized eigenvalue problem: Lf=\\lambda Df Lf=\\lambda Df <span><span class=\"MathJax_Preview\">Lf=\\lambda Df</span><script type=\"math/tex\">Lf=\\lambda Df Step 4. Normalized Eigenvalue Problem (optional) \u00b6 If the graph is fully connected, then \\mathbf{1}=(1,1, \\ldots, 1)^T \\mathbf{1}=(1,1, \\ldots, 1)^T is the only eigenvector with eigenvalue 0. Instead of the above generalized eigenvalue problem, we can solve for the \\text{min tr}(y^TLy) \\text{min tr}(y^TLy) subject to y^TDy=I y^TDy=I and y^TD^{\\frac{1}{2}}y=0 y^TD^{\\frac{1}{2}}y=0 . We can apply the z=D^{\\frac{1}{2}}y z=D^{\\frac{1}{2}}y transformation to yield the following eigenvalue problem: \\text{min tr}(z^T\\mathcal{L}z) \\text{min tr}(z^T\\mathcal{L}z) subject to the constraints z^Tz=I z^Tz=I and z^T\\mathbf{1}=0 z^T\\mathbf{1}=0 where \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} . \\mathcal{L}f=\\lambda f \\mathcal{L}f=\\lambda f ( Need some help understanding the significance of the normalized laplacian. )","title":"6 laplacian eigenmaps"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#laplacian-eigenmaps","text":"Given m m points \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N \\left\\{ x_1, x_2, \\ldots, x_m \\right\\} \\in \\mathcal{R}^N , we assume the m m points belong to an n n dimensional manifold where n n is much smaller or equal to N N . Our objective is to find a lower dimensional representation \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n \\left\\{ y_1, y_2, \\ldots, y_m \\right\\} \\in \\mathcal{R}^n where n<<N n<<N .","title":"Laplacian Eigenmaps"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-1-construct-the-adjacency-matrix","text":"We build a graph G G whose nodes i i and j j are connected if x_i x_i is among k k -NN or \\epsilon \\epsilon -ball graph. The distances between the data points are measured in the euclidean distance metric. This adjacency matrix A A represents the connectivity of the original data where k k , the number of neighbours, is the free parameter.","title":"Step 1. Construct the Adjacency Matrix"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-2-use-the-heat-kernel-as-weights-for-the-adjacency-matrix","text":"We want weighted edges on the graph to represent the proximity of the vertices to their adjacent neighbours. One common kernel to use is the diffusion weight matrix, W W . We can define W W like so: W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right. W_{ij} = \\left\\{ \\begin{array}{ll} e^{-\\frac{||x_i-x_j||^2_2}{2\\sigma^2}} & \\text{if } i,j \\text{ are connected}\\\\ 0 & \\text{otherwise.} \\end{array} \\right.","title":"Step 2. Use the heat kernel as weights for the Adjacency Matrix"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-3-solve-the-eigenvalue-problem","text":"Let D D be a diagonal matrix D_{ii}= \\sum_{j}W_ij D_{ii}= \\sum_{j}W_ij . We can denote the lower dimensional representation by an m m x n n matrix y=(y_1, y_2, \\ldots, y_m)^T y=(y_1, y_2, \\ldots, y_m)^T where each row vector y_i \\in \\mathcal{R}^n y_i \\in \\mathcal{R}^n . Now, we want to minimize the following cost function \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} \\underset{y^TDy=I}{\\text{min }} \\frac{1}{2}\\sum_{i,j}||y_i-y_j||^2W_{ij} which is equivalent to minimizing the following \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) \\underset{y^TDy=I}{\\text{min }} \\text{tr}\\left( y^TLy \\right) where L=D-W L=D-W is an m m x m m laplacian operator and I I is the identity matrix. The constraint y^TDy=I y^TDy=I denotes... The solution to this minimization problem is given by finding the first n n eigenvalue solutions to the generalized eigenvalue problem: Lf=\\lambda Df Lf=\\lambda Df <span><span class=\"MathJax_Preview\">Lf=\\lambda Df</span><script type=\"math/tex\">Lf=\\lambda Df","title":"Step 3. Solve the eigenvalue problem"},{"location":"tutorials/manifold_learning/6_laplacian_eigenmaps/#step-4-normalized-eigenvalue-problem-optional","text":"If the graph is fully connected, then \\mathbf{1}=(1,1, \\ldots, 1)^T \\mathbf{1}=(1,1, \\ldots, 1)^T is the only eigenvector with eigenvalue 0. Instead of the above generalized eigenvalue problem, we can solve for the \\text{min tr}(y^TLy) \\text{min tr}(y^TLy) subject to y^TDy=I y^TDy=I and y^TD^{\\frac{1}{2}}y=0 y^TD^{\\frac{1}{2}}y=0 . We can apply the z=D^{\\frac{1}{2}}y z=D^{\\frac{1}{2}}y transformation to yield the following eigenvalue problem: \\text{min tr}(z^T\\mathcal{L}z) \\text{min tr}(z^T\\mathcal{L}z) subject to the constraints z^Tz=I z^Tz=I and z^T\\mathbf{1}=0 z^T\\mathbf{1}=0 where \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\mathcal{L}=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} . \\mathcal{L}f=\\lambda f \\mathcal{L}f=\\lambda f ( Need some help understanding the significance of the normalized laplacian. )","title":"Step 4. Normalized Eigenvalue Problem (optional)"},{"location":"tutorials/pytorch_nns/","text":"Using PyTorch for Neural Networks \u00b6 Test","title":"Using PyTorch for Neural Networks"},{"location":"tutorials/pytorch_nns/#using-pytorch-for-neural-networks","text":"Test","title":"Using PyTorch for Neural Networks"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/","text":"DL 4 Researchers Part I - NN from scratch \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com PyTorch Colab Notebook: Link Sources: * What is torch.nn really? - PyTorch DL 4 Researchers Part I - NN from scratch Import Data Convert Inputs to Tensors ** PyTorch ** ** TensorFlow ** Parameters ** PyTorch ** Tracking Gradients ** PyTorch ** Model ** PyTorch ** Loss Function ** PyTorch ** Training ** PyTorch ** Appendix Extra Python Packages Used Import Data \u00b6 We will be using the california housing dataset. We will do some standard preprocessing including Normalizing the inputs, removing the mean from the outputs, and splitting the data into train, validation and testing. # Import Data cal_housing = datasets . fetch_california_housing () # extract training and targets X = cal_housing . data y = cal_housing . target # Normalize Input data X = StandardScaler () . fit_transform ( X ) # Remove mean from Output data y = StandardScaler ( with_std = False ) . fit_transform ( X ) # split data into training and test Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , test_size = 0.1 , random_state = 0 ) # split training data into train and validation Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtrain , ytrain , train_size = 0.8 , random_state = 123 ) Convert Inputs to Tensors \u00b6 We need to convert the data from np.ndarray to a Tensor . ** PyTorch ** \u00b6 # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( torch . FloatTensor , ( Xtrain , ytrain , Xvalid , yvalid ) ) ** TensorFlow ** \u00b6 # Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( tf . convert_to_tensor , ( Xtrain , ytrain , Xvalid , yvalid ), ) !> One small thing I've noticed is that TensorFlow handles numpy arrays much better than PyTorch. PyTorch is very strict: you need to convert your data to a PyTorch tensor before you start using PyTorch functions. TensorFlow is a bit more flexible sometimes and can convert your data into tf.tensors. Parameters \u00b6 The first thing we need to do is define the components that we're working with. We are doing a simple linear regression problem so we only need the following components: weight matrix: \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} bias vector: b \\in \\mathcal{R}^{D \\times 1} b \\in \\mathcal{R}^{D \\times 1} ** PyTorch ** \u00b6 import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ) # bias vector bias = torch . zeros ( output_dim ) Tracking Gradients \u00b6 ** PyTorch ** \u00b6 weights . requires_grad_ () bias . requires_grad_ () Model \u00b6 So again, the model is simple: y = \\mathbf{x}_b \\mathbf{W} + b y = \\mathbf{x}_b \\mathbf{W} + b ** PyTorch ** \u00b6 # define the model as a function def model ( x_batch : torch . tensor ): return x_batch @ weights + bias batch_size = 64 # mini-batch from training data xb = x_train [: batch_size ] # predictions preds = model ( xb ) # check if there is grad function preds [ 0 ] You should get the following output: tensor([0.3591], grad_fn=<SelectBackward>) That grad_fn lets you know that we're tracking the gradients. Loss Function \u00b6 We're doing a simple loss: mean squared error. \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 ** PyTorch ** \u00b6 # define mse loss def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) # set loss function to mse loss_func = mse_loss # get sample batch yb = y_train [: batch_size ] # get loss loss = loss_func ( preds , yb ) # check if there is grad function print ( loss ) Training \u00b6 ** PyTorch ** \u00b6 batch_size = 100 learning_rate = 0.01 epochs = 20 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # Loss back propagation loss . backward () # add running loss losses . append ( loss . item ()) # manually calculate gradients with torch . no_grad (): # update the weights individually weights -= learning_rate * weights . grad bias -= learning_rate * bias . grad # zero the weights, bias parameters weights . grad . zero_ () bias . grad . zero_ () # Update status bar postfix = dict ( Epoch = f \" { iepoch } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Your output will look something like this. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00, 2.50it/s, Epoch=19, Loss=0.545] Appendix \u00b6 Extra Python Packages Used \u00b6 scikit-learn The default machine learning package for python. It has a LOT of good algorithms and preprocessing features that are useful. Apparently the biggest use of the sklearn library is the train_test_split function. I'm definitely guilty of that too. tqdm A nice full featured status bar which can help eliminate some of the redundant outputs. typing A built-in type checker. This allows one to type check your inputs. It helps with code readability and to catch silly errors like having bad inputs.","title":"DL 4 Researchers Part I - NN from scratch"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#dl-4-researchers-part-i-nn-from-scratch","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com PyTorch Colab Notebook: Link Sources: * What is torch.nn really? - PyTorch DL 4 Researchers Part I - NN from scratch Import Data Convert Inputs to Tensors ** PyTorch ** ** TensorFlow ** Parameters ** PyTorch ** Tracking Gradients ** PyTorch ** Model ** PyTorch ** Loss Function ** PyTorch ** Training ** PyTorch ** Appendix Extra Python Packages Used","title":"DL 4 Researchers Part I - NN from scratch"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#import-data","text":"We will be using the california housing dataset. We will do some standard preprocessing including Normalizing the inputs, removing the mean from the outputs, and splitting the data into train, validation and testing. # Import Data cal_housing = datasets . fetch_california_housing () # extract training and targets X = cal_housing . data y = cal_housing . target # Normalize Input data X = StandardScaler () . fit_transform ( X ) # Remove mean from Output data y = StandardScaler ( with_std = False ) . fit_transform ( X ) # split data into training and test Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , test_size = 0.1 , random_state = 0 ) # split training data into train and validation Xtrain , Xvalid , ytrain , yvalid = train_test_split ( Xtrain , ytrain , train_size = 0.8 , random_state = 123 )","title":"Import Data"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#convert-inputs-to-tensors","text":"We need to convert the data from np.ndarray to a Tensor .","title":"Convert Inputs to Tensors"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch","text":"# Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( torch . FloatTensor , ( Xtrain , ytrain , Xvalid , yvalid ) )","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#tensorflow","text":"# Create a torch tensor from the data x_train , y_train , x_valid , y_valid = map ( tf . convert_to_tensor , ( Xtrain , ytrain , Xvalid , yvalid ), ) !> One small thing I've noticed is that TensorFlow handles numpy arrays much better than PyTorch. PyTorch is very strict: you need to convert your data to a PyTorch tensor before you start using PyTorch functions. TensorFlow is a bit more flexible sometimes and can convert your data into tf.tensors.","title":"** TensorFlow **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#parameters","text":"The first thing we need to do is define the components that we're working with. We are doing a simple linear regression problem so we only need the following components: weight matrix: \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} \\mathbf{W} \\in \\mathcal{R}^{D \\times 1} bias vector: b \\in \\mathcal{R}^{D \\times 1} b \\in \\mathcal{R}^{D \\times 1}","title":"Parameters"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch_1","text":"import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ) # bias vector bias = torch . zeros ( output_dim )","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#tracking-gradients","text":"","title":"Tracking Gradients"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch_2","text":"weights . requires_grad_ () bias . requires_grad_ ()","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#model","text":"So again, the model is simple: y = \\mathbf{x}_b \\mathbf{W} + b y = \\mathbf{x}_b \\mathbf{W} + b","title":"Model"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch_3","text":"# define the model as a function def model ( x_batch : torch . tensor ): return x_batch @ weights + bias batch_size = 64 # mini-batch from training data xb = x_train [: batch_size ] # predictions preds = model ( xb ) # check if there is grad function preds [ 0 ] You should get the following output: tensor([0.3591], grad_fn=<SelectBackward>) That grad_fn lets you know that we're tracking the gradients.","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#loss-function","text":"We're doing a simple loss: mean squared error. \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2 \\mathcal{L}_{mse} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( \\hat{y}_i - y_i \\right)^2","title":"Loss Function"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch_4","text":"# define mse loss def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) # set loss function to mse loss_func = mse_loss # get sample batch yb = y_train [: batch_size ] # get loss loss = loss_func ( preds , yb ) # check if there is grad function print ( loss )","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#training","text":"","title":"Training"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#pytorch_5","text":"batch_size = 100 learning_rate = 0.01 epochs = 20 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # Loss back propagation loss . backward () # add running loss losses . append ( loss . item ()) # manually calculate gradients with torch . no_grad (): # update the weights individually weights -= learning_rate * weights . grad bias -= learning_rate * bias . grad # zero the weights, bias parameters weights . grad . zero_ () bias . grad . zero_ () # Update status bar postfix = dict ( Epoch = f \" { iepoch } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Your output will look something like this. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00, 2.50it/s, Epoch=19, Loss=0.545]","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#appendix","text":"","title":"Appendix"},{"location":"tutorials/pytorch_nns/1_nn_from_scratch/#extra-python-packages-used","text":"scikit-learn The default machine learning package for python. It has a LOT of good algorithms and preprocessing features that are useful. Apparently the biggest use of the sklearn library is the train_test_split function. I'm definitely guilty of that too. tqdm A nice full featured status bar which can help eliminate some of the redundant outputs. typing A built-in type checker. This allows one to type check your inputs. It helps with code readability and to catch silly errors like having bad inputs.","title":"Extra Python Packages Used"},{"location":"tutorials/pytorch_nns/2_refactor/","text":"DL 4 Researchers Part II - Refactoring \u00b6 Model (and Parameters) \u00b6 Old Way #### ** PyTorch ** from torch import nn import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) # define model def model ( x_batch : torch . tensor ): return x_batch @ weights + bias # set linear model lr_model = model ** PyTorch ** \u00b6 class LinearModel ( nn . Module ): \"\"\"A Linear Model Parameters ---------- input_dim : int, The input dimension for the linear model (# input features) output_dim : int, the output Dimension for the linear model (# outputs) Attributes ---------- weights : torch.Tensor (input_dim x output_dim) the parameter for the linear model weights bias : torch.Tensor (output_dim) the parameter for the linear model bias Methods ------- forward : torch.tensor (input_dim x output_dim) the forward pass through the linear model \"\"\" def __init__ ( self , input_dim : int , output_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) def forward ( self , x_batch : torch . tensor ): return x_batch @ self . weights + self . bias input_dim = x_train . shape [ 1 ] output_dim = y_train . shape [ 1 ] lr_model = LinearModel ( input_dim , output_dim ) So we have effectively encapsulated that entire parameter definition within a single understandable function. We have the parameters defined when we initialize the model and then we have the forward method which allows us to perform the operation. Loss Function \u00b6 We can also look and use the built-in loss functions. The mse is a very common loss function so it should be available within the library. Old Way \u00b6 ** PyTorch ** \u00b6 In PyTorch, we need to look at the nn.functional.mse_loss module or the nn.MSELoss() . The latter has more options as it is a class and not a function but the former will do for now. So we can change the old way: def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) to a simplified version. import torch.nn.functional as F # set loss function to mse loss_func = F . mse_loss Optimizer \u00b6 Another refactor opportunity is to use a built-in optimizer. I don't want to have to calculate the gradient for each of the weights multiplied by the learning rate. ** PyTorch ** \u00b6 from torch import optim learning_rate = 0.01 # use stochastic gradient descent opt = optim . SGD ( lr_model . parameters (), lr = learning_rate ) Training \u00b6 So after all of that hard work, the training procedure will look a lot cleaner because we have encapsulated a lot of operations using the built-in operations. Now we can focus on other things. Old Way \u00b6 New Way \u00b6 ** PyTorch ** \u00b6 batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Datasets and DataLoaders \u00b6 Now there are some extra things we can do to reduce the amount of code and make this neater. We can use Datasets and DataLoaders . ** PyTorch ** \u00b6 Dataset \u00b6 In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. So instead of from torch.utils.data import TensorDataset train_ds = TensorDataset ( x_train , y_train ) lr_model , opt = get_lr_model () batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) DataLoader \u00b6 from torch.utils.data import TensorDataset , DataLoader batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) Validation set \u00b6 So because it's so easy, we can now add that validation set. I would have dreaded doing that before due to the lengthy code. But now, it's a piece of cake. from torch.utils.data import TensorDataset , DataLoader # training set batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size , shuffle = True ) # validation set valid_ds = TensorDataset ( x_valid , y_valid ) valid_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 train_losses , valid_losses = [], [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # put in training mode lr_model . train () # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss train_losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) # put in evaluation model lr_model . eval () with torch . no_grad (): for xbatch , ybatch in valid_dl : loss = loss_func ( lr_model ( xbatch ), ybatch ) valid_losses . append ( loss ) Appendix \u00b6 Python Concepts \u00b6 Python Classes More information can be found here . Comments Do them. Always. It might seem like a sunk cost, but it will save you time in the end.","title":"DL 4 Researchers Part II - Refactoring"},{"location":"tutorials/pytorch_nns/2_refactor/#dl-4-researchers-part-ii-refactoring","text":"","title":"DL 4 Researchers Part II - Refactoring"},{"location":"tutorials/pytorch_nns/2_refactor/#model-and-parameters","text":"Old Way #### ** PyTorch ** from torch import nn import math # dimensions for parameters input_dim = 8 output_dim = 1 n_samples = x_train . shape [ 0 ] # weight 'matrix' weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) # define model def model ( x_batch : torch . tensor ): return x_batch @ weights + bias # set linear model lr_model = model","title":"Model (and Parameters)"},{"location":"tutorials/pytorch_nns/2_refactor/#pytorch","text":"class LinearModel ( nn . Module ): \"\"\"A Linear Model Parameters ---------- input_dim : int, The input dimension for the linear model (# input features) output_dim : int, the output Dimension for the linear model (# outputs) Attributes ---------- weights : torch.Tensor (input_dim x output_dim) the parameter for the linear model weights bias : torch.Tensor (output_dim) the parameter for the linear model bias Methods ------- forward : torch.tensor (input_dim x output_dim) the forward pass through the linear model \"\"\" def __init__ ( self , input_dim : int , output_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , output_dim ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( output_dim ), requires_grad = True ) def forward ( self , x_batch : torch . tensor ): return x_batch @ self . weights + self . bias input_dim = x_train . shape [ 1 ] output_dim = y_train . shape [ 1 ] lr_model = LinearModel ( input_dim , output_dim ) So we have effectively encapsulated that entire parameter definition within a single understandable function. We have the parameters defined when we initialize the model and then we have the forward method which allows us to perform the operation.","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/2_refactor/#loss-function","text":"We can also look and use the built-in loss functions. The mse is a very common loss function so it should be available within the library.","title":"Loss Function"},{"location":"tutorials/pytorch_nns/2_refactor/#old-way","text":"","title":"Old Way"},{"location":"tutorials/pytorch_nns/2_refactor/#pytorch_1","text":"In PyTorch, we need to look at the nn.functional.mse_loss module or the nn.MSELoss() . The latter has more options as it is a class and not a function but the former will do for now. So we can change the old way: def mse_loss ( input : torch . tensor , target : torch . tensor ): return torch . mean (( input - target ) ** 2 ) to a simplified version. import torch.nn.functional as F # set loss function to mse loss_func = F . mse_loss","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/2_refactor/#optimizer","text":"Another refactor opportunity is to use a built-in optimizer. I don't want to have to calculate the gradient for each of the weights multiplied by the learning rate.","title":"Optimizer"},{"location":"tutorials/pytorch_nns/2_refactor/#pytorch_2","text":"from torch import optim learning_rate = 0.01 # use stochastic gradient descent opt = optim . SGD ( lr_model . parameters (), lr = learning_rate )","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/2_refactor/#training","text":"So after all of that hard work, the training procedure will look a lot cleaner because we have encapsulated a lot of operations using the built-in operations. Now we can focus on other things.","title":"Training"},{"location":"tutorials/pytorch_nns/2_refactor/#old-way_1","text":"","title":"Old Way"},{"location":"tutorials/pytorch_nns/2_refactor/#new-way","text":"","title":"New Way"},{"location":"tutorials/pytorch_nns/2_refactor/#pytorch_3","text":"batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/2_refactor/#datasets-and-dataloaders","text":"Now there are some extra things we can do to reduce the amount of code and make this neater. We can use Datasets and DataLoaders .","title":"Datasets and DataLoaders"},{"location":"tutorials/pytorch_nns/2_refactor/#pytorch_4","text":"","title":"** PyTorch **"},{"location":"tutorials/pytorch_nns/2_refactor/#dataset","text":"In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. So instead of from torch.utils.data import TensorDataset train_ds = TensorDataset ( x_train , y_train ) lr_model , opt = get_lr_model () batch_size = 100 epochs = 10 n_samples = x_train . shape [ 0 ] n_batches = ( n_samples - 1 ) // batch_size + 1 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"Dataset"},{"location":"tutorials/pytorch_nns/2_refactor/#dataloader","text":"from torch.utils.data import TensorDataset , DataLoader batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 losses = [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix )","title":"DataLoader"},{"location":"tutorials/pytorch_nns/2_refactor/#validation-set","text":"So because it's so easy, we can now add that validation set. I would have dreaded doing that before due to the lengthy code. But now, it's a piece of cake. from torch.utils.data import TensorDataset , DataLoader # training set batch_size = 100 train_ds = TensorDataset ( x_train , y_train ) train_dl = DataLoader ( train_ds , batch_size = batch_size , shuffle = True ) # validation set valid_ds = TensorDataset ( x_valid , y_valid ) valid_dl = DataLoader ( train_ds , batch_size = batch_size ) # initialize model lr_model , opt = get_lr_model () epochs = 10 train_losses , valid_losses = [], [] with tqdm . trange ( epochs ) as bar : # Loop through epochs with tqdm bar for iepoch in bar : # put in training mode lr_model . train () # Loop through batches for xbatch , ybatch in train_dl : # predictions ypred = lr_model ( xbatch ) # loss loss = loss_func ( ypred , ybatch ) # add running loss train_losses . append ( loss . item ()) # Loss back propagation loss . backward () # optimize weights opt . step () opt . zero_grad () postfix = dict ( Epoch = f \" { iepoch + 1 } \" , Loss = f \" { loss . item () : .3f } \" , ) bar . set_postfix ( postfix ) # put in evaluation model lr_model . eval () with torch . no_grad (): for xbatch , ybatch in valid_dl : loss = loss_func ( lr_model ( xbatch ), ybatch ) valid_losses . append ( loss )","title":"Validation set"},{"location":"tutorials/pytorch_nns/2_refactor/#appendix","text":"","title":"Appendix"},{"location":"tutorials/pytorch_nns/2_refactor/#python-concepts","text":"Python Classes More information can be found here . Comments Do them. Always. It might seem like a sunk cost, but it will save you time in the end.","title":"Python Concepts"},{"location":"tutorials/pytorch_nns/3_subclass/","text":"SubClassing \u00b6 SubClassing \u00b6 Built-In Components \u00b6 Loss Function \u00b6","title":"SubClassing"},{"location":"tutorials/pytorch_nns/3_subclass/#subclassing","text":"","title":"SubClassing"},{"location":"tutorials/pytorch_nns/3_subclass/#subclassing_1","text":"","title":"SubClassing"},{"location":"tutorials/pytorch_nns/3_subclass/#built-in-components","text":"","title":"Built-In Components"},{"location":"tutorials/pytorch_nns/3_subclass/#loss-function","text":"","title":"Loss Function"},{"location":"tutorials/pytorch_nns/4_wrapping/","text":"Wrapping Your Models \u00b6 PyTorch Lightning \u00b6 Skorch \u00b6","title":"Wrapping Your Models"},{"location":"tutorials/pytorch_nns/4_wrapping/#wrapping-your-models","text":"","title":"Wrapping Your Models"},{"location":"tutorials/pytorch_nns/4_wrapping/#pytorch-lightning","text":"","title":"PyTorch Lightning"},{"location":"tutorials/pytorch_nns/4_wrapping/#skorch","text":"","title":"Skorch"},{"location":"tutorials/pytorch_nns/5_extras/","text":"PyTorch Ecosystem Extras \u00b6 Captum - Model Interpretability \u00b6","title":"PyTorch Ecosystem Extras"},{"location":"tutorials/pytorch_nns/5_extras/#pytorch-ecosystem-extras","text":"","title":"PyTorch Ecosystem Extras"},{"location":"tutorials/pytorch_nns/5_extras/#captum-model-interpretability","text":"","title":"Captum - Model Interpretability"},{"location":"tutorials/pytorch_nns/_ideas/","text":"PyTorch Ideas \u00b6 1. Neural Networks from scratch \u00b6 2. \u00b6","title":"PyTorch Ideas"},{"location":"tutorials/pytorch_nns/_ideas/#pytorch-ideas","text":"","title":"PyTorch Ideas"},{"location":"tutorials/pytorch_nns/_ideas/#1-neural-networks-from-scratch","text":"","title":"1. Neural Networks from scratch"},{"location":"tutorials/pytorch_nns/_ideas/#2","text":"","title":"2."},{"location":"tutorials/tf2/slides/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson What is Deep Learning? \u00b6 Deep Learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. - Yann LeCun Deep Learning is a collection of tools to build complex modular differentiable functions. - Danilo Rezende It's more or less a tool... \u00b6 Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Software Perspective \u00b6 Who is your audience? What's your scope? Modular design Influencing minds... User 1 \u00b6 My employer gave me some data of landmass in Africa and wants me to find some huts. He thinks Deep Learning can help. User 2 \u00b6 I think I would like one network for my X X and y y . I also think maybe I should have another network with shared weights and a latent space. Maybe I coud also have two or three input locations. In addition... User 3 \u00b6 I want to implement a Neural Network with convolutional layers and a noise contrastive prior. The weights of the network will be parameterized by Normal distributions. I would also like a training scheme with a mixture of Importance sampling and variational inference with a custom KLD loss. One Deep Learning library to rule them all...! Probably a bad idea... Deep Learning Library Gold Rush \u00b6 Currently more than 10+ mainstream libraries All tech companies want a piece Growth of PyTorch \u00b6 Why? \u00b6 Simple (Pythonic) Great API Performance vs Productivity Tradeoff Easy to Install... Game: Which Library? \u00b6 My Suggestions \u00b6 Productivity: Fastai From Scratch: JAX Research: PyTorch Production/Industry: TensorFlow Basics \u00b6 Tensors Variables Automatic differentiation (AutoGrad) Tensors \u00b6 Constants \u00b6 # create constant x = tf . constant ([[ 5 , 2 ], [ 1 , 3 ]]) print ( x ) tf.Tensor( [[5 2] [1 3]], shape=(2, 2), dtype=int32) \u00b6 Standard \u00b6 # create ones tensor t_ones = tf . ones ( shape = ( 2 , 1 )) # create zeros tensor t_zeros = tf . zeros ( shape = ( 2 , 1 )) \u00b6 Standard Randomized \u00b6 # pretty standard tf . random . normal ( shape = ( 2 , 2 ), mean = 0. , stddev = 1. ) # pretty much the same tf . random . uniform ( shape = ( 2 , 2 ), minval = 0 , maxval = 10 ) Variables \u00b6 # set initial value initial_value = tf . random . normal ( shape = ( 2 , 2 )) # set variable a = tf . Variable ( initial_value ) Options (constraint, trainable, shape) All math operations Updates \u00b6 # new value b = tf . random . uniform ( shape = ( 2 , 2 )) # set value a . assign ( b ) # increment (a + b) a . assign_add ( b ) # dencrement (a - b) a . assign_sub ( new_a ) Gradients \u00b6 Gradient Function \u00b6 # init variable a = tf . Variable ( init_value ) # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # calculate gradient ( dc/da ) dc_da = tf . gradients ( c , a ) # calculate multiple gradients dc_da , dc_db = tf . gradients ( c , [ a , b ]) New : GradientTape Defines the scope literally \"record operations\" # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) Nested Gradients \u00b6 # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as outer_tape : with tf . GradientTape () as inner_tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) # extract gradients ( d2c/da2 ) d2c_da2 = outer_tape . gradient ( dc_da , a ) Gradients in PyTorch \u00b6 Same gradient function torch.autograd.grad There is no Tape Each variable has their own gradient # init variable a = torch . tensor ( init_value , requires_grad = True ) # do operation c = math . sqrt ( a ** 2 + b ** 2 ) # calculate gradients ( dc/da ) c . backward ( a ) # extract gradients dc_da = a . grad TF: Engine Module \u00b6 Layer Network - DAG graph Model Sequential Various Subclasses \u00b6 Layers Metric Loss Callbacks Optimizer Regularizers, Constraints Layer Class \u00b6 The core abstraction Everything is a Layer ...or interacts with a layer Example Layer \u00b6 y = \\mathbf{W}x + b y = \\mathbf{W}x + b # Subclass Layer class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () # Make Parameters def call ( self , inputs ): # Do stuff return inputs 1 - Constructor \u00b6 # Inherit Layer class class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () 2 - Parameters, \\mathbf{W} \\mathbf{W} \u00b6 # initialize weights (random) w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) 2 - Parameter, b b \u00b6 # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) 3 - Call Function, \\mathbf{W}x +b \\mathbf{W}x +b \u00b6 def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b PyTorch (the same...) \u00b6 class Linear ( nn . Module ): def __init__ ( self , units : int , input_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , units ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( units ), requires_grad = True ) def forward ( self , inputs ): return inputs @ self . weights + self . bias Using it \u00b6 # data x_train = ... # initialize linear layer linear_layer = Linear ( units = 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x ) TensorFlow build \u00b6 Know the # of nodes Don't know the input shape More conventional For example... def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) More convenient... # data x_train = ... # initialize linear layer (without input dims) linear_layer = Linear ( units = 4 ) # internally -> calls x.shape y = linear_layer ( x ) We can nest as many Layers as we want. Linear \u00b6 class Linear ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () # call linear layer self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x Linear Block \u00b6 class LinearBlock ( Layer ): def __init__ ( self ): super () . __init__ () self . lin_1 = Linear ( 32 ) self . lin_2 = Linear ( 32 ) self . lin_3 = Linear ( 1 ) def call ( self , inputs ): x = self . lin_1 ( x ) x = self . lin_2 ( x ) x = self . lin_3 ( x ) return x Training TF2.X, PyTorch \u00b6 Losses \u00b6 TensorFlow # example loss function loss_func = torch . nn . MSELoss () PyTorch # example loss function loss_fn = tf . keras . losses . MSELoss () Optimizers \u00b6 TensorFlow # example optimizer optimizer = tf . keras . optimizers . Adam () PyTorch # example optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.01 ) Full Training Loop (PyTorch) \u00b6 # Loop through batches for x , y in dataset : # initialize gradients optimizer . zero_grad () # predictions for minibatch ypred = lr_model ( xbatch ) # loss value for minibatch loss = loss_func ( ypred , ybatch ) # find gradients loss . backward () # apply optimization optimizer . step () Full Training Loop (TF2.X) \u00b6 for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , model . trainable_weights )) TensorFlow Nuggets \u00b6 Training Call \u00b6 Allows training versus inference mode Just need an extra argument training=True in the call method Prob Models, e.g. Batch Norm., Variational Inference Example \u00b6 ... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x Add Loss \u00b6 \"Add Losses on the fly\" Each layer has it's own regularization Examples: KLD, Activation or Weight Regularization Example - Model \u00b6 class MLP ( Layer ): def __init__ ( self , units = 32 , reg = 1e-3 ): super () . __init__ () self . linear = Linear ( units ) self . reg = reg def call ( self , inputs ): x = self . linear ( inputs ) x = tf . nn . relu ( x ) # Add loss during the call self . add_loss ( tf . reduce_sum ( output ** 2 ) * self . reg ) return x \u00b6 Example - Training \u00b6 mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function opt = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) Compile Code \u00b6 Use a decorator, @tf.function Optional Easy performance booster Example - Graphs \u00b6 @tf . function def train_step ( dataset ): for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) return loss Model Class \u00b6 Can do everything a Layer can do Built-in functionality a.k.a. Keras territory TF and PyTorch part ways Definitions \u00b6 Layer : A closed sequence of operation e.g. convolutional layer, recurrent layer, resnet block, attention block. Model : The top layer of your algorithm e.g. Deep learning model, deep neural network. Training Functionality \u00b6 .compile() .fit() .evaulate() .predict() .save() .summary() .plot_model() Example - \u00b6 # loss function loss = tf . keras . losses . MSELoss ( from_logits = True ) # accuracy metrics accuracy = tf . keras . metrics . SparseCategoricalAccuracy () # optimizer optimizer = tf . keras . optimizers . Adam () # compile to graph model . compile ( optimizer = optimizer , loss = loss , metrics = [ accuracy ]) # Fit Model model . fit ( dataset , epochs = 3 ) # Test Data loss , acc = model . evaluate ( test_dataset ) Functional Models \u00b6 Creates DAG Model Class with Extras Only in TF Simple Example \u00b6 # input checks x = tf . keras . layers . Flatten ( shape = 28 , 28 ))( inputs ) # Layer 1 x = tf . keras . layers . Dense ( 512 , activation = tf . nn . relu )( inputs ) # Layer 2 x = tf . keras . layers . Dropout ( 0.2 )( x ) # outputs x = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )( x ) # create model class model = tf . keras . Model ( inputs , outputs ) # compile model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ] ) Example - Graph Output \u00b6 We can go crazy... \u00b6 Sequential Models \u00b6 Predifined PyTorch & TF In TF, Model class PyTorch model = nn . Sequential ( torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 10 ), ) TensorFlow model = tf . keras . Sequential ([ layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 10 ) ]) Datasets \u00b6 Convenience Functions Take care of loading, iterations, batches Normally \u00b6 n_batches = ( n_samples - 1 ) // batch_size + 1 for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # get subset from data xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ] PyTorch - Datasets \u00b6 # create dataset train_ds = TensorDataset ( x_train , y_train ) # Loop through batches for start_idx , end_idx in range ( batch_idx ): # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # Do stuff... Note: In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically. PyTorch - DataLoaders \u00b6 # create dataset train_ds = TensorDataset ( x_train , y_train ) # create dataloader train_dl = DataLoader ( train_ds , batch_size = 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff... TF - Both... \u00b6 # create dataset train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) # create dataloader train_dl = train_ds . batch ( 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff... What We Covered \u00b6 DL Framework Idea Layers and Models Sequential Model What We didn't Cover \u00b6 Callbacks Distributed Training Multiple GPUs All options under the sun Tensorboard (Built-in Jupyter Notebooks!) Summary \u00b6 TensorFlow Training \u00b6","title":"Slides"},{"location":"tutorials/tf2/slides/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"tutorials/tf2/slides/#what-is-deep-learning","text":"Deep Learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. - Yann LeCun Deep Learning is a collection of tools to build complex modular differentiable functions. - Danilo Rezende","title":"What is Deep Learning?"},{"location":"tutorials/tf2/slides/#its-more-or-less-a-tool","text":"Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions","title":"It's more or less a tool..."},{"location":"tutorials/tf2/slides/#software-perspective","text":"Who is your audience? What's your scope? Modular design Influencing minds...","title":"Software Perspective"},{"location":"tutorials/tf2/slides/#user-1","text":"My employer gave me some data of landmass in Africa and wants me to find some huts. He thinks Deep Learning can help.","title":"User 1"},{"location":"tutorials/tf2/slides/#user-2","text":"I think I would like one network for my X X and y y . I also think maybe I should have another network with shared weights and a latent space. Maybe I coud also have two or three input locations. In addition...","title":"User 2"},{"location":"tutorials/tf2/slides/#user-3","text":"I want to implement a Neural Network with convolutional layers and a noise contrastive prior. The weights of the network will be parameterized by Normal distributions. I would also like a training scheme with a mixture of Importance sampling and variational inference with a custom KLD loss. One Deep Learning library to rule them all...! Probably a bad idea...","title":"User 3"},{"location":"tutorials/tf2/slides/#deep-learning-library-gold-rush","text":"Currently more than 10+ mainstream libraries All tech companies want a piece","title":"Deep Learning Library Gold Rush"},{"location":"tutorials/tf2/slides/#growth-of-pytorch","text":"","title":"Growth of PyTorch"},{"location":"tutorials/tf2/slides/#why","text":"Simple (Pythonic) Great API Performance vs Productivity Tradeoff Easy to Install...","title":"Why?"},{"location":"tutorials/tf2/slides/#game-which-library","text":"","title":"Game: Which Library?"},{"location":"tutorials/tf2/slides/#my-suggestions","text":"Productivity: Fastai From Scratch: JAX Research: PyTorch Production/Industry: TensorFlow","title":"My Suggestions"},{"location":"tutorials/tf2/slides/#basics","text":"Tensors Variables Automatic differentiation (AutoGrad)","title":"Basics"},{"location":"tutorials/tf2/slides/#tensors","text":"","title":"Tensors"},{"location":"tutorials/tf2/slides/#constants","text":"# create constant x = tf . constant ([[ 5 , 2 ], [ 1 , 3 ]]) print ( x )","title":"Constants"},{"location":"tutorials/tf2/slides/#tftensor-5-2-1-3-shape2-2-dtypeint32","text":"","title":"tf.Tensor(\n[[5 2]\n [1 3]], shape=(2, 2), dtype=int32)\n"},{"location":"tutorials/tf2/slides/#standard","text":"","title":"Standard"},{"location":"tutorials/tf2/slides/#create-ones-tensor-t_ones-tfonesshape2-1-create-zeros-tensor-t_zeros-tfzerosshape2-1","text":"","title":"# create ones tensor\nt_ones = tf.ones(shape=(2, 1))\n\n# create zeros tensor\nt_zeros = tf.zeros(shape=(2, 1))\n"},{"location":"tutorials/tf2/slides/#standard-randomized","text":"# pretty standard tf . random . normal ( shape = ( 2 , 2 ), mean = 0. , stddev = 1. ) # pretty much the same tf . random . uniform ( shape = ( 2 , 2 ), minval = 0 , maxval = 10 )","title":"Standard Randomized"},{"location":"tutorials/tf2/slides/#variables","text":"# set initial value initial_value = tf . random . normal ( shape = ( 2 , 2 )) # set variable a = tf . Variable ( initial_value ) Options (constraint, trainable, shape) All math operations","title":"Variables"},{"location":"tutorials/tf2/slides/#updates","text":"# new value b = tf . random . uniform ( shape = ( 2 , 2 )) # set value a . assign ( b ) # increment (a + b) a . assign_add ( b ) # dencrement (a - b) a . assign_sub ( new_a )","title":"Updates"},{"location":"tutorials/tf2/slides/#gradients","text":"","title":"Gradients"},{"location":"tutorials/tf2/slides/#gradient-function","text":"# init variable a = tf . Variable ( init_value ) # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # calculate gradient ( dc/da ) dc_da = tf . gradients ( c , a ) # calculate multiple gradients dc_da , dc_db = tf . gradients ( c , [ a , b ]) New : GradientTape Defines the scope literally \"record operations\" # init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a )","title":"Gradient Function"},{"location":"tutorials/tf2/slides/#nested-gradients","text":"# init variable a = tf . Variable ( init_value ) # define gradient scope with tf . GradientTape () as outer_tape : with tf . GradientTape () as inner_tape : # do operation c = tf . sqrt ( tf . square ( a ) + tf . square ( b )) # extract gradients ( dc/da ) dc_da = tape . gradient ( c , a ) # extract gradients ( d2c/da2 ) d2c_da2 = outer_tape . gradient ( dc_da , a )","title":"Nested Gradients"},{"location":"tutorials/tf2/slides/#gradients-in-pytorch","text":"Same gradient function torch.autograd.grad There is no Tape Each variable has their own gradient # init variable a = torch . tensor ( init_value , requires_grad = True ) # do operation c = math . sqrt ( a ** 2 + b ** 2 ) # calculate gradients ( dc/da ) c . backward ( a ) # extract gradients dc_da = a . grad","title":"Gradients in PyTorch"},{"location":"tutorials/tf2/slides/#tf-engine-module","text":"Layer Network - DAG graph Model Sequential","title":"TF: Engine Module"},{"location":"tutorials/tf2/slides/#various-subclasses","text":"Layers Metric Loss Callbacks Optimizer Regularizers, Constraints","title":"Various Subclasses"},{"location":"tutorials/tf2/slides/#layer-class","text":"The core abstraction Everything is a Layer ...or interacts with a layer","title":"Layer Class"},{"location":"tutorials/tf2/slides/#example-layer","text":"y = \\mathbf{W}x + b y = \\mathbf{W}x + b # Subclass Layer class Linear ( tf . keras . Layer ): def __init__ ( self ): super () . __init__ () # Make Parameters def call ( self , inputs ): # Do stuff return inputs","title":"Example Layer"},{"location":"tutorials/tf2/slides/#1-constructor","text":"# Inherit Layer class class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ ()","title":"1 - Constructor"},{"location":"tutorials/tf2/slides/#2-parameters-mathbfwmathbfw","text":"# initialize weights (random) w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True )","title":"2 - Parameters, \\mathbf{W}\\mathbf{W}"},{"location":"tutorials/tf2/slides/#2-parameter-bb","text":"# initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True )","title":"2 - Parameter, bb"},{"location":"tutorials/tf2/slides/#3-call-function-mathbfwx-bmathbfwx-b","text":"def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b class Linear ( tf . keras . Layer ): def __init__ ( self , units = 32 , input_dim = 32 ): super () . __init__ () w_init = tf . random_normal_initializer ()( shape = ( input_dim , units ) ) # weights parameter self . w = tf . Variable ( initial_value = w_init , trainable = True ) # initialize bias (zero) b_init = tf . zeros_initializer ()( shape = ( units ,) ) # bias parameter self . b = tf . Variable ( initial_value = b_init , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + b","title":"3 -  Call Function, \\mathbf{W}x +b\\mathbf{W}x +b"},{"location":"tutorials/tf2/slides/#pytorch-the-same","text":"class Linear ( nn . Module ): def __init__ ( self , units : int , input_dim : int ): super () . __init__ () # weight 'matrix' self . weights = nn . Parameter ( torch . randn ( input_dim , units ) / math . sqrt ( input_dim ), requires_grad = True ) # bias vector self . bias = nn . Parameter ( torch . zeros ( units ), requires_grad = True ) def forward ( self , inputs ): return inputs @ self . weights + self . bias","title":"PyTorch (the same...)"},{"location":"tutorials/tf2/slides/#using-it","text":"# data x_train = ... # initialize linear layer linear_layer = Linear ( units = 4 , input_dim = 2 ) # same thing as linear_layer.call(x) y = linear_layer ( x )","title":"Using it"},{"location":"tutorials/tf2/slides/#tensorflow-build","text":"Know the # of nodes Don't know the input shape More conventional For example... def build ( self , input_shape ): # Weights variable self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) # Bias variable self . b = self . add_weight ( shape = ( self . units ,), initializer = 'zeros' , trainable = True ) More convenient... # data x_train = ... # initialize linear layer (without input dims) linear_layer = Linear ( units = 4 ) # internally -> calls x.shape y = linear_layer ( x ) We can nest as many Layers as we want.","title":"TensorFlow build"},{"location":"tutorials/tf2/slides/#linear","text":"class Linear ( Layer ): def __init__ ( self , units = 32 ): super () . __init__ () # call linear layer self . linear = Linear ( units ) def call ( self , inputs ): x = self . linear ( inputs ) return x","title":"Linear"},{"location":"tutorials/tf2/slides/#linear-block","text":"class LinearBlock ( Layer ): def __init__ ( self ): super () . __init__ () self . lin_1 = Linear ( 32 ) self . lin_2 = Linear ( 32 ) self . lin_3 = Linear ( 1 ) def call ( self , inputs ): x = self . lin_1 ( x ) x = self . lin_2 ( x ) x = self . lin_3 ( x ) return x","title":"Linear Block"},{"location":"tutorials/tf2/slides/#training-tf2x-pytorch","text":"","title":"Training TF2.X, PyTorch"},{"location":"tutorials/tf2/slides/#losses","text":"TensorFlow # example loss function loss_func = torch . nn . MSELoss () PyTorch # example loss function loss_fn = tf . keras . losses . MSELoss ()","title":"Losses"},{"location":"tutorials/tf2/slides/#optimizers","text":"TensorFlow # example optimizer optimizer = tf . keras . optimizers . Adam () PyTorch # example optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.01 )","title":"Optimizers"},{"location":"tutorials/tf2/slides/#full-training-loop-pytorch","text":"# Loop through batches for x , y in dataset : # initialize gradients optimizer . zero_grad () # predictions for minibatch ypred = lr_model ( xbatch ) # loss value for minibatch loss = loss_func ( ypred , ybatch ) # find gradients loss . backward () # apply optimization optimizer . step ()","title":"Full Training Loop (PyTorch)"},{"location":"tutorials/tf2/slides/#full-training-loop-tf2x","text":"for x , y in dataset : with tf . GradientTape () as tape : # predictions for minibatch preds = model ( x ) # loss value for minibatch loss = loss_fn ( y , preds ) # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization optimizer . apply_gradients ( zip ( grads , model . trainable_weights ))","title":"Full Training Loop (TF2.X)"},{"location":"tutorials/tf2/slides/#tensorflow-nuggets","text":"","title":"TensorFlow Nuggets"},{"location":"tutorials/tf2/slides/#training-call","text":"Allows training versus inference mode Just need an extra argument training=True in the call method Prob Models, e.g. Batch Norm., Variational Inference","title":"Training Call"},{"location":"tutorials/tf2/slides/#example","text":"... def call ( self , x , training = True ): if training : # do training stuff else : # do inference stuff return x","title":"Example"},{"location":"tutorials/tf2/slides/#add-loss","text":"\"Add Losses on the fly\" Each layer has it's own regularization Examples: KLD, Activation or Weight Regularization","title":"Add Loss"},{"location":"tutorials/tf2/slides/#example-model","text":"","title":"Example - Model"},{"location":"tutorials/tf2/slides/#class-mlplayer-def-__init__self-units32-reg1e-3-super__init__-selflinear-linearunits-selfreg-reg-def-callself-inputs-x-selflinearinputs-x-tfnnrelux-add-loss-during-the-call-selfadd_losstfreduce_sumoutput-2-selfreg-return-x","text":"","title":"class MLP(Layer):\n    def __init__(self, units=32, reg=1e-3):\n        super().__init__()\n        self.linear = Linear(units)\n        self.reg = reg\n    def call(self, inputs):\n        x = self.linear(inputs)\n        x = tf.nn.relu(x)\n        # Add loss during the call\n        self.add_loss(tf.reduce_sum(output ** 2) * self.reg)\n        return x\n"},{"location":"tutorials/tf2/slides/#example-training","text":"mlp_model = MLP ( 32 ) # initialize model loss_fn = tf . keras . losses . MSELoss () # loss function opt = tf . keras . optimizers . Adam () # optimizer # Loop through dataset for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights ))","title":"Example - Training"},{"location":"tutorials/tf2/slides/#compile-code","text":"Use a decorator, @tf.function Optional Easy performance booster","title":"Compile Code"},{"location":"tutorials/tf2/slides/#example-graphs","text":"@tf . function def train_step ( dataset ): for x , y in dataset : with tf . GradientTape () as tape : preds = mlp_model ( x ) # predictions loss = loss_fn ( y , preds ) # loss value loss += sum ( mlp_model . losses ) # extra losses # find gradients grads = tape . gradients ( loss , model . trainable_weights ) # apply optimization opt . apply_gradients ( zip ( grads , model . trainable_weights )) return loss","title":"Example - Graphs"},{"location":"tutorials/tf2/slides/#model-class","text":"Can do everything a Layer can do Built-in functionality a.k.a. Keras territory TF and PyTorch part ways","title":"Model Class"},{"location":"tutorials/tf2/slides/#definitions","text":"Layer : A closed sequence of operation e.g. convolutional layer, recurrent layer, resnet block, attention block. Model : The top layer of your algorithm e.g. Deep learning model, deep neural network.","title":"Definitions"},{"location":"tutorials/tf2/slides/#training-functionality","text":".compile() .fit() .evaulate() .predict() .save() .summary() .plot_model()","title":"Training Functionality"},{"location":"tutorials/tf2/slides/#example-","text":"# loss function loss = tf . keras . losses . MSELoss ( from_logits = True ) # accuracy metrics accuracy = tf . keras . metrics . SparseCategoricalAccuracy () # optimizer optimizer = tf . keras . optimizers . Adam () # compile to graph model . compile ( optimizer = optimizer , loss = loss , metrics = [ accuracy ]) # Fit Model model . fit ( dataset , epochs = 3 ) # Test Data loss , acc = model . evaluate ( test_dataset )","title":"Example -"},{"location":"tutorials/tf2/slides/#functional-models","text":"Creates DAG Model Class with Extras Only in TF","title":"Functional Models"},{"location":"tutorials/tf2/slides/#simple-example","text":"# input checks x = tf . keras . layers . Flatten ( shape = 28 , 28 ))( inputs ) # Layer 1 x = tf . keras . layers . Dense ( 512 , activation = tf . nn . relu )( inputs ) # Layer 2 x = tf . keras . layers . Dropout ( 0.2 )( x ) # outputs x = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )( x ) # create model class model = tf . keras . Model ( inputs , outputs ) # compile model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ] )","title":"Simple Example"},{"location":"tutorials/tf2/slides/#example-graph-output","text":"","title":"Example - Graph Output"},{"location":"tutorials/tf2/slides/#we-can-go-crazy","text":"","title":"We can go crazy..."},{"location":"tutorials/tf2/slides/#sequential-models","text":"Predifined PyTorch & TF In TF, Model class PyTorch model = nn . Sequential ( torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 256 ), F . reLU (), torch . nn . Linear ( 10 ), ) TensorFlow model = tf . keras . Sequential ([ layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 256 , activation = tf . nn . relu ), layers . Dense ( 10 ) ])","title":"Sequential Models"},{"location":"tutorials/tf2/slides/#datasets","text":"Convenience Functions Take care of loading, iterations, batches","title":"Datasets"},{"location":"tutorials/tf2/slides/#normally","text":"n_batches = ( n_samples - 1 ) // batch_size + 1 for idx in range ( n_batches ): # get indices for batches start_idx = idx * batch_size end_idx = start_idx + batch_size # get subset from data xbatch = x_train [ start_idx : end_idx ] ybatch = y_train [ start_idx : end_idx ]","title":"Normally"},{"location":"tutorials/tf2/slides/#pytorch-datasets","text":"# create dataset train_ds = TensorDataset ( x_train , y_train ) # Loop through batches for start_idx , end_idx in range ( batch_idx ): # Use Dataset to store training data xbatch , ybatch = train_ds [ start_idx : end_idx ] # Do stuff... Note: In PyTorch, the Dataset helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your np.ndarray dataset to a Tensor automatically.","title":"PyTorch - Datasets"},{"location":"tutorials/tf2/slides/#pytorch-dataloaders","text":"# create dataset train_ds = TensorDataset ( x_train , y_train ) # create dataloader train_dl = DataLoader ( train_ds , batch_size = 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff...","title":"PyTorch - DataLoaders"},{"location":"tutorials/tf2/slides/#tf-both","text":"# create dataset train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) # create dataloader train_dl = train_ds . batch ( 100 ) # Loop through batches for xbatch , ybatch in train_dl : # Do stuff...","title":"TF - Both..."},{"location":"tutorials/tf2/slides/#what-we-covered","text":"DL Framework Idea Layers and Models Sequential Model","title":"What We Covered"},{"location":"tutorials/tf2/slides/#what-we-didnt-cover","text":"Callbacks Distributed Training Multiple GPUs All options under the sun Tensorboard (Built-in Jupyter Notebooks!)","title":"What We didn't Cover"},{"location":"tutorials/tf2/slides/#summary","text":"","title":"Summary"},{"location":"tutorials/tf2/slides/#tensorflow-training","text":"","title":"TensorFlow Training"},{"location":"tutorials/tf2/docs/css/theme/","text":"Dependencies \u00b6 Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup Creating a Theme \u00b6 To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Index"},{"location":"tutorials/tf2/docs/css/theme/#dependencies","text":"Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup","title":"Dependencies"},{"location":"tutorials/tf2/docs/css/theme/#creating-a-theme","text":"To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Creating a Theme"},{"location":"tutorials/tf2/docs/plugin/markdown/example/","text":"Markdown Demo \u00b6 External 1.1 \u00b6 Content 1.1 Note: This will only appear in the speaker notes window. External 1.2 \u00b6 Content 1.2 External 2 \u00b6 Content 2.1 External 3.1 \u00b6 Content 3.1 External 3.2 \u00b6 Content 3.2 External 3.3 \u00b6","title":"Markdown Demo"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#markdown-demo","text":"","title":"Markdown Demo"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-11","text":"Content 1.1 Note: This will only appear in the speaker notes window.","title":"External 1.1"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-12","text":"Content 1.2","title":"External 1.2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-2","text":"Content 2.1","title":"External 2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-31","text":"Content 3.1","title":"External 3.1"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-32","text":"Content 3.2","title":"External 3.2"},{"location":"tutorials/tf2/docs/plugin/markdown/example/#external-33","text":"","title":"External 3.3"}]}